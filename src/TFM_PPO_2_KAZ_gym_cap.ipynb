{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"d8JAmEUyj9De"},"outputs":[],"source":["# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n","mount='/content/gdrive'\n","drive_root = mount + \"/My Drive/TFM\"\n","\n","try:\n","  from google.colab import drive\n","  IN_COLAB=True\n","except:\n","  IN_COLAB=False"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42902,"status":"ok","timestamp":1696773138267,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"DrEo9QnxkAne","outputId":"f200a338-92fe-4c90-a780-de64fb02e44a"},"outputs":[{"output_type":"stream","name":"stdout","text":["We're running Colab\n","Colab: mounting Google drive on  /content/gdrive\n","Mounted at /content/gdrive\n","\n","Colab: making sure  /content/gdrive/My Drive/TFM  exists.\n","\n","Colab: Changing directory to  /content/gdrive/My Drive/TFM\n","/content/gdrive/My Drive/TFM\n","Archivos en el directorio: \n","['og_multi_car_racing', 'multi_car_racing', 'MCR_TFM.ipynb', 'Atari_TFM.ipynb', 'Entrenamientos antiguos sin logs', '.ipynb_checkpoints', 'policy_log_eval', 'Entrenamientos_log_no_eval', '=2.13', 'policy_new_log_eval', 'policy_eval.zip', 'TFM_pettingzoo_gym_cap.ipynb', 'TFM_new_pettingzoo_gym_cap.ipynb']\n"]}],"source":["# Switch to the directory on the Google Drive that you want to use\n","import os\n","if IN_COLAB:\n","  print(\"We're running Colab\")\n","\n","  if IN_COLAB:\n","    # Mount the Google Drive at mount\n","    print(\"Colab: mounting Google drive on \", mount)\n","\n","    drive.mount(mount)\n","\n","    # Create drive_root if it doesn't exist\n","    create_drive_root = True\n","    if create_drive_root:\n","      print(\"\\nColab: making sure \", drive_root, \" exists.\")\n","      os.makedirs(drive_root, exist_ok=True)\n","\n","    # Change to the directory\n","    print(\"\\nColab: Changing directory to \", drive_root)\n","    %cd $drive_root\n","# Verify we're in the correct working directory\n","%pwd\n","print(\"Archivos en el directorio: \")\n","print(os.listdir())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":237373,"status":"ok","timestamp":1696773375639,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"xAZDg478kEbs","outputId":"c8321d8d-15f8-4616-a5ee-0f7dfa2b3ca9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gym==0.17.3\n","  Downloading gym-0.17.3.tar.gz (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym==0.17.3) (1.11.3)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.17.3) (1.23.5)\n","Collecting pyglet<=1.5.0,>=1.4.0 (from gym==0.17.3)\n","  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cloudpickle<1.7.0,>=1.2.0 (from gym==0.17.3)\n","  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3) (0.18.3)\n","Building wheels for collected packages: gym\n","  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym: filename=gym-0.17.3-py3-none-any.whl size=1654617 sha256=4d8baddaf4b33e28e24c85888160b081bfaf8224b07bfe955a469a08586da602\n","  Stored in directory: /root/.cache/pip/wheels/af/4b/74/fcfc8238472c34d7f96508a63c962ff3ac9485a9a4137afd4e\n","Successfully built gym\n","Installing collected packages: pyglet, cloudpickle, gym\n","  Attempting uninstall: cloudpickle\n","    Found existing installation: cloudpickle 2.2.1\n","    Uninstalling cloudpickle-2.2.1:\n","      Successfully uninstalled cloudpickle-2.2.1\n","  Attempting uninstall: gym\n","    Found existing installation: gym 0.25.2\n","    Uninstalling gym-0.25.2:\n","      Successfully uninstalled gym-0.25.2\n","Successfully installed cloudpickle-1.6.0 gym-0.17.3 pyglet-1.5.0\n","Collecting git+https://github.com/Kojoley/atari-py.git\n","  Cloning https://github.com/Kojoley/atari-py.git to /tmp/pip-req-build-85mj11gx\n","  Running command git clone --filter=blob:none --quiet https://github.com/Kojoley/atari-py.git /tmp/pip-req-build-85mj11gx\n","  Resolved https://github.com/Kojoley/atari-py.git to commit 86a1e05c0a95e9e6233c3a413521fdb34ca8a089\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from atari-py==1.2.2) (1.23.5)\n","Building wheels for collected packages: atari-py\n","  Building wheel for atari-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for atari-py: filename=atari_py-1.2.2-cp310-cp310-linux_x86_64.whl size=4738731 sha256=531aea64d8edfa86c92516f3d504448c551347684056863315b8821a5252faec\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-xko582bj/wheels/6c/9b/f1/8a80a63a233d6f4eb2e2ee8c7357f4875f21c3ffc7f2613f9f\n","Successfully built atari-py\n","Installing collected packages: atari-py\n","Successfully installed atari-py-1.2.2\n","Collecting keras-rl2==1.0.5\n","  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from keras-rl2==1.0.5) (2.13.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (23.5.26)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.59.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.9.0)\n","Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.13.1)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (16.0.6)\n","Requirement already satisfied: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.23.5)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (23.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.16.0)\n","Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.13.0)\n","Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.13.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.3.0)\n","Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (4.5.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.15.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.34.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2==1.0.5) (0.41.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow->keras-rl2==1.0.5) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow->keras-rl2==1.0.5) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow->keras-rl2==1.0.5) (3.4.4)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow->keras-rl2==1.0.5) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow->keras-rl2==1.0.5) (0.7.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow->keras-rl2==1.0.5) (3.0.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->keras-rl2==1.0.5) (5.3.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->keras-rl2==1.0.5) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->keras-rl2==1.0.5) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow->keras-rl2==1.0.5) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow->keras-rl2==1.0.5) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow->keras-rl2==1.0.5) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow->keras-rl2==1.0.5) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow->keras-rl2==1.0.5) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow->keras-rl2==1.0.5) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->keras-rl2==1.0.5) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow->keras-rl2==1.0.5) (3.2.2)\n","Installing collected packages: keras-rl2\n","Successfully installed keras-rl2-1.0.5\n","Collecting tensorflow==2.8\n","  Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl (497.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.6.3)\n","Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (23.5.26)\n","Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.9.0)\n","Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.8)\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (16.0.6)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.23.5)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.3.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (4.5.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.15.0)\n","Collecting tensorboard<2.9,>=2.8 (from tensorflow==2.8)\n","  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tf-estimator-nightly==2.8.0.dev2021122109 (from tensorflow==2.8)\n","  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting keras<2.9,>=2.8.0rc0 (from tensorflow==2.8)\n","  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.34.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.59.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.8) (0.41.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.17.3)\n","Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.4.4)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.31.0)\n","Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n","  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n","  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.0.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (5.3.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.2.2)\n","Installing collected packages: tf-estimator-nightly, tensorboard-plugin-wit, keras, tensorboard-data-server, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.13.1\n","    Uninstalling keras-2.13.1:\n","      Successfully uninstalled keras-2.13.1\n","  Attempting uninstall: tensorboard-data-server\n","    Found existing installation: tensorboard-data-server 0.7.1\n","    Uninstalling tensorboard-data-server-0.7.1:\n","      Successfully uninstalled tensorboard-data-server-0.7.1\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 1.0.0\n","    Uninstalling google-auth-oauthlib-1.0.0:\n","      Successfully uninstalled google-auth-oauthlib-1.0.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.13.0\n","    Uninstalling tensorboard-2.13.0:\n","      Successfully uninstalled tensorboard-2.13.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.13.0\n","    Uninstalling tensorflow-2.13.0:\n","      Successfully uninstalled tensorflow-2.13.0\n","Successfully installed google-auth-oauthlib-0.4.6 keras-2.8.0 keras-preprocessing-1.1.2 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tf-estimator-nightly-2.8.0.dev2021122109\n","Collecting gym-cap\n","  Downloading gym_cap-0.1.4.12-py3-none-any.whl (32 kB)\n","Requirement already satisfied: gym>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from gym-cap) (0.17.3)\n","Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from gym-cap) (1.23.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym>=0.16.0->gym-cap) (1.11.3)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.16.0->gym-cap) (1.5.0)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.16.0->gym-cap) (1.6.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.16.0->gym-cap) (0.18.3)\n","Installing collected packages: gym-cap\n","Successfully installed gym-cap-0.1.4.12\n","Collecting pettingzoo[butterfly]\n","  Downloading pettingzoo-1.24.1-py3-none-any.whl (840 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.8/840.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[butterfly]) (1.23.5)\n","Collecting gymnasium>=0.28.0 (from pettingzoo[butterfly])\n","  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pygame==2.3.0 (from pettingzoo[butterfly])\n","  Downloading pygame-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pymunk==6.2.0 (from pettingzoo[butterfly])\n","  Downloading pymunk-6.2.0.zip (7.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: cffi>1.14.0 in /usr/local/lib/python3.10/dist-packages (from pymunk==6.2.0->pettingzoo[butterfly]) (1.16.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[butterfly]) (1.6.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[butterfly]) (4.5.0)\n","Collecting farama-notifications>=0.0.1 (from gymnasium>=0.28.0->pettingzoo[butterfly])\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>1.14.0->pymunk==6.2.0->pettingzoo[butterfly]) (2.21)\n","Building wheels for collected packages: pymunk\n","  Building wheel for pymunk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pymunk: filename=pymunk-6.2.0-cp310-cp310-linux_x86_64.whl size=801653 sha256=d4b83f5f3d6dd0dcfa8a1c95f29342966ece31db3cfd5874a78cbee7a7352411\n","  Stored in directory: /root/.cache/pip/wheels/2e/81/a2/f941a9ff417bb4020c37ae218fb7117d12d3fc019ea493d66f\n","Successfully built pymunk\n","Installing collected packages: farama-notifications, pygame, gymnasium, pymunk, pettingzoo\n","  Attempting uninstall: pygame\n","    Found existing installation: pygame 2.5.2\n","    Uninstalling pygame-2.5.2:\n","      Successfully uninstalled pygame-2.5.2\n","Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1 pettingzoo-1.24.1 pygame-2.3.0 pymunk-6.2.0\n"]}],"source":["if IN_COLAB:\n","  %pip install gym==0.17.3\n","  %pip install git+https://github.com/Kojoley/atari-py.git\n","  %pip install keras-rl2==1.0.5\n","  %pip install tensorflow==2.8\n","  %pip install gym-cap\n","  %pip install pettingzoo[butterfly]\n","else:\n","  %pip install gym==0.17.3\n","  %pip install git+https://github.com/Kojoley/atari-py.git\n","  %pip install pyglet==1.5.0\n","  %pip install h5py==3.1.0\n","  %pip install Pillow==9.5.0\n","  %pip install keras-rl2==1.0.5\n","  %pip install Keras==2.2.4\n","  %pip install tensorflow==2.5.3\n","  %pip install torch==2.0.1\n","  %pip install agents==1.4.0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1696773375639,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"G8cw-IX3laE9","outputId":"24ee727d-1c9b-49bd-8e0a-c82f7062e951"},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'multi_car_racing' already exists and is not an empty directory.\n"]}],"source":["!git clone https://github.com/igilitschenski/multi_car_racing.git\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IqbMo3gK7vBG"},"outputs":[],"source":["!cd multi_car_racing\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4549,"status":"ok","timestamp":1696773380606,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"Jekec6f98b3A","outputId":"85a54233-b899-4acd-ba9e-cd7b7870cc37"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting swig\n","  Downloading swig-4.1.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: swig\n","Successfully installed swig-4.1.1\n"]}],"source":["!pip install swig"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":59634,"status":"ok","timestamp":1696773440238,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"KKxRPBFx85k6","outputId":"a2b87006-825c-4c0f-a20a-584d482248af"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting box2d\n","  Downloading Box2D-2.3.2.tar.gz (427 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.9/427.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: box2d\n","  Building wheel for box2d (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d: filename=Box2D-2.3.2-cp310-cp310-linux_x86_64.whl size=2391355 sha256=953fc08e75b5df235d8778065e9557185c5cf0dce60099b7d227a8c7ac9fffc1\n","  Stored in directory: /root/.cache/pip/wheels/eb/cb/be/e663f3ce9aba6580611c0febaf7cd3cf7603f87047de2a52f9\n","Successfully built box2d\n","Installing collected packages: box2d\n","Successfully installed box2d-2.3.2\n"]}],"source":["!pip install box2d"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":58880,"status":"ok","timestamp":1696773499108,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"ijp5V0i09MRF","outputId":"d82d46ee-49d7-4a74-cc0d-d00ed4292520"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting box2d-py\n","  Downloading box2d-py-2.3.8.tar.gz (374 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/374.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/374.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.5/374.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: box2d-py\n","  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-py: filename=box2d_py-2.3.8-cp310-cp310-linux_x86_64.whl size=2373077 sha256=35396071a73be3164ee17ca04baf1679e6e98bb3f6a6e72281f14bb0227e612a\n","  Stored in directory: /root/.cache/pip/wheels/47/01/d2/6a780da77ccb98b1d2facdd520a8d10838a03b590f6f8d50c0\n","Successfully built box2d-py\n","Installing collected packages: box2d-py\n","Successfully installed box2d-py-2.3.8\n"]}],"source":["!pip install box2d-py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26358,"status":"ok","timestamp":1696773525457,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"BwjugqI99g0I","outputId":"f5a2fa59-dd03-4e21-faea-eb7577153da5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Obtaining file:///content/gdrive/MyDrive/TFM/multi_car_racing\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: box2d-py~=2.3.5 in /usr/local/lib/python3.10/dist-packages (from gym-multi-car-racing==0.0.1) (2.3.8)\n","Collecting shapely~=1.7.0 (from gym-multi-car-racing==0.0.1)\n","  Downloading Shapely-1.7.1.tar.gz (383 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.2/383.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from gym-multi-car-racing==0.0.1) (1.23.5)\n","Requirement already satisfied: gym~=0.17.2 in /usr/local/lib/python3.10/dist-packages (from gym-multi-car-racing==0.0.1) (0.17.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym~=0.17.2->gym-multi-car-racing==0.0.1) (1.11.3)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from gym~=0.17.2->gym-multi-car-racing==0.0.1) (1.5.0)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym~=0.17.2->gym-multi-car-racing==0.0.1) (1.6.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym~=0.17.2->gym-multi-car-racing==0.0.1) (0.18.3)\n","Building wheels for collected packages: shapely\n","  Building wheel for shapely (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for shapely: filename=Shapely-1.7.1-cp310-cp310-linux_x86_64.whl size=997925 sha256=f64f53052b3b1ae41b2cb9d2f072f419108e7a27b0534a1ac8dc9041be0b2fb8\n","  Stored in directory: /root/.cache/pip/wheels/2e/fa/97/c85f587c35afcaf4a81c481741d36592518d1e50445572f0d4\n","Successfully built shapely\n","Installing collected packages: shapely, gym-multi-car-racing\n","  Attempting uninstall: shapely\n","    Found existing installation: shapely 2.0.1\n","    Uninstalling shapely-2.0.1:\n","      Successfully uninstalled shapely-2.0.1\n","  Running setup.py develop for gym-multi-car-racing\n","Successfully installed gym-multi-car-racing-0.0.1 shapely-1.7.1\n"]}],"source":["!pip install -e multi_car_racing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9837,"status":"ok","timestamp":1696773535292,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"IrAvXzCW-Z3e","outputId":"28a26325-12f4-4c77-d2b4-5a53edc5de7a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  freeglut3 libegl-dev libgl-dev libgl1-mesa-dev libgles-dev libgles1\n","  libglu1-mesa libglu1-mesa-dev libglvnd-core-dev libglvnd-dev libglx-dev\n","  libice-dev libopengl-dev libsm-dev libxt-dev\n","Suggested packages:\n","  libice-doc libsm-doc libxt-doc\n","The following NEW packages will be installed:\n","  freeglut3 freeglut3-dev libegl-dev libgl-dev libgl1-mesa-dev libgles-dev\n","  libgles1 libglu1-mesa libglu1-mesa-dev libglvnd-core-dev libglvnd-dev\n","  libglx-dev libice-dev libopengl-dev libsm-dev libxt-dev\n","0 upgraded, 16 newly installed, 0 to remove and 18 not upgraded.\n","Need to get 1,261 kB of archives.\n","After this operation, 6,753 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3 amd64 2.8.1-6 [74.0 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglx-dev amd64 1.4.0-1 [14.1 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgl-dev amd64 1.4.0-1 [101 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-core-dev amd64 1.4.0-1 [12.7 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libegl-dev amd64 1.4.0-1 [18.0 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles1 amd64 1.4.0-1 [11.5 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles-dev amd64 1.4.0-1 [49.4 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libopengl-dev amd64 1.4.0-1 [3,400 B]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-dev amd64 1.4.0-1 [3,162 B]\n","Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgl1-mesa-dev amd64 23.0.4-0ubuntu1~22.04.1 [6,510 B]\n","Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n","Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa-dev amd64 9.0.2-1 [231 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 libice-dev amd64 2:1.0.10-1build2 [51.4 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsm-dev amd64 2:1.2.3-1build2 [18.1 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3-dev amd64 2.8.1-6 [126 kB]\n","Fetched 1,261 kB in 0s (5,917 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 16.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package freeglut3:amd64.\n","(Reading database ... 120875 files and directories currently installed.)\n","Preparing to unpack .../00-freeglut3_2.8.1-6_amd64.deb ...\n","Unpacking freeglut3:amd64 (2.8.1-6) ...\n","Selecting previously unselected package libglx-dev:amd64.\n","Preparing to unpack .../01-libglx-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglx-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgl-dev:amd64.\n","Preparing to unpack .../02-libgl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libgl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libglvnd-core-dev:amd64.\n","Preparing to unpack .../03-libglvnd-core-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglvnd-core-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libegl-dev:amd64.\n","Preparing to unpack .../04-libegl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libegl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgles1:amd64.\n","Preparing to unpack .../05-libgles1_1.4.0-1_amd64.deb ...\n","Unpacking libgles1:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgles-dev:amd64.\n","Preparing to unpack .../06-libgles-dev_1.4.0-1_amd64.deb ...\n","Unpacking libgles-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libopengl-dev:amd64.\n","Preparing to unpack .../07-libopengl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libopengl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libglvnd-dev:amd64.\n","Preparing to unpack .../08-libglvnd-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglvnd-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgl1-mesa-dev:amd64.\n","Preparing to unpack .../09-libgl1-mesa-dev_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n","Unpacking libgl1-mesa-dev:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Selecting previously unselected package libglu1-mesa:amd64.\n","Preparing to unpack .../10-libglu1-mesa_9.0.2-1_amd64.deb ...\n","Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n","Selecting previously unselected package libglu1-mesa-dev:amd64.\n","Preparing to unpack .../11-libglu1-mesa-dev_9.0.2-1_amd64.deb ...\n","Unpacking libglu1-mesa-dev:amd64 (9.0.2-1) ...\n","Selecting previously unselected package libice-dev:amd64.\n","Preparing to unpack .../12-libice-dev_2%3a1.0.10-1build2_amd64.deb ...\n","Unpacking libice-dev:amd64 (2:1.0.10-1build2) ...\n","Selecting previously unselected package libsm-dev:amd64.\n","Preparing to unpack .../13-libsm-dev_2%3a1.2.3-1build2_amd64.deb ...\n","Unpacking libsm-dev:amd64 (2:1.2.3-1build2) ...\n","Selecting previously unselected package libxt-dev:amd64.\n","Preparing to unpack .../14-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n","Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n","Selecting previously unselected package freeglut3-dev:amd64.\n","Preparing to unpack .../15-freeglut3-dev_2.8.1-6_amd64.deb ...\n","Unpacking freeglut3-dev:amd64 (2.8.1-6) ...\n","Setting up freeglut3:amd64 (2.8.1-6) ...\n","Setting up libglvnd-core-dev:amd64 (1.4.0-1) ...\n","Setting up libice-dev:amd64 (2:1.0.10-1build2) ...\n","Setting up libsm-dev:amd64 (2:1.2.3-1build2) ...\n","Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n","Setting up libgles1:amd64 (1.4.0-1) ...\n","Setting up libglx-dev:amd64 (1.4.0-1) ...\n","Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n","Setting up libopengl-dev:amd64 (1.4.0-1) ...\n","Setting up libgl-dev:amd64 (1.4.0-1) ...\n","Setting up libegl-dev:amd64 (1.4.0-1) ...\n","Setting up libglu1-mesa-dev:amd64 (9.0.2-1) ...\n","Setting up libgles-dev:amd64 (1.4.0-1) ...\n","Setting up libglvnd-dev:amd64 (1.4.0-1) ...\n","Setting up libgl1-mesa-dev:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Setting up freeglut3-dev:amd64 (2.8.1-6) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n"]}],"source":["!sudo apt-get install freeglut3-dev"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20044,"status":"ok","timestamp":1696773555327,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"cgGdQ6n9EERW","outputId":"2422f824-abe4-4e73-e825-dd916cd62762"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings\n","  xfonts-utils xserver-common\n","The following NEW packages will be installed:\n","  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings\n","  xfonts-utils xserver-common xvfb\n","0 upgraded, 9 newly installed, 0 to remove and 18 not upgraded.\n","Need to get 7,812 kB of archives.\n","After this operation, 11.9 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.1 [28.0 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.1 [863 kB]\n","Fetched 7,812 kB in 0s (43.0 MB/s)\n","Selecting previously unselected package libfontenc1:amd64.\n","(Reading database ... 121333 files and directories currently installed.)\n","Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n","Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n","Selecting previously unselected package libxfont2:amd64.\n","Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n","Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n","Selecting previously unselected package libxkbfile1:amd64.\n","Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n","Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n","Selecting previously unselected package x11-xkb-utils.\n","Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n","Unpacking x11-xkb-utils (7.7+5build4) ...\n","Selecting previously unselected package xfonts-encodings.\n","Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n","Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n","Selecting previously unselected package xfonts-utils.\n","Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n","Unpacking xfonts-utils (1:7.7+6build2) ...\n","Selecting previously unselected package xfonts-base.\n","Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n","Unpacking xfonts-base (1:1.0.5) ...\n","Selecting previously unselected package xserver-common.\n","Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.1_all.deb ...\n","Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.1) ...\n","Selecting previously unselected package xvfb.\n","Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.1_amd64.deb ...\n","Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.1) ...\n","Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n","Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n","Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n","Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n","Setting up x11-xkb-utils (7.7+5build4) ...\n","Setting up xfonts-utils (1:7.7+6build2) ...\n","Setting up xfonts-base (1:1.0.5) ...\n","Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.1) ...\n","Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.1) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","Collecting pyvirtualdisplay\n","  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n","Installing collected packages: pyvirtualdisplay\n","Successfully installed pyvirtualdisplay-3.0\n","Collecting piglet\n","  Downloading piglet-1.0.0-py2.py3-none-any.whl (2.2 kB)\n","Collecting piglet-templates (from piglet)\n","  Downloading piglet_templates-1.3.0-py3-none-any.whl (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.5/67.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from piglet-templates->piglet) (3.1.1)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from piglet-templates->piglet) (23.1.0)\n","Requirement already satisfied: astunparse in /usr/local/lib/python3.10/dist-packages (from piglet-templates->piglet) (1.6.3)\n","Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from piglet-templates->piglet) (2.1.3)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse->piglet-templates->piglet) (0.41.2)\n","Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from astunparse->piglet-templates->piglet) (1.16.0)\n","Installing collected packages: piglet-templates, piglet\n","Successfully installed piglet-1.0.0 piglet-templates-1.3.0\n"]}],"source":["!apt install xvfb -y\n","!pip install pyvirtualdisplay\n","!pip install piglet"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16159,"status":"ok","timestamp":1696773571477,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"5OaWkBSmhm6R","outputId":"d53ef8c5-e927-4eef-a37d-31916880b7a5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting supersuit\n","  Downloading SuperSuit-3.9.0-py3-none-any.whl (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from supersuit) (1.23.5)\n","Requirement already satisfied: gymnasium>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from supersuit) (0.29.1)\n","Collecting tinyscaler>=1.2.6 (from supersuit)\n","  Downloading tinyscaler-1.2.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (517 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.1/517.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (1.6.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (0.0.4)\n","Installing collected packages: tinyscaler, supersuit\n","Successfully installed supersuit-3.9.0 tinyscaler-1.2.7\n","Collecting stable_baselines3\n","  Downloading stable_baselines3-2.1.0-py3-none-any.whl (178 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (0.29.1)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.23.5)\n","Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.0.1+cu118)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.6.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.5.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (3.7.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (0.0.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.12.4)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13->stable_baselines3) (3.27.6)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13->stable_baselines3) (17.0.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.1.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (0.12.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (4.43.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (23.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable_baselines3) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable_baselines3) (1.3.0)\n","Installing collected packages: stable_baselines3\n","Successfully installed stable_baselines3-2.1.0\n"]}],"source":["!pip install supersuit\n","!pip install stable_baselines3"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4461,"status":"ok","timestamp":1696773575935,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"thmOvcHdjKHw","outputId":"31c04828-a32f-4a3d-a8a8-004fbe905318"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting shimmy>=0.2.1\n","  Downloading Shimmy-1.2.1-py3-none-any.whl (37 kB)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from shimmy>=0.2.1) (1.23.5)\n","Requirement already satisfied: gymnasium>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from shimmy>=0.2.1) (0.29.1)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy>=0.2.1) (1.6.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy>=0.2.1) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy>=0.2.1) (0.0.4)\n","Installing collected packages: shimmy\n","Successfully installed shimmy-1.2.1\n"]}],"source":["!pip install 'shimmy>=0.2.1'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20548,"status":"ok","timestamp":1696773596481,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"k0iVvep_spQz","outputId":"4b2de234-e57a-43e9-876e-2255c3058f51"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tf-agents-nightly\n","  Downloading tf_agents_nightly-0.18.0.dev20231008-py3-none-any.whl (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.4.0)\n","Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.6.0)\n","Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (0.5.0)\n","Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (0.17.3)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.23.5)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (9.4.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.16.0)\n","Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (3.20.3)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.15.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (4.5.0)\n","Collecting pygame==2.1.3 (from tf-agents-nightly)\n","  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tfp-nightly (from tf-agents-nightly)\n","  Downloading tfp_nightly-0.23.0.dev20231008-py2.py3-none-any.whl (6.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents-nightly) (1.11.3)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents-nightly) (1.5.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tfp-nightly->tf-agents-nightly) (4.4.2)\n","Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tfp-nightly->tf-agents-nightly) (0.4.0)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tfp-nightly->tf-agents-nightly) (0.1.8)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<=0.23.0,>=0.17.0->tf-agents-nightly) (0.18.3)\n","Installing collected packages: tfp-nightly, pygame, tf-agents-nightly\n","  Attempting uninstall: pygame\n","    Found existing installation: pygame 2.3.0\n","    Uninstalling pygame-2.3.0:\n","      Successfully uninstalled pygame-2.3.0\n","Successfully installed pygame-2.1.3 tf-agents-nightly-0.18.0.dev20231008 tfp-nightly-0.23.0.dev20231008\n"]}],"source":["!pip install tf-agents-nightly"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19130,"status":"ok","timestamp":1696773615609,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"UlXxViz9tdvH","outputId":"c8db012f-66ed-4ae8-b47f-90d078e63cae"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: tensorflow 2.8.0\n","Uninstalling tensorflow-2.8.0:\n","  Would remove:\n","    /usr/local/bin/estimator_ckpt_converter\n","    /usr/local/bin/import_pb_to_tensorboard\n","    /usr/local/bin/saved_model_cli\n","    /usr/local/bin/tensorboard\n","    /usr/local/bin/tf_upgrade_v2\n","    /usr/local/bin/tflite_convert\n","    /usr/local/bin/toco\n","    /usr/local/bin/toco_from_protos\n","    /usr/local/lib/python3.10/dist-packages/tensorflow-2.8.0.dist-info/*\n","    /usr/local/lib/python3.10/dist-packages/tensorflow/*\n","Proceed (Y/n)? Y\n","  Successfully uninstalled tensorflow-2.8.0\n"]}],"source":["!pip uninstall tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dsWlVQ6MtKLj"},"outputs":[],"source":["!pip install tensorflow>=2.13"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4326,"status":"ok","timestamp":1696773667438,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"wE5AiVtFtZDc","outputId":"d67675e4-9cd2-4153-86d0-36c069ed2ac4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Name: tensorflow\n","Version: 2.14.0\n","Summary: TensorFlow is an open source machine learning framework for everyone.\n","Home-page: https://www.tensorflow.org/\n","Author: Google Inc.\n","Author-email: packages@tensorflow.org\n","License: Apache 2.0\n","Location: /usr/local/lib/python3.10/dist-packages\n","Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, libclang, ml-dtypes, numpy, opt-einsum, packaging, protobuf, setuptools, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n","Required-by: dopamine-rl, keras-rl2\n"]}],"source":["!pip show tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13553,"status":"ok","timestamp":1696773680989,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"GnLzCLnICYCz","outputId":"8b202b49-998f-47f9-abf8-e233c23331e6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pettingzoo[butterfly] in /usr/local/lib/python3.10/dist-packages (1.24.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[butterfly]) (1.23.5)\n","Requirement already satisfied: gymnasium>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[butterfly]) (0.29.1)\n","Collecting pygame==2.3.0 (from pettingzoo[butterfly])\n","  Using cached pygame-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\n","Requirement already satisfied: pymunk==6.2.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[butterfly]) (6.2.0)\n","Requirement already satisfied: cffi>1.14.0 in /usr/local/lib/python3.10/dist-packages (from pymunk==6.2.0->pettingzoo[butterfly]) (1.16.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[butterfly]) (1.6.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[butterfly]) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[butterfly]) (0.0.4)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>1.14.0->pymunk==6.2.0->pettingzoo[butterfly]) (2.21)\n","Installing collected packages: pygame\n","  Attempting uninstall: pygame\n","    Found existing installation: pygame 2.1.3\n","    Uninstalling pygame-2.1.3:\n","      Successfully uninstalled pygame-2.1.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tf-agents-nightly 0.18.0.dev20231008 requires pygame==2.1.3, but you have pygame 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed pygame-2.3.0\n"]}],"source":["!pip install pettingzoo[butterfly]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JpA_YhKzCeC2"},"outputs":[],"source":["############################# Código para entrenar KAZ"]},{"cell_type":"code","source":["from stable_baselines3.ppo import MlpPolicy\n","from stable_baselines3 import PPO\n","from pettingzoo.butterfly import knights_archers_zombies_v10\n","import supersuit as ss\n","from stable_baselines3.common.monitor import Monitor"],"metadata":{"id":"48-CZY2SGiT7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eEkqPDJDTB8j","executionInfo":{"status":"ok","timestamp":1696773699467,"user_tz":-120,"elapsed":4,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"6f24b06f-8e57-4ff7-e9e2-169a58d1efdf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["'=2.13'\t\t\t\t     og_multi_car_racing\n"," Atari_TFM.ipynb\t\t     policy_eval.zip\n","'Entrenamientos antiguos sin logs'   policy_log_eval\n"," Entrenamientos_log_no_eval\t     policy_new_log_eval\n"," MCR_TFM.ipynb\t\t\t     TFM_new_pettingzoo_gym_cap.ipynb\n"," multi_car_racing\t\t     TFM_pettingzoo_gym_cap.ipynb\n"]}]},{"cell_type":"code","source":["cd /content/drive/MyDrive/TFM"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nhIwSVpcTFpt","executionInfo":{"status":"ok","timestamp":1696756265986,"user_tz":-120,"elapsed":572,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"e195fbc6-8d06-49af-865d-9bfe498bd295"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/TFM\n"]}]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ghEp9l3UTGOq","executionInfo":{"status":"ok","timestamp":1696756267173,"user_tz":-120,"elapsed":13,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"66996921-1c48-401c-e7f7-e6ae147ef886"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["'=2.13'\t\t\t\t     og_multi_car_racing\n"," Atari_TFM.ipynb\t\t     policy_log_eval\n","'Entrenamientos antiguos sin logs'   policy_new_log_eval\n"," Entrenamientos_log_no_eval\t     TFM_new_pettingzoo_gym_cap.ipynb\n"," MCR_TFM.ipynb\t\t\t     TFM_pettingzoo_gym_cap.ipynb\n"," multi_car_racing\n"]}]},{"cell_type":"code","source":["env = knights_archers_zombies_v10.parallel_env(render_mode=\"rgb_array\")\n","\n","env = ss.black_death_v3(env)\n","env = ss.resize_v1(env, x_size=84, y_size=84)\n","env = ss.frame_stack_v1(env, 3)\n","env = ss.pettingzoo_env_to_vec_env_v1(env)\n","env = ss.concat_vec_envs_v1(env, 8, num_cpus=4, base_class='stable_baselines3')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RLXGeCI2GjiF","executionInfo":{"status":"ok","timestamp":1696773706957,"user_tz":-120,"elapsed":1006,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"62b96861-5f6f-4f56-8606-ffd14d7c2007"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pettingzoo/utils/conversions.py:252: UserWarning: The base environment `knights_archers_zombies_v10` does not have a `render_mode` defined.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/pettingzoo/utils/conversions.py:132: UserWarning: The base environment `aec_observation_lambda<knights_archers_zombies_v10>` does not have a `render_mode` defined.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["from stable_baselines3.common.callbacks import EvalCallback\n","eval_callback = EvalCallback(env, best_model_save_path=\"./policy_new_log_eval/\",\n","                             log_path=\"./policy_new_log_eval/\", eval_freq=500,\n","                             deterministic=True, render=False)"],"metadata":{"id":"1W9MpwEqg3KL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3.common.logger import configure\n","\n","tmp_path = \"./policy_new_log_eval/\"\n","# set up logger\n","new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n","\n","model = PPO(MlpPolicy, env, verbose=3, gamma=0.99, n_steps=512, ent_coef=0.01, learning_rate=0.00062211, vf_coef=0.042202, max_grad_norm=0.9, gae_lambda=0.95, n_epochs=5, clip_range=0.1, batch_size=512)\n","# model.set_logger()\n","model.set_logger(new_logger)\n","model.learn(total_timesteps=5000000,callback=eval_callback)\n","model.save(\"policy_new_eval\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XqgmxYYfg6-7","outputId":"73c2c081-4a83-403d-e6d2-74c0a8e57eb9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Logging to ./policy_new_log_eval/\n","Using cuda device\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n","| time/                   |              |\n","|    total_timesteps      | 1648000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0057712635 |\n","|    clip_fraction        | 0.191        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.64        |\n","|    explained_variance   | 0.882        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0119      |\n","|    n_updates            | 500          |\n","|    policy_gradient_loss | 0.000367     |\n","|    value_loss           | 0.0561       |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 334     |\n","|    iterations      | 101     |\n","|    time_elapsed    | 4941    |\n","|    total_timesteps | 1654784 |\n","--------------------------------\n","Eval num_timesteps=1664000, episode_reward=0.60 +/- 0.80\n","Episode length: 204.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 204          |\n","|    mean_reward          | 0.6          |\n","| time/                   |              |\n","|    total_timesteps      | 1664000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0062769214 |\n","|    clip_fraction        | 0.208        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.63        |\n","|    explained_variance   | 0.843        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0177      |\n","|    n_updates            | 505          |\n","|    policy_gradient_loss | -0.000103    |\n","|    value_loss           | 0.0688       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 334     |\n","|    iterations      | 102     |\n","|    time_elapsed    | 4989    |\n","|    total_timesteps | 1671168 |\n","--------------------------------\n","Eval num_timesteps=1680000, episode_reward=0.40 +/- 0.80\n","Episode length: 164.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 164          |\n","|    mean_reward          | 0.4          |\n","| time/                   |              |\n","|    total_timesteps      | 1680000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0046815914 |\n","|    clip_fraction        | 0.182        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.64        |\n","|    explained_variance   | 0.879        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0162      |\n","|    n_updates            | 510          |\n","|    policy_gradient_loss | 0.000999     |\n","|    value_loss           | 0.0485       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 335     |\n","|    iterations      | 103     |\n","|    time_elapsed    | 5035    |\n","|    total_timesteps | 1687552 |\n","--------------------------------\n","Eval num_timesteps=1696000, episode_reward=1.80 +/- 2.71\n","Episode length: 252.00 +/- 24.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 252          |\n","|    mean_reward          | 1.8          |\n","| time/                   |              |\n","|    total_timesteps      | 1696000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0040933797 |\n","|    clip_fraction        | 0.207        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.65        |\n","|    explained_variance   | 0.867        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0156      |\n","|    n_updates            | 515          |\n","|    policy_gradient_loss | 0.00018      |\n","|    value_loss           | 0.0525       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 335     |\n","|    iterations      | 104     |\n","|    time_elapsed    | 5086    |\n","|    total_timesteps | 1703936 |\n","--------------------------------\n","Eval num_timesteps=1712000, episode_reward=0.80 +/- 0.98\n","Episode length: 180.00 +/- 8.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 180          |\n","|    mean_reward          | 0.8          |\n","| time/                   |              |\n","|    total_timesteps      | 1712000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0054675164 |\n","|    clip_fraction        | 0.181        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.64        |\n","|    explained_variance   | 0.872        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0134      |\n","|    n_updates            | 520          |\n","|    policy_gradient_loss | 0.00153      |\n","|    value_loss           | 0.0523       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 335     |\n","|    iterations      | 105     |\n","|    time_elapsed    | 5132    |\n","|    total_timesteps | 1720320 |\n","--------------------------------\n","Eval num_timesteps=1728000, episode_reward=0.40 +/- 0.49\n","Episode length: 168.00 +/- 8.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 168          |\n","|    mean_reward          | 0.4          |\n","| time/                   |              |\n","|    total_timesteps      | 1728000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0060182475 |\n","|    clip_fraction        | 0.209        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.62        |\n","|    explained_variance   | 0.879        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.00823     |\n","|    n_updates            | 525          |\n","|    policy_gradient_loss | 0.000849     |\n","|    value_loss           | 0.0535       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 335     |\n","|    iterations      | 106     |\n","|    time_elapsed    | 5178    |\n","|    total_timesteps | 1736704 |\n","--------------------------------\n","Eval num_timesteps=1744000, episode_reward=0.20 +/- 0.40\n","Episode length: 168.00 +/- 8.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 168          |\n","|    mean_reward          | 0.2          |\n","| time/                   |              |\n","|    total_timesteps      | 1744000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0047970535 |\n","|    clip_fraction        | 0.189        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.63        |\n","|    explained_variance   | 0.881        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.023       |\n","|    n_updates            | 530          |\n","|    policy_gradient_loss | 0.00022      |\n","|    value_loss           | 0.0561       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 335     |\n","|    iterations      | 107     |\n","|    time_elapsed    | 5225    |\n","|    total_timesteps | 1753088 |\n","--------------------------------\n","Eval num_timesteps=1760000, episode_reward=1.40 +/- 1.96\n","Episode length: 244.00 +/- 40.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 244         |\n","|    mean_reward          | 1.4         |\n","| time/                   |             |\n","|    total_timesteps      | 1760000     |\n","| train/                  |             |\n","|    approx_kl            | 0.004756093 |\n","|    clip_fraction        | 0.187       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.62       |\n","|    explained_variance   | 0.89        |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0127     |\n","|    n_updates            | 535         |\n","|    policy_gradient_loss | 0.000609    |\n","|    value_loss           | 0.0565      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 335     |\n","|    iterations      | 108     |\n","|    time_elapsed    | 5277    |\n","|    total_timesteps | 1769472 |\n","--------------------------------\n","Eval num_timesteps=1776000, episode_reward=2.20 +/- 2.86\n","Episode length: 276.00 +/- 56.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 276          |\n","|    mean_reward          | 2.2          |\n","| time/                   |              |\n","|    total_timesteps      | 1776000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0049188742 |\n","|    clip_fraction        | 0.18         |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.63        |\n","|    explained_variance   | 0.873        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0161      |\n","|    n_updates            | 540          |\n","|    policy_gradient_loss | 0.000241     |\n","|    value_loss           | 0.061        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 334     |\n","|    iterations      | 109     |\n","|    time_elapsed    | 5333    |\n","|    total_timesteps | 1785856 |\n","--------------------------------\n","Eval num_timesteps=1792000, episode_reward=0.80 +/- 1.60\n","Episode length: 212.00 +/- 24.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 212          |\n","|    mean_reward          | 0.8          |\n","| time/                   |              |\n","|    total_timesteps      | 1792000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0050161253 |\n","|    clip_fraction        | 0.169        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.63        |\n","|    explained_variance   | 0.892        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0115      |\n","|    n_updates            | 545          |\n","|    policy_gradient_loss | 0.000607     |\n","|    value_loss           | 0.0544       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 334     |\n","|    iterations      | 110     |\n","|    time_elapsed    | 5383    |\n","|    total_timesteps | 1802240 |\n","--------------------------------\n","Eval num_timesteps=1808000, episode_reward=0.80 +/- 0.98\n","Episode length: 228.00 +/- 8.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 228          |\n","|    mean_reward          | 0.8          |\n","| time/                   |              |\n","|    total_timesteps      | 1808000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0048139687 |\n","|    clip_fraction        | 0.183        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.64        |\n","|    explained_variance   | 0.892        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0236      |\n","|    n_updates            | 550          |\n","|    policy_gradient_loss | -0.000546    |\n","|    value_loss           | 0.0526       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 334     |\n","|    iterations      | 111     |\n","|    time_elapsed    | 5433    |\n","|    total_timesteps | 1818624 |\n","--------------------------------\n","Eval num_timesteps=1824000, episode_reward=0.80 +/- 1.17\n","Episode length: 164.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 164         |\n","|    mean_reward          | 0.8         |\n","| time/                   |             |\n","|    total_timesteps      | 1824000     |\n","| train/                  |             |\n","|    approx_kl            | 0.005088379 |\n","|    clip_fraction        | 0.183       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.62       |\n","|    explained_variance   | 0.881       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0118     |\n","|    n_updates            | 555         |\n","|    policy_gradient_loss | 0.000738    |\n","|    value_loss           | 0.0526      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 334     |\n","|    iterations      | 112     |\n","|    time_elapsed    | 5479    |\n","|    total_timesteps | 1835008 |\n","--------------------------------\n","Eval num_timesteps=1840000, episode_reward=0.40 +/- 0.80\n","Episode length: 164.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 164          |\n","|    mean_reward          | 0.4          |\n","| time/                   |              |\n","|    total_timesteps      | 1840000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0055904724 |\n","|    clip_fraction        | 0.214        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.62        |\n","|    explained_variance   | 0.883        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0184      |\n","|    n_updates            | 560          |\n","|    policy_gradient_loss | 0.000194     |\n","|    value_loss           | 0.0581       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 335     |\n","|    iterations      | 113     |\n","|    time_elapsed    | 5526    |\n","|    total_timesteps | 1851392 |\n","--------------------------------\n","Eval num_timesteps=1856000, episode_reward=1.00 +/- 1.26\n","Episode length: 200.00 +/- 8.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 200          |\n","|    mean_reward          | 1            |\n","| time/                   |              |\n","|    total_timesteps      | 1856000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0062491884 |\n","|    clip_fraction        | 0.194        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.63        |\n","|    explained_variance   | 0.872        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.00592     |\n","|    n_updates            | 565          |\n","|    policy_gradient_loss | -0.00141     |\n","|    value_loss           | 0.0587       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 335     |\n","|    iterations      | 114     |\n","|    time_elapsed    | 5573    |\n","|    total_timesteps | 1867776 |\n","--------------------------------\n","Eval num_timesteps=1872000, episode_reward=1.40 +/- 1.96\n","Episode length: 216.00 +/- 24.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 216          |\n","|    mean_reward          | 1.4          |\n","| time/                   |              |\n","|    total_timesteps      | 1872000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0048179505 |\n","|    clip_fraction        | 0.179        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.62        |\n","|    explained_variance   | 0.885        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0154      |\n","|    n_updates            | 570          |\n","|    policy_gradient_loss | -0.000293    |\n","|    value_loss           | 0.0555       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 334     |\n","|    iterations      | 115     |\n","|    time_elapsed    | 5626    |\n","|    total_timesteps | 1884160 |\n","--------------------------------\n","Eval num_timesteps=1888000, episode_reward=0.20 +/- 0.40\n","Episode length: 176.00 +/- 24.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 176         |\n","|    mean_reward          | 0.2         |\n","| time/                   |             |\n","|    total_timesteps      | 1888000     |\n","| train/                  |             |\n","|    approx_kl            | 0.005515541 |\n","|    clip_fraction        | 0.202       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.61       |\n","|    explained_variance   | 0.856       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0225     |\n","|    n_updates            | 575         |\n","|    policy_gradient_loss | 0.00084     |\n","|    value_loss           | 0.0616      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 334     |\n","|    iterations      | 116     |\n","|    time_elapsed    | 5675    |\n","|    total_timesteps | 1900544 |\n","--------------------------------\n","Eval num_timesteps=1904000, episode_reward=1.20 +/- 1.94\n","Episode length: 260.00 +/- 48.00\n","---------------------------------------\n","| eval/                   |           |\n","|    mean_ep_length       | 260       |\n","|    mean_reward          | 1.2       |\n","| time/                   |           |\n","|    total_timesteps      | 1904000   |\n","| train/                  |           |\n","|    approx_kl            | 0.0056823 |\n","|    clip_fraction        | 0.22      |\n","|    clip_range           | 0.1       |\n","|    entropy_loss         | -1.63     |\n","|    explained_variance   | 0.866     |\n","|    learning_rate        | 0.000622  |\n","|    loss                 | -0.00766  |\n","|    n_updates            | 580       |\n","|    policy_gradient_loss | 0.000878  |\n","|    value_loss           | 0.0564    |\n","---------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 334     |\n","|    iterations      | 117     |\n","|    time_elapsed    | 5727    |\n","|    total_timesteps | 1916928 |\n","--------------------------------\n","Eval num_timesteps=1920000, episode_reward=1.00 +/- 1.26\n","Episode length: 264.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 264          |\n","|    mean_reward          | 1            |\n","| time/                   |              |\n","|    total_timesteps      | 1920000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0059447372 |\n","|    clip_fraction        | 0.199        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.63        |\n","|    explained_variance   | 0.853        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0163      |\n","|    n_updates            | 585          |\n","|    policy_gradient_loss | -0.000145    |\n","|    value_loss           | 0.059        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 334     |\n","|    iterations      | 118     |\n","|    time_elapsed    | 5779    |\n","|    total_timesteps | 1933312 |\n","--------------------------------\n","Eval num_timesteps=1936000, episode_reward=2.20 +/- 3.49\n","Episode length: 296.00 +/- 56.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 296         |\n","|    mean_reward          | 2.2         |\n","| time/                   |             |\n","|    total_timesteps      | 1936000     |\n","| train/                  |             |\n","|    approx_kl            | 0.006905501 |\n","|    clip_fraction        | 0.24        |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.62       |\n","|    explained_variance   | 0.862       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0144     |\n","|    n_updates            | 590         |\n","|    policy_gradient_loss | 0.000888    |\n","|    value_loss           | 0.0618      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 334     |\n","|    iterations      | 119     |\n","|    time_elapsed    | 5834    |\n","|    total_timesteps | 1949696 |\n","--------------------------------\n","Eval num_timesteps=1952000, episode_reward=0.40 +/- 0.80\n","Episode length: 172.00 +/- 16.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 172         |\n","|    mean_reward          | 0.4         |\n","| time/                   |             |\n","|    total_timesteps      | 1952000     |\n","| train/                  |             |\n","|    approx_kl            | 0.006218149 |\n","|    clip_fraction        | 0.213       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.62       |\n","|    explained_variance   | 0.862       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0226     |\n","|    n_updates            | 595         |\n","|    policy_gradient_loss | -2.57e-05   |\n","|    value_loss           | 0.0552      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 334     |\n","|    iterations      | 120     |\n","|    time_elapsed    | 5882    |\n","|    total_timesteps | 1966080 |\n","--------------------------------\n","Eval num_timesteps=1968000, episode_reward=1.60 +/- 1.62\n","Episode length: 284.00 +/- 40.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 284        |\n","|    mean_reward          | 1.6        |\n","| time/                   |            |\n","|    total_timesteps      | 1968000    |\n","| train/                  |            |\n","|    approx_kl            | 0.00499426 |\n","|    clip_fraction        | 0.182      |\n","|    clip_range           | 0.1        |\n","|    entropy_loss         | -1.63      |\n","|    explained_variance   | 0.862      |\n","|    learning_rate        | 0.000622   |\n","|    loss                 | -0.0206    |\n","|    n_updates            | 600        |\n","|    policy_gradient_loss | 0.000309   |\n","|    value_loss           | 0.0504     |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 333     |\n","|    iterations      | 121     |\n","|    time_elapsed    | 5936    |\n","|    total_timesteps | 1982464 |\n","--------------------------------\n","Eval num_timesteps=1984000, episode_reward=1.40 +/- 2.33\n","Episode length: 228.00 +/- 32.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 228         |\n","|    mean_reward          | 1.4         |\n","| time/                   |             |\n","|    total_timesteps      | 1984000     |\n","| train/                  |             |\n","|    approx_kl            | 0.006250388 |\n","|    clip_fraction        | 0.222       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.62       |\n","|    explained_variance   | 0.869       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0169     |\n","|    n_updates            | 605         |\n","|    policy_gradient_loss | 0.000134    |\n","|    value_loss           | 0.0525      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 333     |\n","|    iterations      | 122     |\n","|    time_elapsed    | 5987    |\n","|    total_timesteps | 1998848 |\n","--------------------------------\n","Eval num_timesteps=2000000, episode_reward=0.60 +/- 0.80\n","Episode length: 164.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 164         |\n","|    mean_reward          | 0.6         |\n","| time/                   |             |\n","|    total_timesteps      | 2000000     |\n","| train/                  |             |\n","|    approx_kl            | 0.005947406 |\n","|    clip_fraction        | 0.196       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.62       |\n","|    explained_variance   | 0.845       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.00924    |\n","|    n_updates            | 610         |\n","|    policy_gradient_loss | -0.000142   |\n","|    value_loss           | 0.061       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 333     |\n","|    iterations      | 123     |\n","|    time_elapsed    | 6034    |\n","|    total_timesteps | 2015232 |\n","--------------------------------\n","Eval num_timesteps=2016000, episode_reward=1.40 +/- 1.74\n","Episode length: 196.00 +/- 16.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 196          |\n","|    mean_reward          | 1.4          |\n","| time/                   |              |\n","|    total_timesteps      | 2016000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0058074584 |\n","|    clip_fraction        | 0.198        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.62        |\n","|    explained_variance   | 0.879        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0158      |\n","|    n_updates            | 615          |\n","|    policy_gradient_loss | -0.000387    |\n","|    value_loss           | 0.0517       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 334     |\n","|    iterations      | 124     |\n","|    time_elapsed    | 6082    |\n","|    total_timesteps | 2031616 |\n","--------------------------------\n","Eval num_timesteps=2032000, episode_reward=0.80 +/- 1.17\n","Episode length: 212.00 +/- 96.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 212          |\n","|    mean_reward          | 0.8          |\n","| time/                   |              |\n","|    total_timesteps      | 2032000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0057935286 |\n","|    clip_fraction        | 0.209        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.63        |\n","|    explained_variance   | 0.874        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0225      |\n","|    n_updates            | 620          |\n","|    policy_gradient_loss | 0.000444     |\n","|    value_loss           | 0.0498       |\n","------------------------------------------\n","Eval num_timesteps=2048000, episode_reward=0.80 +/- 0.98\n","Episode length: 184.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 184      |\n","|    mean_reward     | 0.8      |\n","| time/              |          |\n","|    total_timesteps | 2048000  |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 332     |\n","|    iterations      | 125     |\n","|    time_elapsed    | 6152    |\n","|    total_timesteps | 2048000 |\n","--------------------------------\n","Eval num_timesteps=2064000, episode_reward=1.40 +/- 1.50\n","Episode length: 212.00 +/- 24.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 212          |\n","|    mean_reward          | 1.4          |\n","| time/                   |              |\n","|    total_timesteps      | 2064000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0050540497 |\n","|    clip_fraction        | 0.188        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.63        |\n","|    explained_variance   | 0.869        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0168      |\n","|    n_updates            | 625          |\n","|    policy_gradient_loss | 0.000624     |\n","|    value_loss           | 0.053        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 332     |\n","|    iterations      | 126     |\n","|    time_elapsed    | 6200    |\n","|    total_timesteps | 2064384 |\n","--------------------------------\n","Eval num_timesteps=2080000, episode_reward=0.40 +/- 0.80\n","Episode length: 224.00 +/- 40.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 224          |\n","|    mean_reward          | 0.4          |\n","| time/                   |              |\n","|    total_timesteps      | 2080000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0057419064 |\n","|    clip_fraction        | 0.205        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.64        |\n","|    explained_variance   | 0.853        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0155      |\n","|    n_updates            | 630          |\n","|    policy_gradient_loss | 0.00113      |\n","|    value_loss           | 0.059        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 332     |\n","|    iterations      | 127     |\n","|    time_elapsed    | 6254    |\n","|    total_timesteps | 2080768 |\n","--------------------------------\n","Eval num_timesteps=2096000, episode_reward=0.80 +/- 0.98\n","Episode length: 204.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 204         |\n","|    mean_reward          | 0.8         |\n","| time/                   |             |\n","|    total_timesteps      | 2096000     |\n","| train/                  |             |\n","|    approx_kl            | 0.005549637 |\n","|    clip_fraction        | 0.21        |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.62       |\n","|    explained_variance   | 0.864       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0209     |\n","|    n_updates            | 635         |\n","|    policy_gradient_loss | -0.000696   |\n","|    value_loss           | 0.0535      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 332     |\n","|    iterations      | 128     |\n","|    time_elapsed    | 6302    |\n","|    total_timesteps | 2097152 |\n","--------------------------------\n","Eval num_timesteps=2112000, episode_reward=0.40 +/- 0.49\n","Episode length: 236.00 +/- 104.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 236         |\n","|    mean_reward          | 0.4         |\n","| time/                   |             |\n","|    total_timesteps      | 2112000     |\n","| train/                  |             |\n","|    approx_kl            | 0.005323055 |\n","|    clip_fraction        | 0.182       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.62       |\n","|    explained_variance   | 0.868       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0139     |\n","|    n_updates            | 640         |\n","|    policy_gradient_loss | 0.000358    |\n","|    value_loss           | 0.0531      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 332     |\n","|    iterations      | 129     |\n","|    time_elapsed    | 6364    |\n","|    total_timesteps | 2113536 |\n","--------------------------------\n","Eval num_timesteps=2128000, episode_reward=1.20 +/- 1.47\n","Episode length: 184.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 184          |\n","|    mean_reward          | 1.2          |\n","| time/                   |              |\n","|    total_timesteps      | 2128000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0050177323 |\n","|    clip_fraction        | 0.185        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.62        |\n","|    explained_variance   | 0.859        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0162      |\n","|    n_updates            | 645          |\n","|    policy_gradient_loss | 8.58e-05     |\n","|    value_loss           | 0.0599       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 332     |\n","|    iterations      | 130     |\n","|    time_elapsed    | 6410    |\n","|    total_timesteps | 2129920 |\n","--------------------------------\n","Eval num_timesteps=2144000, episode_reward=0.40 +/- 0.49\n","Episode length: 212.00 +/- 16.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 212          |\n","|    mean_reward          | 0.4          |\n","| time/                   |              |\n","|    total_timesteps      | 2144000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0069976556 |\n","|    clip_fraction        | 0.231        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.62        |\n","|    explained_variance   | 0.872        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0178      |\n","|    n_updates            | 650          |\n","|    policy_gradient_loss | 0.000254     |\n","|    value_loss           | 0.0536       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 332     |\n","|    iterations      | 131     |\n","|    time_elapsed    | 6459    |\n","|    total_timesteps | 2146304 |\n","--------------------------------\n","Eval num_timesteps=2160000, episode_reward=0.20 +/- 0.40\n","Episode length: 176.00 +/- 24.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 176          |\n","|    mean_reward          | 0.2          |\n","| time/                   |              |\n","|    total_timesteps      | 2160000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0061293663 |\n","|    clip_fraction        | 0.224        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.62        |\n","|    explained_variance   | 0.894        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0204      |\n","|    n_updates            | 655          |\n","|    policy_gradient_loss | 4.91e-05     |\n","|    value_loss           | 0.0508       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 332     |\n","|    iterations      | 132     |\n","|    time_elapsed    | 6508    |\n","|    total_timesteps | 2162688 |\n","--------------------------------\n","Eval num_timesteps=2176000, episode_reward=1.60 +/- 1.85\n","Episode length: 268.00 +/- 8.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 268          |\n","|    mean_reward          | 1.6          |\n","| time/                   |              |\n","|    total_timesteps      | 2176000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0064609353 |\n","|    clip_fraction        | 0.2          |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.61        |\n","|    explained_variance   | 0.886        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0148      |\n","|    n_updates            | 660          |\n","|    policy_gradient_loss | 7.34e-06     |\n","|    value_loss           | 0.0589       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 332     |\n","|    iterations      | 133     |\n","|    time_elapsed    | 6561    |\n","|    total_timesteps | 2179072 |\n","--------------------------------\n","Eval num_timesteps=2192000, episode_reward=1.60 +/- 1.96\n","Episode length: 200.00 +/- 8.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 200          |\n","|    mean_reward          | 1.6          |\n","| time/                   |              |\n","|    total_timesteps      | 2192000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0059899795 |\n","|    clip_fraction        | 0.198        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.61        |\n","|    explained_variance   | 0.881        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0215      |\n","|    n_updates            | 665          |\n","|    policy_gradient_loss | 0.000372     |\n","|    value_loss           | 0.0625       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 332     |\n","|    iterations      | 134     |\n","|    time_elapsed    | 6608    |\n","|    total_timesteps | 2195456 |\n","--------------------------------\n","Eval num_timesteps=2208000, episode_reward=0.60 +/- 0.80\n","Episode length: 168.00 +/- 8.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 168         |\n","|    mean_reward          | 0.6         |\n","| time/                   |             |\n","|    total_timesteps      | 2208000     |\n","| train/                  |             |\n","|    approx_kl            | 0.005426258 |\n","|    clip_fraction        | 0.19        |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.61       |\n","|    explained_variance   | 0.881       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0137     |\n","|    n_updates            | 670         |\n","|    policy_gradient_loss | -0.00057    |\n","|    value_loss           | 0.0532      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 332     |\n","|    iterations      | 135     |\n","|    time_elapsed    | 6655    |\n","|    total_timesteps | 2211840 |\n","--------------------------------\n","Eval num_timesteps=2224000, episode_reward=0.60 +/- 0.49\n","Episode length: 176.00 +/- 24.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 176          |\n","|    mean_reward          | 0.6          |\n","| time/                   |              |\n","|    total_timesteps      | 2224000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0054965904 |\n","|    clip_fraction        | 0.186        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.61        |\n","|    explained_variance   | 0.902        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0119      |\n","|    n_updates            | 675          |\n","|    policy_gradient_loss | -0.000127    |\n","|    value_loss           | 0.0461       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 332     |\n","|    iterations      | 136     |\n","|    time_elapsed    | 6704    |\n","|    total_timesteps | 2228224 |\n","--------------------------------\n","Eval num_timesteps=2240000, episode_reward=0.60 +/- 0.80\n","Episode length: 196.00 +/- 24.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 196         |\n","|    mean_reward          | 0.6         |\n","| time/                   |             |\n","|    total_timesteps      | 2240000     |\n","| train/                  |             |\n","|    approx_kl            | 0.006999685 |\n","|    clip_fraction        | 0.214       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.61       |\n","|    explained_variance   | 0.898       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0164     |\n","|    n_updates            | 680         |\n","|    policy_gradient_loss | 4.13e-05    |\n","|    value_loss           | 0.0544      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 332     |\n","|    iterations      | 137     |\n","|    time_elapsed    | 6754    |\n","|    total_timesteps | 2244608 |\n","--------------------------------\n","Eval num_timesteps=2256000, episode_reward=0.60 +/- 1.20\n","Episode length: 188.00 +/- 48.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 188         |\n","|    mean_reward          | 0.6         |\n","| time/                   |             |\n","|    total_timesteps      | 2256000     |\n","| train/                  |             |\n","|    approx_kl            | 0.005261591 |\n","|    clip_fraction        | 0.187       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.6        |\n","|    explained_variance   | 0.893       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0168     |\n","|    n_updates            | 685         |\n","|    policy_gradient_loss | -0.00134    |\n","|    value_loss           | 0.0533      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 332     |\n","|    iterations      | 138     |\n","|    time_elapsed    | 6807    |\n","|    total_timesteps | 2260992 |\n","--------------------------------\n","Eval num_timesteps=2272000, episode_reward=0.60 +/- 1.20\n","Episode length: 192.00 +/- 16.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 192          |\n","|    mean_reward          | 0.6          |\n","| time/                   |              |\n","|    total_timesteps      | 2272000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0061885444 |\n","|    clip_fraction        | 0.221        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.6         |\n","|    explained_variance   | 0.874        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0171      |\n","|    n_updates            | 690          |\n","|    policy_gradient_loss | 0.00122      |\n","|    value_loss           | 0.0567       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 332     |\n","|    iterations      | 139     |\n","|    time_elapsed    | 6857    |\n","|    total_timesteps | 2277376 |\n","--------------------------------\n","Eval num_timesteps=2288000, episode_reward=0.80 +/- 1.17\n","Episode length: 184.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 184          |\n","|    mean_reward          | 0.8          |\n","| time/                   |              |\n","|    total_timesteps      | 2288000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0065313186 |\n","|    clip_fraction        | 0.217        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.61        |\n","|    explained_variance   | 0.894        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0173      |\n","|    n_updates            | 695          |\n","|    policy_gradient_loss | 0.00188      |\n","|    value_loss           | 0.0513       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 332     |\n","|    iterations      | 140     |\n","|    time_elapsed    | 6904    |\n","|    total_timesteps | 2293760 |\n","--------------------------------\n","Eval num_timesteps=2304000, episode_reward=1.20 +/- 1.47\n","Episode length: 212.00 +/- 24.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 212          |\n","|    mean_reward          | 1.2          |\n","| time/                   |              |\n","|    total_timesteps      | 2304000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0063590584 |\n","|    clip_fraction        | 0.178        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.59        |\n","|    explained_variance   | 0.898        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0179      |\n","|    n_updates            | 700          |\n","|    policy_gradient_loss | 0.00115      |\n","|    value_loss           | 0.0532       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 332     |\n","|    iterations      | 141     |\n","|    time_elapsed    | 6954    |\n","|    total_timesteps | 2310144 |\n","--------------------------------\n","Eval num_timesteps=2320000, episode_reward=2.20 +/- 2.14\n","Episode length: 340.00 +/- 88.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 340          |\n","|    mean_reward          | 2.2          |\n","| time/                   |              |\n","|    total_timesteps      | 2320000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0064672786 |\n","|    clip_fraction        | 0.2          |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.59        |\n","|    explained_variance   | 0.869        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0119      |\n","|    n_updates            | 705          |\n","|    policy_gradient_loss | 0.00189      |\n","|    value_loss           | 0.0575       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 331     |\n","|    iterations      | 142     |\n","|    time_elapsed    | 7015    |\n","|    total_timesteps | 2326528 |\n","--------------------------------\n","Eval num_timesteps=2336000, episode_reward=1.20 +/- 1.47\n","Episode length: 212.00 +/- 56.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 212         |\n","|    mean_reward          | 1.2         |\n","| time/                   |             |\n","|    total_timesteps      | 2336000     |\n","| train/                  |             |\n","|    approx_kl            | 0.005688753 |\n","|    clip_fraction        | 0.199       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.59       |\n","|    explained_variance   | 0.884       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0233     |\n","|    n_updates            | 710         |\n","|    policy_gradient_loss | -0.00112    |\n","|    value_loss           | 0.0599      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 331     |\n","|    iterations      | 143     |\n","|    time_elapsed    | 7070    |\n","|    total_timesteps | 2342912 |\n","--------------------------------\n","Eval num_timesteps=2352000, episode_reward=1.60 +/- 2.33\n","Episode length: 276.00 +/- 56.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 276          |\n","|    mean_reward          | 1.6          |\n","| time/                   |              |\n","|    total_timesteps      | 2352000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0067068706 |\n","|    clip_fraction        | 0.207        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.58        |\n","|    explained_variance   | 0.882        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0101      |\n","|    n_updates            | 715          |\n","|    policy_gradient_loss | 0.000721     |\n","|    value_loss           | 0.0568       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 331     |\n","|    iterations      | 144     |\n","|    time_elapsed    | 7126    |\n","|    total_timesteps | 2359296 |\n","--------------------------------\n","Eval num_timesteps=2368000, episode_reward=0.80 +/- 1.60\n","Episode length: 180.00 +/- 8.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 180          |\n","|    mean_reward          | 0.8          |\n","| time/                   |              |\n","|    total_timesteps      | 2368000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0058785416 |\n","|    clip_fraction        | 0.186        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.58        |\n","|    explained_variance   | 0.883        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0169      |\n","|    n_updates            | 720          |\n","|    policy_gradient_loss | 0.00111      |\n","|    value_loss           | 0.06         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 331     |\n","|    iterations      | 145     |\n","|    time_elapsed    | 7173    |\n","|    total_timesteps | 2375680 |\n","--------------------------------\n","Eval num_timesteps=2384000, episode_reward=0.20 +/- 0.40\n","Episode length: 168.00 +/- 8.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 168         |\n","|    mean_reward          | 0.2         |\n","| time/                   |             |\n","|    total_timesteps      | 2384000     |\n","| train/                  |             |\n","|    approx_kl            | 0.006125994 |\n","|    clip_fraction        | 0.205       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.58       |\n","|    explained_variance   | 0.874       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0172     |\n","|    n_updates            | 725         |\n","|    policy_gradient_loss | -0.00102    |\n","|    value_loss           | 0.0596      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 331     |\n","|    iterations      | 146     |\n","|    time_elapsed    | 7221    |\n","|    total_timesteps | 2392064 |\n","--------------------------------\n","Eval num_timesteps=2400000, episode_reward=0.20 +/- 0.40\n","Episode length: 168.00 +/- 8.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 168          |\n","|    mean_reward          | 0.2          |\n","| time/                   |              |\n","|    total_timesteps      | 2400000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0052510793 |\n","|    clip_fraction        | 0.191        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.58        |\n","|    explained_variance   | 0.869        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0172      |\n","|    n_updates            | 730          |\n","|    policy_gradient_loss | 0.000749     |\n","|    value_loss           | 0.0603       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 331     |\n","|    iterations      | 147     |\n","|    time_elapsed    | 7269    |\n","|    total_timesteps | 2408448 |\n","--------------------------------\n","Eval num_timesteps=2416000, episode_reward=1.00 +/- 1.26\n","Episode length: 180.00 +/- 8.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 180          |\n","|    mean_reward          | 1            |\n","| time/                   |              |\n","|    total_timesteps      | 2416000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0063662054 |\n","|    clip_fraction        | 0.184        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.57        |\n","|    explained_variance   | 0.883        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0242      |\n","|    n_updates            | 735          |\n","|    policy_gradient_loss | -4.09e-05    |\n","|    value_loss           | 0.0556       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 331     |\n","|    iterations      | 148     |\n","|    time_elapsed    | 7316    |\n","|    total_timesteps | 2424832 |\n","--------------------------------\n","Eval num_timesteps=2432000, episode_reward=0.20 +/- 0.40\n","Episode length: 180.00 +/- 8.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 180          |\n","|    mean_reward          | 0.2          |\n","| time/                   |              |\n","|    total_timesteps      | 2432000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0055416105 |\n","|    clip_fraction        | 0.186        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.56        |\n","|    explained_variance   | 0.888        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0136      |\n","|    n_updates            | 740          |\n","|    policy_gradient_loss | 0.00072      |\n","|    value_loss           | 0.064        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 331     |\n","|    iterations      | 149     |\n","|    time_elapsed    | 7363    |\n","|    total_timesteps | 2441216 |\n","--------------------------------\n","Eval num_timesteps=2448000, episode_reward=1.40 +/- 1.74\n","Episode length: 228.00 +/- 32.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 228        |\n","|    mean_reward          | 1.4        |\n","| time/                   |            |\n","|    total_timesteps      | 2448000    |\n","| train/                  |            |\n","|    approx_kl            | 0.00820322 |\n","|    clip_fraction        | 0.226      |\n","|    clip_range           | 0.1        |\n","|    entropy_loss         | -1.58      |\n","|    explained_variance   | 0.893      |\n","|    learning_rate        | 0.000622   |\n","|    loss                 | -0.0136    |\n","|    n_updates            | 745        |\n","|    policy_gradient_loss | 0.00249    |\n","|    value_loss           | 0.0566     |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 331     |\n","|    iterations      | 150     |\n","|    time_elapsed    | 7415    |\n","|    total_timesteps | 2457600 |\n","--------------------------------\n","Eval num_timesteps=2464000, episode_reward=0.80 +/- 1.17\n","Episode length: 180.00 +/- 32.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 180        |\n","|    mean_reward          | 0.8        |\n","| time/                   |            |\n","|    total_timesteps      | 2464000    |\n","| train/                  |            |\n","|    approx_kl            | 0.00741852 |\n","|    clip_fraction        | 0.219      |\n","|    clip_range           | 0.1        |\n","|    entropy_loss         | -1.56      |\n","|    explained_variance   | 0.856      |\n","|    learning_rate        | 0.000622   |\n","|    loss                 | -0.0181    |\n","|    n_updates            | 750        |\n","|    policy_gradient_loss | 0.00122    |\n","|    value_loss           | 0.0632     |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 331     |\n","|    iterations      | 151     |\n","|    time_elapsed    | 7466    |\n","|    total_timesteps | 2473984 |\n","--------------------------------\n","Eval num_timesteps=2480000, episode_reward=0.80 +/- 1.17\n","Episode length: 212.00 +/- 16.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 212         |\n","|    mean_reward          | 0.8         |\n","| time/                   |             |\n","|    total_timesteps      | 2480000     |\n","| train/                  |             |\n","|    approx_kl            | 0.005731697 |\n","|    clip_fraction        | 0.208       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.57       |\n","|    explained_variance   | 0.82        |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0122     |\n","|    n_updates            | 755         |\n","|    policy_gradient_loss | 0.000702    |\n","|    value_loss           | 0.0807      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 331     |\n","|    iterations      | 152     |\n","|    time_elapsed    | 7518    |\n","|    total_timesteps | 2490368 |\n","--------------------------------\n","Eval num_timesteps=2496000, episode_reward=0.40 +/- 0.49\n","Episode length: 204.00 +/- 40.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 204          |\n","|    mean_reward          | 0.4          |\n","| time/                   |              |\n","|    total_timesteps      | 2496000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0060621654 |\n","|    clip_fraction        | 0.218        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.56        |\n","|    explained_variance   | 0.814        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0138      |\n","|    n_updates            | 760          |\n","|    policy_gradient_loss | 0.000867     |\n","|    value_loss           | 0.0865       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 330     |\n","|    iterations      | 153     |\n","|    time_elapsed    | 7574    |\n","|    total_timesteps | 2506752 |\n","--------------------------------\n","Eval num_timesteps=2512000, episode_reward=1.40 +/- 1.96\n","Episode length: 232.00 +/- 24.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 232         |\n","|    mean_reward          | 1.4         |\n","| time/                   |             |\n","|    total_timesteps      | 2512000     |\n","| train/                  |             |\n","|    approx_kl            | 0.007138426 |\n","|    clip_fraction        | 0.199       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.57       |\n","|    explained_variance   | 0.878       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0173     |\n","|    n_updates            | 765         |\n","|    policy_gradient_loss | 0.00237     |\n","|    value_loss           | 0.0582      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 330     |\n","|    iterations      | 154     |\n","|    time_elapsed    | 7625    |\n","|    total_timesteps | 2523136 |\n","--------------------------------\n","Eval num_timesteps=2528000, episode_reward=0.20 +/- 0.40\n","Episode length: 200.00 +/- 72.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 200         |\n","|    mean_reward          | 0.2         |\n","| time/                   |             |\n","|    total_timesteps      | 2528000     |\n","| train/                  |             |\n","|    approx_kl            | 0.007093301 |\n","|    clip_fraction        | 0.215       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.57       |\n","|    explained_variance   | 0.856       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0126     |\n","|    n_updates            | 770         |\n","|    policy_gradient_loss | 0.00257     |\n","|    value_loss           | 0.0666      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 330     |\n","|    iterations      | 155     |\n","|    time_elapsed    | 7685    |\n","|    total_timesteps | 2539520 |\n","--------------------------------\n","Eval num_timesteps=2544000, episode_reward=0.80 +/- 0.98\n","Episode length: 184.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 184          |\n","|    mean_reward          | 0.8          |\n","| time/                   |              |\n","|    total_timesteps      | 2544000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0064754616 |\n","|    clip_fraction        | 0.164        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.58        |\n","|    explained_variance   | 0.869        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0103      |\n","|    n_updates            | 775          |\n","|    policy_gradient_loss | -0.000187    |\n","|    value_loss           | 0.0669       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 330     |\n","|    iterations      | 156     |\n","|    time_elapsed    | 7734    |\n","|    total_timesteps | 2555904 |\n","--------------------------------\n","Eval num_timesteps=2560000, episode_reward=0.80 +/- 1.17\n","Episode length: 168.00 +/- 8.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 168         |\n","|    mean_reward          | 0.8         |\n","| time/                   |             |\n","|    total_timesteps      | 2560000     |\n","| train/                  |             |\n","|    approx_kl            | 0.007359184 |\n","|    clip_fraction        | 0.195       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.58       |\n","|    explained_variance   | 0.894       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.00896    |\n","|    n_updates            | 780         |\n","|    policy_gradient_loss | 0.000517    |\n","|    value_loss           | 0.058       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 330     |\n","|    iterations      | 157     |\n","|    time_elapsed    | 7782    |\n","|    total_timesteps | 2572288 |\n","--------------------------------\n","Eval num_timesteps=2576000, episode_reward=1.80 +/- 2.40\n","Episode length: 276.00 +/- 24.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 276          |\n","|    mean_reward          | 1.8          |\n","| time/                   |              |\n","|    total_timesteps      | 2576000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0059323814 |\n","|    clip_fraction        | 0.185        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.57        |\n","|    explained_variance   | 0.883        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0134      |\n","|    n_updates            | 785          |\n","|    policy_gradient_loss | 0.000174     |\n","|    value_loss           | 0.0566       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 330     |\n","|    iterations      | 158     |\n","|    time_elapsed    | 7840    |\n","|    total_timesteps | 2588672 |\n","--------------------------------\n","Eval num_timesteps=2592000, episode_reward=1.80 +/- 2.23\n","Episode length: 268.00 +/- 32.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 268          |\n","|    mean_reward          | 1.8          |\n","| time/                   |              |\n","|    total_timesteps      | 2592000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0069721425 |\n","|    clip_fraction        | 0.217        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.58        |\n","|    explained_variance   | 0.884        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0213      |\n","|    n_updates            | 790          |\n","|    policy_gradient_loss | -0.000898    |\n","|    value_loss           | 0.0603       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 329     |\n","|    iterations      | 159     |\n","|    time_elapsed    | 7894    |\n","|    total_timesteps | 2605056 |\n","--------------------------------\n","Eval num_timesteps=2608000, episode_reward=1.60 +/- 2.33\n","Episode length: 268.00 +/- 8.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 268          |\n","|    mean_reward          | 1.6          |\n","| time/                   |              |\n","|    total_timesteps      | 2608000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0068965917 |\n","|    clip_fraction        | 0.202        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.59        |\n","|    explained_variance   | 0.871        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0166      |\n","|    n_updates            | 795          |\n","|    policy_gradient_loss | 0.000729     |\n","|    value_loss           | 0.0609       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 329     |\n","|    iterations      | 160     |\n","|    time_elapsed    | 7947    |\n","|    total_timesteps | 2621440 |\n","--------------------------------\n","Eval num_timesteps=2624000, episode_reward=0.60 +/- 1.20\n","Episode length: 204.00 +/- 40.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 204          |\n","|    mean_reward          | 0.6          |\n","| time/                   |              |\n","|    total_timesteps      | 2624000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0058270385 |\n","|    clip_fraction        | 0.205        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.6         |\n","|    explained_variance   | 0.883        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0188      |\n","|    n_updates            | 800          |\n","|    policy_gradient_loss | -0.000651    |\n","|    value_loss           | 0.0536       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 329     |\n","|    iterations      | 161     |\n","|    time_elapsed    | 8002    |\n","|    total_timesteps | 2637824 |\n","--------------------------------\n","Eval num_timesteps=2640000, episode_reward=1.20 +/- 1.60\n","Episode length: 248.00 +/- 32.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 248         |\n","|    mean_reward          | 1.2         |\n","| time/                   |             |\n","|    total_timesteps      | 2640000     |\n","| train/                  |             |\n","|    approx_kl            | 0.005875938 |\n","|    clip_fraction        | 0.216       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.59       |\n","|    explained_variance   | 0.866       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0109     |\n","|    n_updates            | 805         |\n","|    policy_gradient_loss | 0.000941    |\n","|    value_loss           | 0.056       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 329     |\n","|    iterations      | 162     |\n","|    time_elapsed    | 8053    |\n","|    total_timesteps | 2654208 |\n","--------------------------------\n","Eval num_timesteps=2656000, episode_reward=0.60 +/- 0.80\n","Episode length: 180.00 +/- 8.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 180          |\n","|    mean_reward          | 0.6          |\n","| time/                   |              |\n","|    total_timesteps      | 2656000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0061736642 |\n","|    clip_fraction        | 0.183        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.58        |\n","|    explained_variance   | 0.852        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0198      |\n","|    n_updates            | 810          |\n","|    policy_gradient_loss | 0.000677     |\n","|    value_loss           | 0.0675       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 329     |\n","|    iterations      | 163     |\n","|    time_elapsed    | 8101    |\n","|    total_timesteps | 2670592 |\n","--------------------------------\n","Eval num_timesteps=2672000, episode_reward=0.40 +/- 0.49\n","Episode length: 168.00 +/- 8.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 168         |\n","|    mean_reward          | 0.4         |\n","| time/                   |             |\n","|    total_timesteps      | 2672000     |\n","| train/                  |             |\n","|    approx_kl            | 0.006129241 |\n","|    clip_fraction        | 0.209       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.6        |\n","|    explained_variance   | 0.876       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0143     |\n","|    n_updates            | 815         |\n","|    policy_gradient_loss | -3.27e-05   |\n","|    value_loss           | 0.0559      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 329     |\n","|    iterations      | 164     |\n","|    time_elapsed    | 8149    |\n","|    total_timesteps | 2686976 |\n","--------------------------------\n","Eval num_timesteps=2688000, episode_reward=0.80 +/- 1.17\n","Episode length: 196.00 +/- 16.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 196          |\n","|    mean_reward          | 0.8          |\n","| time/                   |              |\n","|    total_timesteps      | 2688000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0065376842 |\n","|    clip_fraction        | 0.193        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.58        |\n","|    explained_variance   | 0.879        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0235      |\n","|    n_updates            | 820          |\n","|    policy_gradient_loss | 0.00112      |\n","|    value_loss           | 0.0679       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 329     |\n","|    iterations      | 165     |\n","|    time_elapsed    | 8198    |\n","|    total_timesteps | 2703360 |\n","--------------------------------\n","Eval num_timesteps=2704000, episode_reward=0.20 +/- 0.40\n","Episode length: 180.00 +/- 32.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 180          |\n","|    mean_reward          | 0.2          |\n","| time/                   |              |\n","|    total_timesteps      | 2704000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0064352914 |\n","|    clip_fraction        | 0.19         |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.58        |\n","|    explained_variance   | 0.884        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0108      |\n","|    n_updates            | 825          |\n","|    policy_gradient_loss | 3.39e-05     |\n","|    value_loss           | 0.0608       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 329     |\n","|    iterations      | 166     |\n","|    time_elapsed    | 8250    |\n","|    total_timesteps | 2719744 |\n","--------------------------------\n","Eval num_timesteps=2720000, episode_reward=1.00 +/- 1.26\n","Episode length: 184.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 184          |\n","|    mean_reward          | 1            |\n","| time/                   |              |\n","|    total_timesteps      | 2720000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0056969943 |\n","|    clip_fraction        | 0.21         |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.57        |\n","|    explained_variance   | 0.884        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0229      |\n","|    n_updates            | 830          |\n","|    policy_gradient_loss | 0.00125      |\n","|    value_loss           | 0.0513       |\n","------------------------------------------\n","Eval num_timesteps=2736000, episode_reward=0.40 +/- 0.80\n","Episode length: 164.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 164      |\n","|    mean_reward     | 0.4      |\n","| time/              |          |\n","|    total_timesteps | 2736000  |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 329     |\n","|    iterations      | 167     |\n","|    time_elapsed    | 8309    |\n","|    total_timesteps | 2736128 |\n","--------------------------------\n","Eval num_timesteps=2752000, episode_reward=1.60 +/- 2.73\n","Episode length: 276.00 +/- 16.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 276        |\n","|    mean_reward          | 1.6        |\n","| time/                   |            |\n","|    total_timesteps      | 2752000    |\n","| train/                  |            |\n","|    approx_kl            | 0.00547597 |\n","|    clip_fraction        | 0.171      |\n","|    clip_range           | 0.1        |\n","|    entropy_loss         | -1.57      |\n","|    explained_variance   | 0.863      |\n","|    learning_rate        | 0.000622   |\n","|    loss                 | -0.0187    |\n","|    n_updates            | 835        |\n","|    policy_gradient_loss | -0.000421  |\n","|    value_loss           | 0.0622     |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 329     |\n","|    iterations      | 168     |\n","|    time_elapsed    | 8363    |\n","|    total_timesteps | 2752512 |\n","--------------------------------\n","Eval num_timesteps=2768000, episode_reward=1.00 +/- 1.55\n","Episode length: 180.00 +/- 8.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 180          |\n","|    mean_reward          | 1            |\n","| time/                   |              |\n","|    total_timesteps      | 2768000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0061934106 |\n","|    clip_fraction        | 0.18         |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.57        |\n","|    explained_variance   | 0.857        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0138      |\n","|    n_updates            | 840          |\n","|    policy_gradient_loss | 0.000785     |\n","|    value_loss           | 0.0622       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 329     |\n","|    iterations      | 169     |\n","|    time_elapsed    | 8411    |\n","|    total_timesteps | 2768896 |\n","--------------------------------\n","Eval num_timesteps=2784000, episode_reward=0.40 +/- 0.49\n","Episode length: 172.00 +/- 16.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 172          |\n","|    mean_reward          | 0.4          |\n","| time/                   |              |\n","|    total_timesteps      | 2784000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0073930686 |\n","|    clip_fraction        | 0.194        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.57        |\n","|    explained_variance   | 0.855        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0182      |\n","|    n_updates            | 845          |\n","|    policy_gradient_loss | -0.000665    |\n","|    value_loss           | 0.0618       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 329     |\n","|    iterations      | 170     |\n","|    time_elapsed    | 8459    |\n","|    total_timesteps | 2785280 |\n","--------------------------------\n","Eval num_timesteps=2800000, episode_reward=0.80 +/- 1.17\n","Episode length: 176.00 +/- 24.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 176         |\n","|    mean_reward          | 0.8         |\n","| time/                   |             |\n","|    total_timesteps      | 2800000     |\n","| train/                  |             |\n","|    approx_kl            | 0.006493708 |\n","|    clip_fraction        | 0.216       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.59       |\n","|    explained_variance   | 0.887       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.012      |\n","|    n_updates            | 850         |\n","|    policy_gradient_loss | 0.00114     |\n","|    value_loss           | 0.0526      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 329     |\n","|    iterations      | 171     |\n","|    time_elapsed    | 8509    |\n","|    total_timesteps | 2801664 |\n","--------------------------------\n","Eval num_timesteps=2816000, episode_reward=1.00 +/- 1.26\n","Episode length: 228.00 +/- 8.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 228         |\n","|    mean_reward          | 1           |\n","| time/                   |             |\n","|    total_timesteps      | 2816000     |\n","| train/                  |             |\n","|    approx_kl            | 0.006442529 |\n","|    clip_fraction        | 0.18        |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.58       |\n","|    explained_variance   | 0.87        |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0107     |\n","|    n_updates            | 855         |\n","|    policy_gradient_loss | -0.000801   |\n","|    value_loss           | 0.0578      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 329     |\n","|    iterations      | 172     |\n","|    time_elapsed    | 8560    |\n","|    total_timesteps | 2818048 |\n","--------------------------------\n","Eval num_timesteps=2832000, episode_reward=0.60 +/- 0.80\n","Episode length: 200.00 +/- 8.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 200          |\n","|    mean_reward          | 0.6          |\n","| time/                   |              |\n","|    total_timesteps      | 2832000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0062850355 |\n","|    clip_fraction        | 0.199        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.57        |\n","|    explained_variance   | 0.837        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.022       |\n","|    n_updates            | 860          |\n","|    policy_gradient_loss | -0.00119     |\n","|    value_loss           | 0.0627       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 329     |\n","|    iterations      | 173     |\n","|    time_elapsed    | 8610    |\n","|    total_timesteps | 2834432 |\n","--------------------------------\n","Eval num_timesteps=2848000, episode_reward=1.00 +/- 1.55\n","Episode length: 180.00 +/- 8.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 180         |\n","|    mean_reward          | 1           |\n","| time/                   |             |\n","|    total_timesteps      | 2848000     |\n","| train/                  |             |\n","|    approx_kl            | 0.007963725 |\n","|    clip_fraction        | 0.207       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.59       |\n","|    explained_variance   | 0.838       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.014      |\n","|    n_updates            | 865         |\n","|    policy_gradient_loss | 0.000575    |\n","|    value_loss           | 0.0608      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 329     |\n","|    iterations      | 174     |\n","|    time_elapsed    | 8658    |\n","|    total_timesteps | 2850816 |\n","--------------------------------\n","Eval num_timesteps=2864000, episode_reward=3.60 +/- 4.32\n","Episode length: 384.00 +/- 80.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 384         |\n","|    mean_reward          | 3.6         |\n","| time/                   |             |\n","|    total_timesteps      | 2864000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008434119 |\n","|    clip_fraction        | 0.207       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.59       |\n","|    explained_variance   | 0.872       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.016      |\n","|    n_updates            | 870         |\n","|    policy_gradient_loss | 0.00134     |\n","|    value_loss           | 0.0605      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 328     |\n","|    iterations      | 175     |\n","|    time_elapsed    | 8721    |\n","|    total_timesteps | 2867200 |\n","--------------------------------\n","Eval num_timesteps=2880000, episode_reward=0.40 +/- 0.80\n","Episode length: 200.00 +/- 72.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 200         |\n","|    mean_reward          | 0.4         |\n","| time/                   |             |\n","|    total_timesteps      | 2880000     |\n","| train/                  |             |\n","|    approx_kl            | 0.005733153 |\n","|    clip_fraction        | 0.208       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.59       |\n","|    explained_variance   | 0.859       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0148     |\n","|    n_updates            | 875         |\n","|    policy_gradient_loss | 0.001       |\n","|    value_loss           | 0.0582      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 328     |\n","|    iterations      | 176     |\n","|    time_elapsed    | 8777    |\n","|    total_timesteps | 2883584 |\n","--------------------------------\n","Eval num_timesteps=2896000, episode_reward=2.40 +/- 3.01\n","Episode length: 312.00 +/- 64.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 312          |\n","|    mean_reward          | 2.4          |\n","| time/                   |              |\n","|    total_timesteps      | 2896000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0066842837 |\n","|    clip_fraction        | 0.199        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.57        |\n","|    explained_variance   | 0.869        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0186      |\n","|    n_updates            | 880          |\n","|    policy_gradient_loss | 0.000703     |\n","|    value_loss           | 0.0653       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 328     |\n","|    iterations      | 177     |\n","|    time_elapsed    | 8834    |\n","|    total_timesteps | 2899968 |\n","--------------------------------\n","Eval num_timesteps=2912000, episode_reward=1.40 +/- 1.74\n","Episode length: 244.00 +/- 40.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 244          |\n","|    mean_reward          | 1.4          |\n","| time/                   |              |\n","|    total_timesteps      | 2912000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0054781064 |\n","|    clip_fraction        | 0.185        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.57        |\n","|    explained_variance   | 0.858        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0215      |\n","|    n_updates            | 885          |\n","|    policy_gradient_loss | 0.00013      |\n","|    value_loss           | 0.0689       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 328     |\n","|    iterations      | 178     |\n","|    time_elapsed    | 8886    |\n","|    total_timesteps | 2916352 |\n","--------------------------------\n","Eval num_timesteps=2928000, episode_reward=3.60 +/- 4.03\n","Episode length: 404.00 +/- 80.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 404         |\n","|    mean_reward          | 3.6         |\n","| time/                   |             |\n","|    total_timesteps      | 2928000     |\n","| train/                  |             |\n","|    approx_kl            | 0.007561216 |\n","|    clip_fraction        | 0.214       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.57       |\n","|    explained_variance   | 0.89        |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0225     |\n","|    n_updates            | 890         |\n","|    policy_gradient_loss | -0.000284   |\n","|    value_loss           | 0.0606      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 327     |\n","|    iterations      | 179     |\n","|    time_elapsed    | 8949    |\n","|    total_timesteps | 2932736 |\n","--------------------------------\n","Eval num_timesteps=2944000, episode_reward=0.80 +/- 1.17\n","Episode length: 204.00 +/- 40.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 204         |\n","|    mean_reward          | 0.8         |\n","| time/                   |             |\n","|    total_timesteps      | 2944000     |\n","| train/                  |             |\n","|    approx_kl            | 0.005973178 |\n","|    clip_fraction        | 0.187       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.57       |\n","|    explained_variance   | 0.894       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0151     |\n","|    n_updates            | 895         |\n","|    policy_gradient_loss | 0.000569    |\n","|    value_loss           | 0.058       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 327     |\n","|    iterations      | 180     |\n","|    time_elapsed    | 9004    |\n","|    total_timesteps | 2949120 |\n","--------------------------------\n","Eval num_timesteps=2960000, episode_reward=1.40 +/- 1.96\n","Episode length: 228.00 +/- 8.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 228          |\n","|    mean_reward          | 1.4          |\n","| time/                   |              |\n","|    total_timesteps      | 2960000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0060101137 |\n","|    clip_fraction        | 0.181        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.56        |\n","|    explained_variance   | 0.902        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0119      |\n","|    n_updates            | 900          |\n","|    policy_gradient_loss | 0.00138      |\n","|    value_loss           | 0.0561       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 327     |\n","|    iterations      | 181     |\n","|    time_elapsed    | 9056    |\n","|    total_timesteps | 2965504 |\n","--------------------------------\n","Eval num_timesteps=2976000, episode_reward=0.80 +/- 0.75\n","Episode length: 188.00 +/- 48.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 188          |\n","|    mean_reward          | 0.8          |\n","| time/                   |              |\n","|    total_timesteps      | 2976000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0051927953 |\n","|    clip_fraction        | 0.184        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.57        |\n","|    explained_variance   | 0.886        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0109      |\n","|    n_updates            | 905          |\n","|    policy_gradient_loss | 0.000637     |\n","|    value_loss           | 0.052        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 327     |\n","|    iterations      | 182     |\n","|    time_elapsed    | 9108    |\n","|    total_timesteps | 2981888 |\n","--------------------------------\n","Eval num_timesteps=2992000, episode_reward=0.80 +/- 0.98\n","Episode length: 204.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 204         |\n","|    mean_reward          | 0.8         |\n","| time/                   |             |\n","|    total_timesteps      | 2992000     |\n","| train/                  |             |\n","|    approx_kl            | 0.005400088 |\n","|    clip_fraction        | 0.176       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.57       |\n","|    explained_variance   | 0.881       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0158     |\n","|    n_updates            | 910         |\n","|    policy_gradient_loss | 0.000392    |\n","|    value_loss           | 0.0567      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 327     |\n","|    iterations      | 183     |\n","|    time_elapsed    | 9157    |\n","|    total_timesteps | 2998272 |\n","--------------------------------\n","Eval num_timesteps=3008000, episode_reward=0.60 +/- 0.80\n","Episode length: 164.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 164         |\n","|    mean_reward          | 0.6         |\n","| time/                   |             |\n","|    total_timesteps      | 3008000     |\n","| train/                  |             |\n","|    approx_kl            | 0.007788716 |\n","|    clip_fraction        | 0.222       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.57       |\n","|    explained_variance   | 0.9         |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.00929    |\n","|    n_updates            | 915         |\n","|    policy_gradient_loss | 0.00178     |\n","|    value_loss           | 0.0591      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 327     |\n","|    iterations      | 184     |\n","|    time_elapsed    | 9202    |\n","|    total_timesteps | 3014656 |\n","--------------------------------\n","Eval num_timesteps=3024000, episode_reward=0.60 +/- 0.80\n","Episode length: 184.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 184         |\n","|    mean_reward          | 0.6         |\n","| time/                   |             |\n","|    total_timesteps      | 3024000     |\n","| train/                  |             |\n","|    approx_kl            | 0.006756614 |\n","|    clip_fraction        | 0.192       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.56       |\n","|    explained_variance   | 0.863       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0158     |\n","|    n_updates            | 920         |\n","|    policy_gradient_loss | 0.000771    |\n","|    value_loss           | 0.0612      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 327     |\n","|    iterations      | 185     |\n","|    time_elapsed    | 9249    |\n","|    total_timesteps | 3031040 |\n","--------------------------------\n","Eval num_timesteps=3040000, episode_reward=1.00 +/- 1.26\n","Episode length: 176.00 +/- 24.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 176         |\n","|    mean_reward          | 1           |\n","| time/                   |             |\n","|    total_timesteps      | 3040000     |\n","| train/                  |             |\n","|    approx_kl            | 0.005826506 |\n","|    clip_fraction        | 0.183       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.56       |\n","|    explained_variance   | 0.877       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.01       |\n","|    n_updates            | 925         |\n","|    policy_gradient_loss | 0.00116     |\n","|    value_loss           | 0.0557      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 327     |\n","|    iterations      | 186     |\n","|    time_elapsed    | 9298    |\n","|    total_timesteps | 3047424 |\n","--------------------------------\n","Eval num_timesteps=3056000, episode_reward=0.80 +/- 0.98\n","Episode length: 192.00 +/- 16.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 192         |\n","|    mean_reward          | 0.8         |\n","| time/                   |             |\n","|    total_timesteps      | 3056000     |\n","| train/                  |             |\n","|    approx_kl            | 0.005101653 |\n","|    clip_fraction        | 0.181       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.57       |\n","|    explained_variance   | 0.886       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.02       |\n","|    n_updates            | 930         |\n","|    policy_gradient_loss | 0.000136    |\n","|    value_loss           | 0.0638      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 327     |\n","|    iterations      | 187     |\n","|    time_elapsed    | 9348    |\n","|    total_timesteps | 3063808 |\n","--------------------------------\n","Eval num_timesteps=3072000, episode_reward=0.80 +/- 1.60\n","Episode length: 200.00 +/- 8.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 200          |\n","|    mean_reward          | 0.8          |\n","| time/                   |              |\n","|    total_timesteps      | 3072000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0062496364 |\n","|    clip_fraction        | 0.209        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.55        |\n","|    explained_variance   | 0.892        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.00974     |\n","|    n_updates            | 935          |\n","|    policy_gradient_loss | -0.00025     |\n","|    value_loss           | 0.0603       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 327     |\n","|    iterations      | 188     |\n","|    time_elapsed    | 9397    |\n","|    total_timesteps | 3080192 |\n","--------------------------------\n","Eval num_timesteps=3088000, episode_reward=0.40 +/- 0.49\n","Episode length: 180.00 +/- 8.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 180         |\n","|    mean_reward          | 0.4         |\n","| time/                   |             |\n","|    total_timesteps      | 3088000     |\n","| train/                  |             |\n","|    approx_kl            | 0.007008921 |\n","|    clip_fraction        | 0.204       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.54       |\n","|    explained_variance   | 0.884       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.00343    |\n","|    n_updates            | 940         |\n","|    policy_gradient_loss | 0.00158     |\n","|    value_loss           | 0.0651      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 327     |\n","|    iterations      | 189     |\n","|    time_elapsed    | 9445    |\n","|    total_timesteps | 3096576 |\n","--------------------------------\n","Eval num_timesteps=3104000, episode_reward=2.40 +/- 2.94\n","Episode length: 308.00 +/- 72.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 308          |\n","|    mean_reward          | 2.4          |\n","| time/                   |              |\n","|    total_timesteps      | 3104000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0063906303 |\n","|    clip_fraction        | 0.187        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.52        |\n","|    explained_variance   | 0.868        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0098      |\n","|    n_updates            | 945          |\n","|    policy_gradient_loss | 0.00114      |\n","|    value_loss           | 0.0724       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 327     |\n","|    iterations      | 190     |\n","|    time_elapsed    | 9502    |\n","|    total_timesteps | 3112960 |\n","--------------------------------\n","Eval num_timesteps=3120000, episode_reward=0.20 +/- 0.40\n","Episode length: 180.00 +/- 8.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 180          |\n","|    mean_reward          | 0.2          |\n","| time/                   |              |\n","|    total_timesteps      | 3120000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0053472747 |\n","|    clip_fraction        | 0.199        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.54        |\n","|    explained_variance   | 0.889        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0124      |\n","|    n_updates            | 950          |\n","|    policy_gradient_loss | 0.000562     |\n","|    value_loss           | 0.0597       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 327     |\n","|    iterations      | 191     |\n","|    time_elapsed    | 9550    |\n","|    total_timesteps | 3129344 |\n","--------------------------------\n","Eval num_timesteps=3136000, episode_reward=0.60 +/- 0.80\n","Episode length: 180.00 +/- 32.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 180         |\n","|    mean_reward          | 0.6         |\n","| time/                   |             |\n","|    total_timesteps      | 3136000     |\n","| train/                  |             |\n","|    approx_kl            | 0.006020058 |\n","|    clip_fraction        | 0.185       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.54       |\n","|    explained_variance   | 0.91        |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0142     |\n","|    n_updates            | 955         |\n","|    policy_gradient_loss | 0.00161     |\n","|    value_loss           | 0.0546      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 327     |\n","|    iterations      | 192     |\n","|    time_elapsed    | 9601    |\n","|    total_timesteps | 3145728 |\n","--------------------------------\n","Eval num_timesteps=3152000, episode_reward=0.60 +/- 0.80\n","Episode length: 204.00 +/- 40.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 204          |\n","|    mean_reward          | 0.6          |\n","| time/                   |              |\n","|    total_timesteps      | 3152000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0068167206 |\n","|    clip_fraction        | 0.223        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.53        |\n","|    explained_variance   | 0.866        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.00916     |\n","|    n_updates            | 960          |\n","|    policy_gradient_loss | 0.00139      |\n","|    value_loss           | 0.0671       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 327     |\n","|    iterations      | 193     |\n","|    time_elapsed    | 9654    |\n","|    total_timesteps | 3162112 |\n","--------------------------------\n","Eval num_timesteps=3168000, episode_reward=1.80 +/- 2.23\n","Episode length: 228.00 +/- 32.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 228          |\n","|    mean_reward          | 1.8          |\n","| time/                   |              |\n","|    total_timesteps      | 3168000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0070426795 |\n","|    clip_fraction        | 0.199        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.55        |\n","|    explained_variance   | 0.897        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0133      |\n","|    n_updates            | 965          |\n","|    policy_gradient_loss | 0.000482     |\n","|    value_loss           | 0.0546       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 327     |\n","|    iterations      | 194     |\n","|    time_elapsed    | 9706    |\n","|    total_timesteps | 3178496 |\n","--------------------------------\n","Eval num_timesteps=3184000, episode_reward=0.80 +/- 1.17\n","Episode length: 172.00 +/- 16.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 172         |\n","|    mean_reward          | 0.8         |\n","| time/                   |             |\n","|    total_timesteps      | 3184000     |\n","| train/                  |             |\n","|    approx_kl            | 0.006204658 |\n","|    clip_fraction        | 0.193       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.55       |\n","|    explained_variance   | 0.888       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0139     |\n","|    n_updates            | 970         |\n","|    policy_gradient_loss | 0.000108    |\n","|    value_loss           | 0.062       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 327     |\n","|    iterations      | 195     |\n","|    time_elapsed    | 9755    |\n","|    total_timesteps | 3194880 |\n","--------------------------------\n","Eval num_timesteps=3200000, episode_reward=1.00 +/- 0.89\n","Episode length: 240.00 +/- 8.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 240          |\n","|    mean_reward          | 1            |\n","| time/                   |              |\n","|    total_timesteps      | 3200000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0074173533 |\n","|    clip_fraction        | 0.197        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.54        |\n","|    explained_variance   | 0.872        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0162      |\n","|    n_updates            | 975          |\n","|    policy_gradient_loss | 0.00398      |\n","|    value_loss           | 0.0623       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 327     |\n","|    iterations      | 196     |\n","|    time_elapsed    | 9805    |\n","|    total_timesteps | 3211264 |\n","--------------------------------\n","Eval num_timesteps=3216000, episode_reward=0.80 +/- 1.17\n","Episode length: 172.00 +/- 16.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 172         |\n","|    mean_reward          | 0.8         |\n","| time/                   |             |\n","|    total_timesteps      | 3216000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008824289 |\n","|    clip_fraction        | 0.193       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.55       |\n","|    explained_variance   | 0.889       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0124     |\n","|    n_updates            | 980         |\n","|    policy_gradient_loss | 0.000608    |\n","|    value_loss           | 0.0577      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 327     |\n","|    iterations      | 197     |\n","|    time_elapsed    | 9854    |\n","|    total_timesteps | 3227648 |\n","--------------------------------\n","Eval num_timesteps=3232000, episode_reward=2.00 +/- 2.53\n","Episode length: 296.00 +/- 56.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 296         |\n","|    mean_reward          | 2           |\n","| time/                   |             |\n","|    total_timesteps      | 3232000     |\n","| train/                  |             |\n","|    approx_kl            | 0.006556906 |\n","|    clip_fraction        | 0.192       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.56       |\n","|    explained_variance   | 0.899       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0121     |\n","|    n_updates            | 985         |\n","|    policy_gradient_loss | 0.000978    |\n","|    value_loss           | 0.0586      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 327     |\n","|    iterations      | 198     |\n","|    time_elapsed    | 9909    |\n","|    total_timesteps | 3244032 |\n","--------------------------------\n","Eval num_timesteps=3248000, episode_reward=0.60 +/- 0.80\n","Episode length: 204.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 204          |\n","|    mean_reward          | 0.6          |\n","| time/                   |              |\n","|    total_timesteps      | 3248000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0054396726 |\n","|    clip_fraction        | 0.169        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.57        |\n","|    explained_variance   | 0.883        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0148      |\n","|    n_updates            | 990          |\n","|    policy_gradient_loss | 0.0001       |\n","|    value_loss           | 0.0602       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 327     |\n","|    iterations      | 199     |\n","|    time_elapsed    | 9958    |\n","|    total_timesteps | 3260416 |\n","--------------------------------\n","Eval num_timesteps=3264000, episode_reward=0.60 +/- 0.80\n","Episode length: 176.00 +/- 24.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 176         |\n","|    mean_reward          | 0.6         |\n","| time/                   |             |\n","|    total_timesteps      | 3264000     |\n","| train/                  |             |\n","|    approx_kl            | 0.005389544 |\n","|    clip_fraction        | 0.182       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.57       |\n","|    explained_variance   | 0.877       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0198     |\n","|    n_updates            | 995         |\n","|    policy_gradient_loss | 0.000415    |\n","|    value_loss           | 0.064       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 327     |\n","|    iterations      | 200     |\n","|    time_elapsed    | 10007   |\n","|    total_timesteps | 3276800 |\n","--------------------------------\n","Eval num_timesteps=3280000, episode_reward=0.20 +/- 0.40\n","Episode length: 184.00 +/- 40.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 184          |\n","|    mean_reward          | 0.2          |\n","| time/                   |              |\n","|    total_timesteps      | 3280000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0051594507 |\n","|    clip_fraction        | 0.185        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.56        |\n","|    explained_variance   | 0.856        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0211      |\n","|    n_updates            | 1000         |\n","|    policy_gradient_loss | -0.00028     |\n","|    value_loss           | 0.069        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 327     |\n","|    iterations      | 201     |\n","|    time_elapsed    | 10059   |\n","|    total_timesteps | 3293184 |\n","--------------------------------\n","Eval num_timesteps=3296000, episode_reward=0.60 +/- 1.20\n","Episode length: 164.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 164         |\n","|    mean_reward          | 0.6         |\n","| time/                   |             |\n","|    total_timesteps      | 3296000     |\n","| train/                  |             |\n","|    approx_kl            | 0.005870104 |\n","|    clip_fraction        | 0.182       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.56       |\n","|    explained_variance   | 0.854       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0146     |\n","|    n_updates            | 1005        |\n","|    policy_gradient_loss | -2.05e-05   |\n","|    value_loss           | 0.0667      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 327     |\n","|    iterations      | 202     |\n","|    time_elapsed    | 10105   |\n","|    total_timesteps | 3309568 |\n","--------------------------------\n","Eval num_timesteps=3312000, episode_reward=0.80 +/- 0.98\n","Episode length: 216.00 +/- 64.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 216          |\n","|    mean_reward          | 0.8          |\n","| time/                   |              |\n","|    total_timesteps      | 3312000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0060377996 |\n","|    clip_fraction        | 0.163        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.56        |\n","|    explained_variance   | 0.867        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.02        |\n","|    n_updates            | 1010         |\n","|    policy_gradient_loss | -0.00122     |\n","|    value_loss           | 0.0604       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 327     |\n","|    iterations      | 203     |\n","|    time_elapsed    | 10161   |\n","|    total_timesteps | 3325952 |\n","--------------------------------\n","Eval num_timesteps=3328000, episode_reward=1.40 +/- 1.96\n","Episode length: 236.00 +/- 16.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 236          |\n","|    mean_reward          | 1.4          |\n","| time/                   |              |\n","|    total_timesteps      | 3328000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0055880304 |\n","|    clip_fraction        | 0.172        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.57        |\n","|    explained_variance   | 0.894        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0143      |\n","|    n_updates            | 1015         |\n","|    policy_gradient_loss | 0.000749     |\n","|    value_loss           | 0.0591       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 327     |\n","|    iterations      | 204     |\n","|    time_elapsed    | 10212   |\n","|    total_timesteps | 3342336 |\n","--------------------------------\n","Eval num_timesteps=3344000, episode_reward=0.80 +/- 0.98\n","Episode length: 192.00 +/- 16.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 192          |\n","|    mean_reward          | 0.8          |\n","| time/                   |              |\n","|    total_timesteps      | 3344000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0062842807 |\n","|    clip_fraction        | 0.189        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.56        |\n","|    explained_variance   | 0.898        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.02        |\n","|    n_updates            | 1020         |\n","|    policy_gradient_loss | 0.000739     |\n","|    value_loss           | 0.059        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 327     |\n","|    iterations      | 205     |\n","|    time_elapsed    | 10264   |\n","|    total_timesteps | 3358720 |\n","--------------------------------\n","Eval num_timesteps=3360000, episode_reward=1.20 +/- 1.60\n","Episode length: 180.00 +/- 8.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 180         |\n","|    mean_reward          | 1.2         |\n","| time/                   |             |\n","|    total_timesteps      | 3360000     |\n","| train/                  |             |\n","|    approx_kl            | 0.006535613 |\n","|    clip_fraction        | 0.217       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.56       |\n","|    explained_variance   | 0.891       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.015      |\n","|    n_updates            | 1025        |\n","|    policy_gradient_loss | 0.00254     |\n","|    value_loss           | 0.0581      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 327     |\n","|    iterations      | 206     |\n","|    time_elapsed    | 10314   |\n","|    total_timesteps | 3375104 |\n","--------------------------------\n","Eval num_timesteps=3376000, episode_reward=0.80 +/- 0.98\n","Episode length: 164.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 164          |\n","|    mean_reward          | 0.8          |\n","| time/                   |              |\n","|    total_timesteps      | 3376000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0069032544 |\n","|    clip_fraction        | 0.22         |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.56        |\n","|    explained_variance   | 0.867        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0181      |\n","|    n_updates            | 1030         |\n","|    policy_gradient_loss | 0.00128      |\n","|    value_loss           | 0.0703       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 327     |\n","|    iterations      | 207     |\n","|    time_elapsed    | 10362   |\n","|    total_timesteps | 3391488 |\n","--------------------------------\n","Eval num_timesteps=3392000, episode_reward=0.40 +/- 0.80\n","Episode length: 172.00 +/- 16.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 172         |\n","|    mean_reward          | 0.4         |\n","| time/                   |             |\n","|    total_timesteps      | 3392000     |\n","| train/                  |             |\n","|    approx_kl            | 0.007933224 |\n","|    clip_fraction        | 0.22        |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.55       |\n","|    explained_variance   | 0.854       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 0.000158    |\n","|    n_updates            | 1035        |\n","|    policy_gradient_loss | 0.00198     |\n","|    value_loss           | 0.0691      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 327     |\n","|    iterations      | 208     |\n","|    time_elapsed    | 10413   |\n","|    total_timesteps | 3407872 |\n","--------------------------------\n","Eval num_timesteps=3408000, episode_reward=1.00 +/- 1.26\n","Episode length: 168.00 +/- 8.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 168         |\n","|    mean_reward          | 1           |\n","| time/                   |             |\n","|    total_timesteps      | 3408000     |\n","| train/                  |             |\n","|    approx_kl            | 0.005657175 |\n","|    clip_fraction        | 0.179       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.56       |\n","|    explained_variance   | 0.834       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.00905    |\n","|    n_updates            | 1040        |\n","|    policy_gradient_loss | -0.000142   |\n","|    value_loss           | 0.0754      |\n","-----------------------------------------\n","Eval num_timesteps=3424000, episode_reward=1.20 +/- 1.60\n","Episode length: 180.00 +/- 32.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 180      |\n","|    mean_reward     | 1.2      |\n","| time/              |          |\n","|    total_timesteps | 3424000  |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 326     |\n","|    iterations      | 209     |\n","|    time_elapsed    | 10477   |\n","|    total_timesteps | 3424256 |\n","--------------------------------\n","Eval num_timesteps=3440000, episode_reward=1.40 +/- 2.80\n","Episode length: 228.00 +/- 32.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 228         |\n","|    mean_reward          | 1.4         |\n","| time/                   |             |\n","|    total_timesteps      | 3440000     |\n","| train/                  |             |\n","|    approx_kl            | 0.005587353 |\n","|    clip_fraction        | 0.185       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.56       |\n","|    explained_variance   | 0.887       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0155     |\n","|    n_updates            | 1045        |\n","|    policy_gradient_loss | 0.000163    |\n","|    value_loss           | 0.0615      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 326     |\n","|    iterations      | 210     |\n","|    time_elapsed    | 10529   |\n","|    total_timesteps | 3440640 |\n","--------------------------------\n","Eval num_timesteps=3456000, episode_reward=1.40 +/- 1.74\n","Episode length: 196.00 +/- 24.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 196          |\n","|    mean_reward          | 1.4          |\n","| time/                   |              |\n","|    total_timesteps      | 3456000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0061912313 |\n","|    clip_fraction        | 0.17         |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.54        |\n","|    explained_variance   | 0.858        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0222      |\n","|    n_updates            | 1050         |\n","|    policy_gradient_loss | 0.00016      |\n","|    value_loss           | 0.0705       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 326     |\n","|    iterations      | 211     |\n","|    time_elapsed    | 10581   |\n","|    total_timesteps | 3457024 |\n","--------------------------------\n","Eval num_timesteps=3472000, episode_reward=1.20 +/- 1.60\n","Episode length: 244.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 244          |\n","|    mean_reward          | 1.2          |\n","| time/                   |              |\n","|    total_timesteps      | 3472000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0061764424 |\n","|    clip_fraction        | 0.178        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.54        |\n","|    explained_variance   | 0.862        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0127      |\n","|    n_updates            | 1055         |\n","|    policy_gradient_loss | 0.000227     |\n","|    value_loss           | 0.066        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 326     |\n","|    iterations      | 212     |\n","|    time_elapsed    | 10634   |\n","|    total_timesteps | 3473408 |\n","--------------------------------\n","Eval num_timesteps=3488000, episode_reward=0.60 +/- 1.20\n","Episode length: 168.00 +/- 8.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 168          |\n","|    mean_reward          | 0.6          |\n","| time/                   |              |\n","|    total_timesteps      | 3488000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0061195106 |\n","|    clip_fraction        | 0.191        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.54        |\n","|    explained_variance   | 0.867        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0129      |\n","|    n_updates            | 1060         |\n","|    policy_gradient_loss | 0.00131      |\n","|    value_loss           | 0.0568       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 326     |\n","|    iterations      | 213     |\n","|    time_elapsed    | 10683   |\n","|    total_timesteps | 3489792 |\n","--------------------------------\n","Eval num_timesteps=3504000, episode_reward=2.60 +/- 2.06\n","Episode length: 312.00 +/- 16.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 312          |\n","|    mean_reward          | 2.6          |\n","| time/                   |              |\n","|    total_timesteps      | 3504000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0059762774 |\n","|    clip_fraction        | 0.194        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.54        |\n","|    explained_variance   | 0.867        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0129      |\n","|    n_updates            | 1065         |\n","|    policy_gradient_loss | -0.00019     |\n","|    value_loss           | 0.0682       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 326     |\n","|    iterations      | 214     |\n","|    time_elapsed    | 10742   |\n","|    total_timesteps | 3506176 |\n","--------------------------------\n","Eval num_timesteps=3520000, episode_reward=1.20 +/- 1.47\n","Episode length: 200.00 +/- 8.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 200          |\n","|    mean_reward          | 1.2          |\n","| time/                   |              |\n","|    total_timesteps      | 3520000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0061857766 |\n","|    clip_fraction        | 0.17         |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.55        |\n","|    explained_variance   | 0.863        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.014       |\n","|    n_updates            | 1070         |\n","|    policy_gradient_loss | 0.000801     |\n","|    value_loss           | 0.0692       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 326     |\n","|    iterations      | 215     |\n","|    time_elapsed    | 10791   |\n","|    total_timesteps | 3522560 |\n","--------------------------------\n","Eval num_timesteps=3536000, episode_reward=0.60 +/- 0.80\n","Episode length: 168.00 +/- 8.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 168          |\n","|    mean_reward          | 0.6          |\n","| time/                   |              |\n","|    total_timesteps      | 3536000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0052542617 |\n","|    clip_fraction        | 0.186        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.56        |\n","|    explained_variance   | 0.871        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0145      |\n","|    n_updates            | 1075         |\n","|    policy_gradient_loss | -0.00047     |\n","|    value_loss           | 0.0604       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 326     |\n","|    iterations      | 216     |\n","|    time_elapsed    | 10840   |\n","|    total_timesteps | 3538944 |\n","--------------------------------\n","Eval num_timesteps=3552000, episode_reward=1.20 +/- 1.60\n","Episode length: 236.00 +/- 24.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 236         |\n","|    mean_reward          | 1.2         |\n","| time/                   |             |\n","|    total_timesteps      | 3552000     |\n","| train/                  |             |\n","|    approx_kl            | 0.005832524 |\n","|    clip_fraction        | 0.199       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.56       |\n","|    explained_variance   | 0.889       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0154     |\n","|    n_updates            | 1080        |\n","|    policy_gradient_loss | 0.00183     |\n","|    value_loss           | 0.061       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 326     |\n","|    iterations      | 217     |\n","|    time_elapsed    | 10893   |\n","|    total_timesteps | 3555328 |\n","--------------------------------\n","Eval num_timesteps=3568000, episode_reward=0.60 +/- 1.20\n","Episode length: 196.00 +/- 24.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 196         |\n","|    mean_reward          | 0.6         |\n","| time/                   |             |\n","|    total_timesteps      | 3568000     |\n","| train/                  |             |\n","|    approx_kl            | 0.005844755 |\n","|    clip_fraction        | 0.174       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.56       |\n","|    explained_variance   | 0.865       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0116     |\n","|    n_updates            | 1085        |\n","|    policy_gradient_loss | 0.000455    |\n","|    value_loss           | 0.0734      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 326     |\n","|    iterations      | 218     |\n","|    time_elapsed    | 10945   |\n","|    total_timesteps | 3571712 |\n","--------------------------------\n","Eval num_timesteps=3584000, episode_reward=3.00 +/- 3.35\n","Episode length: 316.00 +/- 56.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 316         |\n","|    mean_reward          | 3           |\n","| time/                   |             |\n","|    total_timesteps      | 3584000     |\n","| train/                  |             |\n","|    approx_kl            | 0.007296678 |\n","|    clip_fraction        | 0.192       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.54       |\n","|    explained_variance   | 0.883       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0232     |\n","|    n_updates            | 1090        |\n","|    policy_gradient_loss | -0.000407   |\n","|    value_loss           | 0.0631      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 326     |\n","|    iterations      | 219     |\n","|    time_elapsed    | 11001   |\n","|    total_timesteps | 3588096 |\n","--------------------------------\n","Eval num_timesteps=3600000, episode_reward=2.20 +/- 2.64\n","Episode length: 292.00 +/- 16.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 292          |\n","|    mean_reward          | 2.2          |\n","| time/                   |              |\n","|    total_timesteps      | 3600000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0063472595 |\n","|    clip_fraction        | 0.195        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.56        |\n","|    explained_variance   | 0.876        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0109      |\n","|    n_updates            | 1095         |\n","|    policy_gradient_loss | 0.00136      |\n","|    value_loss           | 0.0588       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 220     |\n","|    time_elapsed    | 11059   |\n","|    total_timesteps | 3604480 |\n","--------------------------------\n","Eval num_timesteps=3616000, episode_reward=1.00 +/- 1.55\n","Episode length: 164.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 164          |\n","|    mean_reward          | 1            |\n","| time/                   |              |\n","|    total_timesteps      | 3616000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0049973233 |\n","|    clip_fraction        | 0.171        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.55        |\n","|    explained_variance   | 0.877        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0154      |\n","|    n_updates            | 1100         |\n","|    policy_gradient_loss | 4.12e-05     |\n","|    value_loss           | 0.065        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 221     |\n","|    time_elapsed    | 11106   |\n","|    total_timesteps | 3620864 |\n","--------------------------------\n","Eval num_timesteps=3632000, episode_reward=0.80 +/- 1.17\n","Episode length: 208.00 +/- 8.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 208          |\n","|    mean_reward          | 0.8          |\n","| time/                   |              |\n","|    total_timesteps      | 3632000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0050811153 |\n","|    clip_fraction        | 0.185        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.57        |\n","|    explained_variance   | 0.887        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0209      |\n","|    n_updates            | 1105         |\n","|    policy_gradient_loss | 0.000377     |\n","|    value_loss           | 0.063        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 222     |\n","|    time_elapsed    | 11157   |\n","|    total_timesteps | 3637248 |\n","--------------------------------\n","Eval num_timesteps=3648000, episode_reward=1.20 +/- 1.94\n","Episode length: 248.00 +/- 48.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 248         |\n","|    mean_reward          | 1.2         |\n","| time/                   |             |\n","|    total_timesteps      | 3648000     |\n","| train/                  |             |\n","|    approx_kl            | 0.004939433 |\n","|    clip_fraction        | 0.177       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.55       |\n","|    explained_variance   | 0.892       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0105     |\n","|    n_updates            | 1110        |\n","|    policy_gradient_loss | -0.000695   |\n","|    value_loss           | 0.06        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 223     |\n","|    time_elapsed    | 11216   |\n","|    total_timesteps | 3653632 |\n","--------------------------------\n","Eval num_timesteps=3664000, episode_reward=1.00 +/- 1.26\n","Episode length: 236.00 +/- 16.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 236         |\n","|    mean_reward          | 1           |\n","| time/                   |             |\n","|    total_timesteps      | 3664000     |\n","| train/                  |             |\n","|    approx_kl            | 0.005236939 |\n","|    clip_fraction        | 0.156       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.56       |\n","|    explained_variance   | 0.872       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0134     |\n","|    n_updates            | 1115        |\n","|    policy_gradient_loss | 0.00107     |\n","|    value_loss           | 0.0698      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 224     |\n","|    time_elapsed    | 11268   |\n","|    total_timesteps | 3670016 |\n","--------------------------------\n","Eval num_timesteps=3680000, episode_reward=1.00 +/- 1.26\n","Episode length: 212.00 +/- 16.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 212         |\n","|    mean_reward          | 1           |\n","| time/                   |             |\n","|    total_timesteps      | 3680000     |\n","| train/                  |             |\n","|    approx_kl            | 0.004728282 |\n","|    clip_fraction        | 0.154       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.55       |\n","|    explained_variance   | 0.893       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0133     |\n","|    n_updates            | 1120        |\n","|    policy_gradient_loss | -0.000152   |\n","|    value_loss           | 0.063       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 225     |\n","|    time_elapsed    | 11320   |\n","|    total_timesteps | 3686400 |\n","--------------------------------\n","Eval num_timesteps=3696000, episode_reward=0.20 +/- 0.40\n","Episode length: 168.00 +/- 8.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 168         |\n","|    mean_reward          | 0.2         |\n","| time/                   |             |\n","|    total_timesteps      | 3696000     |\n","| train/                  |             |\n","|    approx_kl            | 0.005138025 |\n","|    clip_fraction        | 0.169       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.54       |\n","|    explained_variance   | 0.904       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0187     |\n","|    n_updates            | 1125        |\n","|    policy_gradient_loss | 0.00132     |\n","|    value_loss           | 0.0614      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 226     |\n","|    time_elapsed    | 11367   |\n","|    total_timesteps | 3702784 |\n","--------------------------------\n","Eval num_timesteps=3712000, episode_reward=1.00 +/- 1.26\n","Episode length: 184.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 184         |\n","|    mean_reward          | 1           |\n","| time/                   |             |\n","|    total_timesteps      | 3712000     |\n","| train/                  |             |\n","|    approx_kl            | 0.005146185 |\n","|    clip_fraction        | 0.166       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.54       |\n","|    explained_variance   | 0.888       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0177     |\n","|    n_updates            | 1130        |\n","|    policy_gradient_loss | -0.000468   |\n","|    value_loss           | 0.0629      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 227     |\n","|    time_elapsed    | 11415   |\n","|    total_timesteps | 3719168 |\n","--------------------------------\n","Eval num_timesteps=3728000, episode_reward=0.80 +/- 0.75\n","Episode length: 208.00 +/- 8.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 208          |\n","|    mean_reward          | 0.8          |\n","| time/                   |              |\n","|    total_timesteps      | 3728000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0057297247 |\n","|    clip_fraction        | 0.19         |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.54        |\n","|    explained_variance   | 0.887        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0179      |\n","|    n_updates            | 1135         |\n","|    policy_gradient_loss | -3.75e-05    |\n","|    value_loss           | 0.0515       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 228     |\n","|    time_elapsed    | 11465   |\n","|    total_timesteps | 3735552 |\n","--------------------------------\n","Eval num_timesteps=3744000, episode_reward=1.80 +/- 1.47\n","Episode length: 304.00 +/- 40.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 304         |\n","|    mean_reward          | 1.8         |\n","| time/                   |             |\n","|    total_timesteps      | 3744000     |\n","| train/                  |             |\n","|    approx_kl            | 0.007028949 |\n","|    clip_fraction        | 0.183       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.52       |\n","|    explained_variance   | 0.877       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0172     |\n","|    n_updates            | 1140        |\n","|    policy_gradient_loss | 0.000731    |\n","|    value_loss           | 0.0601      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 229     |\n","|    time_elapsed    | 11524   |\n","|    total_timesteps | 3751936 |\n","--------------------------------\n","Eval num_timesteps=3760000, episode_reward=0.40 +/- 0.49\n","Episode length: 180.00 +/- 8.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 180         |\n","|    mean_reward          | 0.4         |\n","| time/                   |             |\n","|    total_timesteps      | 3760000     |\n","| train/                  |             |\n","|    approx_kl            | 0.005141609 |\n","|    clip_fraction        | 0.179       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.53       |\n","|    explained_variance   | 0.87        |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0125     |\n","|    n_updates            | 1145        |\n","|    policy_gradient_loss | -0.000828   |\n","|    value_loss           | 0.0631      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 230     |\n","|    time_elapsed    | 11570   |\n","|    total_timesteps | 3768320 |\n","--------------------------------\n","Eval num_timesteps=3776000, episode_reward=2.80 +/- 3.49\n","Episode length: 356.00 +/- 16.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 356          |\n","|    mean_reward          | 2.8          |\n","| time/                   |              |\n","|    total_timesteps      | 3776000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0068015708 |\n","|    clip_fraction        | 0.188        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.53        |\n","|    explained_variance   | 0.847        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0108      |\n","|    n_updates            | 1150         |\n","|    policy_gradient_loss | 0.000477     |\n","|    value_loss           | 0.0655       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 231     |\n","|    time_elapsed    | 11630   |\n","|    total_timesteps | 3784704 |\n","--------------------------------\n","Eval num_timesteps=3792000, episode_reward=1.20 +/- 1.60\n","Episode length: 196.00 +/- 16.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 196          |\n","|    mean_reward          | 1.2          |\n","| time/                   |              |\n","|    total_timesteps      | 3792000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0060177213 |\n","|    clip_fraction        | 0.166        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.52        |\n","|    explained_variance   | 0.866        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0144      |\n","|    n_updates            | 1155         |\n","|    policy_gradient_loss | -9.6e-05     |\n","|    value_loss           | 0.0625       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 232     |\n","|    time_elapsed    | 11680   |\n","|    total_timesteps | 3801088 |\n","--------------------------------\n","Eval num_timesteps=3808000, episode_reward=1.00 +/- 2.00\n","Episode length: 212.00 +/- 24.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 212         |\n","|    mean_reward          | 1           |\n","| time/                   |             |\n","|    total_timesteps      | 3808000     |\n","| train/                  |             |\n","|    approx_kl            | 0.005306148 |\n","|    clip_fraction        | 0.174       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.52       |\n","|    explained_variance   | 0.895       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0153     |\n","|    n_updates            | 1160        |\n","|    policy_gradient_loss | -1.73e-05   |\n","|    value_loss           | 0.0564      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 233     |\n","|    time_elapsed    | 11730   |\n","|    total_timesteps | 3817472 |\n","--------------------------------\n","Eval num_timesteps=3824000, episode_reward=1.00 +/- 1.26\n","Episode length: 204.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 204        |\n","|    mean_reward          | 1          |\n","| time/                   |            |\n","|    total_timesteps      | 3824000    |\n","| train/                  |            |\n","|    approx_kl            | 0.00557533 |\n","|    clip_fraction        | 0.16       |\n","|    clip_range           | 0.1        |\n","|    entropy_loss         | -1.52      |\n","|    explained_variance   | 0.862      |\n","|    learning_rate        | 0.000622   |\n","|    loss                 | -0.0134    |\n","|    n_updates            | 1165       |\n","|    policy_gradient_loss | -3.58e-05  |\n","|    value_loss           | 0.0623     |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 234     |\n","|    time_elapsed    | 11778   |\n","|    total_timesteps | 3833856 |\n","--------------------------------\n","Eval num_timesteps=3840000, episode_reward=0.80 +/- 1.60\n","Episode length: 196.00 +/- 16.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 196          |\n","|    mean_reward          | 0.8          |\n","| time/                   |              |\n","|    total_timesteps      | 3840000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0056467513 |\n","|    clip_fraction        | 0.177        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.53        |\n","|    explained_variance   | 0.888        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0117      |\n","|    n_updates            | 1170         |\n","|    policy_gradient_loss | 0.000656     |\n","|    value_loss           | 0.0561       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 235     |\n","|    time_elapsed    | 11827   |\n","|    total_timesteps | 3850240 |\n","--------------------------------\n","Eval num_timesteps=3856000, episode_reward=0.40 +/- 0.80\n","Episode length: 168.00 +/- 8.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 168          |\n","|    mean_reward          | 0.4          |\n","| time/                   |              |\n","|    total_timesteps      | 3856000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0057358085 |\n","|    clip_fraction        | 0.183        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.52        |\n","|    explained_variance   | 0.881        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0201      |\n","|    n_updates            | 1175         |\n","|    policy_gradient_loss | -0.000394    |\n","|    value_loss           | 0.0623       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 236     |\n","|    time_elapsed    | 11877   |\n","|    total_timesteps | 3866624 |\n","--------------------------------\n","Eval num_timesteps=3872000, episode_reward=0.40 +/- 0.49\n","Episode length: 164.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 164          |\n","|    mean_reward          | 0.4          |\n","| time/                   |              |\n","|    total_timesteps      | 3872000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0064654993 |\n","|    clip_fraction        | 0.199        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.53        |\n","|    explained_variance   | 0.869        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0168      |\n","|    n_updates            | 1180         |\n","|    policy_gradient_loss | 0.00106      |\n","|    value_loss           | 0.0641       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 237     |\n","|    time_elapsed    | 11924   |\n","|    total_timesteps | 3883008 |\n","--------------------------------\n","Eval num_timesteps=3888000, episode_reward=0.60 +/- 0.80\n","Episode length: 196.00 +/- 16.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 196          |\n","|    mean_reward          | 0.6          |\n","| time/                   |              |\n","|    total_timesteps      | 3888000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0063410043 |\n","|    clip_fraction        | 0.176        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.54        |\n","|    explained_variance   | 0.891        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.00964     |\n","|    n_updates            | 1185         |\n","|    policy_gradient_loss | 0.00153      |\n","|    value_loss           | 0.0555       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 238     |\n","|    time_elapsed    | 11974   |\n","|    total_timesteps | 3899392 |\n","--------------------------------\n","Eval num_timesteps=3904000, episode_reward=0.60 +/- 0.80\n","Episode length: 196.00 +/- 16.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 196          |\n","|    mean_reward          | 0.6          |\n","| time/                   |              |\n","|    total_timesteps      | 3904000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0052690827 |\n","|    clip_fraction        | 0.164        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.53        |\n","|    explained_variance   | 0.872        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.016       |\n","|    n_updates            | 1190         |\n","|    policy_gradient_loss | 0.000794     |\n","|    value_loss           | 0.0632       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 239     |\n","|    time_elapsed    | 12023   |\n","|    total_timesteps | 3915776 |\n","--------------------------------\n","Eval num_timesteps=3920000, episode_reward=1.00 +/- 0.89\n","Episode length: 172.00 +/- 16.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 172          |\n","|    mean_reward          | 1            |\n","| time/                   |              |\n","|    total_timesteps      | 3920000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0048229233 |\n","|    clip_fraction        | 0.15         |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.53        |\n","|    explained_variance   | 0.865        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0107      |\n","|    n_updates            | 1195         |\n","|    policy_gradient_loss | 0.000141     |\n","|    value_loss           | 0.073        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 240     |\n","|    time_elapsed    | 12072   |\n","|    total_timesteps | 3932160 |\n","--------------------------------\n","Eval num_timesteps=3936000, episode_reward=0.40 +/- 0.49\n","Episode length: 164.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 164          |\n","|    mean_reward          | 0.4          |\n","| time/                   |              |\n","|    total_timesteps      | 3936000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0045928587 |\n","|    clip_fraction        | 0.138        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.53        |\n","|    explained_variance   | 0.895        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0117      |\n","|    n_updates            | 1200         |\n","|    policy_gradient_loss | -8.39e-05    |\n","|    value_loss           | 0.0628       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 241     |\n","|    time_elapsed    | 12119   |\n","|    total_timesteps | 3948544 |\n","--------------------------------\n","Eval num_timesteps=3952000, episode_reward=1.20 +/- 1.60\n","Episode length: 180.00 +/- 8.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 180         |\n","|    mean_reward          | 1.2         |\n","| time/                   |             |\n","|    total_timesteps      | 3952000     |\n","| train/                  |             |\n","|    approx_kl            | 0.004994395 |\n","|    clip_fraction        | 0.158       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.54       |\n","|    explained_variance   | 0.898       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0148     |\n","|    n_updates            | 1205        |\n","|    policy_gradient_loss | 0.000186    |\n","|    value_loss           | 0.0555      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 242     |\n","|    time_elapsed    | 12168   |\n","|    total_timesteps | 3964928 |\n","--------------------------------\n","Eval num_timesteps=3968000, episode_reward=1.00 +/- 1.55\n","Episode length: 196.00 +/- 16.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 196          |\n","|    mean_reward          | 1            |\n","| time/                   |              |\n","|    total_timesteps      | 3968000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0075170305 |\n","|    clip_fraction        | 0.154        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.54        |\n","|    explained_variance   | 0.901        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0102      |\n","|    n_updates            | 1210         |\n","|    policy_gradient_loss | -0.000396    |\n","|    value_loss           | 0.0588       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 243     |\n","|    time_elapsed    | 12218   |\n","|    total_timesteps | 3981312 |\n","--------------------------------\n","Eval num_timesteps=3984000, episode_reward=1.40 +/- 2.33\n","Episode length: 232.00 +/- 24.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 232         |\n","|    mean_reward          | 1.4         |\n","| time/                   |             |\n","|    total_timesteps      | 3984000     |\n","| train/                  |             |\n","|    approx_kl            | 0.005962302 |\n","|    clip_fraction        | 0.193       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.52       |\n","|    explained_variance   | 0.9         |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0164     |\n","|    n_updates            | 1215        |\n","|    policy_gradient_loss | 0.00139     |\n","|    value_loss           | 0.0617      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 244     |\n","|    time_elapsed    | 12269   |\n","|    total_timesteps | 3997696 |\n","--------------------------------\n","Eval num_timesteps=4000000, episode_reward=0.80 +/- 1.60\n","Episode length: 172.00 +/- 16.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 172          |\n","|    mean_reward          | 0.8          |\n","| time/                   |              |\n","|    total_timesteps      | 4000000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0053741587 |\n","|    clip_fraction        | 0.175        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.52        |\n","|    explained_variance   | 0.885        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0179      |\n","|    n_updates            | 1220         |\n","|    policy_gradient_loss | -0.00035     |\n","|    value_loss           | 0.0668       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 245     |\n","|    time_elapsed    | 12317   |\n","|    total_timesteps | 4014080 |\n","--------------------------------\n","Eval num_timesteps=4016000, episode_reward=0.60 +/- 0.80\n","Episode length: 168.00 +/- 8.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 168          |\n","|    mean_reward          | 0.6          |\n","| time/                   |              |\n","|    total_timesteps      | 4016000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0067019905 |\n","|    clip_fraction        | 0.175        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.54        |\n","|    explained_variance   | 0.903        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.015       |\n","|    n_updates            | 1225         |\n","|    policy_gradient_loss | 0.00179      |\n","|    value_loss           | 0.0583       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 246     |\n","|    time_elapsed    | 12364   |\n","|    total_timesteps | 4030464 |\n","--------------------------------\n","Eval num_timesteps=4032000, episode_reward=0.80 +/- 0.98\n","Episode length: 180.00 +/- 32.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 180          |\n","|    mean_reward          | 0.8          |\n","| time/                   |              |\n","|    total_timesteps      | 4032000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0053865397 |\n","|    clip_fraction        | 0.148        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.53        |\n","|    explained_variance   | 0.907        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0166      |\n","|    n_updates            | 1230         |\n","|    policy_gradient_loss | 0.00119      |\n","|    value_loss           | 0.056        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 247     |\n","|    time_elapsed    | 12416   |\n","|    total_timesteps | 4046848 |\n","--------------------------------\n","Eval num_timesteps=4048000, episode_reward=2.00 +/- 2.45\n","Episode length: 228.00 +/- 32.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 228          |\n","|    mean_reward          | 2            |\n","| time/                   |              |\n","|    total_timesteps      | 4048000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0049659153 |\n","|    clip_fraction        | 0.149        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.52        |\n","|    explained_variance   | 0.902        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0153      |\n","|    n_updates            | 1235         |\n","|    policy_gradient_loss | 0.00106      |\n","|    value_loss           | 0.0652       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 248     |\n","|    time_elapsed    | 12467   |\n","|    total_timesteps | 4063232 |\n","--------------------------------\n","Eval num_timesteps=4064000, episode_reward=1.60 +/- 2.24\n","Episode length: 260.00 +/- 8.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 260         |\n","|    mean_reward          | 1.6         |\n","| time/                   |             |\n","|    total_timesteps      | 4064000     |\n","| train/                  |             |\n","|    approx_kl            | 0.006222953 |\n","|    clip_fraction        | 0.168       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.53       |\n","|    explained_variance   | 0.897       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0155     |\n","|    n_updates            | 1240        |\n","|    policy_gradient_loss | 0.00346     |\n","|    value_loss           | 0.0649      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 249     |\n","|    time_elapsed    | 12519   |\n","|    total_timesteps | 4079616 |\n","--------------------------------\n","Eval num_timesteps=4080000, episode_reward=0.20 +/- 0.40\n","Episode length: 176.00 +/- 24.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 176         |\n","|    mean_reward          | 0.2         |\n","| time/                   |             |\n","|    total_timesteps      | 4080000     |\n","| train/                  |             |\n","|    approx_kl            | 0.006694241 |\n","|    clip_fraction        | 0.174       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.52       |\n","|    explained_variance   | 0.91        |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0106     |\n","|    n_updates            | 1245        |\n","|    policy_gradient_loss | 0.00254     |\n","|    value_loss           | 0.0618      |\n","-----------------------------------------\n","Eval num_timesteps=4096000, episode_reward=0.40 +/- 0.80\n","Episode length: 168.00 +/- 8.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 168      |\n","|    mean_reward     | 0.4      |\n","| time/              |          |\n","|    total_timesteps | 4096000  |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 250     |\n","|    time_elapsed    | 12579   |\n","|    total_timesteps | 4096000 |\n","--------------------------------\n","Eval num_timesteps=4112000, episode_reward=1.00 +/- 1.26\n","Episode length: 188.00 +/- 8.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 188          |\n","|    mean_reward          | 1            |\n","| time/                   |              |\n","|    total_timesteps      | 4112000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0056810887 |\n","|    clip_fraction        | 0.172        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.53        |\n","|    explained_variance   | 0.918        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.011       |\n","|    n_updates            | 1250         |\n","|    policy_gradient_loss | 0.00154      |\n","|    value_loss           | 0.0589       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 251     |\n","|    time_elapsed    | 12628   |\n","|    total_timesteps | 4112384 |\n","--------------------------------\n","Eval num_timesteps=4128000, episode_reward=0.80 +/- 1.17\n","Episode length: 164.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 164          |\n","|    mean_reward          | 0.8          |\n","| time/                   |              |\n","|    total_timesteps      | 4128000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0057381596 |\n","|    clip_fraction        | 0.151        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.52        |\n","|    explained_variance   | 0.888        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0161      |\n","|    n_updates            | 1255         |\n","|    policy_gradient_loss | 0.000223     |\n","|    value_loss           | 0.0696       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 252     |\n","|    time_elapsed    | 12674   |\n","|    total_timesteps | 4128768 |\n","--------------------------------\n","Eval num_timesteps=4144000, episode_reward=1.40 +/- 1.74\n","Episode length: 200.00 +/- 32.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 200          |\n","|    mean_reward          | 1.4          |\n","| time/                   |              |\n","|    total_timesteps      | 4144000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0072920825 |\n","|    clip_fraction        | 0.193        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.53        |\n","|    explained_variance   | 0.905        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0106      |\n","|    n_updates            | 1260         |\n","|    policy_gradient_loss | 0.00315      |\n","|    value_loss           | 0.0538       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 253     |\n","|    time_elapsed    | 12726   |\n","|    total_timesteps | 4145152 |\n","--------------------------------\n","Eval num_timesteps=4160000, episode_reward=3.20 +/- 3.54\n","Episode length: 396.00 +/- 96.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 396         |\n","|    mean_reward          | 3.2         |\n","| time/                   |             |\n","|    total_timesteps      | 4160000     |\n","| train/                  |             |\n","|    approx_kl            | 0.007138795 |\n","|    clip_fraction        | 0.174       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.53       |\n","|    explained_variance   | 0.914       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0112     |\n","|    n_updates            | 1265        |\n","|    policy_gradient_loss | 0.00165     |\n","|    value_loss           | 0.0621      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 254     |\n","|    time_elapsed    | 12788   |\n","|    total_timesteps | 4161536 |\n","--------------------------------\n","Eval num_timesteps=4176000, episode_reward=1.00 +/- 1.26\n","Episode length: 204.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 204          |\n","|    mean_reward          | 1            |\n","| time/                   |              |\n","|    total_timesteps      | 4176000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0049777376 |\n","|    clip_fraction        | 0.177        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.52        |\n","|    explained_variance   | 0.873        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0112      |\n","|    n_updates            | 1270         |\n","|    policy_gradient_loss | -0.000597    |\n","|    value_loss           | 0.0725       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 255     |\n","|    time_elapsed    | 12836   |\n","|    total_timesteps | 4177920 |\n","--------------------------------\n","Eval num_timesteps=4192000, episode_reward=1.60 +/- 1.62\n","Episode length: 196.00 +/- 16.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 196         |\n","|    mean_reward          | 1.6         |\n","| time/                   |             |\n","|    total_timesteps      | 4192000     |\n","| train/                  |             |\n","|    approx_kl            | 0.004966871 |\n","|    clip_fraction        | 0.16        |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.54       |\n","|    explained_variance   | 0.895       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0123     |\n","|    n_updates            | 1275        |\n","|    policy_gradient_loss | 0.00101     |\n","|    value_loss           | 0.0603      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 256     |\n","|    time_elapsed    | 12886   |\n","|    total_timesteps | 4194304 |\n","--------------------------------\n","Eval num_timesteps=4208000, episode_reward=2.60 +/- 3.32\n","Episode length: 296.00 +/- 56.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 296         |\n","|    mean_reward          | 2.6         |\n","| time/                   |             |\n","|    total_timesteps      | 4208000     |\n","| train/                  |             |\n","|    approx_kl            | 0.005239016 |\n","|    clip_fraction        | 0.147       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.54       |\n","|    explained_variance   | 0.892       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0065     |\n","|    n_updates            | 1280        |\n","|    policy_gradient_loss | 0.002       |\n","|    value_loss           | 0.0618      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 257     |\n","|    time_elapsed    | 12940   |\n","|    total_timesteps | 4210688 |\n","--------------------------------\n","Eval num_timesteps=4224000, episode_reward=2.20 +/- 2.71\n","Episode length: 260.00 +/- 48.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 260          |\n","|    mean_reward          | 2.2          |\n","| time/                   |              |\n","|    total_timesteps      | 4224000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0053486973 |\n","|    clip_fraction        | 0.169        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.55        |\n","|    explained_variance   | 0.895        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0138      |\n","|    n_updates            | 1285         |\n","|    policy_gradient_loss | 0.000849     |\n","|    value_loss           | 0.0569       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 258     |\n","|    time_elapsed    | 12994   |\n","|    total_timesteps | 4227072 |\n","--------------------------------\n","Eval num_timesteps=4240000, episode_reward=1.60 +/- 1.36\n","Episode length: 200.00 +/- 8.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 200         |\n","|    mean_reward          | 1.6         |\n","| time/                   |             |\n","|    total_timesteps      | 4240000     |\n","| train/                  |             |\n","|    approx_kl            | 0.004942983 |\n","|    clip_fraction        | 0.146       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.54       |\n","|    explained_variance   | 0.874       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0155     |\n","|    n_updates            | 1290        |\n","|    policy_gradient_loss | 0.00147     |\n","|    value_loss           | 0.0667      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 259     |\n","|    time_elapsed    | 13043   |\n","|    total_timesteps | 4243456 |\n","--------------------------------\n","Eval num_timesteps=4256000, episode_reward=2.80 +/- 3.43\n","Episode length: 360.00 +/- 88.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 360         |\n","|    mean_reward          | 2.8         |\n","| time/                   |             |\n","|    total_timesteps      | 4256000     |\n","| train/                  |             |\n","|    approx_kl            | 0.004443038 |\n","|    clip_fraction        | 0.164       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.54       |\n","|    explained_variance   | 0.901       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0148     |\n","|    n_updates            | 1295        |\n","|    policy_gradient_loss | -0.000398   |\n","|    value_loss           | 0.0645      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 260     |\n","|    time_elapsed    | 13103   |\n","|    total_timesteps | 4259840 |\n","--------------------------------\n","Eval num_timesteps=4272000, episode_reward=0.20 +/- 0.40\n","Episode length: 164.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 164         |\n","|    mean_reward          | 0.2         |\n","| time/                   |             |\n","|    total_timesteps      | 4272000     |\n","| train/                  |             |\n","|    approx_kl            | 0.005633996 |\n","|    clip_fraction        | 0.166       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.55       |\n","|    explained_variance   | 0.886       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.00912    |\n","|    n_updates            | 1300        |\n","|    policy_gradient_loss | -0.00091    |\n","|    value_loss           | 0.0669      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 261     |\n","|    time_elapsed    | 13149   |\n","|    total_timesteps | 4276224 |\n","--------------------------------\n","Eval num_timesteps=4288000, episode_reward=0.40 +/- 0.80\n","Episode length: 200.00 +/- 32.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 200          |\n","|    mean_reward          | 0.4          |\n","| time/                   |              |\n","|    total_timesteps      | 4288000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0047956486 |\n","|    clip_fraction        | 0.151        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.52        |\n","|    explained_variance   | 0.849        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0088      |\n","|    n_updates            | 1305         |\n","|    policy_gradient_loss | 0.00157      |\n","|    value_loss           | 0.0757       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 262     |\n","|    time_elapsed    | 13201   |\n","|    total_timesteps | 4292608 |\n","--------------------------------\n","Eval num_timesteps=4304000, episode_reward=0.60 +/- 0.80\n","Episode length: 168.00 +/- 8.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 168          |\n","|    mean_reward          | 0.6          |\n","| time/                   |              |\n","|    total_timesteps      | 4304000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0040789805 |\n","|    clip_fraction        | 0.132        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.54        |\n","|    explained_variance   | 0.897        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.02        |\n","|    n_updates            | 1310         |\n","|    policy_gradient_loss | 0.000837     |\n","|    value_loss           | 0.0625       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 263     |\n","|    time_elapsed    | 13250   |\n","|    total_timesteps | 4308992 |\n","--------------------------------\n","Eval num_timesteps=4320000, episode_reward=1.60 +/- 2.73\n","Episode length: 244.00 +/- 40.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 244         |\n","|    mean_reward          | 1.6         |\n","| time/                   |             |\n","|    total_timesteps      | 4320000     |\n","| train/                  |             |\n","|    approx_kl            | 0.004457941 |\n","|    clip_fraction        | 0.134       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.55       |\n","|    explained_variance   | 0.889       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0153     |\n","|    n_updates            | 1315        |\n","|    policy_gradient_loss | 2.41e-05    |\n","|    value_loss           | 0.0712      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 264     |\n","|    time_elapsed    | 13302   |\n","|    total_timesteps | 4325376 |\n","--------------------------------\n","Eval num_timesteps=4336000, episode_reward=0.60 +/- 0.80\n","Episode length: 184.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 184         |\n","|    mean_reward          | 0.6         |\n","| time/                   |             |\n","|    total_timesteps      | 4336000     |\n","| train/                  |             |\n","|    approx_kl            | 0.007258432 |\n","|    clip_fraction        | 0.183       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.54       |\n","|    explained_variance   | 0.891       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0152     |\n","|    n_updates            | 1320        |\n","|    policy_gradient_loss | 0.00168     |\n","|    value_loss           | 0.0663      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 265     |\n","|    time_elapsed    | 13349   |\n","|    total_timesteps | 4341760 |\n","--------------------------------\n","Eval num_timesteps=4352000, episode_reward=0.80 +/- 1.17\n","Episode length: 212.00 +/- 16.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 212         |\n","|    mean_reward          | 0.8         |\n","| time/                   |             |\n","|    total_timesteps      | 4352000     |\n","| train/                  |             |\n","|    approx_kl            | 0.005314365 |\n","|    clip_fraction        | 0.148       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.53       |\n","|    explained_variance   | 0.897       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.00126    |\n","|    n_updates            | 1325        |\n","|    policy_gradient_loss | 0.00171     |\n","|    value_loss           | 0.0687      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 266     |\n","|    time_elapsed    | 13400   |\n","|    total_timesteps | 4358144 |\n","--------------------------------\n","Eval num_timesteps=4368000, episode_reward=1.40 +/- 1.02\n","Episode length: 284.00 +/- 40.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 284          |\n","|    mean_reward          | 1.4          |\n","| time/                   |              |\n","|    total_timesteps      | 4368000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0054764636 |\n","|    clip_fraction        | 0.158        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.55        |\n","|    explained_variance   | 0.903        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.012       |\n","|    n_updates            | 1330         |\n","|    policy_gradient_loss | 0.00097      |\n","|    value_loss           | 0.0727       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 267     |\n","|    time_elapsed    | 13453   |\n","|    total_timesteps | 4374528 |\n","--------------------------------\n","Eval num_timesteps=4384000, episode_reward=1.60 +/- 1.85\n","Episode length: 284.00 +/- 40.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 284        |\n","|    mean_reward          | 1.6        |\n","| time/                   |            |\n","|    total_timesteps      | 4384000    |\n","| train/                  |            |\n","|    approx_kl            | 0.00585958 |\n","|    clip_fraction        | 0.172      |\n","|    clip_range           | 0.1        |\n","|    entropy_loss         | -1.54      |\n","|    explained_variance   | 0.896      |\n","|    learning_rate        | 0.000622   |\n","|    loss                 | -0.0157    |\n","|    n_updates            | 1335       |\n","|    policy_gradient_loss | -0.000337  |\n","|    value_loss           | 0.0665     |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 268     |\n","|    time_elapsed    | 13508   |\n","|    total_timesteps | 4390912 |\n","--------------------------------\n","Eval num_timesteps=4400000, episode_reward=0.80 +/- 1.17\n","Episode length: 200.00 +/- 32.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 200         |\n","|    mean_reward          | 0.8         |\n","| time/                   |             |\n","|    total_timesteps      | 4400000     |\n","| train/                  |             |\n","|    approx_kl            | 0.006365278 |\n","|    clip_fraction        | 0.182       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.54       |\n","|    explained_variance   | 0.904       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0186     |\n","|    n_updates            | 1340        |\n","|    policy_gradient_loss | 0.00242     |\n","|    value_loss           | 0.0633      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 269     |\n","|    time_elapsed    | 13559   |\n","|    total_timesteps | 4407296 |\n","--------------------------------\n","Eval num_timesteps=4416000, episode_reward=1.80 +/- 2.23\n","Episode length: 272.00 +/- 24.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 272         |\n","|    mean_reward          | 1.8         |\n","| time/                   |             |\n","|    total_timesteps      | 4416000     |\n","| train/                  |             |\n","|    approx_kl            | 0.005356453 |\n","|    clip_fraction        | 0.168       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.53       |\n","|    explained_variance   | 0.89        |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0074     |\n","|    n_updates            | 1345        |\n","|    policy_gradient_loss | 0.000726    |\n","|    value_loss           | 0.0728      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 270     |\n","|    time_elapsed    | 13611   |\n","|    total_timesteps | 4423680 |\n","--------------------------------\n","Eval num_timesteps=4432000, episode_reward=0.80 +/- 0.98\n","Episode length: 184.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 184          |\n","|    mean_reward          | 0.8          |\n","| time/                   |              |\n","|    total_timesteps      | 4432000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0053423215 |\n","|    clip_fraction        | 0.166        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.53        |\n","|    explained_variance   | 0.863        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0137      |\n","|    n_updates            | 1350         |\n","|    policy_gradient_loss | -0.000752    |\n","|    value_loss           | 0.0858       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 271     |\n","|    time_elapsed    | 13656   |\n","|    total_timesteps | 4440064 |\n","--------------------------------\n","Eval num_timesteps=4448000, episode_reward=1.20 +/- 1.47\n","Episode length: 220.00 +/- 8.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 220          |\n","|    mean_reward          | 1.2          |\n","| time/                   |              |\n","|    total_timesteps      | 4448000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0048559983 |\n","|    clip_fraction        | 0.14         |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.55        |\n","|    explained_variance   | 0.908        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0149      |\n","|    n_updates            | 1355         |\n","|    policy_gradient_loss | 0.00165      |\n","|    value_loss           | 0.0618       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 325     |\n","|    iterations      | 272     |\n","|    time_elapsed    | 13706   |\n","|    total_timesteps | 4456448 |\n","--------------------------------\n","Eval num_timesteps=4464000, episode_reward=5.20 +/- 6.49\n","Episode length: 544.00 +/- 120.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 544          |\n","|    mean_reward          | 5.2          |\n","| time/                   |              |\n","|    total_timesteps      | 4464000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0055597206 |\n","|    clip_fraction        | 0.177        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.54        |\n","|    explained_variance   | 0.861        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0156      |\n","|    n_updates            | 1360         |\n","|    policy_gradient_loss | 0.000338     |\n","|    value_loss           | 0.0781       |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 324     |\n","|    iterations      | 273     |\n","|    time_elapsed    | 13777   |\n","|    total_timesteps | 4472832 |\n","--------------------------------\n","Eval num_timesteps=4480000, episode_reward=2.60 +/- 3.20\n","Episode length: 344.00 +/- 40.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 344         |\n","|    mean_reward          | 2.6         |\n","| time/                   |             |\n","|    total_timesteps      | 4480000     |\n","| train/                  |             |\n","|    approx_kl            | 0.005117791 |\n","|    clip_fraction        | 0.167       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.54       |\n","|    explained_variance   | 0.858       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0177     |\n","|    n_updates            | 1365        |\n","|    policy_gradient_loss | 0.000996    |\n","|    value_loss           | 0.0785      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 324     |\n","|    iterations      | 274     |\n","|    time_elapsed    | 13833   |\n","|    total_timesteps | 4489216 |\n","--------------------------------\n","Eval num_timesteps=4496000, episode_reward=1.00 +/- 1.55\n","Episode length: 216.00 +/- 16.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 216          |\n","|    mean_reward          | 1            |\n","| time/                   |              |\n","|    total_timesteps      | 4496000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0054476936 |\n","|    clip_fraction        | 0.178        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.55        |\n","|    explained_variance   | 0.863        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0187      |\n","|    n_updates            | 1370         |\n","|    policy_gradient_loss | 0.00193      |\n","|    value_loss           | 0.0741       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 324     |\n","|    iterations      | 275     |\n","|    time_elapsed    | 13881   |\n","|    total_timesteps | 4505600 |\n","--------------------------------\n","Eval num_timesteps=4512000, episode_reward=0.60 +/- 0.80\n","Episode length: 172.00 +/- 16.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 172          |\n","|    mean_reward          | 0.6          |\n","| time/                   |              |\n","|    total_timesteps      | 4512000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0050298553 |\n","|    clip_fraction        | 0.176        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.57        |\n","|    explained_variance   | 0.866        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0125      |\n","|    n_updates            | 1375         |\n","|    policy_gradient_loss | 0.00113      |\n","|    value_loss           | 0.0709       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 324     |\n","|    iterations      | 276     |\n","|    time_elapsed    | 13929   |\n","|    total_timesteps | 4521984 |\n","--------------------------------\n","Eval num_timesteps=4528000, episode_reward=1.20 +/- 1.47\n","Episode length: 212.00 +/- 16.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 212         |\n","|    mean_reward          | 1.2         |\n","| time/                   |             |\n","|    total_timesteps      | 4528000     |\n","| train/                  |             |\n","|    approx_kl            | 0.005636749 |\n","|    clip_fraction        | 0.165       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.53       |\n","|    explained_variance   | 0.808       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.00773    |\n","|    n_updates            | 1380        |\n","|    policy_gradient_loss | 0.000984    |\n","|    value_loss           | 0.0874      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 324     |\n","|    iterations      | 277     |\n","|    time_elapsed    | 13980   |\n","|    total_timesteps | 4538368 |\n","--------------------------------\n","Eval num_timesteps=4544000, episode_reward=1.80 +/- 3.12\n","Episode length: 280.00 +/- 8.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 280          |\n","|    mean_reward          | 1.8          |\n","| time/                   |              |\n","|    total_timesteps      | 4544000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0052840486 |\n","|    clip_fraction        | 0.153        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.55        |\n","|    explained_variance   | 0.892        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0211      |\n","|    n_updates            | 1385         |\n","|    policy_gradient_loss | 0.00144      |\n","|    value_loss           | 0.0624       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 324     |\n","|    iterations      | 278     |\n","|    time_elapsed    | 14031   |\n","|    total_timesteps | 4554752 |\n","--------------------------------\n","Eval num_timesteps=4560000, episode_reward=1.20 +/- 1.47\n","Episode length: 192.00 +/- 16.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 192          |\n","|    mean_reward          | 1.2          |\n","| time/                   |              |\n","|    total_timesteps      | 4560000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0064422796 |\n","|    clip_fraction        | 0.192        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.53        |\n","|    explained_variance   | 0.827        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 8.94e-05     |\n","|    n_updates            | 1390         |\n","|    policy_gradient_loss | 0.00106      |\n","|    value_loss           | 0.081        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 324     |\n","|    iterations      | 279     |\n","|    time_elapsed    | 14080   |\n","|    total_timesteps | 4571136 |\n","--------------------------------\n","Eval num_timesteps=4576000, episode_reward=2.20 +/- 2.86\n","Episode length: 288.00 +/- 8.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 288          |\n","|    mean_reward          | 2.2          |\n","| time/                   |              |\n","|    total_timesteps      | 4576000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0054343105 |\n","|    clip_fraction        | 0.174        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.54        |\n","|    explained_variance   | 0.844        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0182      |\n","|    n_updates            | 1395         |\n","|    policy_gradient_loss | 4.68e-05     |\n","|    value_loss           | 0.0742       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 324     |\n","|    iterations      | 280     |\n","|    time_elapsed    | 14135   |\n","|    total_timesteps | 4587520 |\n","--------------------------------\n","Eval num_timesteps=4592000, episode_reward=3.40 +/- 4.45\n","Episode length: 420.00 +/- 128.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 420          |\n","|    mean_reward          | 3.4          |\n","| time/                   |              |\n","|    total_timesteps      | 4592000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0056725917 |\n","|    clip_fraction        | 0.18         |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.55        |\n","|    explained_variance   | 0.837        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0144      |\n","|    n_updates            | 1400         |\n","|    policy_gradient_loss | 0.000404     |\n","|    value_loss           | 0.0816       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 324     |\n","|    iterations      | 281     |\n","|    time_elapsed    | 14200   |\n","|    total_timesteps | 4603904 |\n","--------------------------------\n","Eval num_timesteps=4608000, episode_reward=1.80 +/- 2.23\n","Episode length: 224.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 224          |\n","|    mean_reward          | 1.8          |\n","| time/                   |              |\n","|    total_timesteps      | 4608000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0054983944 |\n","|    clip_fraction        | 0.169        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.56        |\n","|    explained_variance   | 0.868        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0198      |\n","|    n_updates            | 1405         |\n","|    policy_gradient_loss | 0.00172      |\n","|    value_loss           | 0.0641       |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 324     |\n","|    iterations      | 282     |\n","|    time_elapsed    | 14248   |\n","|    total_timesteps | 4620288 |\n","--------------------------------\n","Eval num_timesteps=4624000, episode_reward=1.80 +/- 2.40\n","Episode length: 216.00 +/- 16.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 216          |\n","|    mean_reward          | 1.8          |\n","| time/                   |              |\n","|    total_timesteps      | 4624000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0059665763 |\n","|    clip_fraction        | 0.16         |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -1.56        |\n","|    explained_variance   | 0.85         |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0169      |\n","|    n_updates            | 1410         |\n","|    policy_gradient_loss | 0.00132      |\n","|    value_loss           | 0.067        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 324     |\n","|    iterations      | 283     |\n","|    time_elapsed    | 14297   |\n","|    total_timesteps | 4636672 |\n","--------------------------------\n","Eval num_timesteps=4640000, episode_reward=4.40 +/- 5.71\n","Episode length: 468.00 +/- 152.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 468         |\n","|    mean_reward          | 4.4         |\n","| time/                   |             |\n","|    total_timesteps      | 4640000     |\n","| train/                  |             |\n","|    approx_kl            | 0.004913548 |\n","|    clip_fraction        | 0.17        |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -1.56       |\n","|    explained_variance   | 0.867       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0151     |\n","|    n_updates            | 1415        |\n","|    policy_gradient_loss | -0.00127    |\n","|    value_loss           | 0.0708      |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 323     |\n","|    iterations      | 284     |\n","|    time_elapsed    | 14364   |\n","|    total_timesteps | 4653056 |\n","--------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n"]},{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-9-b422e3051436>\", line 10, in <cell line: 10>\n","    model.learn(total_timesteps=5000000,callback=eval_callback)\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py\", line 308, in learn\n","    return super().learn(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py\", line 259, in learn\n","    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py\", line 184, in collect_rollouts\n","    if callback.on_step() is False:\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 104, in on_step\n","    return self._on_step()\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 471, in _on_step\n","    np.savez(\n","  File \"<__array_function__ internals>\", line 180, in savez\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 615, in savez\n","    _savez(file, args, kwds, False)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 712, in _savez\n","    zipf = zipfile_factory(file, mode=\"w\", compression=compression)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 103, in zipfile_factory\n","    return zipfile.ZipFile(file, *args, **kwargs)\n","  File \"/usr/lib/python3.10/zipfile.py\", line 1251, in __init__\n","    self.fp = io.open(file, filemode)\n","OSError: [Errno 107] Transport endpoint is not connected: './policy_new_log_eval/evaluations.npz'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'OSError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-9-b422e3051436>\", line 10, in <cell line: 10>\n","    model.learn(total_timesteps=5000000,callback=eval_callback)\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py\", line 308, in learn\n","    return super().learn(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py\", line 259, in learn\n","    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py\", line 184, in collect_rollouts\n","    if callback.on_step() is False:\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 104, in on_step\n","    return self._on_step()\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 471, in _on_step\n","    np.savez(\n","  File \"<__array_function__ internals>\", line 180, in savez\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 615, in savez\n","    _savez(file, args, kwds, False)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 712, in _savez\n","    zipf = zipfile_factory(file, mode=\"w\", compression=compression)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 103, in zipfile_factory\n","    return zipfile.ZipFile(file, *args, **kwargs)\n","  File \"/usr/lib/python3.10/zipfile.py\", line 1251, in __init__\n","    self.fp = io.open(file, filemode)\n","OSError: [Errno 107] Transport endpoint is not connected: './policy_new_log_eval/evaluations.npz'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'OSError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n","    if (await self.run_code(code, result,  async_=asy)):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n","    self.showtraceback(running_compiled_code=True)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n","    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-9-b422e3051436>\", line 10, in <cell line: 10>\n","    model.learn(total_timesteps=5000000,callback=eval_callback)\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py\", line 308, in learn\n","    return super().learn(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py\", line 259, in learn\n","    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py\", line 184, in collect_rollouts\n","    if callback.on_step() is False:\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 104, in on_step\n","    return self._on_step()\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 471, in _on_step\n","    np.savez(\n","  File \"<__array_function__ internals>\", line 180, in savez\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 615, in savez\n","    _savez(file, args, kwds, False)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 712, in _savez\n","    zipf = zipfile_factory(file, mode=\"w\", compression=compression)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 103, in zipfile_factory\n","    return zipfile.ZipFile(file, *args, **kwargs)\n","  File \"/usr/lib/python3.10/zipfile.py\", line 1251, in __init__\n","    self.fp = io.open(file, filemode)\n","OSError: [Errno 107] Transport endpoint is not connected: './policy_new_log_eval/evaluations.npz'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'OSError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n","    if (await self.run_code(code, result,  async_=asy)):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n","    self.showtraceback(running_compiled_code=True)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n","    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n","    return runner(coro)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n","    coro.send(None)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n","    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3492, in run_ast_nodes\n","    self.showtraceback()\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1142, in structured_traceback\n","    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4","mount_file_id":"10ITQr8SSQz72THKnExE61CCGBziVI3d_","authorship_tag":"ABX9TyOEC++ViHaEvJvsWR+ZrMtk"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}