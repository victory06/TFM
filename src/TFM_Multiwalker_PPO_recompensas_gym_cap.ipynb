{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"d8JAmEUyj9De"},"outputs":[],"source":["# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n","mount='/content/gdrive'\n","drive_root = mount + \"/My Drive/TFM\"\n","\n","try:\n","  from google.colab import drive\n","  IN_COLAB=True\n","except:\n","  IN_COLAB=False"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15786,"status":"ok","timestamp":1698933859815,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-60},"id":"DrEo9QnxkAne","outputId":"6c5d10c5-4e81-4ffa-cecb-60c09095d957"},"outputs":[{"output_type":"stream","name":"stdout","text":["We're running Colab\n","Colab: mounting Google drive on  /content/gdrive\n","Mounted at /content/gdrive\n","\n","Colab: making sure  /content/gdrive/My Drive/TFM  exists.\n","\n","Colab: Changing directory to  /content/gdrive/My Drive/TFM\n","/content/gdrive/My Drive/TFM\n","Archivos en el directorio: \n","['Entrenamientos antiguos sin logs', '.ipynb_checkpoints', 'Entrenamientos_log_no_eval', 'PPO_policies', 'DQN_new_pettingzoo_gym_cap.ipynb', 'multi_car_racing', 'policy_log_eval', 'DQN_policies', 'results_rllib', 'MCR_TFM.ipynb', 'multiwalker_ddpg_log_eval', 'multiwalker_sac_log_eval', 'multiwalker_ddpg.zip', 'multiwalker_ppo_log_eval', 'multiwalker_ppo.zip', 'multiwalker_td3_log_eval', 'multiwalker_sac2_log_eval', 'multiwalker_td3_2_log_eval', 'multiwalker_sac3_log_eval', 'multiwalker_sac3.zip', 'multiwalker_ppo_2_log_eval', 'multiwalker_ddpg2_log_eval', 'multiwalker_ppo_2.zip', 'TFM_Multiwalker_DDPG_PPO_gym_cap.ipynb', 'multiwalker_td3_3_log_eval', 'TFM_Multiwalker_TD3_gym_cap.ipynb', 'TFM_Multiwalker_SAC_gym_cap.ipynb', 'TFM_PPO_KAZ_gym_cap.ipynb', 'TFM_PPO_new_KAZ_gym_cap.ipynb', 'TFM_Multiwalker_DDPG_recompensas_gym_cap.ipynb', 'multiwalker_ddpg2_5_log_eval', 'multiwalker_ppo_rew_08_log_eval', 'multiwalker_ppo_rew_08.zip', 'multiwalker_ppo_08_2_log_eval', 'multiwalker_ddpg2_6_log_eval', 'multiwalker_ppo_08_2.zip', 'multiwalker_sac_08_log_eval', 'TFM_Multiwalker_DDPG_gym_cap.ipynb', '=2.13', 'multiwalker_sac2_08_log_eval', 'multiwalker_ppo_rew_04_log_eval', 'multiwalker_ppo_rew_04.zip', 'multiwalker_ppo_04_2_log_eval', 'multiwalker_ppo_04_2.zip', 'TFM_Multiwalker_PPO_recompensas_gym_cap.ipynb', 'TFM_Multiwalker_SAC_recompensas_gym_cap.ipynb']\n"]}],"source":["# Switch to the directory on the Google Drive that you want to use\n","import os\n","if IN_COLAB:\n","  print(\"We're running Colab\")\n","\n","  if IN_COLAB:\n","    # Mount the Google Drive at mount\n","    print(\"Colab: mounting Google drive on \", mount)\n","\n","    drive.mount(mount)\n","\n","    # Create drive_root if it doesn't exist\n","    create_drive_root = True\n","    if create_drive_root:\n","      print(\"\\nColab: making sure \", drive_root, \" exists.\")\n","      os.makedirs(drive_root, exist_ok=True)\n","\n","    # Change to the directory\n","    print(\"\\nColab: Changing directory to \", drive_root)\n","    %cd $drive_root\n","# Verify we're in the correct working directory\n","%pwd\n","print(\"Archivos en el directorio: \")\n","print(os.listdir())"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":232685,"status":"ok","timestamp":1698934092497,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-60},"id":"xAZDg478kEbs","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4abb475d-1f0a-4777-bd4d-28613fdcab69"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gym==0.17.3\n","  Downloading gym-0.17.3.tar.gz (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym==0.17.3) (1.11.3)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.17.3) (1.23.5)\n","Collecting pyglet<=1.5.0,>=1.4.0 (from gym==0.17.3)\n","  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cloudpickle<1.7.0,>=1.2.0 (from gym==0.17.3)\n","  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3) (0.18.3)\n","Building wheels for collected packages: gym\n","  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym: filename=gym-0.17.3-py3-none-any.whl size=1654617 sha256=1a8bc3dcee28d0386f7ebf68a5880dbc724c309f3e2dc17730c10399a8ae91b3\n","  Stored in directory: /root/.cache/pip/wheels/af/4b/74/fcfc8238472c34d7f96508a63c962ff3ac9485a9a4137afd4e\n","Successfully built gym\n","Installing collected packages: pyglet, cloudpickle, gym\n","  Attempting uninstall: cloudpickle\n","    Found existing installation: cloudpickle 2.2.1\n","    Uninstalling cloudpickle-2.2.1:\n","      Successfully uninstalled cloudpickle-2.2.1\n","  Attempting uninstall: gym\n","    Found existing installation: gym 0.25.2\n","    Uninstalling gym-0.25.2:\n","      Successfully uninstalled gym-0.25.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","bigframes 0.10.0 requires cloudpickle>=2.0.0, but you have cloudpickle 1.6.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed cloudpickle-1.6.0 gym-0.17.3 pyglet-1.5.0\n","Collecting git+https://github.com/Kojoley/atari-py.git\n","  Cloning https://github.com/Kojoley/atari-py.git to /tmp/pip-req-build-5o2b4xm_\n","  Running command git clone --filter=blob:none --quiet https://github.com/Kojoley/atari-py.git /tmp/pip-req-build-5o2b4xm_\n","  Resolved https://github.com/Kojoley/atari-py.git to commit 86a1e05c0a95e9e6233c3a413521fdb34ca8a089\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from atari-py==1.2.2) (1.23.5)\n","Building wheels for collected packages: atari-py\n","  Building wheel for atari-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for atari-py: filename=atari_py-1.2.2-cp310-cp310-linux_x86_64.whl size=4738552 sha256=6d2c8f378ddad37c2a10c28cd45a5dcced70f20feaac06edfe17019f7f0a9955\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-pghvvrd3/wheels/6c/9b/f1/8a80a63a233d6f4eb2e2ee8c7357f4875f21c3ffc7f2613f9f\n","Successfully built atari-py\n","Installing collected packages: atari-py\n","Successfully installed atari-py-1.2.2\n","Collecting keras-rl2==1.0.5\n","  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from keras-rl2==1.0.5) (2.14.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (23.5.26)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (16.0.6)\n","Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.2.0)\n","Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.23.5)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (23.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (4.5.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.34.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.59.0)\n","Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.14.1)\n","Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.14.0)\n","Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.14.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2==1.0.5) (0.41.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.2.2)\n","Installing collected packages: keras-rl2\n","Successfully installed keras-rl2-1.0.5\n","Collecting tensorflow==2.8\n","  Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl (497.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.6.3)\n","Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (23.5.26)\n","Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.9.0)\n","Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.8)\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (16.0.6)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.23.5)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.3.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (4.5.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.14.1)\n","Collecting tensorboard<2.9,>=2.8 (from tensorflow==2.8)\n","  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tf-estimator-nightly==2.8.0.dev2021122109 (from tensorflow==2.8)\n","  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting keras<2.9,>=2.8.0rc0 (from tensorflow==2.8)\n","  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.34.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.59.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.8) (0.41.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.17.3)\n","Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.31.0)\n","Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n","  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m112.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n","  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.2.2)\n","Installing collected packages: tf-estimator-nightly, tensorboard-plugin-wit, keras, tensorboard-data-server, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.14.0\n","    Uninstalling keras-2.14.0:\n","      Successfully uninstalled keras-2.14.0\n","  Attempting uninstall: tensorboard-data-server\n","    Found existing installation: tensorboard-data-server 0.7.2\n","    Uninstalling tensorboard-data-server-0.7.2:\n","      Successfully uninstalled tensorboard-data-server-0.7.2\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 1.0.0\n","    Uninstalling google-auth-oauthlib-1.0.0:\n","      Successfully uninstalled google-auth-oauthlib-1.0.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.14.1\n","    Uninstalling tensorboard-2.14.1:\n","      Successfully uninstalled tensorboard-2.14.1\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.14.0\n","    Uninstalling tensorflow-2.14.0:\n","      Successfully uninstalled tensorflow-2.14.0\n","Successfully installed google-auth-oauthlib-0.4.6 keras-2.8.0 keras-preprocessing-1.1.2 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tf-estimator-nightly-2.8.0.dev2021122109\n","Collecting gym-cap\n","  Downloading gym_cap-0.1.4.12-py3-none-any.whl (32 kB)\n","Requirement already satisfied: gym>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from gym-cap) (0.17.3)\n","Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from gym-cap) (1.23.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym>=0.16.0->gym-cap) (1.11.3)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.16.0->gym-cap) (1.5.0)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.16.0->gym-cap) (1.6.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.16.0->gym-cap) (0.18.3)\n","Installing collected packages: gym-cap\n","Successfully installed gym-cap-0.1.4.12\n","Collecting pettingzoo[butterfly]\n","  Downloading pettingzoo-1.24.1-py3-none-any.whl (840 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.8/840.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[butterfly]) (1.23.5)\n","Collecting gymnasium>=0.28.0 (from pettingzoo[butterfly])\n","  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pygame==2.3.0 (from pettingzoo[butterfly])\n","  Downloading pygame-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pymunk==6.2.0 (from pettingzoo[butterfly])\n","  Downloading pymunk-6.2.0.zip (7.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: cffi>1.14.0 in /usr/local/lib/python3.10/dist-packages (from pymunk==6.2.0->pettingzoo[butterfly]) (1.16.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[butterfly]) (1.6.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[butterfly]) (4.5.0)\n","Collecting farama-notifications>=0.0.1 (from gymnasium>=0.28.0->pettingzoo[butterfly])\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>1.14.0->pymunk==6.2.0->pettingzoo[butterfly]) (2.21)\n","Building wheels for collected packages: pymunk\n","  Building wheel for pymunk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pymunk: filename=pymunk-6.2.0-cp310-cp310-linux_x86_64.whl size=801633 sha256=40316c84623cf9a0b88a0dd64afd1cdfab80c417e060a4cee140c82ec7b4da50\n","  Stored in directory: /root/.cache/pip/wheels/2e/81/a2/f941a9ff417bb4020c37ae218fb7117d12d3fc019ea493d66f\n","Successfully built pymunk\n","Installing collected packages: farama-notifications, pygame, gymnasium, pymunk, pettingzoo\n","  Attempting uninstall: pygame\n","    Found existing installation: pygame 2.5.2\n","    Uninstalling pygame-2.5.2:\n","      Successfully uninstalled pygame-2.5.2\n","Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1 pettingzoo-1.24.1 pygame-2.3.0 pymunk-6.2.0\n"]}],"source":["if IN_COLAB:\n","  %pip install gym==0.17.3\n","  %pip install git+https://github.com/Kojoley/atari-py.git\n","  %pip install keras-rl2==1.0.5\n","  %pip install tensorflow==2.8\n","  %pip install gym-cap\n","  %pip install pettingzoo[butterfly]\n","else:\n","  %pip install gym==0.17.3\n","  %pip install git+https://github.com/Kojoley/atari-py.git\n","  %pip install pyglet==1.5.0\n","  %pip install h5py==3.1.0\n","  %pip install Pillow==9.5.0\n","  %pip install keras-rl2==1.0.5\n","  %pip install Keras==2.2.4\n","  %pip install tensorflow==2.5.3\n","  %pip install torch==2.0.1\n","  %pip install agents==1.4.0"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":48,"status":"ok","timestamp":1698934092498,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-60},"id":"G8cw-IX3laE9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3ac05011-bb63-4e10-dcb8-48cb6ad31bb9"},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'multi_car_racing' already exists and is not an empty directory.\n"]}],"source":["!git clone https://github.com/igilitschenski/multi_car_racing.git\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IqbMo3gK7vBG"},"outputs":[],"source":["!cd multi_car_racing\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6493,"status":"ok","timestamp":1698934099285,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-60},"id":"Jekec6f98b3A","colab":{"base_uri":"https://localhost:8080/"},"outputId":"47b31782-7b8f-4b51-9bcf-7eb43ca1fcdd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting swig\n","  Downloading swig-4.1.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: swig\n","Successfully installed swig-4.1.1\n"]}],"source":["!pip install swig"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":61736,"status":"ok","timestamp":1698934161012,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-60},"id":"KKxRPBFx85k6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"923da86e-83f7-426f-ae28-20499e91f1f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting box2d\n","  Downloading Box2D-2.3.2.tar.gz (427 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.9/427.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: box2d\n","  Building wheel for box2d (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d: filename=Box2D-2.3.2-cp310-cp310-linux_x86_64.whl size=2391310 sha256=d05e8c4cf14eb56471d403ebef56527305f3a2c5e9229cebab46535dcb6495fc\n","  Stored in directory: /root/.cache/pip/wheels/eb/cb/be/e663f3ce9aba6580611c0febaf7cd3cf7603f87047de2a52f9\n","Successfully built box2d\n","Installing collected packages: box2d\n","Successfully installed box2d-2.3.2\n"]}],"source":["!pip install box2d"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":57935,"status":"ok","timestamp":1698934218934,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-60},"id":"ijp5V0i09MRF","colab":{"base_uri":"https://localhost:8080/"},"outputId":"25af0ac6-b38a-4520-8ec2-2e7f0c26ab18"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting box2d-py\n","  Downloading box2d-py-2.3.8.tar.gz (374 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/374.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/374.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.5/374.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: box2d-py\n","  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-py: filename=box2d_py-2.3.8-cp310-cp310-linux_x86_64.whl size=2373077 sha256=337e657a126a459031f32d56c14ea0349114c2f58fc1a121dc23cbb4a10088b6\n","  Stored in directory: /root/.cache/pip/wheels/47/01/d2/6a780da77ccb98b1d2facdd520a8d10838a03b590f6f8d50c0\n","Successfully built box2d-py\n","Installing collected packages: box2d-py\n","Successfully installed box2d-py-2.3.8\n"]}],"source":["!pip install box2d-py"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":27026,"status":"ok","timestamp":1698934245950,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-60},"id":"BwjugqI99g0I","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e2bc6962-b3a6-455c-c409-885167a843c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Obtaining file:///content/gdrive/MyDrive/TFM/multi_car_racing\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: box2d-py~=2.3.5 in /usr/local/lib/python3.10/dist-packages (from gym-multi-car-racing==0.0.1) (2.3.8)\n","Collecting shapely~=1.7.0 (from gym-multi-car-racing==0.0.1)\n","  Downloading Shapely-1.7.1.tar.gz (383 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.2/383.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from gym-multi-car-racing==0.0.1) (1.23.5)\n","Requirement already satisfied: gym~=0.17.2 in /usr/local/lib/python3.10/dist-packages (from gym-multi-car-racing==0.0.1) (0.17.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym~=0.17.2->gym-multi-car-racing==0.0.1) (1.11.3)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from gym~=0.17.2->gym-multi-car-racing==0.0.1) (1.5.0)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym~=0.17.2->gym-multi-car-racing==0.0.1) (1.6.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym~=0.17.2->gym-multi-car-racing==0.0.1) (0.18.3)\n","Building wheels for collected packages: shapely\n","  Building wheel for shapely (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for shapely: filename=Shapely-1.7.1-cp310-cp310-linux_x86_64.whl size=997162 sha256=e7165a58aa3fd09741a5677713385a8db1e77a4824c125cd6ef4b0ebf4c72637\n","  Stored in directory: /root/.cache/pip/wheels/2e/fa/97/c85f587c35afcaf4a81c481741d36592518d1e50445572f0d4\n","Successfully built shapely\n","Installing collected packages: shapely, gym-multi-car-racing\n","  Attempting uninstall: shapely\n","    Found existing installation: shapely 2.0.2\n","    Uninstalling shapely-2.0.2:\n","      Successfully uninstalled shapely-2.0.2\n","  Running setup.py develop for gym-multi-car-racing\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires fastapi, which is not installed.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed gym-multi-car-racing-0.0.1 shapely-1.7.1\n"]}],"source":["!pip install -e multi_car_racing"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11249,"status":"ok","timestamp":1698934257152,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-60},"id":"IrAvXzCW-Z3e","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bfc626d9-6b59-468f-f7ff-7bb8a8d8786f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  freeglut3 libegl-dev libgl-dev libgl1-mesa-dev libgles-dev libgles1\n","  libglu1-mesa libglu1-mesa-dev libglvnd-core-dev libglvnd-dev libglx-dev\n","  libice-dev libopengl-dev libsm-dev libxt-dev\n","Suggested packages:\n","  libice-doc libsm-doc libxt-doc\n","The following NEW packages will be installed:\n","  freeglut3 freeglut3-dev libegl-dev libgl-dev libgl1-mesa-dev libgles-dev\n","  libgles1 libglu1-mesa libglu1-mesa-dev libglvnd-core-dev libglvnd-dev\n","  libglx-dev libice-dev libopengl-dev libsm-dev libxt-dev\n","0 upgraded, 16 newly installed, 0 to remove and 19 not upgraded.\n","Need to get 1,261 kB of archives.\n","After this operation, 6,753 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3 amd64 2.8.1-6 [74.0 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglx-dev amd64 1.4.0-1 [14.1 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgl-dev amd64 1.4.0-1 [101 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-core-dev amd64 1.4.0-1 [12.7 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libegl-dev amd64 1.4.0-1 [18.0 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles1 amd64 1.4.0-1 [11.5 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles-dev amd64 1.4.0-1 [49.4 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libopengl-dev amd64 1.4.0-1 [3,400 B]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-dev amd64 1.4.0-1 [3,162 B]\n","Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgl1-mesa-dev amd64 23.0.4-0ubuntu1~22.04.1 [6,510 B]\n","Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n","Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa-dev amd64 9.0.2-1 [231 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 libice-dev amd64 2:1.0.10-1build2 [51.4 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsm-dev amd64 2:1.2.3-1build2 [18.1 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3-dev amd64 2.8.1-6 [126 kB]\n","Fetched 1,261 kB in 2s (619 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 16.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package freeglut3:amd64.\n","(Reading database ... 120874 files and directories currently installed.)\n","Preparing to unpack .../00-freeglut3_2.8.1-6_amd64.deb ...\n","Unpacking freeglut3:amd64 (2.8.1-6) ...\n","Selecting previously unselected package libglx-dev:amd64.\n","Preparing to unpack .../01-libglx-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglx-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgl-dev:amd64.\n","Preparing to unpack .../02-libgl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libgl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libglvnd-core-dev:amd64.\n","Preparing to unpack .../03-libglvnd-core-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglvnd-core-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libegl-dev:amd64.\n","Preparing to unpack .../04-libegl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libegl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgles1:amd64.\n","Preparing to unpack .../05-libgles1_1.4.0-1_amd64.deb ...\n","Unpacking libgles1:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgles-dev:amd64.\n","Preparing to unpack .../06-libgles-dev_1.4.0-1_amd64.deb ...\n","Unpacking libgles-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libopengl-dev:amd64.\n","Preparing to unpack .../07-libopengl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libopengl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libglvnd-dev:amd64.\n","Preparing to unpack .../08-libglvnd-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglvnd-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgl1-mesa-dev:amd64.\n","Preparing to unpack .../09-libgl1-mesa-dev_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n","Unpacking libgl1-mesa-dev:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Selecting previously unselected package libglu1-mesa:amd64.\n","Preparing to unpack .../10-libglu1-mesa_9.0.2-1_amd64.deb ...\n","Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n","Selecting previously unselected package libglu1-mesa-dev:amd64.\n","Preparing to unpack .../11-libglu1-mesa-dev_9.0.2-1_amd64.deb ...\n","Unpacking libglu1-mesa-dev:amd64 (9.0.2-1) ...\n","Selecting previously unselected package libice-dev:amd64.\n","Preparing to unpack .../12-libice-dev_2%3a1.0.10-1build2_amd64.deb ...\n","Unpacking libice-dev:amd64 (2:1.0.10-1build2) ...\n","Selecting previously unselected package libsm-dev:amd64.\n","Preparing to unpack .../13-libsm-dev_2%3a1.2.3-1build2_amd64.deb ...\n","Unpacking libsm-dev:amd64 (2:1.2.3-1build2) ...\n","Selecting previously unselected package libxt-dev:amd64.\n","Preparing to unpack .../14-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n","Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n","Selecting previously unselected package freeglut3-dev:amd64.\n","Preparing to unpack .../15-freeglut3-dev_2.8.1-6_amd64.deb ...\n","Unpacking freeglut3-dev:amd64 (2.8.1-6) ...\n","Setting up freeglut3:amd64 (2.8.1-6) ...\n","Setting up libglvnd-core-dev:amd64 (1.4.0-1) ...\n","Setting up libice-dev:amd64 (2:1.0.10-1build2) ...\n","Setting up libsm-dev:amd64 (2:1.2.3-1build2) ...\n","Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n","Setting up libgles1:amd64 (1.4.0-1) ...\n","Setting up libglx-dev:amd64 (1.4.0-1) ...\n","Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n","Setting up libopengl-dev:amd64 (1.4.0-1) ...\n","Setting up libgl-dev:amd64 (1.4.0-1) ...\n","Setting up libegl-dev:amd64 (1.4.0-1) ...\n","Setting up libglu1-mesa-dev:amd64 (9.0.2-1) ...\n","Setting up libgles-dev:amd64 (1.4.0-1) ...\n","Setting up libglvnd-dev:amd64 (1.4.0-1) ...\n","Setting up libgl1-mesa-dev:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Setting up freeglut3-dev:amd64 (2.8.1-6) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n"]}],"source":["!sudo apt-get install freeglut3-dev"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18988,"status":"ok","timestamp":1698934276091,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-60},"id":"cgGdQ6n9EERW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c501fba8-4717-4236-cfdc-f40017849b27"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n","  xserver-common\n","The following NEW packages will be installed:\n","  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n","  xserver-common xvfb\n","0 upgraded, 9 newly installed, 0 to remove and 19 not upgraded.\n","Need to get 7,814 kB of archives.\n","After this operation, 11.9 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.2 [28.1 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.2 [864 kB]\n","Fetched 7,814 kB in 1s (5,552 kB/s)\n","Selecting previously unselected package libfontenc1:amd64.\n","(Reading database ... 121332 files and directories currently installed.)\n","Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n","Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n","Selecting previously unselected package libxfont2:amd64.\n","Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n","Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n","Selecting previously unselected package libxkbfile1:amd64.\n","Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n","Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n","Selecting previously unselected package x11-xkb-utils.\n","Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n","Unpacking x11-xkb-utils (7.7+5build4) ...\n","Selecting previously unselected package xfonts-encodings.\n","Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n","Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n","Selecting previously unselected package xfonts-utils.\n","Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n","Unpacking xfonts-utils (1:7.7+6build2) ...\n","Selecting previously unselected package xfonts-base.\n","Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n","Unpacking xfonts-base (1:1.0.5) ...\n","Selecting previously unselected package xserver-common.\n","Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.2_all.deb ...\n","Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n","Selecting previously unselected package xvfb.\n","Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.2_amd64.deb ...\n","Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.2) ...\n","Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n","Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n","Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n","Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n","Setting up x11-xkb-utils (7.7+5build4) ...\n","Setting up xfonts-utils (1:7.7+6build2) ...\n","Setting up xfonts-base (1:1.0.5) ...\n","Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n","Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.2) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","Collecting pyvirtualdisplay\n","  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n","Installing collected packages: pyvirtualdisplay\n","Successfully installed pyvirtualdisplay-3.0\n","Collecting piglet\n","  Downloading piglet-1.0.0-py2.py3-none-any.whl (2.2 kB)\n","Collecting piglet-templates (from piglet)\n","  Downloading piglet_templates-1.3.0-py3-none-any.whl (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.5/67.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from piglet-templates->piglet) (3.1.1)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from piglet-templates->piglet) (23.1.0)\n","Requirement already satisfied: astunparse in /usr/local/lib/python3.10/dist-packages (from piglet-templates->piglet) (1.6.3)\n","Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from piglet-templates->piglet) (2.1.3)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse->piglet-templates->piglet) (0.41.2)\n","Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from astunparse->piglet-templates->piglet) (1.16.0)\n","Installing collected packages: piglet-templates, piglet\n","Successfully installed piglet-1.0.0 piglet-templates-1.3.0\n"]}],"source":["!apt install xvfb -y\n","!pip install pyvirtualdisplay\n","!pip install piglet"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14664,"status":"ok","timestamp":1698934290743,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-60},"id":"5OaWkBSmhm6R","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b8fa17fd-699f-467e-93c5-3e7f958588b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting supersuit\n","  Downloading SuperSuit-3.9.0-py3-none-any.whl (49 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/49.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m41.0/49.9 kB\u001b[0m \u001b[31m954.6 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 kB\u001b[0m \u001b[31m937.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from supersuit) (1.23.5)\n","Requirement already satisfied: gymnasium>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from supersuit) (0.29.1)\n","Collecting tinyscaler>=1.2.6 (from supersuit)\n","  Downloading tinyscaler-1.2.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (517 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.1/517.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (1.6.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (0.0.4)\n","Installing collected packages: tinyscaler, supersuit\n","Successfully installed supersuit-3.9.0 tinyscaler-1.2.7\n","Collecting stable_baselines3\n","  Downloading stable_baselines3-2.1.0-py3-none-any.whl (178 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (0.29.1)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.23.5)\n","Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.1.0+cu118)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.6.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.5.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (3.7.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (0.0.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.12.4)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2.1.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.1.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (4.43.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (23.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable_baselines3) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable_baselines3) (1.3.0)\n","Installing collected packages: stable_baselines3\n","Successfully installed stable_baselines3-2.1.0\n"]}],"source":["!pip install supersuit\n","!pip install stable_baselines3"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10451,"status":"ok","timestamp":1698934301146,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-60},"id":"thmOvcHdjKHw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"eb7528b4-0645-4985-d75c-37cdb217ef0c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting shimmy>=0.2.1\n","  Downloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from shimmy>=0.2.1) (1.23.5)\n","Requirement already satisfied: gymnasium>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from shimmy>=0.2.1) (0.29.1)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy>=0.2.1) (1.6.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy>=0.2.1) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy>=0.2.1) (0.0.4)\n","Installing collected packages: shimmy\n","Successfully installed shimmy-1.3.0\n"]}],"source":["!pip install 'shimmy>=0.2.1'"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":21574,"status":"ok","timestamp":1698934322675,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-60},"id":"k0iVvep_spQz","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fa25c58a-1939-4c3c-e690-dfa9d6420d4a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tf-agents-nightly\n","  Downloading tf_agents_nightly-0.19.0.dev20231010-py3-none-any.whl (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.4.0)\n","Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.6.0)\n","Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (0.5.0)\n","Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (0.17.3)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.23.5)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (9.4.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.16.0)\n","Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (3.20.3)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.14.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (4.5.0)\n","Collecting pygame==2.1.3 (from tf-agents-nightly)\n","  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tfp-nightly (from tf-agents-nightly)\n","  Downloading tfp_nightly-0.23.0.dev20231102-py2.py3-none-any.whl (6.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m111.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents-nightly) (1.11.3)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents-nightly) (1.5.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tfp-nightly->tf-agents-nightly) (4.4.2)\n","Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tfp-nightly->tf-agents-nightly) (0.5.4)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tfp-nightly->tf-agents-nightly) (0.1.8)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<=0.23.0,>=0.17.0->tf-agents-nightly) (0.18.3)\n","Installing collected packages: tfp-nightly, pygame, tf-agents-nightly\n","  Attempting uninstall: pygame\n","    Found existing installation: pygame 2.3.0\n","    Uninstalling pygame-2.3.0:\n","      Successfully uninstalled pygame-2.3.0\n","Successfully installed pygame-2.1.3 tf-agents-nightly-0.19.0.dev20231010 tfp-nightly-0.23.0.dev20231102\n"]}],"source":["!pip install tf-agents-nightly"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8696,"status":"ok","timestamp":1698934331322,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-60},"id":"UlXxViz9tdvH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3e81b40c-5329-4e09-d3b9-8e904c26341d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: tensorflow 2.8.0\n","Uninstalling tensorflow-2.8.0:\n","  Would remove:\n","    /usr/local/bin/estimator_ckpt_converter\n","    /usr/local/bin/import_pb_to_tensorboard\n","    /usr/local/bin/saved_model_cli\n","    /usr/local/bin/tensorboard\n","    /usr/local/bin/tf_upgrade_v2\n","    /usr/local/bin/tflite_convert\n","    /usr/local/bin/toco\n","    /usr/local/bin/toco_from_protos\n","    /usr/local/lib/python3.10/dist-packages/tensorflow-2.8.0.dist-info/*\n","    /usr/local/lib/python3.10/dist-packages/tensorflow/*\n","Proceed (Y/n)? Y\n","  Successfully uninstalled tensorflow-2.8.0\n"]}],"source":["!pip uninstall tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dsWlVQ6MtKLj"},"outputs":[],"source":["!pip install tensorflow>=2.13"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4525,"status":"ok","timestamp":1698934381751,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-60},"id":"wE5AiVtFtZDc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"53a74f8f-747a-4b56-9f18-1a16381a8da0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Name: tensorflow\n","Version: 2.14.0\n","Summary: TensorFlow is an open source machine learning framework for everyone.\n","Home-page: https://www.tensorflow.org/\n","Author: Google Inc.\n","Author-email: packages@tensorflow.org\n","License: Apache 2.0\n","Location: /usr/local/lib/python3.10/dist-packages\n","Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, libclang, ml-dtypes, numpy, opt-einsum, packaging, protobuf, setuptools, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n","Required-by: dopamine-rl, keras-rl2\n"]}],"source":["!pip show tensorflow"]},{"cell_type":"code","source":["!pip install pettingzoo[sisl]"],"metadata":{"id":"PZa1qybXZKSX","executionInfo":{"status":"ok","timestamp":1698934448626,"user_tz":-60,"elapsed":66884,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"cb974b4f-2a39-45ac-b611-9572d958017a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pettingzoo[sisl] in /usr/local/lib/python3.10/dist-packages (1.24.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[sisl]) (1.23.5)\n","Requirement already satisfied: gymnasium>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[sisl]) (0.29.1)\n","Collecting pygame==2.3.0 (from pettingzoo[sisl])\n","  Using cached pygame-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\n","Requirement already satisfied: pymunk==6.2.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[sisl]) (6.2.0)\n","Collecting box2d-py==2.3.5 (from pettingzoo[sisl])\n","  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[sisl]) (1.11.3)\n","Requirement already satisfied: cffi>1.14.0 in /usr/local/lib/python3.10/dist-packages (from pymunk==6.2.0->pettingzoo[sisl]) (1.16.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[sisl]) (1.6.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[sisl]) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[sisl]) (0.0.4)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>1.14.0->pymunk==6.2.0->pettingzoo[sisl]) (2.21)\n","Building wheels for collected packages: box2d-py\n","  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2373074 sha256=d8e5480ea93a52f06c5af80809b50e44da4d595f87b6845ef7a36d518734c333\n","  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n","Successfully built box2d-py\n","Installing collected packages: box2d-py, pygame\n","  Attempting uninstall: box2d-py\n","    Found existing installation: box2d-py 2.3.8\n","    Uninstalling box2d-py-2.3.8:\n","      Successfully uninstalled box2d-py-2.3.8\n","  Attempting uninstall: pygame\n","    Found existing installation: pygame 2.1.3\n","    Uninstalling pygame-2.1.3:\n","      Successfully uninstalled pygame-2.1.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tf-agents-nightly 0.19.0.dev20231010 requires pygame==2.1.3, but you have pygame 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed box2d-py-2.3.5 pygame-2.3.0\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"NFzawQ1QZFwj"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JpA_YhKzCeC2"},"outputs":[],"source":["############################# Código para entrenar Multiwalker ######################"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"llJXjwZQTVBe"},"outputs":[],"source":["########### Primero con 0.8 de recompensa grupal"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":556,"status":"ok","timestamp":1697649056527,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"6-kdhb3CI5VC","outputId":"c6003468-c14d-4ee8-e786-b1447befe97a"},"outputs":[{"output_type":"stream","name":"stdout","text":["ls: cannot open directory '.': Transport endpoint is not connected\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":319,"status":"ok","timestamp":1696848224082,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"04CbnRTvI9L2","outputId":"7e3b8b24-a1bb-49f2-e4da-e8287d01683e"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/TFM\n"]}],"source":["cd /content/drive/MyDrive/TFM"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1696848225257,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"bUEH8254I_kb","outputId":"00564d52-42b1-49fd-b614-1e041a847e34"},"outputs":[{"name":"stdout","output_type":"stream","text":["'=2.13'\t\t\t\t     policy3_log_eval\n"," Atari_TFM.ipynb\t\t     policy_eval.zip\n","'Entrenamientos antiguos sin logs'   policy_log_eval\n"," Entrenamientos_log_no_eval\t     policy_new2_log_eval\n"," MCR_TFM.ipynb\t\t\t     policy_new_log_eval\n"," multi_car_racing\t\t     TFM_new_pettingzoo_gym_cap.ipynb\n"," og_multi_car_racing\t\t     TFM_pettingzoo_gym_cap.ipynb\n"," policy2_log_eval\n"]}],"source":["!ls"]},{"cell_type":"code","source":["############# PPO multiwalker"],"metadata":{"id":"I0Q7lFc3xn-c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3 import PPO\n","# from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n","from pettingzoo.sisl import multiwalker_v9\n","import supersuit as ss\n","from stable_baselines3.common.monitor import Monitor"],"metadata":{"id":"7QHhIWDlxxi4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env = multiwalker_v9.parallel_env(render_mode=\"rgb_array\",shared_reward=0.8)\n","\n","# #---------------------------------\n","# log_dir = \"./policy_log\"\n","# os.makedirs(log_dir, exist_ok=True)\n","# env = Monitor(env, log_dir)\n","# #---------------------------------\n","# env = ss.color_reduction_v0(env, mode='B')\n","# env = ss.black_death_v3(env)\n","# env = ss.resize_v1(env, x_size=84, y_size=84)\n","env = ss.frame_stack_v1(env, 3)\n","env = ss.pettingzoo_env_to_vec_env_v1(env)\n","env = ss.concat_vec_envs_v1(env, 8, num_cpus=4, base_class='stable_baselines3')"],"metadata":{"id":"mOueaPEHx6iN"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H0lKH0t2kIP9"},"outputs":[],"source":["from stable_baselines3.common.callbacks import EvalCallback\n","eval_callback = EvalCallback(env, best_model_save_path=\"./multiwalker_ppo_rew_08_log_eval/\",\n","                             log_path=\"./multiwalker_ppo_rew_08_log_eval/\", eval_freq=500,\n","                             deterministic=True, render=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F9i2kMahkL8M","outputId":"37742869-e94f-4e1d-ac83-baa3038cd9ab","executionInfo":{"status":"ok","timestamp":1698840264831,"user_tz":-60,"elapsed":5920674,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Logging to ./multiwalker_ppo_rew_08_log_eval/\n","Using cuda device\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 7.21       |\n","| time/                   |            |\n","|    total_timesteps      | 2832000    |\n","| train/                  |            |\n","|    approx_kl            | 0.00268746 |\n","|    clip_fraction        | 0.13       |\n","|    clip_range           | 0.1        |\n","|    entropy_loss         | -9.95      |\n","|    explained_variance   | 0.509      |\n","|    learning_rate        | 0.000622   |\n","|    loss                 | 0.0111     |\n","|    n_updates            | 1150       |\n","|    policy_gradient_loss | -0.00272   |\n","|    std                  | 2.93       |\n","|    value_loss           | 9.93       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 231     |\n","|    time_elapsed    | 3355    |\n","|    total_timesteps | 2838528 |\n","--------------------------------\n","Eval num_timesteps=2844000, episode_reward=4.80 +/- 0.05\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 4.8          |\n","| time/                   |              |\n","|    total_timesteps      | 2844000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0023884552 |\n","|    clip_fraction        | 0.104        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -9.98        |\n","|    explained_variance   | -0.651       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0897      |\n","|    n_updates            | 1155         |\n","|    policy_gradient_loss | -0.00345     |\n","|    std                  | 2.94         |\n","|    value_loss           | 0.621        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 232     |\n","|    time_elapsed    | 3368    |\n","|    total_timesteps | 2850816 |\n","--------------------------------\n","Eval num_timesteps=2856000, episode_reward=7.15 +/- 1.70\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 7.15         |\n","| time/                   |              |\n","|    total_timesteps      | 2856000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0025124217 |\n","|    clip_fraction        | 0.11         |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -9.99        |\n","|    explained_variance   | 0.0159       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0896      |\n","|    n_updates            | 1160         |\n","|    policy_gradient_loss | -0.00344     |\n","|    std                  | 2.95         |\n","|    value_loss           | 0.256        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 233     |\n","|    time_elapsed    | 3382    |\n","|    total_timesteps | 2863104 |\n","--------------------------------\n","Eval num_timesteps=2868000, episode_reward=8.53 +/- 2.37\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 8.53         |\n","| time/                   |              |\n","|    total_timesteps      | 2868000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021447272 |\n","|    clip_fraction        | 0.108        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10          |\n","|    explained_variance   | 0.00418      |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0908      |\n","|    n_updates            | 1165         |\n","|    policy_gradient_loss | -0.00332     |\n","|    std                  | 2.96         |\n","|    value_loss           | 0.229        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 234     |\n","|    time_elapsed    | 3398    |\n","|    total_timesteps | 2875392 |\n","--------------------------------\n","Eval num_timesteps=2880000, episode_reward=8.97 +/- 1.11\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 8.97         |\n","| time/                   |              |\n","|    total_timesteps      | 2880000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019568617 |\n","|    clip_fraction        | 0.0884       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10          |\n","|    explained_variance   | 0.159        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.103       |\n","|    n_updates            | 1170         |\n","|    policy_gradient_loss | -0.00403     |\n","|    std                  | 2.96         |\n","|    value_loss           | 0.21         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 235     |\n","|    time_elapsed    | 3413    |\n","|    total_timesteps | 2887680 |\n","--------------------------------\n","Eval num_timesteps=2892000, episode_reward=10.87 +/- 3.19\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 10.9        |\n","| time/                   |             |\n","|    total_timesteps      | 2892000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002035681 |\n","|    clip_fraction        | 0.104       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10         |\n","|    explained_variance   | -0.0337     |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0984     |\n","|    n_updates            | 1175        |\n","|    policy_gradient_loss | -0.00382    |\n","|    std                  | 2.97        |\n","|    value_loss           | 0.271       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 236     |\n","|    time_elapsed    | 3429    |\n","|    total_timesteps | 2899968 |\n","--------------------------------\n","Eval num_timesteps=2904000, episode_reward=9.95 +/- 1.65\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 9.95         |\n","| time/                   |              |\n","|    total_timesteps      | 2904000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019413804 |\n","|    clip_fraction        | 0.0983       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10          |\n","|    explained_variance   | 0.255        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0985      |\n","|    n_updates            | 1180         |\n","|    policy_gradient_loss | -0.0042      |\n","|    std                  | 2.97         |\n","|    value_loss           | 0.215        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 237     |\n","|    time_elapsed    | 3444    |\n","|    total_timesteps | 2912256 |\n","--------------------------------\n","Eval num_timesteps=2916000, episode_reward=8.97 +/- 5.63\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 8.97         |\n","| time/                   |              |\n","|    total_timesteps      | 2916000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020976092 |\n","|    clip_fraction        | 0.0972       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10          |\n","|    explained_variance   | 0.2          |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0964      |\n","|    n_updates            | 1185         |\n","|    policy_gradient_loss | -0.00414     |\n","|    std                  | 2.99         |\n","|    value_loss           | 0.24         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 238     |\n","|    time_elapsed    | 3456    |\n","|    total_timesteps | 2924544 |\n","--------------------------------\n","Eval num_timesteps=2928000, episode_reward=15.07 +/- 3.91\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 15.1        |\n","| time/                   |             |\n","|    total_timesteps      | 2928000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002264038 |\n","|    clip_fraction        | 0.119       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10.1       |\n","|    explained_variance   | 0.393       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 0.167       |\n","|    n_updates            | 1190        |\n","|    policy_gradient_loss | -0.00405    |\n","|    std                  | 3.02        |\n","|    value_loss           | 10.4        |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 239     |\n","|    time_elapsed    | 3472    |\n","|    total_timesteps | 2936832 |\n","--------------------------------\n","Eval num_timesteps=2940000, episode_reward=-47.01 +/- 49.12\n","Episode length: 231.80 +/- 218.98\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 232         |\n","|    mean_reward          | -47         |\n","| time/                   |             |\n","|    total_timesteps      | 2940000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002002411 |\n","|    clip_fraction        | 0.0985      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10.1       |\n","|    explained_variance   | -0.308      |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0983     |\n","|    n_updates            | 1195        |\n","|    policy_gradient_loss | -0.00371    |\n","|    std                  | 3.02        |\n","|    value_loss           | 0.259       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 240     |\n","|    time_elapsed    | 3486    |\n","|    total_timesteps | 2949120 |\n","--------------------------------\n","Eval num_timesteps=2952000, episode_reward=11.24 +/- 2.40\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 11.2        |\n","| time/                   |             |\n","|    total_timesteps      | 2952000     |\n","| train/                  |             |\n","|    approx_kl            | 0.001797222 |\n","|    clip_fraction        | 0.0878      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10.1       |\n","|    explained_variance   | -0.0517     |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.1        |\n","|    n_updates            | 1200        |\n","|    policy_gradient_loss | -0.00391    |\n","|    std                  | 3.03        |\n","|    value_loss           | 0.285       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 241     |\n","|    time_elapsed    | 3499    |\n","|    total_timesteps | 2961408 |\n","--------------------------------\n","Eval num_timesteps=2964000, episode_reward=7.41 +/- 0.40\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 7.41         |\n","| time/                   |              |\n","|    total_timesteps      | 2964000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019875618 |\n","|    clip_fraction        | 0.0821       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.1        |\n","|    explained_variance   | 0.0843       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0981      |\n","|    n_updates            | 1205         |\n","|    policy_gradient_loss | -0.00334     |\n","|    std                  | 3.03         |\n","|    value_loss           | 0.228        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 242     |\n","|    time_elapsed    | 3515    |\n","|    total_timesteps | 2973696 |\n","--------------------------------\n","Eval num_timesteps=2976000, episode_reward=11.83 +/- 0.66\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 11.8        |\n","| time/                   |             |\n","|    total_timesteps      | 2976000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002129491 |\n","|    clip_fraction        | 0.1         |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10.1       |\n","|    explained_variance   | -0.0352     |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0951     |\n","|    n_updates            | 1210        |\n","|    policy_gradient_loss | -0.00415    |\n","|    std                  | 3.03        |\n","|    value_loss           | 0.388       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 243     |\n","|    time_elapsed    | 3527    |\n","|    total_timesteps | 2985984 |\n","--------------------------------\n","Eval num_timesteps=2988000, episode_reward=-72.79 +/- 1.65\n","Episode length: 433.00 +/- 29.39\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 433          |\n","|    mean_reward          | -72.8        |\n","| time/                   |              |\n","|    total_timesteps      | 2988000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022195643 |\n","|    clip_fraction        | 0.101        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.1        |\n","|    explained_variance   | 0.138        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0953      |\n","|    n_updates            | 1215         |\n","|    policy_gradient_loss | -0.00316     |\n","|    std                  | 3.03         |\n","|    value_loss           | 0.231        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 244     |\n","|    time_elapsed    | 3541    |\n","|    total_timesteps | 2998272 |\n","--------------------------------\n","Eval num_timesteps=3000000, episode_reward=-26.02 +/- 47.84\n","Episode length: 339.60 +/- 196.45\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 340          |\n","|    mean_reward          | -26          |\n","| time/                   |              |\n","|    total_timesteps      | 3000000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022217664 |\n","|    clip_fraction        | 0.0933       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.1        |\n","|    explained_variance   | 0.118        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0919      |\n","|    n_updates            | 1220         |\n","|    policy_gradient_loss | -0.0033      |\n","|    std                  | 3.05         |\n","|    value_loss           | 0.356        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 245     |\n","|    time_elapsed    | 3557    |\n","|    total_timesteps | 3010560 |\n","--------------------------------\n","Eval num_timesteps=3012000, episode_reward=12.26 +/- 0.41\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 12.3         |\n","| time/                   |              |\n","|    total_timesteps      | 3012000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020549165 |\n","|    clip_fraction        | 0.104        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.1        |\n","|    explained_variance   | 0.157        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.102       |\n","|    n_updates            | 1225         |\n","|    policy_gradient_loss | -0.00341     |\n","|    std                  | 3.07         |\n","|    value_loss           | 0.156        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 246     |\n","|    time_elapsed    | 3569    |\n","|    total_timesteps | 3022848 |\n","--------------------------------\n","Eval num_timesteps=3024000, episode_reward=-39.59 +/- 41.07\n","Episode length: 302.00 +/- 161.67\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 302         |\n","|    mean_reward          | -39.6       |\n","| time/                   |             |\n","|    total_timesteps      | 3024000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002678683 |\n","|    clip_fraction        | 0.12        |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10.2       |\n","|    explained_variance   | 0.0163      |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 0.335       |\n","|    n_updates            | 1230        |\n","|    policy_gradient_loss | -0.00243    |\n","|    std                  | 3.09        |\n","|    value_loss           | 10.4        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 247     |\n","|    time_elapsed    | 3584    |\n","|    total_timesteps | 3035136 |\n","--------------------------------\n","Eval num_timesteps=3036000, episode_reward=14.64 +/- 4.97\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 14.6         |\n","| time/                   |              |\n","|    total_timesteps      | 3036000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0029449342 |\n","|    clip_fraction        | 0.116        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.2        |\n","|    explained_variance   | 0.573        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.569        |\n","|    n_updates            | 1235         |\n","|    policy_gradient_loss | -0.00321     |\n","|    std                  | 3.12         |\n","|    value_loss           | 12           |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 248     |\n","|    time_elapsed    | 3599    |\n","|    total_timesteps | 3047424 |\n","--------------------------------\n","Eval num_timesteps=3048000, episode_reward=13.50 +/- 0.34\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 13.5         |\n","| time/                   |              |\n","|    total_timesteps      | 3048000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0026085887 |\n","|    clip_fraction        | 0.134        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.2        |\n","|    explained_variance   | 0.396        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.169        |\n","|    n_updates            | 1240         |\n","|    policy_gradient_loss | -0.00324     |\n","|    std                  | 3.14         |\n","|    value_loss           | 10.2         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 847     |\n","|    iterations      | 249     |\n","|    time_elapsed    | 3611    |\n","|    total_timesteps | 3059712 |\n","--------------------------------\n","Eval num_timesteps=3060000, episode_reward=15.96 +/- 1.64\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 16           |\n","| time/                   |              |\n","|    total_timesteps      | 3060000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019350345 |\n","|    clip_fraction        | 0.0855       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.2        |\n","|    explained_variance   | -0.462       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0983      |\n","|    n_updates            | 1245         |\n","|    policy_gradient_loss | -0.00335     |\n","|    std                  | 3.14         |\n","|    value_loss           | 0.417        |\n","------------------------------------------\n","New best mean reward!\n","Eval num_timesteps=3072000, episode_reward=15.27 +/- 0.88\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 15.3     |\n","| time/              |          |\n","|    total_timesteps | 3072000  |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 250     |\n","|    time_elapsed    | 3632    |\n","|    total_timesteps | 3072000 |\n","--------------------------------\n","Eval num_timesteps=3084000, episode_reward=22.52 +/- 1.39\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 22.5         |\n","| time/                   |              |\n","|    total_timesteps      | 3084000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0025983287 |\n","|    clip_fraction        | 0.132        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.3        |\n","|    explained_variance   | -0.45        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0948      |\n","|    n_updates            | 1250         |\n","|    policy_gradient_loss | -0.00331     |\n","|    std                  | 3.15         |\n","|    value_loss           | 0.503        |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 251     |\n","|    time_elapsed    | 3648    |\n","|    total_timesteps | 3084288 |\n","--------------------------------\n","Eval num_timesteps=3096000, episode_reward=17.22 +/- 0.07\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 17.2         |\n","| time/                   |              |\n","|    total_timesteps      | 3096000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021137837 |\n","|    clip_fraction        | 0.0948       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.3        |\n","|    explained_variance   | 0.102        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0912      |\n","|    n_updates            | 1255         |\n","|    policy_gradient_loss | -0.00466     |\n","|    std                  | 3.16         |\n","|    value_loss           | 0.323        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 252     |\n","|    time_elapsed    | 3664    |\n","|    total_timesteps | 3096576 |\n","--------------------------------\n","Eval num_timesteps=3108000, episode_reward=15.00 +/- 3.70\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 15           |\n","| time/                   |              |\n","|    total_timesteps      | 3108000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022678438 |\n","|    clip_fraction        | 0.1          |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.3        |\n","|    explained_variance   | 0.19         |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.1         |\n","|    n_updates            | 1260         |\n","|    policy_gradient_loss | -0.00392     |\n","|    std                  | 3.17         |\n","|    value_loss           | 0.232        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 844     |\n","|    iterations      | 253     |\n","|    time_elapsed    | 3679    |\n","|    total_timesteps | 3108864 |\n","--------------------------------\n","Eval num_timesteps=3120000, episode_reward=18.14 +/- 1.05\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 18.1         |\n","| time/                   |              |\n","|    total_timesteps      | 3120000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021547338 |\n","|    clip_fraction        | 0.112        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.3        |\n","|    explained_variance   | 0.188        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0995      |\n","|    n_updates            | 1265         |\n","|    policy_gradient_loss | -0.00375     |\n","|    std                  | 3.19         |\n","|    value_loss           | 0.231        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 844     |\n","|    iterations      | 254     |\n","|    time_elapsed    | 3695    |\n","|    total_timesteps | 3121152 |\n","--------------------------------\n","Eval num_timesteps=3132000, episode_reward=15.75 +/- 1.09\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 15.8        |\n","| time/                   |             |\n","|    total_timesteps      | 3132000     |\n","| train/                  |             |\n","|    approx_kl            | 0.001958892 |\n","|    clip_fraction        | 0.0712      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10.3       |\n","|    explained_variance   | -0.598      |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0911     |\n","|    n_updates            | 1270        |\n","|    policy_gradient_loss | -0.00306    |\n","|    std                  | 3.22        |\n","|    value_loss           | 0.335       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 255     |\n","|    time_elapsed    | 3707    |\n","|    total_timesteps | 3133440 |\n","--------------------------------\n","Eval num_timesteps=3144000, episode_reward=16.70 +/- 1.78\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 16.7         |\n","| time/                   |              |\n","|    total_timesteps      | 3144000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018599886 |\n","|    clip_fraction        | 0.0826       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.3        |\n","|    explained_variance   | 0.2          |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0994      |\n","|    n_updates            | 1275         |\n","|    policy_gradient_loss | -0.00328     |\n","|    std                  | 3.22         |\n","|    value_loss           | 0.246        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 256     |\n","|    time_elapsed    | 3722    |\n","|    total_timesteps | 3145728 |\n","--------------------------------\n","Eval num_timesteps=3156000, episode_reward=-23.52 +/- 37.79\n","Episode length: 460.00 +/- 48.99\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 460          |\n","|    mean_reward          | -23.5        |\n","| time/                   |              |\n","|    total_timesteps      | 3156000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022627546 |\n","|    clip_fraction        | 0.104        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.4        |\n","|    explained_variance   | 0.241        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.104       |\n","|    n_updates            | 1280         |\n","|    policy_gradient_loss | -0.00383     |\n","|    std                  | 3.24         |\n","|    value_loss           | 0.217        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 844     |\n","|    iterations      | 257     |\n","|    time_elapsed    | 3737    |\n","|    total_timesteps | 3158016 |\n","--------------------------------\n","Eval num_timesteps=3168000, episode_reward=15.76 +/- 0.01\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 15.8         |\n","| time/                   |              |\n","|    total_timesteps      | 3168000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018772759 |\n","|    clip_fraction        | 0.0915       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.4        |\n","|    explained_variance   | 0.169        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0996      |\n","|    n_updates            | 1285         |\n","|    policy_gradient_loss | -0.00369     |\n","|    std                  | 3.25         |\n","|    value_loss           | 0.251        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 258     |\n","|    time_elapsed    | 3749    |\n","|    total_timesteps | 3170304 |\n","--------------------------------\n","Eval num_timesteps=3180000, episode_reward=24.31 +/- 1.48\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 24.3         |\n","| time/                   |              |\n","|    total_timesteps      | 3180000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0023964513 |\n","|    clip_fraction        | 0.106        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.4        |\n","|    explained_variance   | 0.163        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0996      |\n","|    n_updates            | 1290         |\n","|    policy_gradient_loss | -0.00346     |\n","|    std                  | 3.27         |\n","|    value_loss           | 0.225        |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 259     |\n","|    time_elapsed    | 3765    |\n","|    total_timesteps | 3182592 |\n","--------------------------------\n","Eval num_timesteps=3192000, episode_reward=14.17 +/- 0.63\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 14.2         |\n","| time/                   |              |\n","|    total_timesteps      | 3192000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0023625898 |\n","|    clip_fraction        | 0.109        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.4        |\n","|    explained_variance   | 0.215        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.102       |\n","|    n_updates            | 1295         |\n","|    policy_gradient_loss | -0.00348     |\n","|    std                  | 3.29         |\n","|    value_loss           | 0.251        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 260     |\n","|    time_elapsed    | 3779    |\n","|    total_timesteps | 3194880 |\n","--------------------------------\n","Eval num_timesteps=3204000, episode_reward=17.45 +/- 1.86\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 17.5         |\n","| time/                   |              |\n","|    total_timesteps      | 3204000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0026541918 |\n","|    clip_fraction        | 0.0959       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.5        |\n","|    explained_variance   | 0.312        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0984      |\n","|    n_updates            | 1300         |\n","|    policy_gradient_loss | -0.00366     |\n","|    std                  | 3.31         |\n","|    value_loss           | 0.254        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 261     |\n","|    time_elapsed    | 3792    |\n","|    total_timesteps | 3207168 |\n","--------------------------------\n","Eval num_timesteps=3216000, episode_reward=20.87 +/- 1.38\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 20.9         |\n","| time/                   |              |\n","|    total_timesteps      | 3216000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018947427 |\n","|    clip_fraction        | 0.0935       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.5        |\n","|    explained_variance   | 0.254        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.103       |\n","|    n_updates            | 1305         |\n","|    policy_gradient_loss | -0.00253     |\n","|    std                  | 3.32         |\n","|    value_loss           | 0.217        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 262     |\n","|    time_elapsed    | 3808    |\n","|    total_timesteps | 3219456 |\n","--------------------------------\n","Eval num_timesteps=3228000, episode_reward=18.09 +/- 2.08\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 18.1         |\n","| time/                   |              |\n","|    total_timesteps      | 3228000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0026485051 |\n","|    clip_fraction        | 0.156        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.5        |\n","|    explained_variance   | 0.0825       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.0336       |\n","|    n_updates            | 1310         |\n","|    policy_gradient_loss | -0.00268     |\n","|    std                  | 3.35         |\n","|    value_loss           | 11.5         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 263     |\n","|    time_elapsed    | 3821    |\n","|    total_timesteps | 3231744 |\n","--------------------------------\n","Eval num_timesteps=3240000, episode_reward=-33.52 +/- 45.19\n","Episode length: 405.20 +/- 77.40\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 405          |\n","|    mean_reward          | -33.5        |\n","| time/                   |              |\n","|    total_timesteps      | 3240000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022309683 |\n","|    clip_fraction        | 0.11         |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.5        |\n","|    explained_variance   | 0.0952       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.103       |\n","|    n_updates            | 1315         |\n","|    policy_gradient_loss | -0.00312     |\n","|    std                  | 3.36         |\n","|    value_loss           | 0.288        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 264     |\n","|    time_elapsed    | 3835    |\n","|    total_timesteps | 3244032 |\n","--------------------------------\n","Eval num_timesteps=3252000, episode_reward=24.34 +/- 1.15\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 24.3        |\n","| time/                   |             |\n","|    total_timesteps      | 3252000     |\n","| train/                  |             |\n","|    approx_kl            | 0.001987134 |\n","|    clip_fraction        | 0.0958      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10.5       |\n","|    explained_variance   | 0.19        |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0978     |\n","|    n_updates            | 1320        |\n","|    policy_gradient_loss | -0.00316    |\n","|    std                  | 3.37        |\n","|    value_loss           | 0.225       |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 265     |\n","|    time_elapsed    | 3851    |\n","|    total_timesteps | 3256320 |\n","--------------------------------\n","Eval num_timesteps=3264000, episode_reward=17.64 +/- 2.95\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 17.6         |\n","| time/                   |              |\n","|    total_timesteps      | 3264000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019088559 |\n","|    clip_fraction        | 0.0905       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.6        |\n","|    explained_variance   | 0.297        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.104       |\n","|    n_updates            | 1325         |\n","|    policy_gradient_loss | -0.00415     |\n","|    std                  | 3.39         |\n","|    value_loss           | 0.254        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 266     |\n","|    time_elapsed    | 3863    |\n","|    total_timesteps | 3268608 |\n","--------------------------------\n","Eval num_timesteps=3276000, episode_reward=17.42 +/- 2.32\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 17.4         |\n","| time/                   |              |\n","|    total_timesteps      | 3276000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0028254117 |\n","|    clip_fraction        | 0.115        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.6        |\n","|    explained_variance   | 0.0227       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.344        |\n","|    n_updates            | 1330         |\n","|    policy_gradient_loss | -0.00278     |\n","|    std                  | 3.43         |\n","|    value_loss           | 15.9         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 267     |\n","|    time_elapsed    | 3877    |\n","|    total_timesteps | 3280896 |\n","--------------------------------\n","Eval num_timesteps=3288000, episode_reward=14.11 +/- 2.86\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 14.1       |\n","| time/                   |            |\n","|    total_timesteps      | 3288000    |\n","| train/                  |            |\n","|    approx_kl            | 0.00191541 |\n","|    clip_fraction        | 0.0892     |\n","|    clip_range           | 0.1        |\n","|    entropy_loss         | -10.6      |\n","|    explained_variance   | 0.0306     |\n","|    learning_rate        | 0.000622   |\n","|    loss                 | -0.103     |\n","|    n_updates            | 1335       |\n","|    policy_gradient_loss | -0.00438   |\n","|    std                  | 3.43       |\n","|    value_loss           | 0.359      |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 268     |\n","|    time_elapsed    | 3893    |\n","|    total_timesteps | 3293184 |\n","--------------------------------\n","Eval num_timesteps=3300000, episode_reward=23.56 +/- 0.84\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 23.6         |\n","| time/                   |              |\n","|    total_timesteps      | 3300000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018232175 |\n","|    clip_fraction        | 0.0927       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.6        |\n","|    explained_variance   | 0.465        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.102       |\n","|    n_updates            | 1340         |\n","|    policy_gradient_loss | -0.00355     |\n","|    std                  | 3.44         |\n","|    value_loss           | 0.221        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 269     |\n","|    time_elapsed    | 3907    |\n","|    total_timesteps | 3305472 |\n","--------------------------------\n","Eval num_timesteps=3312000, episode_reward=18.66 +/- 1.01\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 18.7       |\n","| time/                   |            |\n","|    total_timesteps      | 3312000    |\n","| train/                  |            |\n","|    approx_kl            | 0.00174373 |\n","|    clip_fraction        | 0.0943     |\n","|    clip_range           | 0.1        |\n","|    entropy_loss         | -10.6      |\n","|    explained_variance   | 0.572      |\n","|    learning_rate        | 0.000622   |\n","|    loss                 | -0.105     |\n","|    n_updates            | 1345       |\n","|    policy_gradient_loss | -0.00325   |\n","|    std                  | 3.45       |\n","|    value_loss           | 0.205      |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 270     |\n","|    time_elapsed    | 3924    |\n","|    total_timesteps | 3317760 |\n","--------------------------------\n","Eval num_timesteps=3324000, episode_reward=16.57 +/- 0.15\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 16.6         |\n","| time/                   |              |\n","|    total_timesteps      | 3324000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0017512661 |\n","|    clip_fraction        | 0.0859       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.6        |\n","|    explained_variance   | 0.458        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0993      |\n","|    n_updates            | 1350         |\n","|    policy_gradient_loss | -0.00315     |\n","|    std                  | 3.46         |\n","|    value_loss           | 0.235        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 271     |\n","|    time_elapsed    | 3938    |\n","|    total_timesteps | 3330048 |\n","--------------------------------\n","Eval num_timesteps=3336000, episode_reward=12.15 +/- 0.47\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 12.2         |\n","| time/                   |              |\n","|    total_timesteps      | 3336000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019289497 |\n","|    clip_fraction        | 0.0882       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.7        |\n","|    explained_variance   | 0.616        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.108       |\n","|    n_updates            | 1355         |\n","|    policy_gradient_loss | -0.00418     |\n","|    std                  | 3.47         |\n","|    value_loss           | 0.192        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 272     |\n","|    time_elapsed    | 3951    |\n","|    total_timesteps | 3342336 |\n","--------------------------------\n","Eval num_timesteps=3348000, episode_reward=18.35 +/- 1.98\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 18.4        |\n","| time/                   |             |\n","|    total_timesteps      | 3348000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002082195 |\n","|    clip_fraction        | 0.0931      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10.7       |\n","|    explained_variance   | 0.527       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.107      |\n","|    n_updates            | 1360        |\n","|    policy_gradient_loss | -0.00408    |\n","|    std                  | 3.48        |\n","|    value_loss           | 0.215       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 273     |\n","|    time_elapsed    | 3967    |\n","|    total_timesteps | 3354624 |\n","--------------------------------\n","Eval num_timesteps=3360000, episode_reward=20.50 +/- 4.11\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 20.5         |\n","| time/                   |              |\n","|    total_timesteps      | 3360000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0029690647 |\n","|    clip_fraction        | 0.129        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.7        |\n","|    explained_variance   | 0.0349       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.65         |\n","|    n_updates            | 1365         |\n","|    policy_gradient_loss | -0.00274     |\n","|    std                  | 3.51         |\n","|    value_loss           | 11.6         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 274     |\n","|    time_elapsed    | 3980    |\n","|    total_timesteps | 3366912 |\n","--------------------------------\n","Eval num_timesteps=3372000, episode_reward=21.94 +/- 0.50\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 21.9        |\n","| time/                   |             |\n","|    total_timesteps      | 3372000     |\n","| train/                  |             |\n","|    approx_kl            | 0.001843959 |\n","|    clip_fraction        | 0.0826      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10.7       |\n","|    explained_variance   | -0.378      |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0965     |\n","|    n_updates            | 1370        |\n","|    policy_gradient_loss | -0.00357    |\n","|    std                  | 3.52        |\n","|    value_loss           | 0.546       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 275     |\n","|    time_elapsed    | 3994    |\n","|    total_timesteps | 3379200 |\n","--------------------------------\n","Eval num_timesteps=3384000, episode_reward=22.94 +/- 2.79\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 22.9         |\n","| time/                   |              |\n","|    total_timesteps      | 3384000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0028670803 |\n","|    clip_fraction        | 0.117        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.7        |\n","|    explained_variance   | 0.418        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 2.91         |\n","|    n_updates            | 1375         |\n","|    policy_gradient_loss | -0.00383     |\n","|    std                  | 3.57         |\n","|    value_loss           | 19.6         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 276     |\n","|    time_elapsed    | 4010    |\n","|    total_timesteps | 3391488 |\n","--------------------------------\n","Eval num_timesteps=3396000, episode_reward=22.76 +/- 0.52\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 22.8         |\n","| time/                   |              |\n","|    total_timesteps      | 3396000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0017963607 |\n","|    clip_fraction        | 0.0953       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.8        |\n","|    explained_variance   | 0.543        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.209        |\n","|    n_updates            | 1380         |\n","|    policy_gradient_loss | -0.00346     |\n","|    std                  | 3.57         |\n","|    value_loss           | 12.1         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 277     |\n","|    time_elapsed    | 4023    |\n","|    total_timesteps | 3403776 |\n","--------------------------------\n","Eval num_timesteps=3408000, episode_reward=25.48 +/- 0.58\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 25.5        |\n","| time/                   |             |\n","|    total_timesteps      | 3408000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002286528 |\n","|    clip_fraction        | 0.1         |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10.8       |\n","|    explained_variance   | -0.227      |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0972     |\n","|    n_updates            | 1385        |\n","|    policy_gradient_loss | -0.00521    |\n","|    std                  | 3.55        |\n","|    value_loss           | 0.603       |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 278     |\n","|    time_elapsed    | 4037    |\n","|    total_timesteps | 3416064 |\n","--------------------------------\n","Eval num_timesteps=3420000, episode_reward=0.40 +/- 43.37\n","Episode length: 462.00 +/- 46.54\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 462          |\n","|    mean_reward          | 0.404        |\n","| time/                   |              |\n","|    total_timesteps      | 3420000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0025818981 |\n","|    clip_fraction        | 0.112        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.8        |\n","|    explained_variance   | 0.133        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.103       |\n","|    n_updates            | 1390         |\n","|    policy_gradient_loss | -0.00522     |\n","|    std                  | 3.56         |\n","|    value_loss           | 0.353        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 279     |\n","|    time_elapsed    | 4053    |\n","|    total_timesteps | 3428352 |\n","--------------------------------\n","Eval num_timesteps=3432000, episode_reward=27.44 +/- 1.48\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 27.4         |\n","| time/                   |              |\n","|    total_timesteps      | 3432000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0025328808 |\n","|    clip_fraction        | 0.108        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.8        |\n","|    explained_variance   | 0.436        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0987      |\n","|    n_updates            | 1395         |\n","|    policy_gradient_loss | -0.00435     |\n","|    std                  | 3.57         |\n","|    value_loss           | 0.322        |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 280     |\n","|    time_elapsed    | 4064    |\n","|    total_timesteps | 3440640 |\n","--------------------------------\n","Eval num_timesteps=3444000, episode_reward=30.29 +/- 0.53\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 30.3        |\n","| time/                   |             |\n","|    total_timesteps      | 3444000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002464125 |\n","|    clip_fraction        | 0.107       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10.8       |\n","|    explained_variance   | 0.48        |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.107      |\n","|    n_updates            | 1400        |\n","|    policy_gradient_loss | -0.00455    |\n","|    std                  | 3.58        |\n","|    value_loss           | 0.256       |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 281     |\n","|    time_elapsed    | 4080    |\n","|    total_timesteps | 3452928 |\n","--------------------------------\n","Eval num_timesteps=3456000, episode_reward=29.72 +/- 0.52\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 29.7         |\n","| time/                   |              |\n","|    total_timesteps      | 3456000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018016166 |\n","|    clip_fraction        | 0.0809       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.8        |\n","|    explained_variance   | 0.51         |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0997      |\n","|    n_updates            | 1405         |\n","|    policy_gradient_loss | -0.00352     |\n","|    std                  | 3.59         |\n","|    value_loss           | 0.246        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 282     |\n","|    time_elapsed    | 4094    |\n","|    total_timesteps | 3465216 |\n","--------------------------------\n","Eval num_timesteps=3468000, episode_reward=26.70 +/- 2.81\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 26.7         |\n","| time/                   |              |\n","|    total_timesteps      | 3468000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018899444 |\n","|    clip_fraction        | 0.0771       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.8        |\n","|    explained_variance   | 0.263        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.105       |\n","|    n_updates            | 1410         |\n","|    policy_gradient_loss | -0.00378     |\n","|    std                  | 3.6          |\n","|    value_loss           | 0.321        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 283     |\n","|    time_elapsed    | 4107    |\n","|    total_timesteps | 3477504 |\n","--------------------------------\n","Eval num_timesteps=3480000, episode_reward=31.57 +/- 2.99\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 31.6         |\n","| time/                   |              |\n","|    total_timesteps      | 3480000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021010807 |\n","|    clip_fraction        | 0.0914       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.8        |\n","|    explained_variance   | 0.647        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.101       |\n","|    n_updates            | 1415         |\n","|    policy_gradient_loss | -0.00294     |\n","|    std                  | 3.62         |\n","|    value_loss           | 0.213        |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 284     |\n","|    time_elapsed    | 4123    |\n","|    total_timesteps | 3489792 |\n","--------------------------------\n","Eval num_timesteps=3492000, episode_reward=24.10 +/- 5.29\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 24.1         |\n","| time/                   |              |\n","|    total_timesteps      | 3492000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020082106 |\n","|    clip_fraction        | 0.0878       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.8        |\n","|    explained_variance   | 0.613        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.102       |\n","|    n_updates            | 1420         |\n","|    policy_gradient_loss | -0.00375     |\n","|    std                  | 3.63         |\n","|    value_loss           | 0.231        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 285     |\n","|    time_elapsed    | 4138    |\n","|    total_timesteps | 3502080 |\n","--------------------------------\n","Eval num_timesteps=3504000, episode_reward=32.68 +/- 3.36\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 32.7         |\n","| time/                   |              |\n","|    total_timesteps      | 3504000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0034757166 |\n","|    clip_fraction        | 0.151        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.8        |\n","|    explained_variance   | 0.422        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.415        |\n","|    n_updates            | 1425         |\n","|    policy_gradient_loss | -0.00371     |\n","|    std                  | 3.67         |\n","|    value_loss           | 8.93         |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 286     |\n","|    time_elapsed    | 4153    |\n","|    total_timesteps | 3514368 |\n","--------------------------------\n","Eval num_timesteps=3516000, episode_reward=-17.39 +/- 39.35\n","Episode length: 442.40 +/- 47.03\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 442          |\n","|    mean_reward          | -17.4        |\n","| time/                   |              |\n","|    total_timesteps      | 3516000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0026924387 |\n","|    clip_fraction        | 0.133        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.9        |\n","|    explained_variance   | 0.44         |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.883        |\n","|    n_updates            | 1430         |\n","|    policy_gradient_loss | -0.00471     |\n","|    std                  | 3.7          |\n","|    value_loss           | 12.4         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 287     |\n","|    time_elapsed    | 4171    |\n","|    total_timesteps | 3526656 |\n","--------------------------------\n","Eval num_timesteps=3528000, episode_reward=-19.18 +/- 49.20\n","Episode length: 475.40 +/- 20.09\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 475          |\n","|    mean_reward          | -19.2        |\n","| time/                   |              |\n","|    total_timesteps      | 3528000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019100271 |\n","|    clip_fraction        | 0.0889       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.9        |\n","|    explained_variance   | -0.0338      |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.103       |\n","|    n_updates            | 1435         |\n","|    policy_gradient_loss | -0.0045      |\n","|    std                  | 3.72         |\n","|    value_loss           | 0.456        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 288     |\n","|    time_elapsed    | 4185    |\n","|    total_timesteps | 3538944 |\n","--------------------------------\n","Eval num_timesteps=3540000, episode_reward=40.91 +/- 1.77\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 40.9         |\n","| time/                   |              |\n","|    total_timesteps      | 3540000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0024660109 |\n","|    clip_fraction        | 0.132        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.9        |\n","|    explained_variance   | 0.422        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.671        |\n","|    n_updates            | 1440         |\n","|    policy_gradient_loss | -0.00367     |\n","|    std                  | 3.76         |\n","|    value_loss           | 12           |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 289     |\n","|    time_elapsed    | 4198    |\n","|    total_timesteps | 3551232 |\n","--------------------------------\n","Eval num_timesteps=3552000, episode_reward=27.90 +/- 4.55\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 27.9         |\n","| time/                   |              |\n","|    total_timesteps      | 3552000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021863456 |\n","|    clip_fraction        | 0.0978       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11          |\n","|    explained_variance   | -0.0227      |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.102       |\n","|    n_updates            | 1445         |\n","|    policy_gradient_loss | -0.00465     |\n","|    std                  | 3.76         |\n","|    value_loss           | 0.416        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 290     |\n","|    time_elapsed    | 4214    |\n","|    total_timesteps | 3563520 |\n","--------------------------------\n","Eval num_timesteps=3564000, episode_reward=30.41 +/- 5.99\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 30.4        |\n","| time/                   |             |\n","|    total_timesteps      | 3564000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002041574 |\n","|    clip_fraction        | 0.107       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11         |\n","|    explained_variance   | 0.452       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.106      |\n","|    n_updates            | 1450        |\n","|    policy_gradient_loss | -0.00422    |\n","|    std                  | 3.77        |\n","|    value_loss           | 0.276       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 291     |\n","|    time_elapsed    | 4226    |\n","|    total_timesteps | 3575808 |\n","--------------------------------\n","Eval num_timesteps=3576000, episode_reward=1.12 +/- 37.09\n","Episode length: 482.00 +/- 22.05\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 482          |\n","|    mean_reward          | 1.12         |\n","| time/                   |              |\n","|    total_timesteps      | 3576000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0024516203 |\n","|    clip_fraction        | 0.114        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11          |\n","|    explained_variance   | 0.457        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.104       |\n","|    n_updates            | 1455         |\n","|    policy_gradient_loss | -0.0047      |\n","|    std                  | 3.78         |\n","|    value_loss           | 0.318        |\n","------------------------------------------\n","Eval num_timesteps=3588000, episode_reward=-24.23 +/- 48.72\n","Episode length: 462.80 +/- 30.37\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 463      |\n","|    mean_reward     | -24.2    |\n","| time/              |          |\n","|    total_timesteps | 3588000  |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 844     |\n","|    iterations      | 292     |\n","|    time_elapsed    | 4246    |\n","|    total_timesteps | 3588096 |\n","--------------------------------\n","Eval num_timesteps=3600000, episode_reward=33.93 +/- 8.53\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 33.9         |\n","| time/                   |              |\n","|    total_timesteps      | 3600000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021413935 |\n","|    clip_fraction        | 0.105        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11          |\n","|    explained_variance   | 0.372        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.102       |\n","|    n_updates            | 1460         |\n","|    policy_gradient_loss | -0.00404     |\n","|    std                  | 3.8          |\n","|    value_loss           | 0.447        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 844     |\n","|    iterations      | 293     |\n","|    time_elapsed    | 4261    |\n","|    total_timesteps | 3600384 |\n","--------------------------------\n","Eval num_timesteps=3612000, episode_reward=1.95 +/- 39.69\n","Episode length: 453.60 +/- 56.83\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 454          |\n","|    mean_reward          | 1.95         |\n","| time/                   |              |\n","|    total_timesteps      | 3612000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0030973137 |\n","|    clip_fraction        | 0.127        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11          |\n","|    explained_variance   | 0.506        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.235        |\n","|    n_updates            | 1465         |\n","|    policy_gradient_loss | -0.00326     |\n","|    std                  | 3.84         |\n","|    value_loss           | 12.3         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 844     |\n","|    iterations      | 294     |\n","|    time_elapsed    | 4276    |\n","|    total_timesteps | 3612672 |\n","--------------------------------\n","Eval num_timesteps=3624000, episode_reward=34.22 +/- 0.02\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 34.2         |\n","| time/                   |              |\n","|    total_timesteps      | 3624000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0023422828 |\n","|    clip_fraction        | 0.111        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.1        |\n","|    explained_variance   | 0.226        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.125        |\n","|    n_updates            | 1470         |\n","|    policy_gradient_loss | -0.00353     |\n","|    std                  | 3.89         |\n","|    value_loss           | 8.37         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 295     |\n","|    time_elapsed    | 4288    |\n","|    total_timesteps | 3624960 |\n","--------------------------------\n","Eval num_timesteps=3636000, episode_reward=30.87 +/- 0.29\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 30.9         |\n","| time/                   |              |\n","|    total_timesteps      | 3636000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022454348 |\n","|    clip_fraction        | 0.101        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.1        |\n","|    explained_variance   | 0.727        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.074       |\n","|    n_updates            | 1475         |\n","|    policy_gradient_loss | -0.00469     |\n","|    std                  | 3.92         |\n","|    value_loss           | 1.79         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 844     |\n","|    iterations      | 296     |\n","|    time_elapsed    | 4304    |\n","|    total_timesteps | 3637248 |\n","--------------------------------\n","Eval num_timesteps=3648000, episode_reward=28.56 +/- 9.62\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 28.6        |\n","| time/                   |             |\n","|    total_timesteps      | 3648000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002737556 |\n","|    clip_fraction        | 0.106       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11.2       |\n","|    explained_variance   | 0.208       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 0.854       |\n","|    n_updates            | 1480        |\n","|    policy_gradient_loss | -0.00361    |\n","|    std                  | 3.96        |\n","|    value_loss           | 27.1        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 297     |\n","|    time_elapsed    | 4318    |\n","|    total_timesteps | 3649536 |\n","--------------------------------\n","Eval num_timesteps=3660000, episode_reward=-18.61 +/- 54.17\n","Episode length: 495.80 +/- 3.43\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 496          |\n","|    mean_reward          | -18.6        |\n","| time/                   |              |\n","|    total_timesteps      | 3660000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019841616 |\n","|    clip_fraction        | 0.099        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.2        |\n","|    explained_variance   | -0.318       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0934      |\n","|    n_updates            | 1485         |\n","|    policy_gradient_loss | -0.00357     |\n","|    std                  | 3.97         |\n","|    value_loss           | 0.548        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 298     |\n","|    time_elapsed    | 4331    |\n","|    total_timesteps | 3661824 |\n","--------------------------------\n","Eval num_timesteps=3672000, episode_reward=4.74 +/- 48.01\n","Episode length: 494.80 +/- 6.37\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 495          |\n","|    mean_reward          | 4.74         |\n","| time/                   |              |\n","|    total_timesteps      | 3672000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018442104 |\n","|    clip_fraction        | 0.0835       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.2        |\n","|    explained_variance   | 0.0867       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.11        |\n","|    n_updates            | 1490         |\n","|    policy_gradient_loss | -0.00408     |\n","|    std                  | 3.97         |\n","|    value_loss           | 0.284        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 299     |\n","|    time_elapsed    | 4347    |\n","|    total_timesteps | 3674112 |\n","--------------------------------\n","Eval num_timesteps=3684000, episode_reward=36.08 +/- 0.56\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 36.1         |\n","| time/                   |              |\n","|    total_timesteps      | 3684000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020894809 |\n","|    clip_fraction        | 0.0926       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.2        |\n","|    explained_variance   | 0.402        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.107       |\n","|    n_updates            | 1495         |\n","|    policy_gradient_loss | -0.00396     |\n","|    std                  | 3.99         |\n","|    value_loss           | 0.246        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 300     |\n","|    time_elapsed    | 4360    |\n","|    total_timesteps | 3686400 |\n","--------------------------------\n","Eval num_timesteps=3696000, episode_reward=32.06 +/- 1.71\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 32.1         |\n","| time/                   |              |\n","|    total_timesteps      | 3696000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022431908 |\n","|    clip_fraction        | 0.0945       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.2        |\n","|    explained_variance   | 0.309        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.108        |\n","|    n_updates            | 1500         |\n","|    policy_gradient_loss | -0.00418     |\n","|    std                  | 4.03         |\n","|    value_loss           | 15.9         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 301     |\n","|    time_elapsed    | 4374    |\n","|    total_timesteps | 3698688 |\n","--------------------------------\n","Eval num_timesteps=3708000, episode_reward=-16.17 +/- 41.22\n","Episode length: 426.20 +/- 60.26\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 426          |\n","|    mean_reward          | -16.2        |\n","| time/                   |              |\n","|    total_timesteps      | 3708000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021973231 |\n","|    clip_fraction        | 0.0897       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.2        |\n","|    explained_variance   | 0.105        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.049       |\n","|    n_updates            | 1505         |\n","|    policy_gradient_loss | -0.00463     |\n","|    std                  | 4.02         |\n","|    value_loss           | 2.56         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 302     |\n","|    time_elapsed    | 4390    |\n","|    total_timesteps | 3710976 |\n","--------------------------------\n","Eval num_timesteps=3720000, episode_reward=25.57 +/- 5.07\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 25.6         |\n","| time/                   |              |\n","|    total_timesteps      | 3720000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022377325 |\n","|    clip_fraction        | 0.085        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.2        |\n","|    explained_variance   | -0.314       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0963      |\n","|    n_updates            | 1510         |\n","|    policy_gradient_loss | -0.00469     |\n","|    std                  | 4.02         |\n","|    value_loss           | 0.713        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 844     |\n","|    iterations      | 303     |\n","|    time_elapsed    | 4409    |\n","|    total_timesteps | 3723264 |\n","--------------------------------\n","Eval num_timesteps=3732000, episode_reward=36.28 +/- 3.11\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 36.3         |\n","| time/                   |              |\n","|    total_timesteps      | 3732000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0034037065 |\n","|    clip_fraction        | 0.128        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.3        |\n","|    explained_variance   | 0.181        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.122        |\n","|    n_updates            | 1515         |\n","|    policy_gradient_loss | -0.00246     |\n","|    std                  | 4.07         |\n","|    value_loss           | 7.45         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 844     |\n","|    iterations      | 304     |\n","|    time_elapsed    | 4422    |\n","|    total_timesteps | 3735552 |\n","--------------------------------\n","Eval num_timesteps=3744000, episode_reward=33.97 +/- 6.47\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 34           |\n","| time/                   |              |\n","|    total_timesteps      | 3744000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0029150785 |\n","|    clip_fraction        | 0.0987       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.3        |\n","|    explained_variance   | 0.531        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.136        |\n","|    n_updates            | 1520         |\n","|    policy_gradient_loss | -0.00219     |\n","|    std                  | 4.12         |\n","|    value_loss           | 8.8          |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 844     |\n","|    iterations      | 305     |\n","|    time_elapsed    | 4438    |\n","|    total_timesteps | 3747840 |\n","--------------------------------\n","Eval num_timesteps=3756000, episode_reward=-53.34 +/- 8.87\n","Episode length: 492.00 +/- 4.90\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 492          |\n","|    mean_reward          | -53.3        |\n","| time/                   |              |\n","|    total_timesteps      | 3756000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0033072766 |\n","|    clip_fraction        | 0.144        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.4        |\n","|    explained_variance   | 0.331        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.463        |\n","|    n_updates            | 1525         |\n","|    policy_gradient_loss | -0.00404     |\n","|    std                  | 4.15         |\n","|    value_loss           | 13.7         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 844     |\n","|    iterations      | 306     |\n","|    time_elapsed    | 4451    |\n","|    total_timesteps | 3760128 |\n","--------------------------------\n","Eval num_timesteps=3768000, episode_reward=-2.29 +/- 40.48\n","Episode length: 440.40 +/- 72.99\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 440          |\n","|    mean_reward          | -2.29        |\n","| time/                   |              |\n","|    total_timesteps      | 3768000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0024931717 |\n","|    clip_fraction        | 0.122        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.4        |\n","|    explained_variance   | 0.623        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0464      |\n","|    n_updates            | 1530         |\n","|    policy_gradient_loss | -0.00351     |\n","|    std                  | 4.18         |\n","|    value_loss           | 4.9          |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 844     |\n","|    iterations      | 307     |\n","|    time_elapsed    | 4465    |\n","|    total_timesteps | 3772416 |\n","--------------------------------\n","Eval num_timesteps=3780000, episode_reward=31.82 +/- 7.09\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 31.8         |\n","| time/                   |              |\n","|    total_timesteps      | 3780000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021865182 |\n","|    clip_fraction        | 0.093        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.4        |\n","|    explained_variance   | -0.645       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0875      |\n","|    n_updates            | 1535         |\n","|    policy_gradient_loss | -0.00424     |\n","|    std                  | 4.16         |\n","|    value_loss           | 0.797        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 844     |\n","|    iterations      | 308     |\n","|    time_elapsed    | 4481    |\n","|    total_timesteps | 3784704 |\n","--------------------------------\n","Eval num_timesteps=3792000, episode_reward=-14.99 +/- 38.10\n","Episode length: 435.20 +/- 52.91\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 435          |\n","|    mean_reward          | -15          |\n","| time/                   |              |\n","|    total_timesteps      | 3792000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022597536 |\n","|    clip_fraction        | 0.122        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.4        |\n","|    explained_variance   | 0.35         |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0993      |\n","|    n_updates            | 1540         |\n","|    policy_gradient_loss | -0.00497     |\n","|    std                  | 4.16         |\n","|    value_loss           | 0.564        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 844     |\n","|    iterations      | 309     |\n","|    time_elapsed    | 4493    |\n","|    total_timesteps | 3796992 |\n","--------------------------------\n","Eval num_timesteps=3804000, episode_reward=34.36 +/- 1.99\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 34.4         |\n","| time/                   |              |\n","|    total_timesteps      | 3804000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019937379 |\n","|    clip_fraction        | 0.0827       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.4        |\n","|    explained_variance   | 0.231        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.11        |\n","|    n_updates            | 1545         |\n","|    policy_gradient_loss | -0.00372     |\n","|    std                  | 4.17         |\n","|    value_loss           | 0.259        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 844     |\n","|    iterations      | 310     |\n","|    time_elapsed    | 4508    |\n","|    total_timesteps | 3809280 |\n","--------------------------------\n","Eval num_timesteps=3816000, episode_reward=-50.55 +/- 3.11\n","Episode length: 363.20 +/- 13.72\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 363          |\n","|    mean_reward          | -50.6        |\n","| time/                   |              |\n","|    total_timesteps      | 3816000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021371932 |\n","|    clip_fraction        | 0.1          |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.4        |\n","|    explained_variance   | -0.077       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.109       |\n","|    n_updates            | 1550         |\n","|    policy_gradient_loss | -0.00515     |\n","|    std                  | 4.17         |\n","|    value_loss           | 0.388        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 311     |\n","|    time_elapsed    | 4521    |\n","|    total_timesteps | 3821568 |\n","--------------------------------\n","Eval num_timesteps=3828000, episode_reward=-6.45 +/- 49.40\n","Episode length: 430.40 +/- 85.24\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 430          |\n","|    mean_reward          | -6.45        |\n","| time/                   |              |\n","|    total_timesteps      | 3828000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0030809788 |\n","|    clip_fraction        | 0.112        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.4        |\n","|    explained_variance   | 0.138        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.855        |\n","|    n_updates            | 1555         |\n","|    policy_gradient_loss | -0.00271     |\n","|    std                  | 4.22         |\n","|    value_loss           | 18.2         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 312     |\n","|    time_elapsed    | 4533    |\n","|    total_timesteps | 3833856 |\n","--------------------------------\n","Eval num_timesteps=3840000, episode_reward=37.16 +/- 2.43\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 37.2         |\n","| time/                   |              |\n","|    total_timesteps      | 3840000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019491737 |\n","|    clip_fraction        | 0.0836       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.4        |\n","|    explained_variance   | -0.0922      |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.11        |\n","|    n_updates            | 1560         |\n","|    policy_gradient_loss | -0.00488     |\n","|    std                  | 4.21         |\n","|    value_loss           | 0.354        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 313     |\n","|    time_elapsed    | 4549    |\n","|    total_timesteps | 3846144 |\n","--------------------------------\n","Eval num_timesteps=3852000, episode_reward=31.81 +/- 4.91\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 31.8         |\n","| time/                   |              |\n","|    total_timesteps      | 3852000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0024478713 |\n","|    clip_fraction        | 0.117        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.4        |\n","|    explained_variance   | 0.454        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.403        |\n","|    n_updates            | 1565         |\n","|    policy_gradient_loss | -0.00378     |\n","|    std                  | 4.27         |\n","|    value_loss           | 13.5         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 314     |\n","|    time_elapsed    | 4563    |\n","|    total_timesteps | 3858432 |\n","--------------------------------\n","Eval num_timesteps=3864000, episode_reward=-4.90 +/- 49.80\n","Episode length: 447.60 +/- 64.18\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 448          |\n","|    mean_reward          | -4.9         |\n","| time/                   |              |\n","|    total_timesteps      | 3864000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020102854 |\n","|    clip_fraction        | 0.0821       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.5        |\n","|    explained_variance   | -0.271       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.108       |\n","|    n_updates            | 1570         |\n","|    policy_gradient_loss | -0.00412     |\n","|    std                  | 4.3          |\n","|    value_loss           | 0.515        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 315     |\n","|    time_elapsed    | 4576    |\n","|    total_timesteps | 3870720 |\n","--------------------------------\n","Eval num_timesteps=3876000, episode_reward=-3.77 +/- 52.82\n","Episode length: 427.60 +/- 88.67\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 428         |\n","|    mean_reward          | -3.77       |\n","| time/                   |             |\n","|    total_timesteps      | 3876000     |\n","| train/                  |             |\n","|    approx_kl            | 0.001996976 |\n","|    clip_fraction        | 0.0859      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11.5       |\n","|    explained_variance   | 0.214       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.112      |\n","|    n_updates            | 1575        |\n","|    policy_gradient_loss | -0.0037     |\n","|    std                  | 4.3         |\n","|    value_loss           | 0.301       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 316     |\n","|    time_elapsed    | 4592    |\n","|    total_timesteps | 3883008 |\n","--------------------------------\n","Eval num_timesteps=3888000, episode_reward=35.85 +/- 5.08\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 35.9         |\n","| time/                   |              |\n","|    total_timesteps      | 3888000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018840906 |\n","|    clip_fraction        | 0.092        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.5        |\n","|    explained_variance   | 0.376        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.11        |\n","|    n_updates            | 1580         |\n","|    policy_gradient_loss | -0.0038      |\n","|    std                  | 4.31         |\n","|    value_loss           | 0.297        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 317     |\n","|    time_elapsed    | 4605    |\n","|    total_timesteps | 3895296 |\n","--------------------------------\n","Eval num_timesteps=3900000, episode_reward=33.31 +/- 0.83\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 33.3         |\n","| time/                   |              |\n","|    total_timesteps      | 3900000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020684467 |\n","|    clip_fraction        | 0.1          |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.5        |\n","|    explained_variance   | 0.35         |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.113       |\n","|    n_updates            | 1585         |\n","|    policy_gradient_loss | -0.00456     |\n","|    std                  | 4.32         |\n","|    value_loss           | 0.297        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 318     |\n","|    time_elapsed    | 4619    |\n","|    total_timesteps | 3907584 |\n","--------------------------------\n","Eval num_timesteps=3912000, episode_reward=-1.21 +/- 45.03\n","Episode length: 415.20 +/- 103.86\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 415         |\n","|    mean_reward          | -1.21       |\n","| time/                   |             |\n","|    total_timesteps      | 3912000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002455449 |\n","|    clip_fraction        | 0.089       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11.5       |\n","|    explained_variance   | 0.26        |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.111      |\n","|    n_updates            | 1590        |\n","|    policy_gradient_loss | -0.00445    |\n","|    std                  | 4.34        |\n","|    value_loss           | 0.277       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 319     |\n","|    time_elapsed    | 4635    |\n","|    total_timesteps | 3919872 |\n","--------------------------------\n","Eval num_timesteps=3924000, episode_reward=-3.42 +/- 43.40\n","Episode length: 497.60 +/- 2.94\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 498          |\n","|    mean_reward          | -3.42        |\n","| time/                   |              |\n","|    total_timesteps      | 3924000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019555306 |\n","|    clip_fraction        | 0.09         |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.5        |\n","|    explained_variance   | 0.409        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.109       |\n","|    n_updates            | 1595         |\n","|    policy_gradient_loss | -0.0046      |\n","|    std                  | 4.35         |\n","|    value_loss           | 0.303        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 320     |\n","|    time_elapsed    | 4651    |\n","|    total_timesteps | 3932160 |\n","--------------------------------\n","Eval num_timesteps=3936000, episode_reward=-8.94 +/- 39.49\n","Episode length: 479.60 +/- 16.66\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 480          |\n","|    mean_reward          | -8.94        |\n","| time/                   |              |\n","|    total_timesteps      | 3936000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018953554 |\n","|    clip_fraction        | 0.0876       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.6        |\n","|    explained_variance   | 0.459        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.116       |\n","|    n_updates            | 1600         |\n","|    policy_gradient_loss | -0.00467     |\n","|    std                  | 4.35         |\n","|    value_loss           | 0.257        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 321     |\n","|    time_elapsed    | 4666    |\n","|    total_timesteps | 3944448 |\n","--------------------------------\n","Eval num_timesteps=3948000, episode_reward=27.00 +/- 8.69\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 27           |\n","| time/                   |              |\n","|    total_timesteps      | 3948000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020387454 |\n","|    clip_fraction        | 0.0865       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.6        |\n","|    explained_variance   | 0.401        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.115       |\n","|    n_updates            | 1605         |\n","|    policy_gradient_loss | -0.00432     |\n","|    std                  | 4.37         |\n","|    value_loss           | 0.288        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 322     |\n","|    time_elapsed    | 4681    |\n","|    total_timesteps | 3956736 |\n","--------------------------------\n","Eval num_timesteps=3960000, episode_reward=-13.57 +/- 46.44\n","Episode length: 423.20 +/- 62.71\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 423          |\n","|    mean_reward          | -13.6        |\n","| time/                   |              |\n","|    total_timesteps      | 3960000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0034900913 |\n","|    clip_fraction        | 0.132        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.6        |\n","|    explained_variance   | 0.106        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.234        |\n","|    n_updates            | 1610         |\n","|    policy_gradient_loss | -0.00239     |\n","|    std                  | 4.42         |\n","|    value_loss           | 7.54         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 323     |\n","|    time_elapsed    | 4693    |\n","|    total_timesteps | 3969024 |\n","--------------------------------\n","Eval num_timesteps=3972000, episode_reward=35.12 +/- 1.86\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 35.1         |\n","| time/                   |              |\n","|    total_timesteps      | 3972000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019936902 |\n","|    clip_fraction        | 0.0983       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.6        |\n","|    explained_variance   | 0.0999       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.105       |\n","|    n_updates            | 1615         |\n","|    policy_gradient_loss | -0.00313     |\n","|    std                  | 4.43         |\n","|    value_loss           | 1            |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 324     |\n","|    time_elapsed    | 4709    |\n","|    total_timesteps | 3981312 |\n","--------------------------------\n","Eval num_timesteps=3984000, episode_reward=39.50 +/- 3.68\n","Episode length: 500.00 +/- 0.00\n","---------------------------------------\n","| eval/                   |           |\n","|    mean_ep_length       | 500       |\n","|    mean_reward          | 39.5      |\n","| time/                   |           |\n","|    total_timesteps      | 3984000   |\n","| train/                  |           |\n","|    approx_kl            | 0.0020298 |\n","|    clip_fraction        | 0.0792    |\n","|    clip_range           | 0.1       |\n","|    entropy_loss         | -11.6     |\n","|    explained_variance   | 0.0885    |\n","|    learning_rate        | 0.000622  |\n","|    loss                 | -0.101    |\n","|    n_updates            | 1620      |\n","|    policy_gradient_loss | -0.00409  |\n","|    std                  | 4.46      |\n","|    value_loss           | 0.724     |\n","---------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 325     |\n","|    time_elapsed    | 4724    |\n","|    total_timesteps | 3993600 |\n","--------------------------------\n","Eval num_timesteps=3996000, episode_reward=-7.49 +/- 55.59\n","Episode length: 366.80 +/- 163.14\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 367          |\n","|    mean_reward          | -7.49        |\n","| time/                   |              |\n","|    total_timesteps      | 3996000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020783416 |\n","|    clip_fraction        | 0.0888       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.7        |\n","|    explained_variance   | 0.517        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.112       |\n","|    n_updates            | 1625         |\n","|    policy_gradient_loss | -0.00405     |\n","|    std                  | 4.49         |\n","|    value_loss           | 0.279        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 326     |\n","|    time_elapsed    | 4736    |\n","|    total_timesteps | 4005888 |\n","--------------------------------\n","Eval num_timesteps=4008000, episode_reward=34.48 +/- 2.01\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 34.5         |\n","| time/                   |              |\n","|    total_timesteps      | 4008000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020755336 |\n","|    clip_fraction        | 0.0905       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.7        |\n","|    explained_variance   | 0.245        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.149        |\n","|    n_updates            | 1630         |\n","|    policy_gradient_loss | -0.00381     |\n","|    std                  | 4.51         |\n","|    value_loss           | 11.4         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 327     |\n","|    time_elapsed    | 4752    |\n","|    total_timesteps | 4018176 |\n","--------------------------------\n","Eval num_timesteps=4020000, episode_reward=-56.34 +/- 13.76\n","Episode length: 287.20 +/- 123.45\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 287          |\n","|    mean_reward          | -56.3        |\n","| time/                   |              |\n","|    total_timesteps      | 4020000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022308032 |\n","|    clip_fraction        | 0.105        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.7        |\n","|    explained_variance   | 0.43         |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.385        |\n","|    n_updates            | 1635         |\n","|    policy_gradient_loss | -0.00345     |\n","|    std                  | 4.54         |\n","|    value_loss           | 11.9         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 328     |\n","|    time_elapsed    | 4764    |\n","|    total_timesteps | 4030464 |\n","--------------------------------\n","Eval num_timesteps=4032000, episode_reward=-71.72 +/- 7.81\n","Episode length: 252.20 +/- 121.49\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 252         |\n","|    mean_reward          | -71.7       |\n","| time/                   |             |\n","|    total_timesteps      | 4032000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002500696 |\n","|    clip_fraction        | 0.116       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11.7       |\n","|    explained_variance   | 0.0324      |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 1.58        |\n","|    n_updates            | 1640        |\n","|    policy_gradient_loss | -0.00342    |\n","|    std                  | 4.56        |\n","|    value_loss           | 26.9        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 329     |\n","|    time_elapsed    | 4777    |\n","|    total_timesteps | 4042752 |\n","--------------------------------\n","Eval num_timesteps=4044000, episode_reward=-19.35 +/- 58.14\n","Episode length: 337.40 +/- 132.76\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 337         |\n","|    mean_reward          | -19.4       |\n","| time/                   |             |\n","|    total_timesteps      | 4044000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002401783 |\n","|    clip_fraction        | 0.0776      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11.8       |\n","|    explained_variance   | 0.424       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 1.14        |\n","|    n_updates            | 1645        |\n","|    policy_gradient_loss | -0.00258    |\n","|    std                  | 4.62        |\n","|    value_loss           | 31.8        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 330     |\n","|    time_elapsed    | 4792    |\n","|    total_timesteps | 4055040 |\n","--------------------------------\n","Eval num_timesteps=4056000, episode_reward=43.77 +/- 1.46\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 43.8         |\n","| time/                   |              |\n","|    total_timesteps      | 4056000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022852195 |\n","|    clip_fraction        | 0.115        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.8        |\n","|    explained_variance   | 0.0322       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 1.55         |\n","|    n_updates            | 1650         |\n","|    policy_gradient_loss | -0.00307     |\n","|    std                  | 4.67         |\n","|    value_loss           | 23.3         |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 331     |\n","|    time_elapsed    | 4804    |\n","|    total_timesteps | 4067328 |\n","--------------------------------\n","Eval num_timesteps=4068000, episode_reward=38.62 +/- 3.54\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 38.6         |\n","| time/                   |              |\n","|    total_timesteps      | 4068000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018802384 |\n","|    clip_fraction        | 0.0759       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.8        |\n","|    explained_variance   | 0.494        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.107       |\n","|    n_updates            | 1655         |\n","|    policy_gradient_loss | -0.00373     |\n","|    std                  | 4.66         |\n","|    value_loss           | 0.675        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 332     |\n","|    time_elapsed    | 4819    |\n","|    total_timesteps | 4079616 |\n","--------------------------------\n","Eval num_timesteps=4080000, episode_reward=-58.89 +/- 5.09\n","Episode length: 370.80 +/- 77.40\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 371          |\n","|    mean_reward          | -58.9        |\n","| time/                   |              |\n","|    total_timesteps      | 4080000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0016852269 |\n","|    clip_fraction        | 0.0962       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.8        |\n","|    explained_variance   | 0.64         |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.112       |\n","|    n_updates            | 1660         |\n","|    policy_gradient_loss | -0.00375     |\n","|    std                  | 4.68         |\n","|    value_loss           | 0.283        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 333     |\n","|    time_elapsed    | 4833    |\n","|    total_timesteps | 4091904 |\n","--------------------------------\n","Eval num_timesteps=4092000, episode_reward=-61.83 +/- 0.81\n","Episode length: 382.20 +/- 5.88\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 382          |\n","|    mean_reward          | -61.8        |\n","| time/                   |              |\n","|    total_timesteps      | 4092000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021780487 |\n","|    clip_fraction        | 0.0935       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.9        |\n","|    explained_variance   | 0.0692       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.714        |\n","|    n_updates            | 1665         |\n","|    policy_gradient_loss | -0.00479     |\n","|    std                  | 4.72         |\n","|    value_loss           | 41           |\n","------------------------------------------\n","Eval num_timesteps=4104000, episode_reward=39.25 +/- 2.46\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 39.3     |\n","| time/              |          |\n","|    total_timesteps | 4104000  |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 334     |\n","|    time_elapsed    | 4851    |\n","|    total_timesteps | 4104192 |\n","--------------------------------\n","Eval num_timesteps=4116000, episode_reward=-20.02 +/- 47.41\n","Episode length: 471.80 +/- 23.03\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 472          |\n","|    mean_reward          | -20          |\n","| time/                   |              |\n","|    total_timesteps      | 4116000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0028119863 |\n","|    clip_fraction        | 0.105        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.9        |\n","|    explained_variance   | 0.737        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.29         |\n","|    n_updates            | 1670         |\n","|    policy_gradient_loss | -0.00434     |\n","|    std                  | 4.75         |\n","|    value_loss           | 18.3         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 335     |\n","|    time_elapsed    | 4865    |\n","|    total_timesteps | 4116480 |\n","--------------------------------\n","Eval num_timesteps=4128000, episode_reward=-10.56 +/- 59.47\n","Episode length: 343.20 +/- 192.04\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 343        |\n","|    mean_reward          | -10.6      |\n","| time/                   |            |\n","|    total_timesteps      | 4128000    |\n","| train/                  |            |\n","|    approx_kl            | 0.00302459 |\n","|    clip_fraction        | 0.129      |\n","|    clip_range           | 0.1        |\n","|    entropy_loss         | -11.9      |\n","|    explained_variance   | 0.194      |\n","|    learning_rate        | 0.000622   |\n","|    loss                 | 0.233      |\n","|    n_updates            | 1675       |\n","|    policy_gradient_loss | -0.00362   |\n","|    std                  | 4.8        |\n","|    value_loss           | 11.5       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 336     |\n","|    time_elapsed    | 4881    |\n","|    total_timesteps | 4128768 |\n","--------------------------------\n","Eval num_timesteps=4140000, episode_reward=36.61 +/- 0.78\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 36.6        |\n","| time/                   |             |\n","|    total_timesteps      | 4140000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002984862 |\n","|    clip_fraction        | 0.126       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12         |\n","|    explained_variance   | 0.391       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 1.05        |\n","|    n_updates            | 1680        |\n","|    policy_gradient_loss | -0.00344    |\n","|    std                  | 4.83        |\n","|    value_loss           | 19.6        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 337     |\n","|    time_elapsed    | 4898    |\n","|    total_timesteps | 4141056 |\n","--------------------------------\n","Eval num_timesteps=4152000, episode_reward=-83.52 +/- 2.00\n","Episode length: 140.20 +/- 10.78\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 140          |\n","|    mean_reward          | -83.5        |\n","| time/                   |              |\n","|    total_timesteps      | 4152000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0025299543 |\n","|    clip_fraction        | 0.0956       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12          |\n","|    explained_variance   | 0.539        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.387        |\n","|    n_updates            | 1685         |\n","|    policy_gradient_loss | -0.00397     |\n","|    std                  | 4.88         |\n","|    value_loss           | 8.32         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 338     |\n","|    time_elapsed    | 4909    |\n","|    total_timesteps | 4153344 |\n","--------------------------------\n","Eval num_timesteps=4164000, episode_reward=-38.81 +/- 51.73\n","Episode length: 382.40 +/- 96.02\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 382         |\n","|    mean_reward          | -38.8       |\n","| time/                   |             |\n","|    total_timesteps      | 4164000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002849644 |\n","|    clip_fraction        | 0.135       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12         |\n","|    explained_variance   | 0.539       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 0.496       |\n","|    n_updates            | 1690        |\n","|    policy_gradient_loss | -0.00381    |\n","|    std                  | 4.89        |\n","|    value_loss           | 16.6        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 339     |\n","|    time_elapsed    | 4922    |\n","|    total_timesteps | 4165632 |\n","--------------------------------\n","Eval num_timesteps=4176000, episode_reward=1.24 +/- 60.40\n","Episode length: 391.20 +/- 133.25\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 391          |\n","|    mean_reward          | 1.24         |\n","| time/                   |              |\n","|    total_timesteps      | 4176000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021252297 |\n","|    clip_fraction        | 0.077        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12          |\n","|    explained_variance   | 0.245        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.093       |\n","|    n_updates            | 1695         |\n","|    policy_gradient_loss | -0.00374     |\n","|    std                  | 4.9          |\n","|    value_loss           | 0.952        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 340     |\n","|    time_elapsed    | 4936    |\n","|    total_timesteps | 4177920 |\n","--------------------------------\n","Eval num_timesteps=4188000, episode_reward=37.11 +/- 0.22\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 37.1         |\n","| time/                   |              |\n","|    total_timesteps      | 4188000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0029776401 |\n","|    clip_fraction        | 0.13         |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12          |\n","|    explained_variance   | 0.605        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0428      |\n","|    n_updates            | 1700         |\n","|    policy_gradient_loss | -0.00276     |\n","|    std                  | 4.96         |\n","|    value_loss           | 3.84         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 341     |\n","|    time_elapsed    | 4952    |\n","|    total_timesteps | 4190208 |\n","--------------------------------\n","Eval num_timesteps=4200000, episode_reward=-7.82 +/- 44.72\n","Episode length: 458.00 +/- 51.44\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 458        |\n","|    mean_reward          | -7.82      |\n","| time/                   |            |\n","|    total_timesteps      | 4200000    |\n","| train/                  |            |\n","|    approx_kl            | 0.00202693 |\n","|    clip_fraction        | 0.104      |\n","|    clip_range           | 0.1        |\n","|    entropy_loss         | -12.1      |\n","|    explained_variance   | 0.0117     |\n","|    learning_rate        | 0.000622   |\n","|    loss                 | -0.0911    |\n","|    n_updates            | 1705       |\n","|    policy_gradient_loss | -0.00319   |\n","|    std                  | 4.96       |\n","|    value_loss           | 0.901      |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 342     |\n","|    time_elapsed    | 4964    |\n","|    total_timesteps | 4202496 |\n","--------------------------------\n","Eval num_timesteps=4212000, episode_reward=7.31 +/- 41.99\n","Episode length: 450.00 +/- 61.24\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 450          |\n","|    mean_reward          | 7.31         |\n","| time/                   |              |\n","|    total_timesteps      | 4212000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020761804 |\n","|    clip_fraction        | 0.0936       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.1        |\n","|    explained_variance   | 0.371        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.11        |\n","|    n_updates            | 1710         |\n","|    policy_gradient_loss | -0.00448     |\n","|    std                  | 4.96         |\n","|    value_loss           | 0.475        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 343     |\n","|    time_elapsed    | 4979    |\n","|    total_timesteps | 4214784 |\n","--------------------------------\n","Eval num_timesteps=4224000, episode_reward=-26.38 +/- 53.31\n","Episode length: 359.60 +/- 114.64\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 360          |\n","|    mean_reward          | -26.4        |\n","| time/                   |              |\n","|    total_timesteps      | 4224000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0024160629 |\n","|    clip_fraction        | 0.121        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.1        |\n","|    explained_variance   | 0.218        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.649        |\n","|    n_updates            | 1715         |\n","|    policy_gradient_loss | -0.00353     |\n","|    std                  | 5.01         |\n","|    value_loss           | 12           |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 344     |\n","|    time_elapsed    | 4994    |\n","|    total_timesteps | 4227072 |\n","--------------------------------\n","Eval num_timesteps=4236000, episode_reward=5.25 +/- 48.32\n","Episode length: 472.80 +/- 33.31\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 473         |\n","|    mean_reward          | 5.25        |\n","| time/                   |             |\n","|    total_timesteps      | 4236000     |\n","| train/                  |             |\n","|    approx_kl            | 0.001968417 |\n","|    clip_fraction        | 0.0919      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12.1       |\n","|    explained_variance   | -0.0497     |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.109      |\n","|    n_updates            | 1720        |\n","|    policy_gradient_loss | -0.00472    |\n","|    std                  | 5.01        |\n","|    value_loss           | 0.403       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 345     |\n","|    time_elapsed    | 5006    |\n","|    total_timesteps | 4239360 |\n","--------------------------------\n","Eval num_timesteps=4248000, episode_reward=-21.77 +/- 46.39\n","Episode length: 425.60 +/- 60.75\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 426          |\n","|    mean_reward          | -21.8        |\n","| time/                   |              |\n","|    total_timesteps      | 4248000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0029869445 |\n","|    clip_fraction        | 0.104        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.1        |\n","|    explained_variance   | 0.296        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0723      |\n","|    n_updates            | 1725         |\n","|    policy_gradient_loss | -0.00265     |\n","|    std                  | 5.07         |\n","|    value_loss           | 5.68         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 346     |\n","|    time_elapsed    | 5022    |\n","|    total_timesteps | 4251648 |\n","--------------------------------\n","Eval num_timesteps=4260000, episode_reward=2.06 +/- 35.93\n","Episode length: 483.60 +/- 20.09\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 484         |\n","|    mean_reward          | 2.06        |\n","| time/                   |             |\n","|    total_timesteps      | 4260000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002846896 |\n","|    clip_fraction        | 0.132       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12.2       |\n","|    explained_variance   | -0.0561     |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 0.85        |\n","|    n_updates            | 1730        |\n","|    policy_gradient_loss | -0.00336    |\n","|    std                  | 5.14        |\n","|    value_loss           | 17.9        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 347     |\n","|    time_elapsed    | 5036    |\n","|    total_timesteps | 4263936 |\n","--------------------------------\n","Eval num_timesteps=4272000, episode_reward=-10.25 +/- 55.19\n","Episode length: 374.40 +/- 153.83\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 374          |\n","|    mean_reward          | -10.2        |\n","| time/                   |              |\n","|    total_timesteps      | 4272000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0017821997 |\n","|    clip_fraction        | 0.0701       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.2        |\n","|    explained_variance   | -0.393       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.11        |\n","|    n_updates            | 1735         |\n","|    policy_gradient_loss | -0.00371     |\n","|    std                  | 5.16         |\n","|    value_loss           | 0.432        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 348     |\n","|    time_elapsed    | 5048    |\n","|    total_timesteps | 4276224 |\n","--------------------------------\n","Eval num_timesteps=4284000, episode_reward=-0.63 +/- 36.24\n","Episode length: 485.60 +/- 17.64\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 486          |\n","|    mean_reward          | -0.631       |\n","| time/                   |              |\n","|    total_timesteps      | 4284000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019308793 |\n","|    clip_fraction        | 0.0703       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.2        |\n","|    explained_variance   | 0.278        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.115       |\n","|    n_updates            | 1740         |\n","|    policy_gradient_loss | -0.00421     |\n","|    std                  | 5.17         |\n","|    value_loss           | 0.278        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 349     |\n","|    time_elapsed    | 5065    |\n","|    total_timesteps | 4288512 |\n","--------------------------------\n","Eval num_timesteps=4296000, episode_reward=-10.31 +/- 37.29\n","Episode length: 473.60 +/- 21.56\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 474          |\n","|    mean_reward          | -10.3        |\n","| time/                   |              |\n","|    total_timesteps      | 4296000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019149348 |\n","|    clip_fraction        | 0.0816       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.2        |\n","|    explained_variance   | 0.336        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.117       |\n","|    n_updates            | 1745         |\n","|    policy_gradient_loss | -0.00377     |\n","|    std                  | 5.18         |\n","|    value_loss           | 0.271        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 350     |\n","|    time_elapsed    | 5079    |\n","|    total_timesteps | 4300800 |\n","--------------------------------\n","Eval num_timesteps=4308000, episode_reward=37.05 +/- 0.47\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 37.1         |\n","| time/                   |              |\n","|    total_timesteps      | 4308000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020037147 |\n","|    clip_fraction        | 0.087        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.3        |\n","|    explained_variance   | 0.328        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.113       |\n","|    n_updates            | 1750         |\n","|    policy_gradient_loss | -0.00454     |\n","|    std                  | 5.19         |\n","|    value_loss           | 0.317        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 351     |\n","|    time_elapsed    | 5092    |\n","|    total_timesteps | 4313088 |\n","--------------------------------\n","Eval num_timesteps=4320000, episode_reward=-18.78 +/- 37.81\n","Episode length: 477.80 +/- 18.13\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 478          |\n","|    mean_reward          | -18.8        |\n","| time/                   |              |\n","|    total_timesteps      | 4320000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0025678745 |\n","|    clip_fraction        | 0.11         |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.3        |\n","|    explained_variance   | -0.00868     |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 1.53         |\n","|    n_updates            | 1755         |\n","|    policy_gradient_loss | -0.00304     |\n","|    std                  | 5.25         |\n","|    value_loss           | 17.5         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 352     |\n","|    time_elapsed    | 5108    |\n","|    total_timesteps | 4325376 |\n","--------------------------------\n","Eval num_timesteps=4332000, episode_reward=-25.88 +/- 43.25\n","Episode length: 480.20 +/- 16.17\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 480          |\n","|    mean_reward          | -25.9        |\n","| time/                   |              |\n","|    total_timesteps      | 4332000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0028348444 |\n","|    clip_fraction        | 0.102        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.3        |\n","|    explained_variance   | 0.0819       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.383        |\n","|    n_updates            | 1760         |\n","|    policy_gradient_loss | -0.00309     |\n","|    std                  | 5.32         |\n","|    value_loss           | 16.6         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 353     |\n","|    time_elapsed    | 5121    |\n","|    total_timesteps | 4337664 |\n","--------------------------------\n","Eval num_timesteps=4344000, episode_reward=19.15 +/- 7.91\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 19.2        |\n","| time/                   |             |\n","|    total_timesteps      | 4344000     |\n","| train/                  |             |\n","|    approx_kl            | 0.003032867 |\n","|    clip_fraction        | 0.102       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12.4       |\n","|    explained_variance   | 0.456       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.00666    |\n","|    n_updates            | 1765        |\n","|    policy_gradient_loss | -0.00298    |\n","|    std                  | 5.36        |\n","|    value_loss           | 4.57        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 847     |\n","|    iterations      | 354     |\n","|    time_elapsed    | 5135    |\n","|    total_timesteps | 4349952 |\n","--------------------------------\n","Eval num_timesteps=4356000, episode_reward=-12.82 +/- 43.93\n","Episode length: 448.80 +/- 62.71\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 449          |\n","|    mean_reward          | -12.8        |\n","| time/                   |              |\n","|    total_timesteps      | 4356000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0023461576 |\n","|    clip_fraction        | 0.0858       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.4        |\n","|    explained_variance   | 0.751        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0423      |\n","|    n_updates            | 1770         |\n","|    policy_gradient_loss | -0.00291     |\n","|    std                  | 5.4          |\n","|    value_loss           | 2.18         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 355     |\n","|    time_elapsed    | 5154    |\n","|    total_timesteps | 4362240 |\n","--------------------------------\n","Eval num_timesteps=4368000, episode_reward=29.09 +/- 1.05\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 29.1         |\n","| time/                   |              |\n","|    total_timesteps      | 4368000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0034403924 |\n","|    clip_fraction        | 0.124        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.4        |\n","|    explained_variance   | -0.0366      |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.291        |\n","|    n_updates            | 1775         |\n","|    policy_gradient_loss | -0.00311     |\n","|    std                  | 5.47         |\n","|    value_loss           | 17.4         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 356     |\n","|    time_elapsed    | 5166    |\n","|    total_timesteps | 4374528 |\n","--------------------------------\n","Eval num_timesteps=4380000, episode_reward=33.12 +/- 0.19\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 33.1         |\n","| time/                   |              |\n","|    total_timesteps      | 4380000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0025316083 |\n","|    clip_fraction        | 0.126        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.5        |\n","|    explained_variance   | 0.0295       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.401        |\n","|    n_updates            | 1780         |\n","|    policy_gradient_loss | -0.00472     |\n","|    std                  | 5.53         |\n","|    value_loss           | 29           |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 357     |\n","|    time_elapsed    | 5182    |\n","|    total_timesteps | 4386816 |\n","--------------------------------\n","Eval num_timesteps=4392000, episode_reward=-36.74 +/- 56.07\n","Episode length: 243.80 +/- 209.19\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 244          |\n","|    mean_reward          | -36.7        |\n","| time/                   |              |\n","|    total_timesteps      | 4392000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0023431052 |\n","|    clip_fraction        | 0.0982       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.5        |\n","|    explained_variance   | 0.124        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.812        |\n","|    n_updates            | 1785         |\n","|    policy_gradient_loss | -0.00412     |\n","|    std                  | 5.6          |\n","|    value_loss           | 13.5         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 358     |\n","|    time_elapsed    | 5197    |\n","|    total_timesteps | 4399104 |\n","--------------------------------\n","Eval num_timesteps=4404000, episode_reward=-4.26 +/- 46.37\n","Episode length: 494.80 +/- 6.37\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 495          |\n","|    mean_reward          | -4.26        |\n","| time/                   |              |\n","|    total_timesteps      | 4404000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0017534117 |\n","|    clip_fraction        | 0.0655       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.6        |\n","|    explained_variance   | -0.398       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.107       |\n","|    n_updates            | 1790         |\n","|    policy_gradient_loss | -0.00482     |\n","|    std                  | 5.59         |\n","|    value_loss           | 0.933        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 359     |\n","|    time_elapsed    | 5209    |\n","|    total_timesteps | 4411392 |\n","--------------------------------\n","Eval num_timesteps=4416000, episode_reward=30.41 +/- 2.54\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 30.4         |\n","| time/                   |              |\n","|    total_timesteps      | 4416000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020287195 |\n","|    clip_fraction        | 0.0788       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.6        |\n","|    explained_variance   | -0.0473      |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.116       |\n","|    n_updates            | 1795         |\n","|    policy_gradient_loss | -0.00441     |\n","|    std                  | 5.59         |\n","|    value_loss           | 0.412        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 360     |\n","|    time_elapsed    | 5225    |\n","|    total_timesteps | 4423680 |\n","--------------------------------\n","Eval num_timesteps=4428000, episode_reward=33.61 +/- 1.09\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 33.6         |\n","| time/                   |              |\n","|    total_timesteps      | 4428000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0023729168 |\n","|    clip_fraction        | 0.089        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.6        |\n","|    explained_variance   | 0.319        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.109        |\n","|    n_updates            | 1800         |\n","|    policy_gradient_loss | -0.00458     |\n","|    std                  | 5.67         |\n","|    value_loss           | 12.2         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 361     |\n","|    time_elapsed    | 5239    |\n","|    total_timesteps | 4435968 |\n","--------------------------------\n","Eval num_timesteps=4440000, episode_reward=32.19 +/- 0.31\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 32.2         |\n","| time/                   |              |\n","|    total_timesteps      | 4440000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019852521 |\n","|    clip_fraction        | 0.083        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.6        |\n","|    explained_variance   | 0.0896       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.124       |\n","|    n_updates            | 1805         |\n","|    policy_gradient_loss | -0.00511     |\n","|    std                  | 5.68         |\n","|    value_loss           | 0.318        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 362     |\n","|    time_elapsed    | 5252    |\n","|    total_timesteps | 4448256 |\n","--------------------------------\n","Eval num_timesteps=4452000, episode_reward=-47.70 +/- 1.72\n","Episode length: 461.60 +/- 31.35\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 462          |\n","|    mean_reward          | -47.7        |\n","| time/                   |              |\n","|    total_timesteps      | 4452000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0017935174 |\n","|    clip_fraction        | 0.0877       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.6        |\n","|    explained_variance   | 0.0509       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.12        |\n","|    n_updates            | 1810         |\n","|    policy_gradient_loss | -0.00476     |\n","|    std                  | 5.68         |\n","|    value_loss           | 0.227        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 363     |\n","|    time_elapsed    | 5268    |\n","|    total_timesteps | 4460544 |\n","--------------------------------\n","Eval num_timesteps=4464000, episode_reward=-0.50 +/- 43.16\n","Episode length: 460.40 +/- 48.50\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 460          |\n","|    mean_reward          | -0.497       |\n","| time/                   |              |\n","|    total_timesteps      | 4464000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020983107 |\n","|    clip_fraction        | 0.0884       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.6        |\n","|    explained_variance   | 0.00112      |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.123       |\n","|    n_updates            | 1815         |\n","|    policy_gradient_loss | -0.00442     |\n","|    std                  | 5.7          |\n","|    value_loss           | 0.262        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 364     |\n","|    time_elapsed    | 5281    |\n","|    total_timesteps | 4472832 |\n","--------------------------------\n","Eval num_timesteps=4476000, episode_reward=-18.52 +/- 39.66\n","Episode length: 449.60 +/- 41.15\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 450          |\n","|    mean_reward          | -18.5        |\n","| time/                   |              |\n","|    total_timesteps      | 4476000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018680668 |\n","|    clip_fraction        | 0.067        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.7        |\n","|    explained_variance   | 0.0831       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.125       |\n","|    n_updates            | 1820         |\n","|    policy_gradient_loss | -0.00431     |\n","|    std                  | 5.73         |\n","|    value_loss           | 0.267        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 365     |\n","|    time_elapsed    | 5295    |\n","|    total_timesteps | 4485120 |\n","--------------------------------\n","Eval num_timesteps=4488000, episode_reward=33.15 +/- 5.46\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 33.1        |\n","| time/                   |             |\n","|    total_timesteps      | 4488000     |\n","| train/                  |             |\n","|    approx_kl            | 0.001890108 |\n","|    clip_fraction        | 0.082       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12.7       |\n","|    explained_variance   | -0.00506    |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.128      |\n","|    n_updates            | 1825        |\n","|    policy_gradient_loss | -0.00417    |\n","|    std                  | 5.73        |\n","|    value_loss           | 0.26        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 366     |\n","|    time_elapsed    | 5311    |\n","|    total_timesteps | 4497408 |\n","--------------------------------\n","Eval num_timesteps=4500000, episode_reward=31.70 +/- 3.20\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 31.7         |\n","| time/                   |              |\n","|    total_timesteps      | 4500000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0025266483 |\n","|    clip_fraction        | 0.0989       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.7        |\n","|    explained_variance   | 0.085        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.122       |\n","|    n_updates            | 1830         |\n","|    policy_gradient_loss | -0.00437     |\n","|    std                  | 5.75         |\n","|    value_loss           | 0.291        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 367     |\n","|    time_elapsed    | 5324    |\n","|    total_timesteps | 4509696 |\n","--------------------------------\n","Eval num_timesteps=4512000, episode_reward=27.75 +/- 4.21\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 27.7         |\n","| time/                   |              |\n","|    total_timesteps      | 4512000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018533369 |\n","|    clip_fraction        | 0.0773       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.7        |\n","|    explained_variance   | -0.0547      |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.122       |\n","|    n_updates            | 1835         |\n","|    policy_gradient_loss | -0.0039      |\n","|    std                  | 5.77         |\n","|    value_loss           | 0.375        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 847     |\n","|    iterations      | 368     |\n","|    time_elapsed    | 5338    |\n","|    total_timesteps | 4521984 |\n","--------------------------------\n","Eval num_timesteps=4524000, episode_reward=32.67 +/- 0.07\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 32.7         |\n","| time/                   |              |\n","|    total_timesteps      | 4524000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019173296 |\n","|    clip_fraction        | 0.0827       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.7        |\n","|    explained_variance   | 0.12         |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.122       |\n","|    n_updates            | 1840         |\n","|    policy_gradient_loss | -0.00395     |\n","|    std                  | 5.8          |\n","|    value_loss           | 0.262        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 369     |\n","|    time_elapsed    | 5354    |\n","|    total_timesteps | 4534272 |\n","--------------------------------\n","Eval num_timesteps=4536000, episode_reward=-47.19 +/- 0.25\n","Episode length: 487.80 +/- 13.72\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 488          |\n","|    mean_reward          | -47.2        |\n","| time/                   |              |\n","|    total_timesteps      | 4536000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020154223 |\n","|    clip_fraction        | 0.0906       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.7        |\n","|    explained_variance   | 0.0933       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.124       |\n","|    n_updates            | 1845         |\n","|    policy_gradient_loss | -0.00481     |\n","|    std                  | 5.8          |\n","|    value_loss           | 0.255        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 847     |\n","|    iterations      | 370     |\n","|    time_elapsed    | 5366    |\n","|    total_timesteps | 4546560 |\n","--------------------------------\n","Eval num_timesteps=4548000, episode_reward=32.69 +/- 0.35\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 32.7         |\n","| time/                   |              |\n","|    total_timesteps      | 4548000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021060742 |\n","|    clip_fraction        | 0.0942       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.7        |\n","|    explained_variance   | 0.00565      |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.379        |\n","|    n_updates            | 1850         |\n","|    policy_gradient_loss | -0.00404     |\n","|    std                  | 5.86         |\n","|    value_loss           | 16.3         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 847     |\n","|    iterations      | 371     |\n","|    time_elapsed    | 5381    |\n","|    total_timesteps | 4558848 |\n","--------------------------------\n","Eval num_timesteps=4560000, episode_reward=33.26 +/- 1.88\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 33.3         |\n","| time/                   |              |\n","|    total_timesteps      | 4560000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0016970774 |\n","|    clip_fraction        | 0.0735       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.7        |\n","|    explained_variance   | 0.261        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.125       |\n","|    n_updates            | 1855         |\n","|    policy_gradient_loss | -0.00483     |\n","|    std                  | 5.86         |\n","|    value_loss           | 0.397        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 372     |\n","|    time_elapsed    | 5398    |\n","|    total_timesteps | 4571136 |\n","--------------------------------\n","Eval num_timesteps=4572000, episode_reward=27.81 +/- 4.70\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 27.8         |\n","| time/                   |              |\n","|    total_timesteps      | 4572000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019242619 |\n","|    clip_fraction        | 0.0802       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.7        |\n","|    explained_variance   | 0.214        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.118       |\n","|    n_updates            | 1860         |\n","|    policy_gradient_loss | -0.00399     |\n","|    std                  | 5.86         |\n","|    value_loss           | 0.32         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 373     |\n","|    time_elapsed    | 5413    |\n","|    total_timesteps | 4583424 |\n","--------------------------------\n","Eval num_timesteps=4584000, episode_reward=29.66 +/- 5.09\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 29.7         |\n","| time/                   |              |\n","|    total_timesteps      | 4584000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018549412 |\n","|    clip_fraction        | 0.0716       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.7        |\n","|    explained_variance   | 0.0326       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.119       |\n","|    n_updates            | 1865         |\n","|    policy_gradient_loss | -0.00436     |\n","|    std                  | 5.86         |\n","|    value_loss           | 0.266        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 374     |\n","|    time_elapsed    | 5429    |\n","|    total_timesteps | 4595712 |\n","--------------------------------\n","Eval num_timesteps=4596000, episode_reward=30.97 +/- 1.79\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 31           |\n","| time/                   |              |\n","|    total_timesteps      | 4596000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020018597 |\n","|    clip_fraction        | 0.0844       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.8        |\n","|    explained_variance   | 0.228        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.118       |\n","|    n_updates            | 1870         |\n","|    policy_gradient_loss | -0.00475     |\n","|    std                  | 5.89         |\n","|    value_loss           | 0.319        |\n","------------------------------------------\n","Eval num_timesteps=4608000, episode_reward=24.36 +/- 5.10\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 24.4     |\n","| time/              |          |\n","|    total_timesteps | 4608000  |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 375     |\n","|    time_elapsed    | 5450    |\n","|    total_timesteps | 4608000 |\n","--------------------------------\n","Eval num_timesteps=4620000, episode_reward=32.29 +/- 1.61\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 32.3         |\n","| time/                   |              |\n","|    total_timesteps      | 4620000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018827999 |\n","|    clip_fraction        | 0.0856       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.8        |\n","|    explained_variance   | -0.163       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.119       |\n","|    n_updates            | 1875         |\n","|    policy_gradient_loss | -0.00409     |\n","|    std                  | 5.91         |\n","|    value_loss           | 0.402        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 376     |\n","|    time_elapsed    | 5464    |\n","|    total_timesteps | 4620288 |\n","--------------------------------\n","Eval num_timesteps=4632000, episode_reward=31.67 +/- 2.39\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 31.7         |\n","| time/                   |              |\n","|    total_timesteps      | 4632000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021580092 |\n","|    clip_fraction        | 0.0865       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.8        |\n","|    explained_variance   | 0.0776       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.686        |\n","|    n_updates            | 1880         |\n","|    policy_gradient_loss | -0.00359     |\n","|    std                  | 5.95         |\n","|    value_loss           | 16.7         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 377     |\n","|    time_elapsed    | 5476    |\n","|    total_timesteps | 4632576 |\n","--------------------------------\n","Eval num_timesteps=4644000, episode_reward=22.91 +/- 4.46\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 22.9         |\n","| time/                   |              |\n","|    total_timesteps      | 4644000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0025086084 |\n","|    clip_fraction        | 0.0945       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.8        |\n","|    explained_variance   | -0.168       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0766      |\n","|    n_updates            | 1885         |\n","|    policy_gradient_loss | -0.00422     |\n","|    std                  | 5.95         |\n","|    value_loss           | 1.95         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 378     |\n","|    time_elapsed    | 5493    |\n","|    total_timesteps | 4644864 |\n","--------------------------------\n","Eval num_timesteps=4656000, episode_reward=26.53 +/- 4.06\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 26.5         |\n","| time/                   |              |\n","|    total_timesteps      | 4656000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0026599455 |\n","|    clip_fraction        | 0.123        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.8        |\n","|    explained_variance   | 0.533        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.838        |\n","|    n_updates            | 1890         |\n","|    policy_gradient_loss | -0.00445     |\n","|    std                  | 6            |\n","|    value_loss           | 25.1         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 379     |\n","|    time_elapsed    | 5506    |\n","|    total_timesteps | 4657152 |\n","--------------------------------\n","Eval num_timesteps=4668000, episode_reward=29.22 +/- 0.00\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 29.2       |\n","| time/                   |            |\n","|    total_timesteps      | 4668000    |\n","| train/                  |            |\n","|    approx_kl            | 0.00221329 |\n","|    clip_fraction        | 0.1        |\n","|    clip_range           | 0.1        |\n","|    entropy_loss         | -12.9      |\n","|    explained_variance   | 0.424      |\n","|    learning_rate        | 0.000622   |\n","|    loss                 | 0.932      |\n","|    n_updates            | 1895       |\n","|    policy_gradient_loss | -0.0045    |\n","|    std                  | 6.07       |\n","|    value_loss           | 30.1       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 380     |\n","|    time_elapsed    | 5520    |\n","|    total_timesteps | 4669440 |\n","--------------------------------\n","Eval num_timesteps=4680000, episode_reward=25.95 +/- 0.93\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 26           |\n","| time/                   |              |\n","|    total_timesteps      | 4680000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022243888 |\n","|    clip_fraction        | 0.0961       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.9        |\n","|    explained_variance   | -0.233       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0102      |\n","|    n_updates            | 1900         |\n","|    policy_gradient_loss | -0.00488     |\n","|    std                  | 6.06         |\n","|    value_loss           | 2.96         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 381     |\n","|    time_elapsed    | 5536    |\n","|    total_timesteps | 4681728 |\n","--------------------------------\n","Eval num_timesteps=4692000, episode_reward=30.39 +/- 0.90\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 30.4         |\n","| time/                   |              |\n","|    total_timesteps      | 4692000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018390383 |\n","|    clip_fraction        | 0.0831       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.9        |\n","|    explained_variance   | -0.206       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.116       |\n","|    n_updates            | 1905         |\n","|    policy_gradient_loss | -0.00401     |\n","|    std                  | 6.04         |\n","|    value_loss           | 0.615        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 382     |\n","|    time_elapsed    | 5548    |\n","|    total_timesteps | 4694016 |\n","--------------------------------\n","Eval num_timesteps=4704000, episode_reward=29.43 +/- 3.22\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 29.4         |\n","| time/                   |              |\n","|    total_timesteps      | 4704000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020221218 |\n","|    clip_fraction        | 0.0865       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.9        |\n","|    explained_variance   | 0.86         |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0914      |\n","|    n_updates            | 1910         |\n","|    policy_gradient_loss | -0.00504     |\n","|    std                  | 6.07         |\n","|    value_loss           | 0.926        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 383     |\n","|    time_elapsed    | 5562    |\n","|    total_timesteps | 4706304 |\n","--------------------------------\n","Eval num_timesteps=4716000, episode_reward=21.65 +/- 0.41\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 21.6         |\n","| time/                   |              |\n","|    total_timesteps      | 4716000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0024172943 |\n","|    clip_fraction        | 0.116        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.9        |\n","|    explained_variance   | 0.593        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.103        |\n","|    n_updates            | 1915         |\n","|    policy_gradient_loss | -0.00347     |\n","|    std                  | 6.14         |\n","|    value_loss           | 12.6         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 384     |\n","|    time_elapsed    | 5579    |\n","|    total_timesteps | 4718592 |\n","--------------------------------\n","Eval num_timesteps=4728000, episode_reward=30.32 +/- 1.10\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 30.3        |\n","| time/                   |             |\n","|    total_timesteps      | 4728000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002181797 |\n","|    clip_fraction        | 0.0912      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12.9       |\n","|    explained_variance   | -0.178      |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0895     |\n","|    n_updates            | 1920        |\n","|    policy_gradient_loss | -0.00468    |\n","|    std                  | 6.15        |\n","|    value_loss           | 3.43        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 385     |\n","|    time_elapsed    | 5591    |\n","|    total_timesteps | 4730880 |\n","--------------------------------\n","Eval num_timesteps=4740000, episode_reward=31.20 +/- 2.62\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 31.2         |\n","| time/                   |              |\n","|    total_timesteps      | 4740000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0024087538 |\n","|    clip_fraction        | 0.0853       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13          |\n","|    explained_variance   | -0.0343      |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.12        |\n","|    n_updates            | 1925         |\n","|    policy_gradient_loss | -0.00446     |\n","|    std                  | 6.18         |\n","|    value_loss           | 0.369        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 386     |\n","|    time_elapsed    | 5606    |\n","|    total_timesteps | 4743168 |\n","--------------------------------\n","Eval num_timesteps=4752000, episode_reward=-1.87 +/- 38.76\n","Episode length: 472.40 +/- 33.80\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 472          |\n","|    mean_reward          | -1.87        |\n","| time/                   |              |\n","|    total_timesteps      | 4752000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0015883291 |\n","|    clip_fraction        | 0.0858       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13          |\n","|    explained_variance   | -0.113       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.123       |\n","|    n_updates            | 1930         |\n","|    policy_gradient_loss | -0.00401     |\n","|    std                  | 6.18         |\n","|    value_loss           | 0.311        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 387     |\n","|    time_elapsed    | 5621    |\n","|    total_timesteps | 4755456 |\n","--------------------------------\n","Eval num_timesteps=4764000, episode_reward=26.74 +/- 3.75\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 26.7         |\n","| time/                   |              |\n","|    total_timesteps      | 4764000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0027676944 |\n","|    clip_fraction        | 0.129        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13          |\n","|    explained_variance   | 0.451        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.033        |\n","|    n_updates            | 1935         |\n","|    policy_gradient_loss | -0.00276     |\n","|    std                  | 6.25         |\n","|    value_loss           | 5.04         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 388     |\n","|    time_elapsed    | 5633    |\n","|    total_timesteps | 4767744 |\n","--------------------------------\n","Eval num_timesteps=4776000, episode_reward=24.89 +/- 0.09\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 24.9         |\n","| time/                   |              |\n","|    total_timesteps      | 4776000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0025030384 |\n","|    clip_fraction        | 0.106        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13          |\n","|    explained_variance   | -0.0903      |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0218      |\n","|    n_updates            | 1940         |\n","|    policy_gradient_loss | -0.00419     |\n","|    std                  | 6.27         |\n","|    value_loss           | 2.46         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 389     |\n","|    time_elapsed    | 5649    |\n","|    total_timesteps | 4780032 |\n","--------------------------------\n","Eval num_timesteps=4788000, episode_reward=20.12 +/- 0.40\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 20.1         |\n","| time/                   |              |\n","|    total_timesteps      | 4788000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018093773 |\n","|    clip_fraction        | 0.0682       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13          |\n","|    explained_variance   | -0.208       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.127       |\n","|    n_updates            | 1945         |\n","|    policy_gradient_loss | -0.00408     |\n","|    std                  | 6.27         |\n","|    value_loss           | 0.276        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 390     |\n","|    time_elapsed    | 5668    |\n","|    total_timesteps | 4792320 |\n","--------------------------------\n","Eval num_timesteps=4800000, episode_reward=-34.75 +/- 43.73\n","Episode length: 491.00 +/- 7.35\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 491          |\n","|    mean_reward          | -34.8        |\n","| time/                   |              |\n","|    total_timesteps      | 4800000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022904968 |\n","|    clip_fraction        | 0.0861       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13          |\n","|    explained_variance   | 0.0072       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.125       |\n","|    n_updates            | 1950         |\n","|    policy_gradient_loss | -0.00454     |\n","|    std                  | 6.28         |\n","|    value_loss           | 0.284        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 391     |\n","|    time_elapsed    | 5680    |\n","|    total_timesteps | 4804608 |\n","--------------------------------\n","Eval num_timesteps=4812000, episode_reward=-16.46 +/- 44.43\n","Episode length: 478.80 +/- 25.96\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 479          |\n","|    mean_reward          | -16.5        |\n","| time/                   |              |\n","|    total_timesteps      | 4812000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019896633 |\n","|    clip_fraction        | 0.0929       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13          |\n","|    explained_variance   | 0.101        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.12        |\n","|    n_updates            | 1955         |\n","|    policy_gradient_loss | -0.00398     |\n","|    std                  | 6.32         |\n","|    value_loss           | 0.26         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 392     |\n","|    time_elapsed    | 5696    |\n","|    total_timesteps | 4816896 |\n","--------------------------------\n","Eval num_timesteps=4824000, episode_reward=22.15 +/- 0.53\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 22.2         |\n","| time/                   |              |\n","|    total_timesteps      | 4824000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019299564 |\n","|    clip_fraction        | 0.0834       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13          |\n","|    explained_variance   | 0.0399       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.122       |\n","|    n_updates            | 1960         |\n","|    policy_gradient_loss | -0.00379     |\n","|    std                  | 6.33         |\n","|    value_loss           | 0.235        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 393     |\n","|    time_elapsed    | 5710    |\n","|    total_timesteps | 4829184 |\n","--------------------------------\n","Eval num_timesteps=4836000, episode_reward=23.45 +/- 5.75\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 23.4         |\n","| time/                   |              |\n","|    total_timesteps      | 4836000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018963832 |\n","|    clip_fraction        | 0.0854       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.1        |\n","|    explained_variance   | -0.544       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.124       |\n","|    n_updates            | 1965         |\n","|    policy_gradient_loss | -0.00399     |\n","|    std                  | 6.35         |\n","|    value_loss           | 0.459        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 394     |\n","|    time_elapsed    | 5722    |\n","|    total_timesteps | 4841472 |\n","--------------------------------\n","Eval num_timesteps=4848000, episode_reward=16.19 +/- 2.27\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 16.2         |\n","| time/                   |              |\n","|    total_timesteps      | 4848000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018296279 |\n","|    clip_fraction        | 0.077        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.1        |\n","|    explained_variance   | 0.0861       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.126       |\n","|    n_updates            | 1970         |\n","|    policy_gradient_loss | -0.00386     |\n","|    std                  | 6.37         |\n","|    value_loss           | 0.231        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 395     |\n","|    time_elapsed    | 5739    |\n","|    total_timesteps | 4853760 |\n","--------------------------------\n","Eval num_timesteps=4860000, episode_reward=26.29 +/- 1.80\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 26.3         |\n","| time/                   |              |\n","|    total_timesteps      | 4860000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019279813 |\n","|    clip_fraction        | 0.0778       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.1        |\n","|    explained_variance   | -0.066       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.131       |\n","|    n_updates            | 1975         |\n","|    policy_gradient_loss | -0.00412     |\n","|    std                  | 6.4          |\n","|    value_loss           | 0.271        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 396     |\n","|    time_elapsed    | 5753    |\n","|    total_timesteps | 4866048 |\n","--------------------------------\n","Eval num_timesteps=4872000, episode_reward=21.63 +/- 1.72\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 21.6         |\n","| time/                   |              |\n","|    total_timesteps      | 4872000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020714947 |\n","|    clip_fraction        | 0.0893       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.1        |\n","|    explained_variance   | 0.0941       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.129       |\n","|    n_updates            | 1980         |\n","|    policy_gradient_loss | -0.00415     |\n","|    std                  | 6.45         |\n","|    value_loss           | 0.23         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 397     |\n","|    time_elapsed    | 5766    |\n","|    total_timesteps | 4878336 |\n","--------------------------------\n","Eval num_timesteps=4884000, episode_reward=17.17 +/- 1.12\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 17.2         |\n","| time/                   |              |\n","|    total_timesteps      | 4884000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018525826 |\n","|    clip_fraction        | 0.0719       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.1        |\n","|    explained_variance   | 0.12         |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.13        |\n","|    n_updates            | 1985         |\n","|    policy_gradient_loss | -0.00418     |\n","|    std                  | 6.43         |\n","|    value_loss           | 0.23         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 398     |\n","|    time_elapsed    | 5782    |\n","|    total_timesteps | 4890624 |\n","--------------------------------\n","Eval num_timesteps=4896000, episode_reward=17.38 +/- 1.41\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 17.4         |\n","| time/                   |              |\n","|    total_timesteps      | 4896000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022056838 |\n","|    clip_fraction        | 0.0845       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.1        |\n","|    explained_variance   | 0.602        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.111       |\n","|    n_updates            | 1990         |\n","|    policy_gradient_loss | -0.003       |\n","|    std                  | 6.5          |\n","|    value_loss           | 11.5         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 399     |\n","|    time_elapsed    | 5795    |\n","|    total_timesteps | 4902912 |\n","--------------------------------\n","Eval num_timesteps=4908000, episode_reward=13.75 +/- 1.64\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 13.7         |\n","| time/                   |              |\n","|    total_timesteps      | 4908000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0023355465 |\n","|    clip_fraction        | 0.078        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.2        |\n","|    explained_variance   | 0.65         |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.233        |\n","|    n_updates            | 1995         |\n","|    policy_gradient_loss | -0.00279     |\n","|    std                  | 6.59         |\n","|    value_loss           | 11.6         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 400     |\n","|    time_elapsed    | 5809    |\n","|    total_timesteps | 4915200 |\n","--------------------------------\n","Eval num_timesteps=4920000, episode_reward=19.66 +/- 0.24\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 19.7        |\n","| time/                   |             |\n","|    total_timesteps      | 4920000     |\n","| train/                  |             |\n","|    approx_kl            | 0.001531139 |\n","|    clip_fraction        | 0.055       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -13.2       |\n","|    explained_variance   | -0.0418     |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.131      |\n","|    n_updates            | 2000        |\n","|    policy_gradient_loss | -0.00298    |\n","|    std                  | 6.61        |\n","|    value_loss           | 0.412       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 401     |\n","|    time_elapsed    | 5825    |\n","|    total_timesteps | 4927488 |\n","--------------------------------\n","Eval num_timesteps=4932000, episode_reward=22.90 +/- 6.57\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 22.9         |\n","| time/                   |              |\n","|    total_timesteps      | 4932000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019744656 |\n","|    clip_fraction        | 0.0777       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.2        |\n","|    explained_variance   | -0.00173     |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.129       |\n","|    n_updates            | 2005         |\n","|    policy_gradient_loss | -0.00471     |\n","|    std                  | 6.6          |\n","|    value_loss           | 0.34         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 402     |\n","|    time_elapsed    | 5838    |\n","|    total_timesteps | 4939776 |\n","--------------------------------\n","Eval num_timesteps=4944000, episode_reward=16.97 +/- 2.79\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 17          |\n","| time/                   |             |\n","|    total_timesteps      | 4944000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002106311 |\n","|    clip_fraction        | 0.0872      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -13.2       |\n","|    explained_variance   | 0.123       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.129      |\n","|    n_updates            | 2010        |\n","|    policy_gradient_loss | -0.00427    |\n","|    std                  | 6.64        |\n","|    value_loss           | 0.246       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 403     |\n","|    time_elapsed    | 5852    |\n","|    total_timesteps | 4952064 |\n","--------------------------------\n","Eval num_timesteps=4956000, episode_reward=20.69 +/- 0.38\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 20.7         |\n","| time/                   |              |\n","|    total_timesteps      | 4956000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019483002 |\n","|    clip_fraction        | 0.0805       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.3        |\n","|    explained_variance   | 0.0978       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.131       |\n","|    n_updates            | 2015         |\n","|    policy_gradient_loss | -0.00428     |\n","|    std                  | 6.66         |\n","|    value_loss           | 0.265        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 404     |\n","|    time_elapsed    | 5868    |\n","|    total_timesteps | 4964352 |\n","--------------------------------\n","Eval num_timesteps=4968000, episode_reward=20.55 +/- 5.63\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 20.5         |\n","| time/                   |              |\n","|    total_timesteps      | 4968000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0024236094 |\n","|    clip_fraction        | 0.0948       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.3        |\n","|    explained_variance   | 0.413        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0427      |\n","|    n_updates            | 2020         |\n","|    policy_gradient_loss | -0.00379     |\n","|    std                  | 6.74         |\n","|    value_loss           | 5.24         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 405     |\n","|    time_elapsed    | 5881    |\n","|    total_timesteps | 4976640 |\n","--------------------------------\n","Eval num_timesteps=4980000, episode_reward=17.98 +/- 1.65\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 18           |\n","| time/                   |              |\n","|    total_timesteps      | 4980000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019036541 |\n","|    clip_fraction        | 0.0822       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.3        |\n","|    explained_variance   | -0.183       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0998      |\n","|    n_updates            | 2025         |\n","|    policy_gradient_loss | -0.00405     |\n","|    std                  | 6.73         |\n","|    value_loss           | 2.34         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 846     |\n","|    iterations      | 406     |\n","|    time_elapsed    | 5895    |\n","|    total_timesteps | 4988928 |\n","--------------------------------\n","Eval num_timesteps=4992000, episode_reward=15.63 +/- 2.15\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 15.6         |\n","| time/                   |              |\n","|    total_timesteps      | 4992000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021135563 |\n","|    clip_fraction        | 0.0952       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.3        |\n","|    explained_variance   | -0.417       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.126       |\n","|    n_updates            | 2030         |\n","|    policy_gradient_loss | -0.00493     |\n","|    std                  | 6.77         |\n","|    value_loss           | 0.383        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 845     |\n","|    iterations      | 407     |\n","|    time_elapsed    | 5911    |\n","|    total_timesteps | 5001216 |\n","--------------------------------\n"]}],"source":["from stable_baselines3.common.logger import configure\n","\n","tmp_path = \"./multiwalker_ppo_rew_08_log_eval/\"\n","# set up logger\n","new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n","\n","model = PPO(\"MlpPolicy\", env, verbose=3, gamma=0.99, n_steps=512, ent_coef=0.01, learning_rate=0.00062211, vf_coef=0.042202, max_grad_norm=0.9, gae_lambda=0.95, n_epochs=5, clip_range=0.1, batch_size=512)\n","# model.set_logger()\n","model.set_logger(new_logger)\n","model.learn(total_timesteps=5000000,callback=eval_callback)\n","model.save(\"multiwalker_ppo_rew_08\")"]},{"cell_type":"code","source":["from stable_baselines3 import PPO\n","# from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n","from pettingzoo.sisl import multiwalker_v9\n","import supersuit as ss\n","from stable_baselines3.common.monitor import Monitor"],"metadata":{"id":"MNRZ7Uu67dfg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env = multiwalker_v9.parallel_env(render_mode=\"rgb_array\",shared_reward=0.8)\n","\n","# #---------------------------------\n","# log_dir = \"./policy_log\"\n","# os.makedirs(log_dir, exist_ok=True)\n","# env = Monitor(env, log_dir)\n","# #---------------------------------\n","# env = ss.color_reduction_v0(env, mode='B')\n","# env = ss.black_death_v3(env)\n","# env = ss.resize_v1(env, x_size=84, y_size=84)\n","env = ss.frame_stack_v1(env, 3)\n","env = ss.pettingzoo_env_to_vec_env_v1(env)\n","env = ss.concat_vec_envs_v1(env, 8, num_cpus=4, base_class='stable_baselines3')"],"metadata":{"id":"SAh1CXvi7gIG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3.common.callbacks import EvalCallback\n","eval_callback = EvalCallback(env, best_model_save_path=\"./multiwalker_ppo_08_2_log_eval/\",\n","                             log_path=\"./multiwalker_ppo_08_2_log_eval/\", eval_freq=500,\n","                             deterministic=True, render=False)"],"metadata":{"id":"U9sXQH8q7iOG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3.common.logger import configure\n","\n","tmp_path = \"./multiwalker_ppo_08_2_log_eval/\"\n","# set up logger\n","new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n","\n","model = PPO(\"MlpPolicy\", env, verbose=3, n_steps=512,batch_size=512)\n","# model.set_logger()\n","model.set_logger(new_logger)\n","model.learn(total_timesteps=5000000,callback=eval_callback)\n","model.save(\"multiwalker_ppo_08_2\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I9knvCAg7iSY","executionInfo":{"status":"ok","timestamp":1698851694454,"user_tz":-60,"elapsed":5661334,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"9988e7a0-3b56-47a4-ffc8-397e02671034"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Logging to ./multiwalker_ppo_08_2_log_eval/\n","Using cuda device\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n","|    iterations      | 230     |\n","|    time_elapsed    | 3186    |\n","|    total_timesteps | 2826240 |\n","--------------------------------\n","Eval num_timesteps=2832000, episode_reward=30.24 +/- 4.17\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 30.2        |\n","| time/                   |             |\n","|    total_timesteps      | 2832000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012295887 |\n","|    clip_fraction        | 0.16        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.78       |\n","|    explained_variance   | 0.435       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.142       |\n","|    n_updates            | 2300        |\n","|    policy_gradient_loss | -0.00874    |\n","|    std                  | 0.497       |\n","|    value_loss           | 0.346       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 887     |\n","|    iterations      | 231     |\n","|    time_elapsed    | 3199    |\n","|    total_timesteps | 2838528 |\n","--------------------------------\n","Eval num_timesteps=2844000, episode_reward=29.46 +/- 0.85\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 29.5        |\n","| time/                   |             |\n","|    total_timesteps      | 2844000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012355745 |\n","|    clip_fraction        | 0.151       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.77       |\n","|    explained_variance   | 0.483       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.152       |\n","|    n_updates            | 2310        |\n","|    policy_gradient_loss | -0.0111     |\n","|    std                  | 0.496       |\n","|    value_loss           | 0.328       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 232     |\n","|    time_elapsed    | 3215    |\n","|    total_timesteps | 2850816 |\n","--------------------------------\n","Eval num_timesteps=2856000, episode_reward=32.02 +/- 4.06\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 32          |\n","| time/                   |             |\n","|    total_timesteps      | 2856000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011311188 |\n","|    clip_fraction        | 0.143       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.77       |\n","|    explained_variance   | 0.42        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.148       |\n","|    n_updates            | 2320        |\n","|    policy_gradient_loss | -0.0103     |\n","|    std                  | 0.496       |\n","|    value_loss           | 0.44        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 233     |\n","|    time_elapsed    | 3228    |\n","|    total_timesteps | 2863104 |\n","--------------------------------\n","Eval num_timesteps=2868000, episode_reward=-3.81 +/- 37.03\n","Episode length: 460.80 +/- 48.01\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 461         |\n","|    mean_reward          | -3.81       |\n","| time/                   |             |\n","|    total_timesteps      | 2868000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015301228 |\n","|    clip_fraction        | 0.152       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.75       |\n","|    explained_variance   | 0.506       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.151       |\n","|    n_updates            | 2330        |\n","|    policy_gradient_loss | -0.0109     |\n","|    std                  | 0.494       |\n","|    value_loss           | 0.464       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 887     |\n","|    iterations      | 234     |\n","|    time_elapsed    | 3241    |\n","|    total_timesteps | 2875392 |\n","--------------------------------\n","Eval num_timesteps=2880000, episode_reward=-7.29 +/- 45.38\n","Episode length: 443.60 +/- 69.08\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 444        |\n","|    mean_reward          | -7.29      |\n","| time/                   |            |\n","|    total_timesteps      | 2880000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01277407 |\n","|    clip_fraction        | 0.16       |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.75      |\n","|    explained_variance   | 0.0181     |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 21.9       |\n","|    n_updates            | 2340       |\n","|    policy_gradient_loss | -0.00634   |\n","|    std                  | 0.496      |\n","|    value_loss           | 17.6       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 235     |\n","|    time_elapsed    | 3257    |\n","|    total_timesteps | 2887680 |\n","--------------------------------\n","Eval num_timesteps=2892000, episode_reward=-1.58 +/- 52.34\n","Episode length: 421.60 +/- 96.02\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 422         |\n","|    mean_reward          | -1.58       |\n","| time/                   |             |\n","|    total_timesteps      | 2892000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012039677 |\n","|    clip_fraction        | 0.126       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.73       |\n","|    explained_variance   | 0.329       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.229       |\n","|    n_updates            | 2350        |\n","|    policy_gradient_loss | -0.0129     |\n","|    std                  | 0.491       |\n","|    value_loss           | 0.522       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 236     |\n","|    time_elapsed    | 3270    |\n","|    total_timesteps | 2899968 |\n","--------------------------------\n","Eval num_timesteps=2904000, episode_reward=36.78 +/- 1.18\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 36.8        |\n","| time/                   |             |\n","|    total_timesteps      | 2904000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011663333 |\n","|    clip_fraction        | 0.146       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.71       |\n","|    explained_variance   | 0.515       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.136       |\n","|    n_updates            | 2360        |\n","|    policy_gradient_loss | -0.0114     |\n","|    std                  | 0.49        |\n","|    value_loss           | 0.4         |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 237     |\n","|    time_elapsed    | 3283    |\n","|    total_timesteps | 2912256 |\n","--------------------------------\n","Eval num_timesteps=2916000, episode_reward=31.39 +/- 0.12\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 31.4        |\n","| time/                   |             |\n","|    total_timesteps      | 2916000     |\n","| train/                  |             |\n","|    approx_kl            | 0.007988048 |\n","|    clip_fraction        | 0.0922      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.71       |\n","|    explained_variance   | 0.0977      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.09        |\n","|    n_updates            | 2370        |\n","|    policy_gradient_loss | -0.00775    |\n","|    std                  | 0.491       |\n","|    value_loss           | 11.5        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 238     |\n","|    time_elapsed    | 3299    |\n","|    total_timesteps | 2924544 |\n","--------------------------------\n","Eval num_timesteps=2928000, episode_reward=-2.11 +/- 49.31\n","Episode length: 458.40 +/- 50.95\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 458         |\n","|    mean_reward          | -2.11       |\n","| time/                   |             |\n","|    total_timesteps      | 2928000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013138789 |\n","|    clip_fraction        | 0.123       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.71       |\n","|    explained_variance   | 0.0854      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.662       |\n","|    n_updates            | 2380        |\n","|    policy_gradient_loss | -0.00935    |\n","|    std                  | 0.489       |\n","|    value_loss           | 1.03        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 239     |\n","|    time_elapsed    | 3312    |\n","|    total_timesteps | 2936832 |\n","--------------------------------\n","Eval num_timesteps=2940000, episode_reward=3.69 +/- 38.50\n","Episode length: 468.00 +/- 39.19\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 468         |\n","|    mean_reward          | 3.69        |\n","| time/                   |             |\n","|    total_timesteps      | 2940000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012746449 |\n","|    clip_fraction        | 0.146       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.71       |\n","|    explained_variance   | 0.653       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.119       |\n","|    n_updates            | 2390        |\n","|    policy_gradient_loss | -0.0107     |\n","|    std                  | 0.489       |\n","|    value_loss           | 0.379       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 240     |\n","|    time_elapsed    | 3325    |\n","|    total_timesteps | 2949120 |\n","--------------------------------\n","Eval num_timesteps=2952000, episode_reward=-18.18 +/- 48.36\n","Episode length: 369.80 +/- 106.31\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 370         |\n","|    mean_reward          | -18.2       |\n","| time/                   |             |\n","|    total_timesteps      | 2952000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012147802 |\n","|    clip_fraction        | 0.153       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.7        |\n","|    explained_variance   | 0.582       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.145       |\n","|    n_updates            | 2400        |\n","|    policy_gradient_loss | -0.0121     |\n","|    std                  | 0.488       |\n","|    value_loss           | 0.372       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 241     |\n","|    time_elapsed    | 3340    |\n","|    total_timesteps | 2961408 |\n","--------------------------------\n","Eval num_timesteps=2964000, episode_reward=-19.34 +/- 39.06\n","Episode length: 386.00 +/- 93.08\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 386         |\n","|    mean_reward          | -19.3       |\n","| time/                   |             |\n","|    total_timesteps      | 2964000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011352941 |\n","|    clip_fraction        | 0.136       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.67       |\n","|    explained_variance   | 0.526       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.278       |\n","|    n_updates            | 2410        |\n","|    policy_gradient_loss | -0.0104     |\n","|    std                  | 0.484       |\n","|    value_loss           | 0.576       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 242     |\n","|    time_elapsed    | 3353    |\n","|    total_timesteps | 2973696 |\n","--------------------------------\n","Eval num_timesteps=2976000, episode_reward=27.22 +/- 1.85\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 27.2         |\n","| time/                   |              |\n","|    total_timesteps      | 2976000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0058401986 |\n","|    clip_fraction        | 0.0683       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.65        |\n","|    explained_variance   | 0.0912       |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 15.8         |\n","|    n_updates            | 2420         |\n","|    policy_gradient_loss | -0.00585     |\n","|    std                  | 0.484        |\n","|    value_loss           | 45.2         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 243     |\n","|    time_elapsed    | 3367    |\n","|    total_timesteps | 2985984 |\n","--------------------------------\n","Eval num_timesteps=2988000, episode_reward=27.68 +/- 2.09\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 27.7         |\n","| time/                   |              |\n","|    total_timesteps      | 2988000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0045753215 |\n","|    clip_fraction        | 0.0338       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.66        |\n","|    explained_variance   | 0.36         |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 5.93         |\n","|    n_updates            | 2430         |\n","|    policy_gradient_loss | -0.00539     |\n","|    std                  | 0.484        |\n","|    value_loss           | 24.1         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 244     |\n","|    time_elapsed    | 3382    |\n","|    total_timesteps | 2998272 |\n","--------------------------------\n","Eval num_timesteps=3000000, episode_reward=21.40 +/- 3.29\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 21.4        |\n","| time/                   |             |\n","|    total_timesteps      | 3000000     |\n","| train/                  |             |\n","|    approx_kl            | 0.005790878 |\n","|    clip_fraction        | 0.0592      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.66       |\n","|    explained_variance   | 0.613       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.61        |\n","|    n_updates            | 2440        |\n","|    policy_gradient_loss | -0.00631    |\n","|    std                  | 0.484       |\n","|    value_loss           | 7.32        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 245     |\n","|    time_elapsed    | 3395    |\n","|    total_timesteps | 3010560 |\n","--------------------------------\n","Eval num_timesteps=3012000, episode_reward=2.59 +/- 40.56\n","Episode length: 450.80 +/- 60.26\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 451         |\n","|    mean_reward          | 2.59        |\n","| time/                   |             |\n","|    total_timesteps      | 3012000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008948423 |\n","|    clip_fraction        | 0.0894      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.64       |\n","|    explained_variance   | 0.504       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.93        |\n","|    n_updates            | 2450        |\n","|    policy_gradient_loss | -0.00796    |\n","|    std                  | 0.48        |\n","|    value_loss           | 2.79        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 246     |\n","|    time_elapsed    | 3408    |\n","|    total_timesteps | 3022848 |\n","--------------------------------\n","Eval num_timesteps=3024000, episode_reward=37.05 +/- 5.20\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 37.1        |\n","| time/                   |             |\n","|    total_timesteps      | 3024000     |\n","| train/                  |             |\n","|    approx_kl            | 0.007946273 |\n","|    clip_fraction        | 0.0721      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.63       |\n","|    explained_variance   | 0.565       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.68        |\n","|    n_updates            | 2460        |\n","|    policy_gradient_loss | -0.00757    |\n","|    std                  | 0.481       |\n","|    value_loss           | 10.5        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 247     |\n","|    time_elapsed    | 3424    |\n","|    total_timesteps | 3035136 |\n","--------------------------------\n","Eval num_timesteps=3036000, episode_reward=33.86 +/- 3.31\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 33.9        |\n","| time/                   |             |\n","|    total_timesteps      | 3036000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011700462 |\n","|    clip_fraction        | 0.128       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.61       |\n","|    explained_variance   | -0.0748     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.01        |\n","|    n_updates            | 2470        |\n","|    policy_gradient_loss | -0.00727    |\n","|    std                  | 0.477       |\n","|    value_loss           | 3.11        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 248     |\n","|    time_elapsed    | 3436    |\n","|    total_timesteps | 3047424 |\n","--------------------------------\n","Eval num_timesteps=3048000, episode_reward=35.60 +/- 0.71\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 35.6        |\n","| time/                   |             |\n","|    total_timesteps      | 3048000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014994688 |\n","|    clip_fraction        | 0.174       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.58       |\n","|    explained_variance   | 0.477       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.135       |\n","|    n_updates            | 2480        |\n","|    policy_gradient_loss | -0.0117     |\n","|    std                  | 0.475       |\n","|    value_loss           | 0.456       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 249     |\n","|    time_elapsed    | 3450    |\n","|    total_timesteps | 3059712 |\n","--------------------------------\n","Eval num_timesteps=3060000, episode_reward=35.59 +/- 1.23\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 35.6        |\n","| time/                   |             |\n","|    total_timesteps      | 3060000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012002662 |\n","|    clip_fraction        | 0.158       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.57       |\n","|    explained_variance   | 0.219       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.36        |\n","|    n_updates            | 2490        |\n","|    policy_gradient_loss | -0.00526    |\n","|    std                  | 0.474       |\n","|    value_loss           | 7.31        |\n","-----------------------------------------\n","Eval num_timesteps=3072000, episode_reward=36.98 +/- 0.73\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 37       |\n","| time/              |          |\n","|    total_timesteps | 3072000  |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 250     |\n","|    time_elapsed    | 3471    |\n","|    total_timesteps | 3072000 |\n","--------------------------------\n","Eval num_timesteps=3084000, episode_reward=3.99 +/- 40.88\n","Episode length: 449.20 +/- 62.22\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 449          |\n","|    mean_reward          | 3.99         |\n","| time/                   |              |\n","|    total_timesteps      | 3084000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0055823377 |\n","|    clip_fraction        | 0.0448       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.57        |\n","|    explained_variance   | 0.365        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 5.16         |\n","|    n_updates            | 2500         |\n","|    policy_gradient_loss | -0.00471     |\n","|    std                  | 0.474        |\n","|    value_loss           | 21.1         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 251     |\n","|    time_elapsed    | 3486    |\n","|    total_timesteps | 3084288 |\n","--------------------------------\n","Eval num_timesteps=3096000, episode_reward=33.15 +/- 0.27\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 33.1        |\n","| time/                   |             |\n","|    total_timesteps      | 3096000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014112473 |\n","|    clip_fraction        | 0.129       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.55       |\n","|    explained_variance   | -0.132      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.415       |\n","|    n_updates            | 2510        |\n","|    policy_gradient_loss | -0.00895    |\n","|    std                  | 0.47        |\n","|    value_loss           | 2.04        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 252     |\n","|    time_elapsed    | 3497    |\n","|    total_timesteps | 3096576 |\n","--------------------------------\n","Eval num_timesteps=3108000, episode_reward=-15.02 +/- 32.41\n","Episode length: 471.20 +/- 23.52\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 471         |\n","|    mean_reward          | -15         |\n","| time/                   |             |\n","|    total_timesteps      | 3108000     |\n","| train/                  |             |\n","|    approx_kl            | 0.007974851 |\n","|    clip_fraction        | 0.0833      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.53       |\n","|    explained_variance   | 0.137       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 6.12        |\n","|    n_updates            | 2520        |\n","|    policy_gradient_loss | -0.0073     |\n","|    std                  | 0.469       |\n","|    value_loss           | 19.1        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 253     |\n","|    time_elapsed    | 3512    |\n","|    total_timesteps | 3108864 |\n","--------------------------------\n","Eval num_timesteps=3120000, episode_reward=35.37 +/- 6.30\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 35.4        |\n","| time/                   |             |\n","|    total_timesteps      | 3120000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012094933 |\n","|    clip_fraction        | 0.105       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.54       |\n","|    explained_variance   | 0.491       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.35        |\n","|    n_updates            | 2530        |\n","|    policy_gradient_loss | -0.00673    |\n","|    std                  | 0.472       |\n","|    value_loss           | 5.74        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 254     |\n","|    time_elapsed    | 3527    |\n","|    total_timesteps | 3121152 |\n","--------------------------------\n","Eval num_timesteps=3132000, episode_reward=33.86 +/- 5.93\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 33.9        |\n","| time/                   |             |\n","|    total_timesteps      | 3132000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008483578 |\n","|    clip_fraction        | 0.0786      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.55       |\n","|    explained_variance   | 0.488       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.5         |\n","|    n_updates            | 2540        |\n","|    policy_gradient_loss | -0.00788    |\n","|    std                  | 0.47        |\n","|    value_loss           | 14.9        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 255     |\n","|    time_elapsed    | 3538    |\n","|    total_timesteps | 3133440 |\n","--------------------------------\n","Eval num_timesteps=3144000, episode_reward=32.97 +/- 5.59\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 33          |\n","| time/                   |             |\n","|    total_timesteps      | 3144000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014271036 |\n","|    clip_fraction        | 0.166       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.54       |\n","|    explained_variance   | 0.551       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.499       |\n","|    n_updates            | 2550        |\n","|    policy_gradient_loss | -0.0084     |\n","|    std                  | 0.469       |\n","|    value_loss           | 1.59        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 256     |\n","|    time_elapsed    | 3554    |\n","|    total_timesteps | 3145728 |\n","--------------------------------\n","Eval num_timesteps=3156000, episode_reward=24.50 +/- 10.96\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 24.5        |\n","| time/                   |             |\n","|    total_timesteps      | 3156000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015827067 |\n","|    clip_fraction        | 0.2         |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.52       |\n","|    explained_variance   | 0.443       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.192       |\n","|    n_updates            | 2560        |\n","|    policy_gradient_loss | -0.00569    |\n","|    std                  | 0.467       |\n","|    value_loss           | 0.733       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 257     |\n","|    time_elapsed    | 3569    |\n","|    total_timesteps | 3158016 |\n","--------------------------------\n","Eval num_timesteps=3168000, episode_reward=31.95 +/- 0.72\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 31.9        |\n","| time/                   |             |\n","|    total_timesteps      | 3168000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014822385 |\n","|    clip_fraction        | 0.181       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.5        |\n","|    explained_variance   | 0.685       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.177       |\n","|    n_updates            | 2570        |\n","|    policy_gradient_loss | -0.0103     |\n","|    std                  | 0.464       |\n","|    value_loss           | 0.375       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 258     |\n","|    time_elapsed    | 3581    |\n","|    total_timesteps | 3170304 |\n","--------------------------------\n","Eval num_timesteps=3180000, episode_reward=23.63 +/- 3.65\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 23.6        |\n","| time/                   |             |\n","|    total_timesteps      | 3180000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013128887 |\n","|    clip_fraction        | 0.165       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.49       |\n","|    explained_variance   | 0.622       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.178       |\n","|    n_updates            | 2580        |\n","|    policy_gradient_loss | -0.0115     |\n","|    std                  | 0.463       |\n","|    value_loss           | 0.397       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 259     |\n","|    time_elapsed    | 3596    |\n","|    total_timesteps | 3182592 |\n","--------------------------------\n","Eval num_timesteps=3192000, episode_reward=-8.33 +/- 36.26\n","Episode length: 463.20 +/- 45.07\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 463         |\n","|    mean_reward          | -8.33       |\n","| time/                   |             |\n","|    total_timesteps      | 3192000     |\n","| train/                  |             |\n","|    approx_kl            | 0.006532676 |\n","|    clip_fraction        | 0.101       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.49       |\n","|    explained_variance   | 0.328       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.61        |\n","|    n_updates            | 2590        |\n","|    policy_gradient_loss | -0.00706    |\n","|    std                  | 0.464       |\n","|    value_loss           | 17          |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 260     |\n","|    time_elapsed    | 3610    |\n","|    total_timesteps | 3194880 |\n","--------------------------------\n","Eval num_timesteps=3204000, episode_reward=26.22 +/- 1.15\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 26.2        |\n","| time/                   |             |\n","|    total_timesteps      | 3204000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012889035 |\n","|    clip_fraction        | 0.144       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.45       |\n","|    explained_variance   | 0.721       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.144       |\n","|    n_updates            | 2600        |\n","|    policy_gradient_loss | -0.0115     |\n","|    std                  | 0.455       |\n","|    value_loss           | 0.377       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 261     |\n","|    time_elapsed    | 3623    |\n","|    total_timesteps | 3207168 |\n","--------------------------------\n","Eval num_timesteps=3216000, episode_reward=29.47 +/- 4.92\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 29.5        |\n","| time/                   |             |\n","|    total_timesteps      | 3216000     |\n","| train/                  |             |\n","|    approx_kl            | 0.007931191 |\n","|    clip_fraction        | 0.105       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.42       |\n","|    explained_variance   | 0.236       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 11          |\n","|    n_updates            | 2610        |\n","|    policy_gradient_loss | -0.00716    |\n","|    std                  | 0.455       |\n","|    value_loss           | 13          |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 262     |\n","|    time_elapsed    | 3638    |\n","|    total_timesteps | 3219456 |\n","--------------------------------\n","Eval num_timesteps=3228000, episode_reward=6.17 +/- 42.42\n","Episode length: 485.60 +/- 17.64\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 486         |\n","|    mean_reward          | 6.17        |\n","| time/                   |             |\n","|    total_timesteps      | 3228000     |\n","| train/                  |             |\n","|    approx_kl            | 0.006892594 |\n","|    clip_fraction        | 0.0534      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.42       |\n","|    explained_variance   | 0.213       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 13.7        |\n","|    n_updates            | 2620        |\n","|    policy_gradient_loss | -0.00536    |\n","|    std                  | 0.455       |\n","|    value_loss           | 32.1        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 263     |\n","|    time_elapsed    | 3653    |\n","|    total_timesteps | 3231744 |\n","--------------------------------\n","Eval num_timesteps=3240000, episode_reward=23.82 +/- 5.52\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 23.8        |\n","| time/                   |             |\n","|    total_timesteps      | 3240000     |\n","| train/                  |             |\n","|    approx_kl            | 0.007973253 |\n","|    clip_fraction        | 0.0556      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.42       |\n","|    explained_variance   | 0.47        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.3         |\n","|    n_updates            | 2630        |\n","|    policy_gradient_loss | -0.00762    |\n","|    std                  | 0.456       |\n","|    value_loss           | 11.1        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 264     |\n","|    time_elapsed    | 3665    |\n","|    total_timesteps | 3244032 |\n","--------------------------------\n","Eval num_timesteps=3252000, episode_reward=-10.00 +/- 36.03\n","Episode length: 458.00 +/- 34.29\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 458          |\n","|    mean_reward          | -10          |\n","| time/                   |              |\n","|    total_timesteps      | 3252000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0068175606 |\n","|    clip_fraction        | 0.0541       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.42        |\n","|    explained_variance   | 0.523        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 12.8         |\n","|    n_updates            | 2640         |\n","|    policy_gradient_loss | -0.0061      |\n","|    std                  | 0.455        |\n","|    value_loss           | 24.2         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 265     |\n","|    time_elapsed    | 3681    |\n","|    total_timesteps | 3256320 |\n","--------------------------------\n","Eval num_timesteps=3264000, episode_reward=29.58 +/- 5.64\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 29.6        |\n","| time/                   |             |\n","|    total_timesteps      | 3264000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008432254 |\n","|    clip_fraction        | 0.0601      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.42       |\n","|    explained_variance   | 0.511       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 12          |\n","|    n_updates            | 2650        |\n","|    policy_gradient_loss | -0.00679    |\n","|    std                  | 0.455       |\n","|    value_loss           | 15.3        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 266     |\n","|    time_elapsed    | 3695    |\n","|    total_timesteps | 3268608 |\n","--------------------------------\n","Eval num_timesteps=3276000, episode_reward=36.31 +/- 0.68\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 36.3        |\n","| time/                   |             |\n","|    total_timesteps      | 3276000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014675092 |\n","|    clip_fraction        | 0.18        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.4        |\n","|    explained_variance   | 0.629       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.228       |\n","|    n_updates            | 2660        |\n","|    policy_gradient_loss | -0.00983    |\n","|    std                  | 0.452       |\n","|    value_loss           | 0.595       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 267     |\n","|    time_elapsed    | 3707    |\n","|    total_timesteps | 3280896 |\n","--------------------------------\n","Eval num_timesteps=3288000, episode_reward=36.81 +/- 4.10\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 36.8        |\n","| time/                   |             |\n","|    total_timesteps      | 3288000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009063333 |\n","|    clip_fraction        | 0.138       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.39       |\n","|    explained_variance   | 0.304       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 7.46        |\n","|    n_updates            | 2670        |\n","|    policy_gradient_loss | -0.00653    |\n","|    std                  | 0.453       |\n","|    value_loss           | 11.9        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 268     |\n","|    time_elapsed    | 3723    |\n","|    total_timesteps | 3293184 |\n","--------------------------------\n","Eval num_timesteps=3300000, episode_reward=0.44 +/- 47.88\n","Episode length: 445.20 +/- 67.12\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 445         |\n","|    mean_reward          | 0.441       |\n","| time/                   |             |\n","|    total_timesteps      | 3300000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012700766 |\n","|    clip_fraction        | 0.149       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.38       |\n","|    explained_variance   | 0.521       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.44        |\n","|    n_updates            | 2680        |\n","|    policy_gradient_loss | -0.00786    |\n","|    std                  | 0.451       |\n","|    value_loss           | 2.61        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 269     |\n","|    time_elapsed    | 3737    |\n","|    total_timesteps | 3305472 |\n","--------------------------------\n","Eval num_timesteps=3312000, episode_reward=4.90 +/- 34.42\n","Episode length: 468.80 +/- 38.21\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 469         |\n","|    mean_reward          | 4.9         |\n","| time/                   |             |\n","|    total_timesteps      | 3312000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013588603 |\n","|    clip_fraction        | 0.173       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.38       |\n","|    explained_variance   | 0.502       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.521       |\n","|    n_updates            | 2690        |\n","|    policy_gradient_loss | -0.00594    |\n","|    std                  | 0.45        |\n","|    value_loss           | 1.49        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 270     |\n","|    time_elapsed    | 3749    |\n","|    total_timesteps | 3317760 |\n","--------------------------------\n","Eval num_timesteps=3324000, episode_reward=-24.36 +/- 41.95\n","Episode length: 458.60 +/- 33.80\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 459         |\n","|    mean_reward          | -24.4       |\n","| time/                   |             |\n","|    total_timesteps      | 3324000     |\n","| train/                  |             |\n","|    approx_kl            | 0.016251853 |\n","|    clip_fraction        | 0.176       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.35       |\n","|    explained_variance   | 0.812       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.129       |\n","|    n_updates            | 2700        |\n","|    policy_gradient_loss | -0.0115     |\n","|    std                  | 0.446       |\n","|    value_loss           | 0.469       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 271     |\n","|    time_elapsed    | 3765    |\n","|    total_timesteps | 3330048 |\n","--------------------------------\n","Eval num_timesteps=3336000, episode_reward=35.40 +/- 0.44\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 35.4        |\n","| time/                   |             |\n","|    total_timesteps      | 3336000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009554839 |\n","|    clip_fraction        | 0.162       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.34       |\n","|    explained_variance   | 0.209       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 5.02        |\n","|    n_updates            | 2710        |\n","|    policy_gradient_loss | -0.00496    |\n","|    std                  | 0.447       |\n","|    value_loss           | 19.2        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 272     |\n","|    time_elapsed    | 3779    |\n","|    total_timesteps | 3342336 |\n","--------------------------------\n","Eval num_timesteps=3348000, episode_reward=37.66 +/- 2.71\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 37.7        |\n","| time/                   |             |\n","|    total_timesteps      | 3348000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012942825 |\n","|    clip_fraction        | 0.142       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.32       |\n","|    explained_variance   | 0.416       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.372       |\n","|    n_updates            | 2720        |\n","|    policy_gradient_loss | -0.00902    |\n","|    std                  | 0.443       |\n","|    value_loss           | 1.56        |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 273     |\n","|    time_elapsed    | 3791    |\n","|    total_timesteps | 3354624 |\n","--------------------------------\n","Eval num_timesteps=3360000, episode_reward=31.57 +/- 4.50\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 31.6        |\n","| time/                   |             |\n","|    total_timesteps      | 3360000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014897286 |\n","|    clip_fraction        | 0.158       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.31       |\n","|    explained_variance   | 0.0775      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.25        |\n","|    n_updates            | 2730        |\n","|    policy_gradient_loss | -0.00916    |\n","|    std                  | 0.443       |\n","|    value_loss           | 13.3        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 274     |\n","|    time_elapsed    | 3807    |\n","|    total_timesteps | 3366912 |\n","--------------------------------\n","Eval num_timesteps=3372000, episode_reward=32.08 +/- 6.80\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 32.1        |\n","| time/                   |             |\n","|    total_timesteps      | 3372000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010620254 |\n","|    clip_fraction        | 0.114       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.31       |\n","|    explained_variance   | 0.419       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.41        |\n","|    n_updates            | 2740        |\n","|    policy_gradient_loss | -0.00728    |\n","|    std                  | 0.443       |\n","|    value_loss           | 7.64        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 275     |\n","|    time_elapsed    | 3821    |\n","|    total_timesteps | 3379200 |\n","--------------------------------\n","Eval num_timesteps=3384000, episode_reward=33.85 +/- 4.90\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 33.9        |\n","| time/                   |             |\n","|    total_timesteps      | 3384000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009732384 |\n","|    clip_fraction        | 0.11        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.31       |\n","|    explained_variance   | 0.367       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.68        |\n","|    n_updates            | 2750        |\n","|    policy_gradient_loss | -0.00748    |\n","|    std                  | 0.444       |\n","|    value_loss           | 12.5        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 276     |\n","|    time_elapsed    | 3833    |\n","|    total_timesteps | 3391488 |\n","--------------------------------\n","Eval num_timesteps=3396000, episode_reward=24.34 +/- 0.05\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 24.3        |\n","| time/                   |             |\n","|    total_timesteps      | 3396000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008204591 |\n","|    clip_fraction        | 0.0776      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.31       |\n","|    explained_variance   | 0.462       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 7.64        |\n","|    n_updates            | 2760        |\n","|    policy_gradient_loss | -0.00724    |\n","|    std                  | 0.444       |\n","|    value_loss           | 16.6        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 277     |\n","|    time_elapsed    | 3849    |\n","|    total_timesteps | 3403776 |\n","--------------------------------\n","Eval num_timesteps=3408000, episode_reward=32.83 +/- 2.98\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 32.8        |\n","| time/                   |             |\n","|    total_timesteps      | 3408000     |\n","| train/                  |             |\n","|    approx_kl            | 0.016047167 |\n","|    clip_fraction        | 0.193       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.3        |\n","|    explained_variance   | 0.395       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.372       |\n","|    n_updates            | 2770        |\n","|    policy_gradient_loss | -0.00932    |\n","|    std                  | 0.443       |\n","|    value_loss           | 0.749       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 278     |\n","|    time_elapsed    | 3863    |\n","|    total_timesteps | 3416064 |\n","--------------------------------\n","Eval num_timesteps=3420000, episode_reward=36.81 +/- 2.75\n","Episode length: 500.00 +/- 0.00\n","---------------------------------------\n","| eval/                   |           |\n","|    mean_ep_length       | 500       |\n","|    mean_reward          | 36.8      |\n","| time/                   |           |\n","|    total_timesteps      | 3420000   |\n","| train/                  |           |\n","|    approx_kl            | 0.0170851 |\n","|    clip_fraction        | 0.207     |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -2.27     |\n","|    explained_variance   | 0.0478    |\n","|    learning_rate        | 0.0003    |\n","|    loss                 | 0.392     |\n","|    n_updates            | 2780      |\n","|    policy_gradient_loss | -0.0102   |\n","|    std                  | 0.438     |\n","|    value_loss           | 0.788     |\n","---------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 279     |\n","|    time_elapsed    | 3875    |\n","|    total_timesteps | 3428352 |\n","--------------------------------\n","Eval num_timesteps=3432000, episode_reward=31.71 +/- 2.94\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 31.7         |\n","| time/                   |              |\n","|    total_timesteps      | 3432000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0144888265 |\n","|    clip_fraction        | 0.176        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.26        |\n","|    explained_variance   | 0.638        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.207        |\n","|    n_updates            | 2790         |\n","|    policy_gradient_loss | -0.0126      |\n","|    std                  | 0.44         |\n","|    value_loss           | 0.461        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 280     |\n","|    time_elapsed    | 3891    |\n","|    total_timesteps | 3440640 |\n","--------------------------------\n","Eval num_timesteps=3444000, episode_reward=35.02 +/- 0.90\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 35          |\n","| time/                   |             |\n","|    total_timesteps      | 3444000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012244289 |\n","|    clip_fraction        | 0.141       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.26       |\n","|    explained_variance   | 0.784       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.176       |\n","|    n_updates            | 2800        |\n","|    policy_gradient_loss | -0.012      |\n","|    std                  | 0.438       |\n","|    value_loss           | 0.369       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 281     |\n","|    time_elapsed    | 3904    |\n","|    total_timesteps | 3452928 |\n","--------------------------------\n","Eval num_timesteps=3456000, episode_reward=32.29 +/- 6.34\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 32.3        |\n","| time/                   |             |\n","|    total_timesteps      | 3456000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011603379 |\n","|    clip_fraction        | 0.144       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.25       |\n","|    explained_variance   | 0.738       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.108       |\n","|    n_updates            | 2810        |\n","|    policy_gradient_loss | -0.00994    |\n","|    std                  | 0.438       |\n","|    value_loss           | 0.323       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 282     |\n","|    time_elapsed    | 3917    |\n","|    total_timesteps | 3465216 |\n","--------------------------------\n","Eval num_timesteps=3468000, episode_reward=32.52 +/- 2.24\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 32.5        |\n","| time/                   |             |\n","|    total_timesteps      | 3468000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013400872 |\n","|    clip_fraction        | 0.153       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.23       |\n","|    explained_variance   | 0.691       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.163       |\n","|    n_updates            | 2820        |\n","|    policy_gradient_loss | -0.0109     |\n","|    std                  | 0.435       |\n","|    value_loss           | 0.332       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 283     |\n","|    time_elapsed    | 3933    |\n","|    total_timesteps | 3477504 |\n","--------------------------------\n","Eval num_timesteps=3480000, episode_reward=-15.41 +/- 37.41\n","Episode length: 443.00 +/- 46.54\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 443         |\n","|    mean_reward          | -15.4       |\n","| time/                   |             |\n","|    total_timesteps      | 3480000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011475458 |\n","|    clip_fraction        | 0.15        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.22       |\n","|    explained_variance   | 0.556       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.2         |\n","|    n_updates            | 2830        |\n","|    policy_gradient_loss | -0.0109     |\n","|    std                  | 0.434       |\n","|    value_loss           | 0.405       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 284     |\n","|    time_elapsed    | 3946    |\n","|    total_timesteps | 3489792 |\n","--------------------------------\n","Eval num_timesteps=3492000, episode_reward=30.82 +/- 2.39\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 30.8        |\n","| time/                   |             |\n","|    total_timesteps      | 3492000     |\n","| train/                  |             |\n","|    approx_kl            | 0.007752107 |\n","|    clip_fraction        | 0.103       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.21       |\n","|    explained_variance   | 0.0685      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 11.3        |\n","|    n_updates            | 2840        |\n","|    policy_gradient_loss | -0.00804    |\n","|    std                  | 0.434       |\n","|    value_loss           | 28.7        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 285     |\n","|    time_elapsed    | 3960    |\n","|    total_timesteps | 3502080 |\n","--------------------------------\n","Eval num_timesteps=3504000, episode_reward=30.10 +/- 0.54\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 30.1        |\n","| time/                   |             |\n","|    total_timesteps      | 3504000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011900286 |\n","|    clip_fraction        | 0.137       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.2        |\n","|    explained_variance   | 0.152       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.281       |\n","|    n_updates            | 2850        |\n","|    policy_gradient_loss | -0.0114     |\n","|    std                  | 0.431       |\n","|    value_loss           | 0.637       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 883     |\n","|    iterations      | 286     |\n","|    time_elapsed    | 3975    |\n","|    total_timesteps | 3514368 |\n","--------------------------------\n","Eval num_timesteps=3516000, episode_reward=32.62 +/- 0.08\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 32.6         |\n","| time/                   |              |\n","|    total_timesteps      | 3516000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0134131275 |\n","|    clip_fraction        | 0.156        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.17        |\n","|    explained_variance   | 0.576        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.245        |\n","|    n_updates            | 2860         |\n","|    policy_gradient_loss | -0.0105      |\n","|    std                  | 0.428        |\n","|    value_loss           | 0.432        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 287     |\n","|    time_elapsed    | 3988    |\n","|    total_timesteps | 3526656 |\n","--------------------------------\n","Eval num_timesteps=3528000, episode_reward=23.87 +/- 1.63\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 23.9        |\n","| time/                   |             |\n","|    total_timesteps      | 3528000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012610662 |\n","|    clip_fraction        | 0.14        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.16       |\n","|    explained_variance   | 0.218       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 14.8        |\n","|    n_updates            | 2870        |\n","|    policy_gradient_loss | -0.00597    |\n","|    std                  | 0.429       |\n","|    value_loss           | 13.9        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 288     |\n","|    time_elapsed    | 4001    |\n","|    total_timesteps | 3538944 |\n","--------------------------------\n","Eval num_timesteps=3540000, episode_reward=35.18 +/- 2.95\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 35.2        |\n","| time/                   |             |\n","|    total_timesteps      | 3540000     |\n","| train/                  |             |\n","|    approx_kl            | 0.007674057 |\n","|    clip_fraction        | 0.0788      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.16       |\n","|    explained_variance   | 0.368       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.73        |\n","|    n_updates            | 2880        |\n","|    policy_gradient_loss | -0.00512    |\n","|    std                  | 0.428       |\n","|    value_loss           | 7.54        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 289     |\n","|    time_elapsed    | 4017    |\n","|    total_timesteps | 3551232 |\n","--------------------------------\n","Eval num_timesteps=3552000, episode_reward=32.78 +/- 6.56\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 32.8        |\n","| time/                   |             |\n","|    total_timesteps      | 3552000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013995849 |\n","|    clip_fraction        | 0.168       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.15       |\n","|    explained_variance   | 0.115       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.224       |\n","|    n_updates            | 2890        |\n","|    policy_gradient_loss | -0.00723    |\n","|    std                  | 0.425       |\n","|    value_loss           | 0.578       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 290     |\n","|    time_elapsed    | 4029    |\n","|    total_timesteps | 3563520 |\n","--------------------------------\n","Eval num_timesteps=3564000, episode_reward=3.98 +/- 37.58\n","Episode length: 466.40 +/- 41.15\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 466          |\n","|    mean_reward          | 3.98         |\n","| time/                   |              |\n","|    total_timesteps      | 3564000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0091003245 |\n","|    clip_fraction        | 0.113        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.13        |\n","|    explained_variance   | 0.191        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 9.21         |\n","|    n_updates            | 2900         |\n","|    policy_gradient_loss | -0.00691     |\n","|    std                  | 0.425        |\n","|    value_loss           | 13.4         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 291     |\n","|    time_elapsed    | 4042    |\n","|    total_timesteps | 3575808 |\n","--------------------------------\n","Eval num_timesteps=3576000, episode_reward=-6.70 +/- 43.98\n","Episode length: 486.00 +/- 17.15\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 486         |\n","|    mean_reward          | -6.7        |\n","| time/                   |             |\n","|    total_timesteps      | 3576000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012405564 |\n","|    clip_fraction        | 0.153       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.14       |\n","|    explained_variance   | 0.771       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.271       |\n","|    n_updates            | 2910        |\n","|    policy_gradient_loss | -0.0101     |\n","|    std                  | 0.426       |\n","|    value_loss           | 0.568       |\n","-----------------------------------------\n","Eval num_timesteps=3588000, episode_reward=28.35 +/- 4.18\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 28.3     |\n","| time/              |          |\n","|    total_timesteps | 3588000  |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 883     |\n","|    iterations      | 292     |\n","|    time_elapsed    | 4062    |\n","|    total_timesteps | 3588096 |\n","--------------------------------\n","Eval num_timesteps=3600000, episode_reward=-8.87 +/- 48.85\n","Episode length: 428.40 +/- 87.69\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 428         |\n","|    mean_reward          | -8.87       |\n","| time/                   |             |\n","|    total_timesteps      | 3600000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011980247 |\n","|    clip_fraction        | 0.148       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.14       |\n","|    explained_variance   | 0.671       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.176       |\n","|    n_updates            | 2920        |\n","|    policy_gradient_loss | -0.0114     |\n","|    std                  | 0.426       |\n","|    value_loss           | 0.435       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 882     |\n","|    iterations      | 293     |\n","|    time_elapsed    | 4077    |\n","|    total_timesteps | 3600384 |\n","--------------------------------\n","Eval num_timesteps=3612000, episode_reward=28.42 +/- 4.79\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 28.4        |\n","| time/                   |             |\n","|    total_timesteps      | 3612000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011434202 |\n","|    clip_fraction        | 0.144       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.14       |\n","|    explained_variance   | 0.766       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.139       |\n","|    n_updates            | 2930        |\n","|    policy_gradient_loss | -0.00935    |\n","|    std                  | 0.425       |\n","|    value_loss           | 0.355       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 883     |\n","|    iterations      | 294     |\n","|    time_elapsed    | 4088    |\n","|    total_timesteps | 3612672 |\n","--------------------------------\n","Eval num_timesteps=3624000, episode_reward=25.41 +/- 4.12\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 25.4        |\n","| time/                   |             |\n","|    total_timesteps      | 3624000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008795543 |\n","|    clip_fraction        | 0.107       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.14       |\n","|    explained_variance   | 0.276       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.82        |\n","|    n_updates            | 2940        |\n","|    policy_gradient_loss | -0.00544    |\n","|    std                  | 0.426       |\n","|    value_loss           | 11.9        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 883     |\n","|    iterations      | 295     |\n","|    time_elapsed    | 4103    |\n","|    total_timesteps | 3624960 |\n","--------------------------------\n","Eval num_timesteps=3636000, episode_reward=32.80 +/- 0.97\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 32.8        |\n","| time/                   |             |\n","|    total_timesteps      | 3636000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008256397 |\n","|    clip_fraction        | 0.0855      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.15       |\n","|    explained_variance   | 0.676       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.06        |\n","|    n_updates            | 2950        |\n","|    policy_gradient_loss | -0.00786    |\n","|    std                  | 0.426       |\n","|    value_loss           | 3.1         |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 883     |\n","|    iterations      | 296     |\n","|    time_elapsed    | 4117    |\n","|    total_timesteps | 3637248 |\n","--------------------------------\n","Eval num_timesteps=3648000, episode_reward=34.83 +/- 0.85\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 34.8        |\n","| time/                   |             |\n","|    total_timesteps      | 3648000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015303987 |\n","|    clip_fraction        | 0.152       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.12       |\n","|    explained_variance   | 0.831       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.181       |\n","|    n_updates            | 2960        |\n","|    policy_gradient_loss | -0.00914    |\n","|    std                  | 0.422       |\n","|    value_loss           | 0.436       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 883     |\n","|    iterations      | 297     |\n","|    time_elapsed    | 4128    |\n","|    total_timesteps | 3649536 |\n","--------------------------------\n","Eval num_timesteps=3660000, episode_reward=28.39 +/- 2.46\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 28.4        |\n","| time/                   |             |\n","|    total_timesteps      | 3660000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011935525 |\n","|    clip_fraction        | 0.148       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.1        |\n","|    explained_variance   | 0.735       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.207       |\n","|    n_updates            | 2970        |\n","|    policy_gradient_loss | -0.0112     |\n","|    std                  | 0.421       |\n","|    value_loss           | 0.408       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 883     |\n","|    iterations      | 298     |\n","|    time_elapsed    | 4143    |\n","|    total_timesteps | 3661824 |\n","--------------------------------\n","Eval num_timesteps=3672000, episode_reward=1.64 +/- 42.23\n","Episode length: 485.60 +/- 17.64\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 486         |\n","|    mean_reward          | 1.64        |\n","| time/                   |             |\n","|    total_timesteps      | 3672000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013014715 |\n","|    clip_fraction        | 0.163       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.1        |\n","|    explained_variance   | 0.615       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.218       |\n","|    n_updates            | 2980        |\n","|    policy_gradient_loss | -0.01       |\n","|    std                  | 0.422       |\n","|    value_loss           | 0.428       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 883     |\n","|    iterations      | 299     |\n","|    time_elapsed    | 4157    |\n","|    total_timesteps | 3674112 |\n","--------------------------------\n","Eval num_timesteps=3684000, episode_reward=35.17 +/- 0.80\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 35.2        |\n","| time/                   |             |\n","|    total_timesteps      | 3684000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012642376 |\n","|    clip_fraction        | 0.153       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.08       |\n","|    explained_variance   | 0.782       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.19        |\n","|    n_updates            | 2990        |\n","|    policy_gradient_loss | -0.00936    |\n","|    std                  | 0.417       |\n","|    value_loss           | 0.488       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 300     |\n","|    time_elapsed    | 4168    |\n","|    total_timesteps | 3686400 |\n","--------------------------------\n","Eval num_timesteps=3696000, episode_reward=35.22 +/- 1.48\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 35.2        |\n","| time/                   |             |\n","|    total_timesteps      | 3696000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008920564 |\n","|    clip_fraction        | 0.108       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.06       |\n","|    explained_variance   | 0.379       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.07        |\n","|    n_updates            | 3000        |\n","|    policy_gradient_loss | -0.00742    |\n","|    std                  | 0.418       |\n","|    value_loss           | 6.87        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 883     |\n","|    iterations      | 301     |\n","|    time_elapsed    | 4184    |\n","|    total_timesteps | 3698688 |\n","--------------------------------\n","Eval num_timesteps=3708000, episode_reward=24.82 +/- 2.62\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 24.8        |\n","| time/                   |             |\n","|    total_timesteps      | 3708000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008019707 |\n","|    clip_fraction        | 0.121       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.05       |\n","|    explained_variance   | 0.0563      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 7.11        |\n","|    n_updates            | 3010        |\n","|    policy_gradient_loss | -0.00642    |\n","|    std                  | 0.417       |\n","|    value_loss           | 13.5        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 302     |\n","|    time_elapsed    | 4197    |\n","|    total_timesteps | 3710976 |\n","--------------------------------\n","Eval num_timesteps=3720000, episode_reward=32.34 +/- 1.57\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 32.3       |\n","| time/                   |            |\n","|    total_timesteps      | 3720000    |\n","| train/                  |            |\n","|    approx_kl            | 0.00801858 |\n","|    clip_fraction        | 0.104      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.06      |\n","|    explained_variance   | 0.24       |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 1.88       |\n","|    n_updates            | 3020       |\n","|    policy_gradient_loss | -0.00515   |\n","|    std                  | 0.418      |\n","|    value_loss           | 9.21       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 303     |\n","|    time_elapsed    | 4209    |\n","|    total_timesteps | 3723264 |\n","--------------------------------\n","Eval num_timesteps=3732000, episode_reward=28.22 +/- 4.10\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 28.2       |\n","| time/                   |            |\n","|    total_timesteps      | 3732000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01550407 |\n","|    clip_fraction        | 0.11       |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.06      |\n","|    explained_variance   | 0.523      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 1.97       |\n","|    n_updates            | 3030       |\n","|    policy_gradient_loss | -0.00658   |\n","|    std                  | 0.418      |\n","|    value_loss           | 5.47       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 304     |\n","|    time_elapsed    | 4224    |\n","|    total_timesteps | 3735552 |\n","--------------------------------\n","Eval num_timesteps=3744000, episode_reward=-13.14 +/- 37.71\n","Episode length: 458.60 +/- 33.80\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 459         |\n","|    mean_reward          | -13.1       |\n","| time/                   |             |\n","|    total_timesteps      | 3744000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015193026 |\n","|    clip_fraction        | 0.179       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.06       |\n","|    explained_variance   | 0.693       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.189       |\n","|    n_updates            | 3040        |\n","|    policy_gradient_loss | -0.011      |\n","|    std                  | 0.418       |\n","|    value_loss           | 0.512       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 305     |\n","|    time_elapsed    | 4237    |\n","|    total_timesteps | 3747840 |\n","--------------------------------\n","Eval num_timesteps=3756000, episode_reward=29.09 +/- 7.27\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 29.1         |\n","| time/                   |              |\n","|    total_timesteps      | 3756000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0130468905 |\n","|    clip_fraction        | 0.159        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.04        |\n","|    explained_variance   | 0.749        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.198        |\n","|    n_updates            | 3050         |\n","|    policy_gradient_loss | -0.00922     |\n","|    std                  | 0.415        |\n","|    value_loss           | 0.44         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 306     |\n","|    time_elapsed    | 4250    |\n","|    total_timesteps | 3760128 |\n","--------------------------------\n","Eval num_timesteps=3768000, episode_reward=30.87 +/- 2.49\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 30.9        |\n","| time/                   |             |\n","|    total_timesteps      | 3768000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013277665 |\n","|    clip_fraction        | 0.155       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.04       |\n","|    explained_variance   | 0.0766      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 15.9        |\n","|    n_updates            | 3060        |\n","|    policy_gradient_loss | -0.00907    |\n","|    std                  | 0.416       |\n","|    value_loss           | 14.3        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 307     |\n","|    time_elapsed    | 4265    |\n","|    total_timesteps | 3772416 |\n","--------------------------------\n","Eval num_timesteps=3780000, episode_reward=30.16 +/- 1.77\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 30.2        |\n","| time/                   |             |\n","|    total_timesteps      | 3780000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012519334 |\n","|    clip_fraction        | 0.158       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.05       |\n","|    explained_variance   | 0.437       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.51        |\n","|    n_updates            | 3070        |\n","|    policy_gradient_loss | -0.00733    |\n","|    std                  | 0.417       |\n","|    value_loss           | 1.46        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 308     |\n","|    time_elapsed    | 4278    |\n","|    total_timesteps | 3784704 |\n","--------------------------------\n","Eval num_timesteps=3792000, episode_reward=32.19 +/- 2.04\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 32.2         |\n","| time/                   |              |\n","|    total_timesteps      | 3792000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0075419354 |\n","|    clip_fraction        | 0.0875       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.04        |\n","|    explained_variance   | 0.443        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 8.46         |\n","|    n_updates            | 3080         |\n","|    policy_gradient_loss | -0.00538     |\n","|    std                  | 0.417        |\n","|    value_loss           | 20.6         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 309     |\n","|    time_elapsed    | 4291    |\n","|    total_timesteps | 3796992 |\n","--------------------------------\n","Eval num_timesteps=3804000, episode_reward=33.37 +/- 2.24\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 33.4        |\n","| time/                   |             |\n","|    total_timesteps      | 3804000     |\n","| train/                  |             |\n","|    approx_kl            | 0.016097317 |\n","|    clip_fraction        | 0.182       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.03       |\n","|    explained_variance   | 0.283       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.202       |\n","|    n_updates            | 3090        |\n","|    policy_gradient_loss | -0.0116     |\n","|    std                  | 0.413       |\n","|    value_loss           | 0.574       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 310     |\n","|    time_elapsed    | 4306    |\n","|    total_timesteps | 3809280 |\n","--------------------------------\n","Eval num_timesteps=3816000, episode_reward=35.99 +/- 1.01\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 36          |\n","| time/                   |             |\n","|    total_timesteps      | 3816000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012394068 |\n","|    clip_fraction        | 0.154       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.01       |\n","|    explained_variance   | 0.632       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.38        |\n","|    n_updates            | 3100        |\n","|    policy_gradient_loss | -0.00544    |\n","|    std                  | 0.412       |\n","|    value_loss           | 2.78        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 311     |\n","|    time_elapsed    | 4318    |\n","|    total_timesteps | 3821568 |\n","--------------------------------\n","Eval num_timesteps=3828000, episode_reward=35.08 +/- 0.56\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 35.1        |\n","| time/                   |             |\n","|    total_timesteps      | 3828000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012984912 |\n","|    clip_fraction        | 0.136       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.01       |\n","|    explained_variance   | 0.509       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.99        |\n","|    n_updates            | 3110        |\n","|    policy_gradient_loss | -0.004      |\n","|    std                  | 0.413       |\n","|    value_loss           | 4.95        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 312     |\n","|    time_elapsed    | 4331    |\n","|    total_timesteps | 3833856 |\n","--------------------------------\n","Eval num_timesteps=3840000, episode_reward=28.17 +/- 0.68\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 28.2       |\n","| time/                   |            |\n","|    total_timesteps      | 3840000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01477397 |\n","|    clip_fraction        | 0.175      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.02      |\n","|    explained_variance   | 0.7        |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.158      |\n","|    n_updates            | 3120       |\n","|    policy_gradient_loss | -0.00914   |\n","|    std                  | 0.413      |\n","|    value_loss           | 0.411      |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 313     |\n","|    time_elapsed    | 4346    |\n","|    total_timesteps | 3846144 |\n","--------------------------------\n","Eval num_timesteps=3852000, episode_reward=33.00 +/- 2.09\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 33          |\n","| time/                   |             |\n","|    total_timesteps      | 3852000     |\n","| train/                  |             |\n","|    approx_kl            | 0.016785212 |\n","|    clip_fraction        | 0.182       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.02       |\n","|    explained_variance   | 0.668       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.265       |\n","|    n_updates            | 3130        |\n","|    policy_gradient_loss | -0.0103     |\n","|    std                  | 0.413       |\n","|    value_loss           | 0.54        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 314     |\n","|    time_elapsed    | 4358    |\n","|    total_timesteps | 3858432 |\n","--------------------------------\n","Eval num_timesteps=3864000, episode_reward=25.89 +/- 9.18\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 25.9        |\n","| time/                   |             |\n","|    total_timesteps      | 3864000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012968349 |\n","|    clip_fraction        | 0.151       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.01       |\n","|    explained_variance   | 0.738       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.167       |\n","|    n_updates            | 3140        |\n","|    policy_gradient_loss | -0.0102     |\n","|    std                  | 0.411       |\n","|    value_loss           | 0.321       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 315     |\n","|    time_elapsed    | 4372    |\n","|    total_timesteps | 3870720 |\n","--------------------------------\n","Eval num_timesteps=3876000, episode_reward=35.62 +/- 1.18\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 35.6        |\n","| time/                   |             |\n","|    total_timesteps      | 3876000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013088952 |\n","|    clip_fraction        | 0.152       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.99       |\n","|    explained_variance   | 0.809       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.154       |\n","|    n_updates            | 3150        |\n","|    policy_gradient_loss | -0.0098     |\n","|    std                  | 0.409       |\n","|    value_loss           | 0.325       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 316     |\n","|    time_elapsed    | 4386    |\n","|    total_timesteps | 3883008 |\n","--------------------------------\n","Eval num_timesteps=3888000, episode_reward=32.15 +/- 2.23\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 32.2        |\n","| time/                   |             |\n","|    total_timesteps      | 3888000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012383826 |\n","|    clip_fraction        | 0.141       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.97       |\n","|    explained_variance   | 0.782       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.141       |\n","|    n_updates            | 3160        |\n","|    policy_gradient_loss | -0.00931    |\n","|    std                  | 0.408       |\n","|    value_loss           | 0.333       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 317     |\n","|    time_elapsed    | 4397    |\n","|    total_timesteps | 3895296 |\n","--------------------------------\n","Eval num_timesteps=3900000, episode_reward=31.96 +/- 6.56\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 32          |\n","| time/                   |             |\n","|    total_timesteps      | 3900000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010027501 |\n","|    clip_fraction        | 0.13        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.97       |\n","|    explained_variance   | 0.268       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 8.51        |\n","|    n_updates            | 3170        |\n","|    policy_gradient_loss | -0.00738    |\n","|    std                  | 0.408       |\n","|    value_loss           | 13.3        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 318     |\n","|    time_elapsed    | 4412    |\n","|    total_timesteps | 3907584 |\n","--------------------------------\n","Eval num_timesteps=3912000, episode_reward=32.91 +/- 3.23\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 32.9        |\n","| time/                   |             |\n","|    total_timesteps      | 3912000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012070411 |\n","|    clip_fraction        | 0.157       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.96       |\n","|    explained_variance   | 0.55        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.262       |\n","|    n_updates            | 3180        |\n","|    policy_gradient_loss | -0.0107     |\n","|    std                  | 0.406       |\n","|    value_loss           | 0.486       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 319     |\n","|    time_elapsed    | 4426    |\n","|    total_timesteps | 3919872 |\n","--------------------------------\n","Eval num_timesteps=3924000, episode_reward=35.77 +/- 1.22\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 35.8        |\n","| time/                   |             |\n","|    total_timesteps      | 3924000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013190997 |\n","|    clip_fraction        | 0.161       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.94       |\n","|    explained_variance   | 0.71        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.142       |\n","|    n_updates            | 3190        |\n","|    policy_gradient_loss | -0.0128     |\n","|    std                  | 0.403       |\n","|    value_loss           | 0.422       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 320     |\n","|    time_elapsed    | 4438    |\n","|    total_timesteps | 3932160 |\n","--------------------------------\n","Eval num_timesteps=3936000, episode_reward=35.11 +/- 1.14\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 35.1        |\n","| time/                   |             |\n","|    total_timesteps      | 3936000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010849685 |\n","|    clip_fraction        | 0.148       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.92       |\n","|    explained_variance   | 0.762       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.15        |\n","|    n_updates            | 3200        |\n","|    policy_gradient_loss | -0.0108     |\n","|    std                  | 0.402       |\n","|    value_loss           | 0.366       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 321     |\n","|    time_elapsed    | 4453    |\n","|    total_timesteps | 3944448 |\n","--------------------------------\n","Eval num_timesteps=3948000, episode_reward=6.28 +/- 38.91\n","Episode length: 481.20 +/- 23.03\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 481          |\n","|    mean_reward          | 6.28         |\n","| time/                   |              |\n","|    total_timesteps      | 3948000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0072565526 |\n","|    clip_fraction        | 0.122        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -1.92        |\n","|    explained_variance   | 0.144        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 4            |\n","|    n_updates            | 3210         |\n","|    policy_gradient_loss | -0.00637     |\n","|    std                  | 0.403        |\n","|    value_loss           | 12.5         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 322     |\n","|    time_elapsed    | 4467    |\n","|    total_timesteps | 3956736 |\n","--------------------------------\n","Eval num_timesteps=3960000, episode_reward=26.21 +/- 2.54\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 26.2        |\n","| time/                   |             |\n","|    total_timesteps      | 3960000     |\n","| train/                  |             |\n","|    approx_kl            | 0.007939178 |\n","|    clip_fraction        | 0.0811      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.93       |\n","|    explained_variance   | 0.455       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.02        |\n","|    n_updates            | 3220        |\n","|    policy_gradient_loss | -0.0073     |\n","|    std                  | 0.404       |\n","|    value_loss           | 11.2        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 323     |\n","|    time_elapsed    | 4479    |\n","|    total_timesteps | 3969024 |\n","--------------------------------\n","Eval num_timesteps=3972000, episode_reward=31.42 +/- 0.59\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 31.4        |\n","| time/                   |             |\n","|    total_timesteps      | 3972000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012821298 |\n","|    clip_fraction        | 0.132       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.93       |\n","|    explained_variance   | -0.0797     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.17        |\n","|    n_updates            | 3230        |\n","|    policy_gradient_loss | -0.00896    |\n","|    std                  | 0.402       |\n","|    value_loss           | 2.61        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 324     |\n","|    time_elapsed    | 4494    |\n","|    total_timesteps | 3981312 |\n","--------------------------------\n","Eval num_timesteps=3984000, episode_reward=35.21 +/- 2.13\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 35.2         |\n","| time/                   |              |\n","|    total_timesteps      | 3984000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0073437165 |\n","|    clip_fraction        | 0.0737       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -1.92        |\n","|    explained_variance   | 0.353        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 10           |\n","|    n_updates            | 3240         |\n","|    policy_gradient_loss | -0.00736     |\n","|    std                  | 0.402        |\n","|    value_loss           | 10.6         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 325     |\n","|    time_elapsed    | 4507    |\n","|    total_timesteps | 3993600 |\n","--------------------------------\n","Eval num_timesteps=3996000, episode_reward=-7.39 +/- 36.17\n","Episode length: 478.40 +/- 17.64\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 478         |\n","|    mean_reward          | -7.39       |\n","| time/                   |             |\n","|    total_timesteps      | 3996000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009887166 |\n","|    clip_fraction        | 0.121       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.93       |\n","|    explained_variance   | 0.797       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.55        |\n","|    n_updates            | 3250        |\n","|    policy_gradient_loss | -0.00605    |\n","|    std                  | 0.404       |\n","|    value_loss           | 4.26        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 326     |\n","|    time_elapsed    | 4520    |\n","|    total_timesteps | 4005888 |\n","--------------------------------\n","Eval num_timesteps=4008000, episode_reward=33.35 +/- 0.24\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 33.3        |\n","| time/                   |             |\n","|    total_timesteps      | 4008000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011268462 |\n","|    clip_fraction        | 0.109       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.93       |\n","|    explained_variance   | 0.653       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.21        |\n","|    n_updates            | 3260        |\n","|    policy_gradient_loss | -0.00608    |\n","|    std                  | 0.402       |\n","|    value_loss           | 3.48        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 327     |\n","|    time_elapsed    | 4535    |\n","|    total_timesteps | 4018176 |\n","--------------------------------\n","Eval num_timesteps=4020000, episode_reward=36.31 +/- 0.59\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 36.3        |\n","| time/                   |             |\n","|    total_timesteps      | 4020000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012089281 |\n","|    clip_fraction        | 0.138       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.91       |\n","|    explained_variance   | 0.714       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.8         |\n","|    n_updates            | 3270        |\n","|    policy_gradient_loss | -0.00826    |\n","|    std                  | 0.401       |\n","|    value_loss           | 5.75        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 328     |\n","|    time_elapsed    | 4547    |\n","|    total_timesteps | 4030464 |\n","--------------------------------\n","Eval num_timesteps=4032000, episode_reward=6.69 +/- 35.58\n","Episode length: 495.20 +/- 5.88\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 495         |\n","|    mean_reward          | 6.69        |\n","| time/                   |             |\n","|    total_timesteps      | 4032000     |\n","| train/                  |             |\n","|    approx_kl            | 0.017772818 |\n","|    clip_fraction        | 0.206       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.9        |\n","|    explained_variance   | 0.66        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.267       |\n","|    n_updates            | 3280        |\n","|    policy_gradient_loss | -0.0098     |\n","|    std                  | 0.399       |\n","|    value_loss           | 0.723       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 329     |\n","|    time_elapsed    | 4560    |\n","|    total_timesteps | 4042752 |\n","--------------------------------\n","Eval num_timesteps=4044000, episode_reward=37.61 +/- 0.01\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 37.6        |\n","| time/                   |             |\n","|    total_timesteps      | 4044000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010588843 |\n","|    clip_fraction        | 0.153       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.9        |\n","|    explained_variance   | 0.478       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.564       |\n","|    n_updates            | 3290        |\n","|    policy_gradient_loss | -0.00628    |\n","|    std                  | 0.401       |\n","|    value_loss           | 3.8         |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 330     |\n","|    time_elapsed    | 4576    |\n","|    total_timesteps | 4055040 |\n","--------------------------------\n","Eval num_timesteps=4056000, episode_reward=35.93 +/- 0.11\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 35.9        |\n","| time/                   |             |\n","|    total_timesteps      | 4056000     |\n","| train/                  |             |\n","|    approx_kl            | 0.016899155 |\n","|    clip_fraction        | 0.208       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.91       |\n","|    explained_variance   | 0.678       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.256       |\n","|    n_updates            | 3300        |\n","|    policy_gradient_loss | -0.00889    |\n","|    std                  | 0.401       |\n","|    value_loss           | 0.598       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 331     |\n","|    time_elapsed    | 4587    |\n","|    total_timesteps | 4067328 |\n","--------------------------------\n","Eval num_timesteps=4068000, episode_reward=-6.86 +/- 44.43\n","Episode length: 472.40 +/- 33.80\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 472         |\n","|    mean_reward          | -6.86       |\n","| time/                   |             |\n","|    total_timesteps      | 4068000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015601963 |\n","|    clip_fraction        | 0.188       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.89       |\n","|    explained_variance   | 0.566       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.226       |\n","|    n_updates            | 3310        |\n","|    policy_gradient_loss | -0.00987    |\n","|    std                  | 0.399       |\n","|    value_loss           | 0.545       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 332     |\n","|    time_elapsed    | 4601    |\n","|    total_timesteps | 4079616 |\n","--------------------------------\n","Eval num_timesteps=4080000, episode_reward=28.26 +/- 7.57\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 28.3       |\n","| time/                   |            |\n","|    total_timesteps      | 4080000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01186763 |\n","|    clip_fraction        | 0.163      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.89      |\n","|    explained_variance   | 0.309      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 1.61       |\n","|    n_updates            | 3320       |\n","|    policy_gradient_loss | -0.00739   |\n","|    std                  | 0.399      |\n","|    value_loss           | 4.99       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 333     |\n","|    time_elapsed    | 4616    |\n","|    total_timesteps | 4091904 |\n","--------------------------------\n","Eval num_timesteps=4092000, episode_reward=-20.49 +/- 49.87\n","Episode length: 431.00 +/- 56.34\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 431          |\n","|    mean_reward          | -20.5        |\n","| time/                   |              |\n","|    total_timesteps      | 4092000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0060489886 |\n","|    clip_fraction        | 0.0475       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -1.89        |\n","|    explained_variance   | 0.497        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 6.7          |\n","|    n_updates            | 3330         |\n","|    policy_gradient_loss | -0.0054      |\n","|    std                  | 0.399        |\n","|    value_loss           | 19.7         |\n","------------------------------------------\n","Eval num_timesteps=4104000, episode_reward=34.42 +/- 3.61\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 34.4     |\n","| time/              |          |\n","|    total_timesteps | 4104000  |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 334     |\n","|    time_elapsed    | 4635    |\n","|    total_timesteps | 4104192 |\n","--------------------------------\n","Eval num_timesteps=4116000, episode_reward=-19.35 +/- 43.78\n","Episode length: 498.20 +/- 1.47\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 498         |\n","|    mean_reward          | -19.3       |\n","| time/                   |             |\n","|    total_timesteps      | 4116000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015272836 |\n","|    clip_fraction        | 0.188       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.9        |\n","|    explained_variance   | 0.514       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.276       |\n","|    n_updates            | 3340        |\n","|    policy_gradient_loss | -0.00912    |\n","|    std                  | 0.401       |\n","|    value_loss           | 0.539       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 335     |\n","|    time_elapsed    | 4647    |\n","|    total_timesteps | 4116480 |\n","--------------------------------\n","Eval num_timesteps=4128000, episode_reward=41.90 +/- 0.69\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 41.9        |\n","| time/                   |             |\n","|    total_timesteps      | 4128000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012877733 |\n","|    clip_fraction        | 0.15        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.91       |\n","|    explained_variance   | 0.69        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.545       |\n","|    n_updates            | 3350        |\n","|    policy_gradient_loss | -0.00579    |\n","|    std                  | 0.402       |\n","|    value_loss           | 3.55        |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 336     |\n","|    time_elapsed    | 4662    |\n","|    total_timesteps | 4128768 |\n","--------------------------------\n","Eval num_timesteps=4140000, episode_reward=38.70 +/- 1.18\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 38.7        |\n","| time/                   |             |\n","|    total_timesteps      | 4140000     |\n","| train/                  |             |\n","|    approx_kl            | 0.016761426 |\n","|    clip_fraction        | 0.195       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.89       |\n","|    explained_variance   | 0.764       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.206       |\n","|    n_updates            | 3360        |\n","|    policy_gradient_loss | -0.0107     |\n","|    std                  | 0.397       |\n","|    value_loss           | 0.454       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 337     |\n","|    time_elapsed    | 4675    |\n","|    total_timesteps | 4141056 |\n","--------------------------------\n","Eval num_timesteps=4152000, episode_reward=38.86 +/- 1.29\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 38.9        |\n","| time/                   |             |\n","|    total_timesteps      | 4152000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009202663 |\n","|    clip_fraction        | 0.149       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.88       |\n","|    explained_variance   | 0.134       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 7.38        |\n","|    n_updates            | 3370        |\n","|    policy_gradient_loss | -0.00716    |\n","|    std                  | 0.398       |\n","|    value_loss           | 14.6        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 338     |\n","|    time_elapsed    | 4687    |\n","|    total_timesteps | 4153344 |\n","--------------------------------\n","Eval num_timesteps=4164000, episode_reward=32.25 +/- 2.64\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 32.3       |\n","| time/                   |            |\n","|    total_timesteps      | 4164000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01533024 |\n","|    clip_fraction        | 0.183      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.87      |\n","|    explained_variance   | 0.699      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.23       |\n","|    n_updates            | 3380       |\n","|    policy_gradient_loss | -0.011     |\n","|    std                  | 0.395      |\n","|    value_loss           | 0.524      |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 339     |\n","|    time_elapsed    | 4702    |\n","|    total_timesteps | 4165632 |\n","--------------------------------\n","Eval num_timesteps=4176000, episode_reward=28.47 +/- 6.30\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 28.5        |\n","| time/                   |             |\n","|    total_timesteps      | 4176000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012452518 |\n","|    clip_fraction        | 0.166       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.83       |\n","|    explained_variance   | 0.833       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.153       |\n","|    n_updates            | 3390        |\n","|    policy_gradient_loss | -0.0105     |\n","|    std                  | 0.392       |\n","|    value_loss           | 0.433       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 340     |\n","|    time_elapsed    | 4716    |\n","|    total_timesteps | 4177920 |\n","--------------------------------\n","Eval num_timesteps=4188000, episode_reward=32.27 +/- 0.34\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 32.3        |\n","| time/                   |             |\n","|    total_timesteps      | 4188000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013086473 |\n","|    clip_fraction        | 0.17        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.8        |\n","|    explained_variance   | 0.721       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.216       |\n","|    n_updates            | 3400        |\n","|    policy_gradient_loss | -0.0119     |\n","|    std                  | 0.391       |\n","|    value_loss           | 0.515       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 341     |\n","|    time_elapsed    | 4728    |\n","|    total_timesteps | 4190208 |\n","--------------------------------\n","Eval num_timesteps=4200000, episode_reward=34.20 +/- 1.45\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 34.2        |\n","| time/                   |             |\n","|    total_timesteps      | 4200000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013460397 |\n","|    clip_fraction        | 0.178       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.81       |\n","|    explained_variance   | 0.629       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.183       |\n","|    n_updates            | 3410        |\n","|    policy_gradient_loss | -0.0099     |\n","|    std                  | 0.391       |\n","|    value_loss           | 0.525       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 342     |\n","|    time_elapsed    | 4743    |\n","|    total_timesteps | 4202496 |\n","--------------------------------\n","Eval num_timesteps=4212000, episode_reward=34.08 +/- 5.21\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 34.1         |\n","| time/                   |              |\n","|    total_timesteps      | 4212000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0075856126 |\n","|    clip_fraction        | 0.105        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -1.81        |\n","|    explained_variance   | 0.441        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 1.69         |\n","|    n_updates            | 3420         |\n","|    policy_gradient_loss | -0.0055      |\n","|    std                  | 0.392        |\n","|    value_loss           | 6.15         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 343     |\n","|    time_elapsed    | 4756    |\n","|    total_timesteps | 4214784 |\n","--------------------------------\n","Eval num_timesteps=4224000, episode_reward=-1.79 +/- 43.02\n","Episode length: 478.40 +/- 26.45\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 478         |\n","|    mean_reward          | -1.79       |\n","| time/                   |             |\n","|    total_timesteps      | 4224000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013727389 |\n","|    clip_fraction        | 0.159       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.82       |\n","|    explained_variance   | 0.492       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.227       |\n","|    n_updates            | 3430        |\n","|    policy_gradient_loss | -0.0115     |\n","|    std                  | 0.393       |\n","|    value_loss           | 0.557       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 344     |\n","|    time_elapsed    | 4769    |\n","|    total_timesteps | 4227072 |\n","--------------------------------\n","Eval num_timesteps=4236000, episode_reward=32.79 +/- 0.66\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 32.8        |\n","| time/                   |             |\n","|    total_timesteps      | 4236000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010841445 |\n","|    clip_fraction        | 0.126       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.82       |\n","|    explained_variance   | 0.153       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.49        |\n","|    n_updates            | 3440        |\n","|    policy_gradient_loss | -0.00604    |\n","|    std                  | 0.393       |\n","|    value_loss           | 10.8        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 345     |\n","|    time_elapsed    | 4784    |\n","|    total_timesteps | 4239360 |\n","--------------------------------\n","Eval num_timesteps=4248000, episode_reward=34.72 +/- 0.60\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 34.7        |\n","| time/                   |             |\n","|    total_timesteps      | 4248000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011823411 |\n","|    clip_fraction        | 0.132       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.81       |\n","|    explained_variance   | -0.24       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.625       |\n","|    n_updates            | 3450        |\n","|    policy_gradient_loss | -0.0101     |\n","|    std                  | 0.391       |\n","|    value_loss           | 1.8         |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 346     |\n","|    time_elapsed    | 4795    |\n","|    total_timesteps | 4251648 |\n","--------------------------------\n","Eval num_timesteps=4260000, episode_reward=35.75 +/- 0.34\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 35.7        |\n","| time/                   |             |\n","|    total_timesteps      | 4260000     |\n","| train/                  |             |\n","|    approx_kl            | 0.016916744 |\n","|    clip_fraction        | 0.163       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.8        |\n","|    explained_variance   | 0.286       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.261       |\n","|    n_updates            | 3460        |\n","|    policy_gradient_loss | -0.0104     |\n","|    std                  | 0.39        |\n","|    value_loss           | 0.833       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 347     |\n","|    time_elapsed    | 4809    |\n","|    total_timesteps | 4263936 |\n","--------------------------------\n","Eval num_timesteps=4272000, episode_reward=40.42 +/- 2.40\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 40.4        |\n","| time/                   |             |\n","|    total_timesteps      | 4272000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014679913 |\n","|    clip_fraction        | 0.186       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.8        |\n","|    explained_variance   | 0.771       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.162       |\n","|    n_updates            | 3470        |\n","|    policy_gradient_loss | -0.0124     |\n","|    std                  | 0.391       |\n","|    value_loss           | 0.368       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 348     |\n","|    time_elapsed    | 4824    |\n","|    total_timesteps | 4276224 |\n","--------------------------------\n","Eval num_timesteps=4284000, episode_reward=36.24 +/- 5.39\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 36.2        |\n","| time/                   |             |\n","|    total_timesteps      | 4284000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010545907 |\n","|    clip_fraction        | 0.15        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.79       |\n","|    explained_variance   | 0.713       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.29        |\n","|    n_updates            | 3480        |\n","|    policy_gradient_loss | -0.00859    |\n","|    std                  | 0.39        |\n","|    value_loss           | 3.85        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 349     |\n","|    time_elapsed    | 4835    |\n","|    total_timesteps | 4288512 |\n","--------------------------------\n","Eval num_timesteps=4296000, episode_reward=28.61 +/- 0.90\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 28.6        |\n","| time/                   |             |\n","|    total_timesteps      | 4296000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008101029 |\n","|    clip_fraction        | 0.0839      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.79       |\n","|    explained_variance   | 0.351       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.31        |\n","|    n_updates            | 3490        |\n","|    policy_gradient_loss | -0.00552    |\n","|    std                  | 0.39        |\n","|    value_loss           | 10.3        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 350     |\n","|    time_elapsed    | 4850    |\n","|    total_timesteps | 4300800 |\n","--------------------------------\n","Eval num_timesteps=4308000, episode_reward=28.77 +/- 1.91\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 28.8        |\n","| time/                   |             |\n","|    total_timesteps      | 4308000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008174365 |\n","|    clip_fraction        | 0.0974      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.8        |\n","|    explained_variance   | 0.296       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.37        |\n","|    n_updates            | 3500        |\n","|    policy_gradient_loss | -0.00686    |\n","|    std                  | 0.391       |\n","|    value_loss           | 13.8        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 351     |\n","|    time_elapsed    | 4865    |\n","|    total_timesteps | 4313088 |\n","--------------------------------\n","Eval num_timesteps=4320000, episode_reward=40.76 +/- 1.68\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 40.8        |\n","| time/                   |             |\n","|    total_timesteps      | 4320000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014833377 |\n","|    clip_fraction        | 0.156       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.78       |\n","|    explained_variance   | 0.847       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.232       |\n","|    n_updates            | 3510        |\n","|    policy_gradient_loss | -0.00934    |\n","|    std                  | 0.386       |\n","|    value_loss           | 0.734       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 352     |\n","|    time_elapsed    | 4876    |\n","|    total_timesteps | 4325376 |\n","--------------------------------\n","Eval num_timesteps=4332000, episode_reward=28.01 +/- 9.63\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 28           |\n","| time/                   |              |\n","|    total_timesteps      | 4332000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0095754815 |\n","|    clip_fraction        | 0.135        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -1.76        |\n","|    explained_variance   | 0.46         |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 9.03         |\n","|    n_updates            | 3520         |\n","|    policy_gradient_loss | -0.00665     |\n","|    std                  | 0.387        |\n","|    value_loss           | 13.2         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 353     |\n","|    time_elapsed    | 4891    |\n","|    total_timesteps | 4337664 |\n","--------------------------------\n","Eval num_timesteps=4344000, episode_reward=40.13 +/- 3.27\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 40.1        |\n","| time/                   |             |\n","|    total_timesteps      | 4344000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014142689 |\n","|    clip_fraction        | 0.168       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.75       |\n","|    explained_variance   | 0.427       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.213       |\n","|    n_updates            | 3530        |\n","|    policy_gradient_loss | -0.01       |\n","|    std                  | 0.383       |\n","|    value_loss           | 0.622       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 354     |\n","|    time_elapsed    | 4906    |\n","|    total_timesteps | 4349952 |\n","--------------------------------\n","Eval num_timesteps=4356000, episode_reward=32.21 +/- 0.13\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 32.2        |\n","| time/                   |             |\n","|    total_timesteps      | 4356000     |\n","| train/                  |             |\n","|    approx_kl            | 0.016216679 |\n","|    clip_fraction        | 0.178       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.71       |\n","|    explained_variance   | 0.775       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.192       |\n","|    n_updates            | 3540        |\n","|    policy_gradient_loss | -0.013      |\n","|    std                  | 0.38        |\n","|    value_loss           | 0.553       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 355     |\n","|    time_elapsed    | 4918    |\n","|    total_timesteps | 4362240 |\n","--------------------------------\n","Eval num_timesteps=4368000, episode_reward=39.06 +/- 1.00\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 39.1       |\n","| time/                   |            |\n","|    total_timesteps      | 4368000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01418937 |\n","|    clip_fraction        | 0.162      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.68      |\n","|    explained_variance   | 0.814      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.178      |\n","|    n_updates            | 3550       |\n","|    policy_gradient_loss | -0.0109    |\n","|    std                  | 0.378      |\n","|    value_loss           | 0.356      |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 356     |\n","|    time_elapsed    | 4933    |\n","|    total_timesteps | 4374528 |\n","--------------------------------\n","Eval num_timesteps=4380000, episode_reward=38.18 +/- 8.34\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 38.2        |\n","| time/                   |             |\n","|    total_timesteps      | 4380000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011366077 |\n","|    clip_fraction        | 0.159       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.68       |\n","|    explained_variance   | 0.369       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.3         |\n","|    n_updates            | 3560        |\n","|    policy_gradient_loss | -0.00619    |\n","|    std                  | 0.379       |\n","|    value_loss           | 3.41        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 357     |\n","|    time_elapsed    | 4948    |\n","|    total_timesteps | 4386816 |\n","--------------------------------\n","Eval num_timesteps=4392000, episode_reward=30.00 +/- 2.56\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 30         |\n","| time/                   |            |\n","|    total_timesteps      | 4392000    |\n","| train/                  |            |\n","|    approx_kl            | 0.00828195 |\n","|    clip_fraction        | 0.124      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.68      |\n","|    explained_variance   | 0.294      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 2.98       |\n","|    n_updates            | 3570       |\n","|    policy_gradient_loss | -0.00748   |\n","|    std                  | 0.379      |\n","|    value_loss           | 11.4       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 358     |\n","|    time_elapsed    | 4959    |\n","|    total_timesteps | 4399104 |\n","--------------------------------\n","Eval num_timesteps=4404000, episode_reward=35.46 +/- 0.39\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 35.5        |\n","| time/                   |             |\n","|    total_timesteps      | 4404000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009962995 |\n","|    clip_fraction        | 0.0759      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.68       |\n","|    explained_variance   | 0.623       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.581       |\n","|    n_updates            | 3580        |\n","|    policy_gradient_loss | -0.00503    |\n","|    std                  | 0.378       |\n","|    value_loss           | 9.79        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 359     |\n","|    time_elapsed    | 4974    |\n","|    total_timesteps | 4411392 |\n","--------------------------------\n","Eval num_timesteps=4416000, episode_reward=33.57 +/- 3.51\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 33.6        |\n","| time/                   |             |\n","|    total_timesteps      | 4416000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010688151 |\n","|    clip_fraction        | 0.109       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.67       |\n","|    explained_variance   | 0.488       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.23        |\n","|    n_updates            | 3590        |\n","|    policy_gradient_loss | -0.0077     |\n","|    std                  | 0.377       |\n","|    value_loss           | 12.3        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 360     |\n","|    time_elapsed    | 4989    |\n","|    total_timesteps | 4423680 |\n","--------------------------------\n","Eval num_timesteps=4428000, episode_reward=40.60 +/- 1.62\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 40.6         |\n","| time/                   |              |\n","|    total_timesteps      | 4428000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0128445355 |\n","|    clip_fraction        | 0.121        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -1.65        |\n","|    explained_variance   | 0.347        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 2.95         |\n","|    n_updates            | 3600         |\n","|    policy_gradient_loss | -0.00809     |\n","|    std                  | 0.375        |\n","|    value_loss           | 7.21         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 887     |\n","|    iterations      | 361     |\n","|    time_elapsed    | 5000    |\n","|    total_timesteps | 4435968 |\n","--------------------------------\n","Eval num_timesteps=4440000, episode_reward=31.86 +/- 3.57\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 31.9        |\n","| time/                   |             |\n","|    total_timesteps      | 4440000     |\n","| train/                  |             |\n","|    approx_kl            | 0.016638437 |\n","|    clip_fraction        | 0.193       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.65       |\n","|    explained_variance   | 0.313       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.99        |\n","|    n_updates            | 3610        |\n","|    policy_gradient_loss | -0.00614    |\n","|    std                  | 0.375       |\n","|    value_loss           | 11.6        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 362     |\n","|    time_elapsed    | 5016    |\n","|    total_timesteps | 4448256 |\n","--------------------------------\n","Eval num_timesteps=4452000, episode_reward=-18.44 +/- 45.46\n","Episode length: 447.20 +/- 43.11\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 447         |\n","|    mean_reward          | -18.4       |\n","| time/                   |             |\n","|    total_timesteps      | 4452000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009810084 |\n","|    clip_fraction        | 0.131       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.66       |\n","|    explained_variance   | 0.489       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.12        |\n","|    n_updates            | 3620        |\n","|    policy_gradient_loss | -0.0078     |\n","|    std                  | 0.377       |\n","|    value_loss           | 11          |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 363     |\n","|    time_elapsed    | 5030    |\n","|    total_timesteps | 4460544 |\n","--------------------------------\n","Eval num_timesteps=4464000, episode_reward=43.01 +/- 3.37\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 43          |\n","| time/                   |             |\n","|    total_timesteps      | 4464000     |\n","| train/                  |             |\n","|    approx_kl            | 0.016433487 |\n","|    clip_fraction        | 0.217       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.64       |\n","|    explained_variance   | 0.649       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.22        |\n","|    n_updates            | 3630        |\n","|    policy_gradient_loss | -0.0109     |\n","|    std                  | 0.373       |\n","|    value_loss           | 0.534       |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 887     |\n","|    iterations      | 364     |\n","|    time_elapsed    | 5042    |\n","|    total_timesteps | 4472832 |\n","--------------------------------\n","Eval num_timesteps=4476000, episode_reward=33.24 +/- 0.02\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 33.2        |\n","| time/                   |             |\n","|    total_timesteps      | 4476000     |\n","| train/                  |             |\n","|    approx_kl            | 0.018163905 |\n","|    clip_fraction        | 0.211       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.61       |\n","|    explained_variance   | 0.797       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.247       |\n","|    n_updates            | 3640        |\n","|    policy_gradient_loss | -0.00911    |\n","|    std                  | 0.37        |\n","|    value_loss           | 0.801       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 365     |\n","|    time_elapsed    | 5057    |\n","|    total_timesteps | 4485120 |\n","--------------------------------\n","Eval num_timesteps=4488000, episode_reward=-7.70 +/- 50.19\n","Episode length: 460.80 +/- 48.01\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 461         |\n","|    mean_reward          | -7.7        |\n","| time/                   |             |\n","|    total_timesteps      | 4488000     |\n","| train/                  |             |\n","|    approx_kl            | 0.018483652 |\n","|    clip_fraction        | 0.206       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.6        |\n","|    explained_variance   | 0.68        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.58        |\n","|    n_updates            | 3650        |\n","|    policy_gradient_loss | -0.00474    |\n","|    std                  | 0.37        |\n","|    value_loss           | 1.63        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 366     |\n","|    time_elapsed    | 5071    |\n","|    total_timesteps | 4497408 |\n","--------------------------------\n","Eval num_timesteps=4500000, episode_reward=35.14 +/- 1.53\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 35.1        |\n","| time/                   |             |\n","|    total_timesteps      | 4500000     |\n","| train/                  |             |\n","|    approx_kl            | 0.016035913 |\n","|    clip_fraction        | 0.199       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.58       |\n","|    explained_variance   | 0.811       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.216       |\n","|    n_updates            | 3660        |\n","|    policy_gradient_loss | -0.0113     |\n","|    std                  | 0.368       |\n","|    value_loss           | 0.467       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 887     |\n","|    iterations      | 367     |\n","|    time_elapsed    | 5083    |\n","|    total_timesteps | 4509696 |\n","--------------------------------\n","Eval num_timesteps=4512000, episode_reward=39.71 +/- 2.14\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 39.7       |\n","| time/                   |            |\n","|    total_timesteps      | 4512000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01580108 |\n","|    clip_fraction        | 0.177      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.58      |\n","|    explained_variance   | 0.693      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.249      |\n","|    n_updates            | 3670       |\n","|    policy_gradient_loss | -0.00947   |\n","|    std                  | 0.369      |\n","|    value_loss           | 0.636      |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 368     |\n","|    time_elapsed    | 5099    |\n","|    total_timesteps | 4521984 |\n","--------------------------------\n","Eval num_timesteps=4524000, episode_reward=-4.05 +/- 55.12\n","Episode length: 454.80 +/- 55.36\n","---------------------------------------\n","| eval/                   |           |\n","|    mean_ep_length       | 455       |\n","|    mean_reward          | -4.05     |\n","| time/                   |           |\n","|    total_timesteps      | 4524000   |\n","| train/                  |           |\n","|    approx_kl            | 0.0082319 |\n","|    clip_fraction        | 0.131     |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -1.58     |\n","|    explained_variance   | 0.207     |\n","|    learning_rate        | 0.0003    |\n","|    loss                 | 3.4       |\n","|    n_updates            | 3680      |\n","|    policy_gradient_loss | -0.00466  |\n","|    std                  | 0.368     |\n","|    value_loss           | 10.6      |\n","---------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 369     |\n","|    time_elapsed    | 5113    |\n","|    total_timesteps | 4534272 |\n","--------------------------------\n","Eval num_timesteps=4536000, episode_reward=-25.90 +/- 49.23\n","Episode length: 498.20 +/- 1.47\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 498         |\n","|    mean_reward          | -25.9       |\n","| time/                   |             |\n","|    total_timesteps      | 4536000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014826149 |\n","|    clip_fraction        | 0.171       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.57       |\n","|    explained_variance   | 0.726       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.253       |\n","|    n_updates            | 3690        |\n","|    policy_gradient_loss | -0.00966    |\n","|    std                  | 0.367       |\n","|    value_loss           | 0.505       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 370     |\n","|    time_elapsed    | 5126    |\n","|    total_timesteps | 4546560 |\n","--------------------------------\n","Eval num_timesteps=4548000, episode_reward=34.02 +/- 1.84\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 34          |\n","| time/                   |             |\n","|    total_timesteps      | 4548000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013634466 |\n","|    clip_fraction        | 0.167       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.55       |\n","|    explained_variance   | 0.841       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.134       |\n","|    n_updates            | 3700        |\n","|    policy_gradient_loss | -0.0114     |\n","|    std                  | 0.364       |\n","|    value_loss           | 0.366       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 371     |\n","|    time_elapsed    | 5141    |\n","|    total_timesteps | 4558848 |\n","--------------------------------\n","Eval num_timesteps=4560000, episode_reward=6.51 +/- 41.67\n","Episode length: 489.20 +/- 13.23\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 489        |\n","|    mean_reward          | 6.51       |\n","| time/                   |            |\n","|    total_timesteps      | 4560000    |\n","| train/                  |            |\n","|    approx_kl            | 0.00866004 |\n","|    clip_fraction        | 0.131      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.53      |\n","|    explained_variance   | 0.215      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 5.88       |\n","|    n_updates            | 3710       |\n","|    policy_gradient_loss | -0.00752   |\n","|    std                  | 0.364      |\n","|    value_loss           | 19.2       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 372     |\n","|    time_elapsed    | 5156    |\n","|    total_timesteps | 4571136 |\n","--------------------------------\n","Eval num_timesteps=4572000, episode_reward=31.60 +/- 0.23\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 31.6        |\n","| time/                   |             |\n","|    total_timesteps      | 4572000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012125411 |\n","|    clip_fraction        | 0.153       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.54       |\n","|    explained_variance   | 0.0537      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 7.19        |\n","|    n_updates            | 3720        |\n","|    policy_gradient_loss | -0.00937    |\n","|    std                  | 0.365       |\n","|    value_loss           | 17.8        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 373     |\n","|    time_elapsed    | 5168    |\n","|    total_timesteps | 4583424 |\n","--------------------------------\n","Eval num_timesteps=4584000, episode_reward=28.86 +/- 7.42\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 28.9        |\n","| time/                   |             |\n","|    total_timesteps      | 4584000     |\n","| train/                  |             |\n","|    approx_kl            | 0.016107364 |\n","|    clip_fraction        | 0.178       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.54       |\n","|    explained_variance   | 0.409       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.348       |\n","|    n_updates            | 3730        |\n","|    policy_gradient_loss | -0.0117     |\n","|    std                  | 0.363       |\n","|    value_loss           | 0.801       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 374     |\n","|    time_elapsed    | 5183    |\n","|    total_timesteps | 4595712 |\n","--------------------------------\n","Eval num_timesteps=4596000, episode_reward=40.99 +/- 1.33\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 41          |\n","| time/                   |             |\n","|    total_timesteps      | 4596000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010870892 |\n","|    clip_fraction        | 0.134       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.53       |\n","|    explained_variance   | 0.228       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 8.52        |\n","|    n_updates            | 3740        |\n","|    policy_gradient_loss | -0.00539    |\n","|    std                  | 0.364       |\n","|    value_loss           | 10.4        |\n","-----------------------------------------\n","Eval num_timesteps=4608000, episode_reward=36.49 +/- 1.21\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 36.5     |\n","| time/              |          |\n","|    total_timesteps | 4608000  |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 375     |\n","|    time_elapsed    | 5203    |\n","|    total_timesteps | 4608000 |\n","--------------------------------\n","Eval num_timesteps=4620000, episode_reward=40.52 +/- 1.75\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 40.5        |\n","| time/                   |             |\n","|    total_timesteps      | 4620000     |\n","| train/                  |             |\n","|    approx_kl            | 0.019984225 |\n","|    clip_fraction        | 0.216       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.51       |\n","|    explained_variance   | -0.0276     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.349       |\n","|    n_updates            | 3750        |\n","|    policy_gradient_loss | -0.00967    |\n","|    std                  | 0.361       |\n","|    value_loss           | 0.87        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 376     |\n","|    time_elapsed    | 5217    |\n","|    total_timesteps | 4620288 |\n","--------------------------------\n","Eval num_timesteps=4632000, episode_reward=34.53 +/- 3.05\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 34.5        |\n","| time/                   |             |\n","|    total_timesteps      | 4632000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015387905 |\n","|    clip_fraction        | 0.17        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.51       |\n","|    explained_variance   | 0.686       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.321       |\n","|    n_updates            | 3760        |\n","|    policy_gradient_loss | -0.0143     |\n","|    std                  | 0.362       |\n","|    value_loss           | 0.648       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 377     |\n","|    time_elapsed    | 5229    |\n","|    total_timesteps | 4632576 |\n","--------------------------------\n","Eval num_timesteps=4644000, episode_reward=34.86 +/- 5.05\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 34.9        |\n","| time/                   |             |\n","|    total_timesteps      | 4644000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015936894 |\n","|    clip_fraction        | 0.187       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.5        |\n","|    explained_variance   | 0.723       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.328       |\n","|    n_updates            | 3770        |\n","|    policy_gradient_loss | -0.0135     |\n","|    std                  | 0.359       |\n","|    value_loss           | 0.551       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 378     |\n","|    time_elapsed    | 5245    |\n","|    total_timesteps | 4644864 |\n","--------------------------------\n","Eval num_timesteps=4656000, episode_reward=-9.97 +/- 39.64\n","Episode length: 434.00 +/- 53.89\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 434         |\n","|    mean_reward          | -9.97       |\n","| time/                   |             |\n","|    total_timesteps      | 4656000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015209592 |\n","|    clip_fraction        | 0.18        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.49       |\n","|    explained_variance   | 0.668       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.298       |\n","|    n_updates            | 3780        |\n","|    policy_gradient_loss | -0.011      |\n","|    std                  | 0.359       |\n","|    value_loss           | 0.624       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 379     |\n","|    time_elapsed    | 5258    |\n","|    total_timesteps | 4657152 |\n","--------------------------------\n","Eval num_timesteps=4668000, episode_reward=38.05 +/- 0.12\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 38.1        |\n","| time/                   |             |\n","|    total_timesteps      | 4668000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014106815 |\n","|    clip_fraction        | 0.178       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.48       |\n","|    explained_variance   | 0.479       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.21        |\n","|    n_updates            | 3790        |\n","|    policy_gradient_loss | -0.0117     |\n","|    std                  | 0.359       |\n","|    value_loss           | 0.602       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 380     |\n","|    time_elapsed    | 5271    |\n","|    total_timesteps | 4669440 |\n","--------------------------------\n","Eval num_timesteps=4680000, episode_reward=36.10 +/- 1.87\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 36.1        |\n","| time/                   |             |\n","|    total_timesteps      | 4680000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014222495 |\n","|    clip_fraction        | 0.168       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.47       |\n","|    explained_variance   | 0.707       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.283       |\n","|    n_updates            | 3800        |\n","|    policy_gradient_loss | -0.0125     |\n","|    std                  | 0.358       |\n","|    value_loss           | 0.508       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 381     |\n","|    time_elapsed    | 5287    |\n","|    total_timesteps | 4681728 |\n","--------------------------------\n","Eval num_timesteps=4692000, episode_reward=42.38 +/- 0.39\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 42.4         |\n","| time/                   |              |\n","|    total_timesteps      | 4692000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0151054235 |\n","|    clip_fraction        | 0.169        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -1.47        |\n","|    explained_variance   | 0.452        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.299        |\n","|    n_updates            | 3810         |\n","|    policy_gradient_loss | -0.0116      |\n","|    std                  | 0.357        |\n","|    value_loss           | 0.665        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 382     |\n","|    time_elapsed    | 5300    |\n","|    total_timesteps | 4694016 |\n","--------------------------------\n","Eval num_timesteps=4704000, episode_reward=39.81 +/- 2.49\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 39.8        |\n","| time/                   |             |\n","|    total_timesteps      | 4704000     |\n","| train/                  |             |\n","|    approx_kl            | 0.016004361 |\n","|    clip_fraction        | 0.171       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.47       |\n","|    explained_variance   | 0.548       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.287       |\n","|    n_updates            | 3820        |\n","|    policy_gradient_loss | -0.0113     |\n","|    std                  | 0.358       |\n","|    value_loss           | 0.595       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 383     |\n","|    time_elapsed    | 5313    |\n","|    total_timesteps | 4706304 |\n","--------------------------------\n","Eval num_timesteps=4716000, episode_reward=38.72 +/- 0.59\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 38.7        |\n","| time/                   |             |\n","|    total_timesteps      | 4716000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013878438 |\n","|    clip_fraction        | 0.15        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.47       |\n","|    explained_variance   | 0.53        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.349       |\n","|    n_updates            | 3830        |\n","|    policy_gradient_loss | -0.00994    |\n","|    std                  | 0.358       |\n","|    value_loss           | 0.66        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 384     |\n","|    time_elapsed    | 5328    |\n","|    total_timesteps | 4718592 |\n","--------------------------------\n","Eval num_timesteps=4728000, episode_reward=35.45 +/- 4.88\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 35.4        |\n","| time/                   |             |\n","|    total_timesteps      | 4728000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009818759 |\n","|    clip_fraction        | 0.121       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.47       |\n","|    explained_variance   | 0.0938      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 5.4         |\n","|    n_updates            | 3840        |\n","|    policy_gradient_loss | -0.00584    |\n","|    std                  | 0.358       |\n","|    value_loss           | 8.85        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 385     |\n","|    time_elapsed    | 5341    |\n","|    total_timesteps | 4730880 |\n","--------------------------------\n","Eval num_timesteps=4740000, episode_reward=41.40 +/- 0.27\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 41.4        |\n","| time/                   |             |\n","|    total_timesteps      | 4740000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009626746 |\n","|    clip_fraction        | 0.116       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.47       |\n","|    explained_variance   | 0.0664      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.46        |\n","|    n_updates            | 3850        |\n","|    policy_gradient_loss | -0.00893    |\n","|    std                  | 0.358       |\n","|    value_loss           | 8.58        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 386     |\n","|    time_elapsed    | 5354    |\n","|    total_timesteps | 4743168 |\n","--------------------------------\n","Eval num_timesteps=4752000, episode_reward=38.57 +/- 1.53\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 38.6        |\n","| time/                   |             |\n","|    total_timesteps      | 4752000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015027777 |\n","|    clip_fraction        | 0.189       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.47       |\n","|    explained_variance   | 0.258       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.301       |\n","|    n_updates            | 3860        |\n","|    policy_gradient_loss | -0.0145     |\n","|    std                  | 0.358       |\n","|    value_loss           | 0.726       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 387     |\n","|    time_elapsed    | 5370    |\n","|    total_timesteps | 4755456 |\n","--------------------------------\n","Eval num_timesteps=4764000, episode_reward=36.64 +/- 7.58\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 36.6        |\n","| time/                   |             |\n","|    total_timesteps      | 4764000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014962192 |\n","|    clip_fraction        | 0.17        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.47       |\n","|    explained_variance   | 0.334       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.233       |\n","|    n_updates            | 3870        |\n","|    policy_gradient_loss | -0.0102     |\n","|    std                  | 0.357       |\n","|    value_loss           | 0.82        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 388     |\n","|    time_elapsed    | 5383    |\n","|    total_timesteps | 4767744 |\n","--------------------------------\n","Eval num_timesteps=4776000, episode_reward=30.06 +/- 0.79\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 30.1        |\n","| time/                   |             |\n","|    total_timesteps      | 4776000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011543696 |\n","|    clip_fraction        | 0.154       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.47       |\n","|    explained_variance   | 0.737       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.55        |\n","|    n_updates            | 3880        |\n","|    policy_gradient_loss | -0.00787    |\n","|    std                  | 0.359       |\n","|    value_loss           | 3.61        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 389     |\n","|    time_elapsed    | 5396    |\n","|    total_timesteps | 4780032 |\n","--------------------------------\n","Eval num_timesteps=4788000, episode_reward=32.08 +/- 0.76\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 32.1        |\n","| time/                   |             |\n","|    total_timesteps      | 4788000     |\n","| train/                  |             |\n","|    approx_kl            | 0.007708512 |\n","|    clip_fraction        | 0.0871      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.48       |\n","|    explained_variance   | 0.115       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 20.4        |\n","|    n_updates            | 3890        |\n","|    policy_gradient_loss | -0.00775    |\n","|    std                  | 0.359       |\n","|    value_loss           | 38.8        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 390     |\n","|    time_elapsed    | 5412    |\n","|    total_timesteps | 4792320 |\n","--------------------------------\n","Eval num_timesteps=4800000, episode_reward=40.23 +/- 0.34\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 40.2        |\n","| time/                   |             |\n","|    total_timesteps      | 4800000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010975766 |\n","|    clip_fraction        | 0.117       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.48       |\n","|    explained_variance   | 0.0761      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.95        |\n","|    n_updates            | 3900        |\n","|    policy_gradient_loss | -0.00642    |\n","|    std                  | 0.359       |\n","|    value_loss           | 13.7        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 391     |\n","|    time_elapsed    | 5424    |\n","|    total_timesteps | 4804608 |\n","--------------------------------\n","Eval num_timesteps=4812000, episode_reward=42.84 +/- 0.17\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 42.8        |\n","| time/                   |             |\n","|    total_timesteps      | 4812000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015432759 |\n","|    clip_fraction        | 0.174       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.46       |\n","|    explained_variance   | -0.136      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.416       |\n","|    n_updates            | 3910        |\n","|    policy_gradient_loss | -0.0102     |\n","|    std                  | 0.356       |\n","|    value_loss           | 1.26        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 392     |\n","|    time_elapsed    | 5438    |\n","|    total_timesteps | 4816896 |\n","--------------------------------\n","Eval num_timesteps=4824000, episode_reward=31.79 +/- 7.13\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 31.8       |\n","| time/                   |            |\n","|    total_timesteps      | 4824000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01714996 |\n","|    clip_fraction        | 0.194      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.42      |\n","|    explained_variance   | 0.477      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.489      |\n","|    n_updates            | 3920       |\n","|    policy_gradient_loss | -0.0109    |\n","|    std                  | 0.352      |\n","|    value_loss           | 0.858      |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 393     |\n","|    time_elapsed    | 5453    |\n","|    total_timesteps | 4829184 |\n","--------------------------------\n","Eval num_timesteps=4836000, episode_reward=35.37 +/- 0.41\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 35.4        |\n","| time/                   |             |\n","|    total_timesteps      | 4836000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015642447 |\n","|    clip_fraction        | 0.175       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.39       |\n","|    explained_variance   | 0.548       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.252       |\n","|    n_updates            | 3930        |\n","|    policy_gradient_loss | -0.011      |\n","|    std                  | 0.351       |\n","|    value_loss           | 0.538       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 394     |\n","|    time_elapsed    | 5466    |\n","|    total_timesteps | 4841472 |\n","--------------------------------\n","Eval num_timesteps=4848000, episode_reward=35.84 +/- 1.72\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 35.8        |\n","| time/                   |             |\n","|    total_timesteps      | 4848000     |\n","| train/                  |             |\n","|    approx_kl            | 0.017093489 |\n","|    clip_fraction        | 0.178       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.39       |\n","|    explained_variance   | 0.659       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.212       |\n","|    n_updates            | 3940        |\n","|    policy_gradient_loss | -0.0117     |\n","|    std                  | 0.351       |\n","|    value_loss           | 0.605       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 395     |\n","|    time_elapsed    | 5479    |\n","|    total_timesteps | 4853760 |\n","--------------------------------\n","Eval num_timesteps=4860000, episode_reward=31.83 +/- 0.10\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 31.8        |\n","| time/                   |             |\n","|    total_timesteps      | 4860000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015641363 |\n","|    clip_fraction        | 0.179       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.38       |\n","|    explained_variance   | 0.57        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.211       |\n","|    n_updates            | 3950        |\n","|    policy_gradient_loss | -0.0115     |\n","|    std                  | 0.35        |\n","|    value_loss           | 0.513       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 396     |\n","|    time_elapsed    | 5495    |\n","|    total_timesteps | 4866048 |\n","--------------------------------\n","Eval num_timesteps=4872000, episode_reward=28.71 +/- 16.17\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 28.7        |\n","| time/                   |             |\n","|    total_timesteps      | 4872000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014093481 |\n","|    clip_fraction        | 0.17        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.37       |\n","|    explained_variance   | 0.507       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.275       |\n","|    n_updates            | 3960        |\n","|    policy_gradient_loss | -0.0134     |\n","|    std                  | 0.35        |\n","|    value_loss           | 0.635       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 397     |\n","|    time_elapsed    | 5508    |\n","|    total_timesteps | 4878336 |\n","--------------------------------\n","Eval num_timesteps=4884000, episode_reward=30.18 +/- 5.95\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 30.2       |\n","| time/                   |            |\n","|    total_timesteps      | 4884000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01460491 |\n","|    clip_fraction        | 0.169      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.36      |\n","|    explained_variance   | 0.469      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.193      |\n","|    n_updates            | 3970       |\n","|    policy_gradient_loss | -0.0115    |\n","|    std                  | 0.347      |\n","|    value_loss           | 0.533      |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 398     |\n","|    time_elapsed    | 5521    |\n","|    total_timesteps | 4890624 |\n","--------------------------------\n","Eval num_timesteps=4896000, episode_reward=37.11 +/- 1.30\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 37.1        |\n","| time/                   |             |\n","|    total_timesteps      | 4896000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013724137 |\n","|    clip_fraction        | 0.17        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.36       |\n","|    explained_variance   | 0.738       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.24        |\n","|    n_updates            | 3980        |\n","|    policy_gradient_loss | -0.0118     |\n","|    std                  | 0.348       |\n","|    value_loss           | 0.503       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 399     |\n","|    time_elapsed    | 5537    |\n","|    total_timesteps | 4902912 |\n","--------------------------------\n","Eval num_timesteps=4908000, episode_reward=38.86 +/- 3.40\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 38.9        |\n","| time/                   |             |\n","|    total_timesteps      | 4908000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008716478 |\n","|    clip_fraction        | 0.125       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.35       |\n","|    explained_variance   | 0.149       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.48        |\n","|    n_updates            | 3990        |\n","|    policy_gradient_loss | -0.00788    |\n","|    std                  | 0.347       |\n","|    value_loss           | 12.8        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 400     |\n","|    time_elapsed    | 5550    |\n","|    total_timesteps | 4915200 |\n","--------------------------------\n","Eval num_timesteps=4920000, episode_reward=38.19 +/- 6.34\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 38.2         |\n","| time/                   |              |\n","|    total_timesteps      | 4920000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0140424045 |\n","|    clip_fraction        | 0.169        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -1.35        |\n","|    explained_variance   | 0.421        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.294        |\n","|    n_updates            | 4000         |\n","|    policy_gradient_loss | -0.0128      |\n","|    std                  | 0.346        |\n","|    value_loss           | 0.741        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 401     |\n","|    time_elapsed    | 5563    |\n","|    total_timesteps | 4927488 |\n","--------------------------------\n","Eval num_timesteps=4932000, episode_reward=41.07 +/- 7.61\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 41.1        |\n","| time/                   |             |\n","|    total_timesteps      | 4932000     |\n","| train/                  |             |\n","|    approx_kl            | 0.018205114 |\n","|    clip_fraction        | 0.176       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.32       |\n","|    explained_variance   | 0.466       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.272       |\n","|    n_updates            | 4010        |\n","|    policy_gradient_loss | -0.0104     |\n","|    std                  | 0.344       |\n","|    value_loss           | 0.765       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 402     |\n","|    time_elapsed    | 5579    |\n","|    total_timesteps | 4939776 |\n","--------------------------------\n","Eval num_timesteps=4944000, episode_reward=39.07 +/- 1.24\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 39.1        |\n","| time/                   |             |\n","|    total_timesteps      | 4944000     |\n","| train/                  |             |\n","|    approx_kl            | 0.017289845 |\n","|    clip_fraction        | 0.175       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.3        |\n","|    explained_variance   | 0.775       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.18        |\n","|    n_updates            | 4020        |\n","|    policy_gradient_loss | -0.0119     |\n","|    std                  | 0.342       |\n","|    value_loss           | 0.523       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 403     |\n","|    time_elapsed    | 5591    |\n","|    total_timesteps | 4952064 |\n","--------------------------------\n","Eval num_timesteps=4956000, episode_reward=35.34 +/- 5.72\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 35.3         |\n","| time/                   |              |\n","|    total_timesteps      | 4956000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0154832555 |\n","|    clip_fraction        | 0.179        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -1.28        |\n","|    explained_variance   | 0.484        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.413        |\n","|    n_updates            | 4030         |\n","|    policy_gradient_loss | -0.0106      |\n","|    std                  | 0.34         |\n","|    value_loss           | 1.01         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 404     |\n","|    time_elapsed    | 5605    |\n","|    total_timesteps | 4964352 |\n","--------------------------------\n","Eval num_timesteps=4968000, episode_reward=34.25 +/- 5.69\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 34.3        |\n","| time/                   |             |\n","|    total_timesteps      | 4968000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014158744 |\n","|    clip_fraction        | 0.171       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.27       |\n","|    explained_variance   | 0.752       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.205       |\n","|    n_updates            | 4040        |\n","|    policy_gradient_loss | -0.0106     |\n","|    std                  | 0.339       |\n","|    value_loss           | 0.497       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 405     |\n","|    time_elapsed    | 5621    |\n","|    total_timesteps | 4976640 |\n","--------------------------------\n","Eval num_timesteps=4980000, episode_reward=43.40 +/- 0.55\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 43.4        |\n","| time/                   |             |\n","|    total_timesteps      | 4980000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014740574 |\n","|    clip_fraction        | 0.18        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.26       |\n","|    explained_variance   | 0.612       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.275       |\n","|    n_updates            | 4050        |\n","|    policy_gradient_loss | -0.0116     |\n","|    std                  | 0.338       |\n","|    value_loss           | 0.547       |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 406     |\n","|    time_elapsed    | 5633    |\n","|    total_timesteps | 4988928 |\n","--------------------------------\n","Eval num_timesteps=4992000, episode_reward=42.29 +/- 2.43\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 42.3        |\n","| time/                   |             |\n","|    total_timesteps      | 4992000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012840189 |\n","|    clip_fraction        | 0.16        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.25       |\n","|    explained_variance   | 0.664       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.312       |\n","|    n_updates            | 4060        |\n","|    policy_gradient_loss | -0.0122     |\n","|    std                  | 0.338       |\n","|    value_loss           | 0.614       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 407     |\n","|    time_elapsed    | 5647    |\n","|    total_timesteps | 5001216 |\n","--------------------------------\n"]}]},{"cell_type":"code","source":["############# Ahora con 0.4"],"metadata":{"id":"B1LiIUiIHaa8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3 import PPO\n","# from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n","from pettingzoo.sisl import multiwalker_v9\n","import supersuit as ss\n","from stable_baselines3.common.monitor import Monitor"],"metadata":{"id":"KiwYrrU5Hjxq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env = multiwalker_v9.parallel_env(render_mode=\"rgb_array\",shared_reward=0.4)\n","\n","env = ss.frame_stack_v1(env, 3)\n","env = ss.pettingzoo_env_to_vec_env_v1(env)\n","env = ss.concat_vec_envs_v1(env, 8, num_cpus=4, base_class='stable_baselines3')"],"metadata":{"id":"NTsR96TbHjxs"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DbmTcKhlHjxt"},"outputs":[],"source":["from stable_baselines3.common.callbacks import EvalCallback\n","eval_callback = EvalCallback(env, best_model_save_path=\"./multiwalker_ppo_rew_04_log_eval/\",\n","                             log_path=\"./multiwalker_ppo_rew_04_log_eval/\", eval_freq=500,\n","                             deterministic=True, render=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f1ede2a0-13e4-42ff-8ca7-f659198a1c48","executionInfo":{"status":"ok","timestamp":1698919409248,"user_tz":-60,"elapsed":5298132,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"id":"eaYfx2L0Hjxu"},"outputs":[{"output_type":"stream","name":"stdout","text":["Logging to ./multiwalker_ppo_rew_04_log_eval/\n","Using cuda device\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n","|    mean_reward          | 3.47         |\n","| time/                   |              |\n","|    total_timesteps      | 2832000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0031872757 |\n","|    clip_fraction        | 0.136        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.1        |\n","|    explained_variance   | 0.663        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 1.35         |\n","|    n_updates            | 1150         |\n","|    policy_gradient_loss | -0.00367     |\n","|    std                  | 3.02         |\n","|    value_loss           | 22.8         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 956     |\n","|    iterations      | 231     |\n","|    time_elapsed    | 2969    |\n","|    total_timesteps | 2838528 |\n","--------------------------------\n","Eval num_timesteps=2844000, episode_reward=5.44 +/- 3.34\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 5.44         |\n","| time/                   |              |\n","|    total_timesteps      | 2844000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020760037 |\n","|    clip_fraction        | 0.106        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.1        |\n","|    explained_variance   | 0.207        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0873      |\n","|    n_updates            | 1155         |\n","|    policy_gradient_loss | -0.0042      |\n","|    std                  | 3.02         |\n","|    value_loss           | 0.467        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 955     |\n","|    iterations      | 232     |\n","|    time_elapsed    | 2983    |\n","|    total_timesteps | 2850816 |\n","--------------------------------\n","Eval num_timesteps=2856000, episode_reward=3.80 +/- 0.16\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 3.8          |\n","| time/                   |              |\n","|    total_timesteps      | 2856000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0025695758 |\n","|    clip_fraction        | 0.115        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.1        |\n","|    explained_variance   | -0.222       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0999      |\n","|    n_updates            | 1160         |\n","|    policy_gradient_loss | -0.00449     |\n","|    std                  | 3.03         |\n","|    value_loss           | 0.364        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 955     |\n","|    iterations      | 233     |\n","|    time_elapsed    | 2995    |\n","|    total_timesteps | 2863104 |\n","--------------------------------\n","Eval num_timesteps=2868000, episode_reward=4.41 +/- 0.41\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 4.41         |\n","| time/                   |              |\n","|    total_timesteps      | 2868000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0017376173 |\n","|    clip_fraction        | 0.0805       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.1        |\n","|    explained_variance   | 0.0925       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.104       |\n","|    n_updates            | 1165         |\n","|    policy_gradient_loss | -0.00395     |\n","|    std                  | 3.02         |\n","|    value_loss           | 0.173        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 955     |\n","|    iterations      | 234     |\n","|    time_elapsed    | 3008    |\n","|    total_timesteps | 2875392 |\n","--------------------------------\n","Eval num_timesteps=2880000, episode_reward=6.63 +/- 3.61\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 6.63        |\n","| time/                   |             |\n","|    total_timesteps      | 2880000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002173894 |\n","|    clip_fraction        | 0.105       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10.1       |\n","|    explained_variance   | 0.478       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.103      |\n","|    n_updates            | 1170        |\n","|    policy_gradient_loss | -0.00388    |\n","|    std                  | 3.03        |\n","|    value_loss           | 0.176       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 955     |\n","|    iterations      | 235     |\n","|    time_elapsed    | 3022    |\n","|    total_timesteps | 2887680 |\n","--------------------------------\n","Eval num_timesteps=2892000, episode_reward=-43.77 +/- 41.17\n","Episode length: 222.80 +/- 226.33\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 223          |\n","|    mean_reward          | -43.8        |\n","| time/                   |              |\n","|    total_timesteps      | 2892000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0024178608 |\n","|    clip_fraction        | 0.102        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.1        |\n","|    explained_variance   | 0.333        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0997      |\n","|    n_updates            | 1175         |\n","|    policy_gradient_loss | -0.00309     |\n","|    std                  | 3.04         |\n","|    value_loss           | 0.169        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 955     |\n","|    iterations      | 236     |\n","|    time_elapsed    | 3034    |\n","|    total_timesteps | 2899968 |\n","--------------------------------\n","Eval num_timesteps=2904000, episode_reward=5.41 +/- 1.35\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 5.41         |\n","| time/                   |              |\n","|    total_timesteps      | 2904000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020813032 |\n","|    clip_fraction        | 0.112        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.1        |\n","|    explained_variance   | 0.415        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.105       |\n","|    n_updates            | 1180         |\n","|    policy_gradient_loss | -0.00407     |\n","|    std                  | 3.05         |\n","|    value_loss           | 0.196        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 954     |\n","|    iterations      | 237     |\n","|    time_elapsed    | 3051    |\n","|    total_timesteps | 2912256 |\n","--------------------------------\n","Eval num_timesteps=2916000, episode_reward=2.94 +/- 1.05\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 2.94         |\n","| time/                   |              |\n","|    total_timesteps      | 2916000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0023044161 |\n","|    clip_fraction        | 0.107        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.1        |\n","|    explained_variance   | 0.354        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0933      |\n","|    n_updates            | 1185         |\n","|    policy_gradient_loss | -0.00391     |\n","|    std                  | 3.06         |\n","|    value_loss           | 0.172        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 955     |\n","|    iterations      | 238     |\n","|    time_elapsed    | 3062    |\n","|    total_timesteps | 2924544 |\n","--------------------------------\n","Eval num_timesteps=2928000, episode_reward=8.03 +/- 4.74\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 8.03         |\n","| time/                   |              |\n","|    total_timesteps      | 2928000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022565494 |\n","|    clip_fraction        | 0.0934       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.1        |\n","|    explained_variance   | 0.392        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.101       |\n","|    n_updates            | 1190         |\n","|    policy_gradient_loss | -0.00381     |\n","|    std                  | 3.06         |\n","|    value_loss           | 0.155        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 954     |\n","|    iterations      | 239     |\n","|    time_elapsed    | 3075    |\n","|    total_timesteps | 2936832 |\n","--------------------------------\n","Eval num_timesteps=2940000, episode_reward=4.25 +/- 1.33\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 4.25        |\n","| time/                   |             |\n","|    total_timesteps      | 2940000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002331712 |\n","|    clip_fraction        | 0.106       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10.1       |\n","|    explained_variance   | 0.426       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.102      |\n","|    n_updates            | 1195        |\n","|    policy_gradient_loss | -0.00322    |\n","|    std                  | 3.07        |\n","|    value_loss           | 0.152       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 954     |\n","|    iterations      | 240     |\n","|    time_elapsed    | 3089    |\n","|    total_timesteps | 2949120 |\n","--------------------------------\n","Eval num_timesteps=2952000, episode_reward=6.29 +/- 0.99\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 6.29         |\n","| time/                   |              |\n","|    total_timesteps      | 2952000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018699124 |\n","|    clip_fraction        | 0.096        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.1        |\n","|    explained_variance   | 0.561        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.102       |\n","|    n_updates            | 1200         |\n","|    policy_gradient_loss | -0.00274     |\n","|    std                  | 3.07         |\n","|    value_loss           | 0.136        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 955     |\n","|    iterations      | 241     |\n","|    time_elapsed    | 3100    |\n","|    total_timesteps | 2961408 |\n","--------------------------------\n","Eval num_timesteps=2964000, episode_reward=2.93 +/- 0.68\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 2.93         |\n","| time/                   |              |\n","|    total_timesteps      | 2964000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0042166575 |\n","|    clip_fraction        | 0.19         |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.2        |\n","|    explained_variance   | 0.0439       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.0167       |\n","|    n_updates            | 1205         |\n","|    policy_gradient_loss | -0.00191     |\n","|    std                  | 3.11         |\n","|    value_loss           | 3.94         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 954     |\n","|    iterations      | 242     |\n","|    time_elapsed    | 3115    |\n","|    total_timesteps | 2973696 |\n","--------------------------------\n","Eval num_timesteps=2976000, episode_reward=-44.72 +/- 39.43\n","Episode length: 229.40 +/- 220.94\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 229          |\n","|    mean_reward          | -44.7        |\n","| time/                   |              |\n","|    total_timesteps      | 2976000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020412316 |\n","|    clip_fraction        | 0.108        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.2        |\n","|    explained_variance   | -0.593       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.098       |\n","|    n_updates            | 1210         |\n","|    policy_gradient_loss | -0.00292     |\n","|    std                  | 3.12         |\n","|    value_loss           | 0.297        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 954     |\n","|    iterations      | 243     |\n","|    time_elapsed    | 3127    |\n","|    total_timesteps | 2985984 |\n","--------------------------------\n","Eval num_timesteps=2988000, episode_reward=11.33 +/- 1.40\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 11.3         |\n","| time/                   |              |\n","|    total_timesteps      | 2988000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0025556332 |\n","|    clip_fraction        | 0.101        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.2        |\n","|    explained_variance   | 0.426        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0996      |\n","|    n_updates            | 1215         |\n","|    policy_gradient_loss | -0.00328     |\n","|    std                  | 3.11         |\n","|    value_loss           | 0.179        |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 954     |\n","|    iterations      | 244     |\n","|    time_elapsed    | 3140    |\n","|    total_timesteps | 2998272 |\n","--------------------------------\n","Eval num_timesteps=3000000, episode_reward=3.84 +/- 0.25\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 3.84         |\n","| time/                   |              |\n","|    total_timesteps      | 3000000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0025976102 |\n","|    clip_fraction        | 0.1          |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.2        |\n","|    explained_variance   | 0.477        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.103       |\n","|    n_updates            | 1220         |\n","|    policy_gradient_loss | -0.00295     |\n","|    std                  | 3.12         |\n","|    value_loss           | 0.163        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 954     |\n","|    iterations      | 245     |\n","|    time_elapsed    | 3155    |\n","|    total_timesteps | 3010560 |\n","--------------------------------\n","Eval num_timesteps=3012000, episode_reward=-25.03 +/- 42.84\n","Episode length: 319.20 +/- 221.43\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 319          |\n","|    mean_reward          | -25          |\n","| time/                   |              |\n","|    total_timesteps      | 3012000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020845176 |\n","|    clip_fraction        | 0.108        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.2        |\n","|    explained_variance   | 0.517        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.104       |\n","|    n_updates            | 1225         |\n","|    policy_gradient_loss | -0.00411     |\n","|    std                  | 3.13         |\n","|    value_loss           | 0.149        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 954     |\n","|    iterations      | 246     |\n","|    time_elapsed    | 3166    |\n","|    total_timesteps | 3022848 |\n","--------------------------------\n","Eval num_timesteps=3024000, episode_reward=-44.14 +/- 41.06\n","Episode length: 233.60 +/- 217.51\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 234          |\n","|    mean_reward          | -44.1        |\n","| time/                   |              |\n","|    total_timesteps      | 3024000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019909022 |\n","|    clip_fraction        | 0.095        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.2        |\n","|    explained_variance   | 0.421        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.1         |\n","|    n_updates            | 1230         |\n","|    policy_gradient_loss | -0.00336     |\n","|    std                  | 3.13         |\n","|    value_loss           | 0.195        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 954     |\n","|    iterations      | 247     |\n","|    time_elapsed    | 3180    |\n","|    total_timesteps | 3035136 |\n","--------------------------------\n","Eval num_timesteps=3036000, episode_reward=-44.40 +/- 41.28\n","Episode length: 231.20 +/- 219.47\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 231         |\n","|    mean_reward          | -44.4       |\n","| time/                   |             |\n","|    total_timesteps      | 3036000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002355149 |\n","|    clip_fraction        | 0.11        |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10.2       |\n","|    explained_variance   | 0.319       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 0.541       |\n","|    n_updates            | 1235        |\n","|    policy_gradient_loss | -0.00365    |\n","|    std                  | 3.16        |\n","|    value_loss           | 20.2        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 953     |\n","|    iterations      | 248     |\n","|    time_elapsed    | 3194    |\n","|    total_timesteps | 3047424 |\n","--------------------------------\n","Eval num_timesteps=3048000, episode_reward=7.51 +/- 0.59\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 7.51         |\n","| time/                   |              |\n","|    total_timesteps      | 3048000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0023098004 |\n","|    clip_fraction        | 0.117        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.3        |\n","|    explained_variance   | -0.809       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.076       |\n","|    n_updates            | 1240         |\n","|    policy_gradient_loss | -0.00401     |\n","|    std                  | 3.15         |\n","|    value_loss           | 0.931        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 954     |\n","|    iterations      | 249     |\n","|    time_elapsed    | 3205    |\n","|    total_timesteps | 3059712 |\n","--------------------------------\n","Eval num_timesteps=3060000, episode_reward=4.36 +/- 1.57\n","Episode length: 500.00 +/- 0.00\n","---------------------------------------\n","| eval/                   |           |\n","|    mean_ep_length       | 500       |\n","|    mean_reward          | 4.36      |\n","| time/                   |           |\n","|    total_timesteps      | 3060000   |\n","| train/                  |           |\n","|    approx_kl            | 0.0025991 |\n","|    clip_fraction        | 0.122     |\n","|    clip_range           | 0.1       |\n","|    entropy_loss         | -10.3     |\n","|    explained_variance   | 0.659     |\n","|    learning_rate        | 0.000622  |\n","|    loss                 | 0.295     |\n","|    n_updates            | 1245      |\n","|    policy_gradient_loss | -0.00361  |\n","|    std                  | 3.18      |\n","|    value_loss           | 7.26      |\n","---------------------------------------\n","Eval num_timesteps=3072000, episode_reward=6.54 +/- 1.12\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.54     |\n","| time/              |          |\n","|    total_timesteps | 3072000  |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 952     |\n","|    iterations      | 250     |\n","|    time_elapsed    | 3225    |\n","|    total_timesteps | 3072000 |\n","--------------------------------\n","Eval num_timesteps=3084000, episode_reward=-26.55 +/- 41.54\n","Episode length: 322.40 +/- 217.51\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 322         |\n","|    mean_reward          | -26.6       |\n","| time/                   |             |\n","|    total_timesteps      | 3084000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002664891 |\n","|    clip_fraction        | 0.104       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10.3       |\n","|    explained_variance   | 0.612       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 0.068       |\n","|    n_updates            | 1250        |\n","|    policy_gradient_loss | -0.00321    |\n","|    std                  | 3.22        |\n","|    value_loss           | 3.94        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 951     |\n","|    iterations      | 251     |\n","|    time_elapsed    | 3240    |\n","|    total_timesteps | 3084288 |\n","--------------------------------\n","Eval num_timesteps=3096000, episode_reward=6.88 +/- 2.40\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 6.88         |\n","| time/                   |              |\n","|    total_timesteps      | 3096000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0024279489 |\n","|    clip_fraction        | 0.123        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.3        |\n","|    explained_variance   | 0.687        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.207        |\n","|    n_updates            | 1255         |\n","|    policy_gradient_loss | -0.00267     |\n","|    std                  | 3.22         |\n","|    value_loss           | 5.63         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 952     |\n","|    iterations      | 252     |\n","|    time_elapsed    | 3250    |\n","|    total_timesteps | 3096576 |\n","--------------------------------\n","Eval num_timesteps=3108000, episode_reward=4.83 +/- 0.63\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 4.83         |\n","| time/                   |              |\n","|    total_timesteps      | 3108000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021121574 |\n","|    clip_fraction        | 0.122        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.3        |\n","|    explained_variance   | -0.106       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0442      |\n","|    n_updates            | 1260         |\n","|    policy_gradient_loss | -0.00331     |\n","|    std                  | 3.22         |\n","|    value_loss           | 1.51         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 952     |\n","|    iterations      | 253     |\n","|    time_elapsed    | 3264    |\n","|    total_timesteps | 3108864 |\n","--------------------------------\n","Eval num_timesteps=3120000, episode_reward=-25.09 +/- 41.76\n","Episode length: 324.00 +/- 215.56\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 324          |\n","|    mean_reward          | -25.1        |\n","| time/                   |              |\n","|    total_timesteps      | 3120000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019424978 |\n","|    clip_fraction        | 0.0895       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.3        |\n","|    explained_variance   | 0.176        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.103       |\n","|    n_updates            | 1265         |\n","|    policy_gradient_loss | -0.00413     |\n","|    std                  | 3.22         |\n","|    value_loss           | 0.215        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 952     |\n","|    iterations      | 254     |\n","|    time_elapsed    | 3278    |\n","|    total_timesteps | 3121152 |\n","--------------------------------\n","Eval num_timesteps=3132000, episode_reward=5.53 +/- 1.78\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 5.53         |\n","| time/                   |              |\n","|    total_timesteps      | 3132000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0026130136 |\n","|    clip_fraction        | 0.118        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.4        |\n","|    explained_variance   | 0.169        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0996      |\n","|    n_updates            | 1270         |\n","|    policy_gradient_loss | -0.00305     |\n","|    std                  | 3.23         |\n","|    value_loss           | 0.286        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 952     |\n","|    iterations      | 255     |\n","|    time_elapsed    | 3289    |\n","|    total_timesteps | 3133440 |\n","--------------------------------\n","Eval num_timesteps=3144000, episode_reward=-25.61 +/- 41.25\n","Episode length: 367.20 +/- 162.65\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 367          |\n","|    mean_reward          | -25.6        |\n","| time/                   |              |\n","|    total_timesteps      | 3144000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019772889 |\n","|    clip_fraction        | 0.086        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.4        |\n","|    explained_variance   | 0.437        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.1         |\n","|    n_updates            | 1275         |\n","|    policy_gradient_loss | -0.00303     |\n","|    std                  | 3.25         |\n","|    value_loss           | 0.163        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 952     |\n","|    iterations      | 256     |\n","|    time_elapsed    | 3304    |\n","|    total_timesteps | 3145728 |\n","--------------------------------\n","Eval num_timesteps=3156000, episode_reward=6.72 +/- 1.21\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 6.72         |\n","| time/                   |              |\n","|    total_timesteps      | 3156000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022992163 |\n","|    clip_fraction        | 0.0995       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.4        |\n","|    explained_variance   | 0.591        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.105       |\n","|    n_updates            | 1280         |\n","|    policy_gradient_loss | -0.00376     |\n","|    std                  | 3.25         |\n","|    value_loss           | 0.16         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 951     |\n","|    iterations      | 257     |\n","|    time_elapsed    | 3319    |\n","|    total_timesteps | 3158016 |\n","--------------------------------\n","Eval num_timesteps=3168000, episode_reward=-44.27 +/- 39.72\n","Episode length: 237.80 +/- 214.09\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 238          |\n","|    mean_reward          | -44.3        |\n","| time/                   |              |\n","|    total_timesteps      | 3168000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0029988186 |\n","|    clip_fraction        | 0.159        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.4        |\n","|    explained_variance   | 0.289        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.37         |\n","|    n_updates            | 1285         |\n","|    policy_gradient_loss | -0.00409     |\n","|    std                  | 3.29         |\n","|    value_loss           | 19.4         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 951     |\n","|    iterations      | 258     |\n","|    time_elapsed    | 3333    |\n","|    total_timesteps | 3170304 |\n","--------------------------------\n","Eval num_timesteps=3180000, episode_reward=7.80 +/- 0.54\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 7.8          |\n","| time/                   |              |\n","|    total_timesteps      | 3180000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020911724 |\n","|    clip_fraction        | 0.0953       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.4        |\n","|    explained_variance   | 0.141        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.101       |\n","|    n_updates            | 1290         |\n","|    policy_gradient_loss | -0.00423     |\n","|    std                  | 3.29         |\n","|    value_loss           | 0.338        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 950     |\n","|    iterations      | 259     |\n","|    time_elapsed    | 3347    |\n","|    total_timesteps | 3182592 |\n","--------------------------------\n","Eval num_timesteps=3192000, episode_reward=6.47 +/- 1.45\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 6.47         |\n","| time/                   |              |\n","|    total_timesteps      | 3192000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019091442 |\n","|    clip_fraction        | 0.104        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.4        |\n","|    explained_variance   | 0.0717       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.102       |\n","|    n_updates            | 1295         |\n","|    policy_gradient_loss | -0.00345     |\n","|    std                  | 3.3          |\n","|    value_loss           | 0.27         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 951     |\n","|    iterations      | 260     |\n","|    time_elapsed    | 3357    |\n","|    total_timesteps | 3194880 |\n","--------------------------------\n","Eval num_timesteps=3204000, episode_reward=-44.88 +/- 39.59\n","Episode length: 233.60 +/- 217.51\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 234          |\n","|    mean_reward          | -44.9        |\n","| time/                   |              |\n","|    total_timesteps      | 3204000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0029537568 |\n","|    clip_fraction        | 0.157        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.5        |\n","|    explained_variance   | 0.524        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.411        |\n","|    n_updates            | 1300         |\n","|    policy_gradient_loss | -0.00347     |\n","|    std                  | 3.33         |\n","|    value_loss           | 8.2          |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 950     |\n","|    iterations      | 261     |\n","|    time_elapsed    | 3372    |\n","|    total_timesteps | 3207168 |\n","--------------------------------\n","Eval num_timesteps=3216000, episode_reward=7.27 +/- 0.83\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 7.27         |\n","| time/                   |              |\n","|    total_timesteps      | 3216000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018493021 |\n","|    clip_fraction        | 0.105        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.5        |\n","|    explained_variance   | -0.0195      |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0705      |\n","|    n_updates            | 1305         |\n","|    policy_gradient_loss | -0.00257     |\n","|    std                  | 3.33         |\n","|    value_loss           | 1.44         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 951     |\n","|    iterations      | 262     |\n","|    time_elapsed    | 3384    |\n","|    total_timesteps | 3219456 |\n","--------------------------------\n","Eval num_timesteps=3228000, episode_reward=4.12 +/- 0.64\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 4.12         |\n","| time/                   |              |\n","|    total_timesteps      | 3228000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019826444 |\n","|    clip_fraction        | 0.085        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.5        |\n","|    explained_variance   | 0.49         |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.105       |\n","|    n_updates            | 1310         |\n","|    policy_gradient_loss | -0.0043      |\n","|    std                  | 3.33         |\n","|    value_loss           | 0.164        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 951     |\n","|    iterations      | 263     |\n","|    time_elapsed    | 3396    |\n","|    total_timesteps | 3231744 |\n","--------------------------------\n","Eval num_timesteps=3240000, episode_reward=-44.12 +/- 40.52\n","Episode length: 223.40 +/- 225.84\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 223          |\n","|    mean_reward          | -44.1        |\n","| time/                   |              |\n","|    total_timesteps      | 3240000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0023355016 |\n","|    clip_fraction        | 0.0983       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.5        |\n","|    explained_variance   | 0.185        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.099       |\n","|    n_updates            | 1315         |\n","|    policy_gradient_loss | -0.00437     |\n","|    std                  | 3.32         |\n","|    value_loss           | 0.335        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 950     |\n","|    iterations      | 264     |\n","|    time_elapsed    | 3411    |\n","|    total_timesteps | 3244032 |\n","--------------------------------\n","Eval num_timesteps=3252000, episode_reward=-77.44 +/- 0.95\n","Episode length: 46.60 +/- 5.39\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 46.6        |\n","|    mean_reward          | -77.4       |\n","| time/                   |             |\n","|    total_timesteps      | 3252000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002145578 |\n","|    clip_fraction        | 0.0979      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10.5       |\n","|    explained_variance   | 0.481       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.103      |\n","|    n_updates            | 1320        |\n","|    policy_gradient_loss | -0.00385    |\n","|    std                  | 3.32        |\n","|    value_loss           | 0.183       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 952     |\n","|    iterations      | 265     |\n","|    time_elapsed    | 3417    |\n","|    total_timesteps | 3256320 |\n","--------------------------------\n","Eval num_timesteps=3264000, episode_reward=7.77 +/- 0.51\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 7.77        |\n","| time/                   |             |\n","|    total_timesteps      | 3264000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002818685 |\n","|    clip_fraction        | 0.133       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10.5       |\n","|    explained_variance   | 0.449       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 0.151       |\n","|    n_updates            | 1325        |\n","|    policy_gradient_loss | -0.00316    |\n","|    std                  | 3.37        |\n","|    value_loss           | 6.61        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 952     |\n","|    iterations      | 266     |\n","|    time_elapsed    | 3432    |\n","|    total_timesteps | 3268608 |\n","--------------------------------\n","Eval num_timesteps=3276000, episode_reward=-24.58 +/- 43.63\n","Episode length: 317.20 +/- 223.88\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 317          |\n","|    mean_reward          | -24.6        |\n","| time/                   |              |\n","|    total_timesteps      | 3276000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0034093475 |\n","|    clip_fraction        | 0.136        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.6        |\n","|    explained_variance   | 0.498        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.218        |\n","|    n_updates            | 1330         |\n","|    policy_gradient_loss | -0.00415     |\n","|    std                  | 3.42         |\n","|    value_loss           | 16.7         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 952     |\n","|    iterations      | 267     |\n","|    time_elapsed    | 3442    |\n","|    total_timesteps | 3280896 |\n","--------------------------------\n","Eval num_timesteps=3288000, episode_reward=9.32 +/- 2.75\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 9.32       |\n","| time/                   |            |\n","|    total_timesteps      | 3288000    |\n","| train/                  |            |\n","|    approx_kl            | 0.00217498 |\n","|    clip_fraction        | 0.0953     |\n","|    clip_range           | 0.1        |\n","|    entropy_loss         | -10.6      |\n","|    explained_variance   | 0.0932     |\n","|    learning_rate        | 0.000622   |\n","|    loss                 | -0.0978    |\n","|    n_updates            | 1335       |\n","|    policy_gradient_loss | -0.00336   |\n","|    std                  | 3.43       |\n","|    value_loss           | 0.354      |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 952     |\n","|    iterations      | 268     |\n","|    time_elapsed    | 3457    |\n","|    total_timesteps | 3293184 |\n","--------------------------------\n","Eval num_timesteps=3300000, episode_reward=11.42 +/- 0.46\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 11.4         |\n","| time/                   |              |\n","|    total_timesteps      | 3300000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021631627 |\n","|    clip_fraction        | 0.0975       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.6        |\n","|    explained_variance   | -0.25        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.097       |\n","|    n_updates            | 1340         |\n","|    policy_gradient_loss | -0.00395     |\n","|    std                  | 3.45         |\n","|    value_loss           | 0.596        |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 952     |\n","|    iterations      | 269     |\n","|    time_elapsed    | 3470    |\n","|    total_timesteps | 3305472 |\n","--------------------------------\n","Eval num_timesteps=3312000, episode_reward=-45.98 +/- 42.70\n","Episode length: 378.20 +/- 99.45\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 378          |\n","|    mean_reward          | -46          |\n","| time/                   |              |\n","|    total_timesteps      | 3312000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018537586 |\n","|    clip_fraction        | 0.101        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.6        |\n","|    explained_variance   | 0.366        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.103       |\n","|    n_updates            | 1345         |\n","|    policy_gradient_loss | -0.00362     |\n","|    std                  | 3.45         |\n","|    value_loss           | 0.179        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 952     |\n","|    iterations      | 270     |\n","|    time_elapsed    | 3482    |\n","|    total_timesteps | 3317760 |\n","--------------------------------\n","Eval num_timesteps=3324000, episode_reward=10.27 +/- 3.60\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 10.3         |\n","| time/                   |              |\n","|    total_timesteps      | 3324000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020349731 |\n","|    clip_fraction        | 0.091        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.6        |\n","|    explained_variance   | 0.529        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.111       |\n","|    n_updates            | 1350         |\n","|    policy_gradient_loss | -0.00368     |\n","|    std                  | 3.46         |\n","|    value_loss           | 0.159        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 952     |\n","|    iterations      | 271     |\n","|    time_elapsed    | 3496    |\n","|    total_timesteps | 3330048 |\n","--------------------------------\n","Eval num_timesteps=3336000, episode_reward=6.26 +/- 0.45\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 6.26        |\n","| time/                   |             |\n","|    total_timesteps      | 3336000     |\n","| train/                  |             |\n","|    approx_kl            | 0.003130554 |\n","|    clip_fraction        | 0.112       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10.7       |\n","|    explained_variance   | 0.437       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0422     |\n","|    n_updates            | 1355        |\n","|    policy_gradient_loss | -0.00431    |\n","|    std                  | 3.52        |\n","|    value_loss           | 8.54        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 952     |\n","|    iterations      | 272     |\n","|    time_elapsed    | 3508    |\n","|    total_timesteps | 3342336 |\n","--------------------------------\n","Eval num_timesteps=3348000, episode_reward=-23.77 +/- 43.73\n","Episode length: 317.60 +/- 223.39\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 318         |\n","|    mean_reward          | -23.8       |\n","| time/                   |             |\n","|    total_timesteps      | 3348000     |\n","| train/                  |             |\n","|    approx_kl            | 0.003981412 |\n","|    clip_fraction        | 0.154       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10.7       |\n","|    explained_variance   | 0.666       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 0.695       |\n","|    n_updates            | 1360        |\n","|    policy_gradient_loss | -0.00224    |\n","|    std                  | 3.57        |\n","|    value_loss           | 14.2        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 952     |\n","|    iterations      | 273     |\n","|    time_elapsed    | 3521    |\n","|    total_timesteps | 3354624 |\n","--------------------------------\n","Eval num_timesteps=3360000, episode_reward=6.28 +/- 1.32\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 6.28         |\n","| time/                   |              |\n","|    total_timesteps      | 3360000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019054731 |\n","|    clip_fraction        | 0.0837       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.8        |\n","|    explained_variance   | -0.17        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0951      |\n","|    n_updates            | 1365         |\n","|    policy_gradient_loss | -0.00401     |\n","|    std                  | 3.57         |\n","|    value_loss           | 0.695        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 952     |\n","|    iterations      | 274     |\n","|    time_elapsed    | 3535    |\n","|    total_timesteps | 3366912 |\n","--------------------------------\n","Eval num_timesteps=3372000, episode_reward=9.70 +/- 1.79\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 9.7          |\n","| time/                   |              |\n","|    total_timesteps      | 3372000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020269346 |\n","|    clip_fraction        | 0.085        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.8        |\n","|    explained_variance   | 0.333        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.103       |\n","|    n_updates            | 1370         |\n","|    policy_gradient_loss | -0.00385     |\n","|    std                  | 3.57         |\n","|    value_loss           | 0.378        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 952     |\n","|    iterations      | 275     |\n","|    time_elapsed    | 3546    |\n","|    total_timesteps | 3379200 |\n","--------------------------------\n","Eval num_timesteps=3384000, episode_reward=6.66 +/- 2.07\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 6.66         |\n","| time/                   |              |\n","|    total_timesteps      | 3384000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018220819 |\n","|    clip_fraction        | 0.0734       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.8        |\n","|    explained_variance   | 0.307        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.108       |\n","|    n_updates            | 1375         |\n","|    policy_gradient_loss | -0.00411     |\n","|    std                  | 3.58         |\n","|    value_loss           | 0.225        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 952     |\n","|    iterations      | 276     |\n","|    time_elapsed    | 3561    |\n","|    total_timesteps | 3391488 |\n","--------------------------------\n","Eval num_timesteps=3396000, episode_reward=5.94 +/- 0.41\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 5.94         |\n","| time/                   |              |\n","|    total_timesteps      | 3396000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0032446391 |\n","|    clip_fraction        | 0.146        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.8        |\n","|    explained_variance   | 0.0866       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.122        |\n","|    n_updates            | 1380         |\n","|    policy_gradient_loss | -0.00245     |\n","|    std                  | 3.59         |\n","|    value_loss           | 4.78         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 952     |\n","|    iterations      | 277     |\n","|    time_elapsed    | 3573    |\n","|    total_timesteps | 3403776 |\n","--------------------------------\n","Eval num_timesteps=3408000, episode_reward=10.17 +/- 0.51\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 10.2         |\n","| time/                   |              |\n","|    total_timesteps      | 3408000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0031760223 |\n","|    clip_fraction        | 0.165        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.8        |\n","|    explained_variance   | 0.639        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 1.14         |\n","|    n_updates            | 1385         |\n","|    policy_gradient_loss | -0.00315     |\n","|    std                  | 3.63         |\n","|    value_loss           | 17.7         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 952     |\n","|    iterations      | 278     |\n","|    time_elapsed    | 3585    |\n","|    total_timesteps | 3416064 |\n","--------------------------------\n","Eval num_timesteps=3420000, episode_reward=6.84 +/- 2.95\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 6.84        |\n","| time/                   |             |\n","|    total_timesteps      | 3420000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002329815 |\n","|    clip_fraction        | 0.101       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10.8       |\n","|    explained_variance   | -0.58       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0972     |\n","|    n_updates            | 1390        |\n","|    policy_gradient_loss | -0.00414    |\n","|    std                  | 3.64        |\n","|    value_loss           | 0.55        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 951     |\n","|    iterations      | 279     |\n","|    time_elapsed    | 3603    |\n","|    total_timesteps | 3428352 |\n","--------------------------------\n","Eval num_timesteps=3432000, episode_reward=-28.44 +/- 38.94\n","Episode length: 349.60 +/- 184.20\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 350          |\n","|    mean_reward          | -28.4        |\n","| time/                   |              |\n","|    total_timesteps      | 3432000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0028825633 |\n","|    clip_fraction        | 0.124        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.9        |\n","|    explained_variance   | 0.666        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.0884       |\n","|    n_updates            | 1395         |\n","|    policy_gradient_loss | -0.00336     |\n","|    std                  | 3.68         |\n","|    value_loss           | 6.75         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 951     |\n","|    iterations      | 280     |\n","|    time_elapsed    | 3614    |\n","|    total_timesteps | 3440640 |\n","--------------------------------\n","Eval num_timesteps=3444000, episode_reward=7.74 +/- 6.78\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 7.74         |\n","| time/                   |              |\n","|    total_timesteps      | 3444000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018491689 |\n","|    clip_fraction        | 0.081        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.9        |\n","|    explained_variance   | -0.0358      |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.106       |\n","|    n_updates            | 1400         |\n","|    policy_gradient_loss | -0.00368     |\n","|    std                  | 3.68         |\n","|    value_loss           | 0.229        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 951     |\n","|    iterations      | 281     |\n","|    time_elapsed    | 3629    |\n","|    total_timesteps | 3452928 |\n","--------------------------------\n","Eval num_timesteps=3456000, episode_reward=-43.73 +/- 40.15\n","Episode length: 227.00 +/- 222.90\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 227        |\n","|    mean_reward          | -43.7      |\n","| time/                   |            |\n","|    total_timesteps      | 3456000    |\n","| train/                  |            |\n","|    approx_kl            | 0.00220935 |\n","|    clip_fraction        | 0.0883     |\n","|    clip_range           | 0.1        |\n","|    entropy_loss         | -10.9      |\n","|    explained_variance   | 0.332      |\n","|    learning_rate        | 0.000622   |\n","|    loss                 | -0.109     |\n","|    n_updates            | 1405       |\n","|    policy_gradient_loss | -0.00307   |\n","|    std                  | 3.7        |\n","|    value_loss           | 0.156      |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 951     |\n","|    iterations      | 282     |\n","|    time_elapsed    | 3641    |\n","|    total_timesteps | 3465216 |\n","--------------------------------\n","Eval num_timesteps=3468000, episode_reward=4.22 +/- 0.56\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 4.22        |\n","| time/                   |             |\n","|    total_timesteps      | 3468000     |\n","| train/                  |             |\n","|    approx_kl            | 0.001703767 |\n","|    clip_fraction        | 0.0829      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10.9       |\n","|    explained_variance   | 0.146       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.107      |\n","|    n_updates            | 1410        |\n","|    policy_gradient_loss | -0.00433    |\n","|    std                  | 3.7         |\n","|    value_loss           | 0.265       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 951     |\n","|    iterations      | 283     |\n","|    time_elapsed    | 3654    |\n","|    total_timesteps | 3477504 |\n","--------------------------------\n","Eval num_timesteps=3480000, episode_reward=3.71 +/- 0.86\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 3.71         |\n","| time/                   |              |\n","|    total_timesteps      | 3480000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0030701421 |\n","|    clip_fraction        | 0.129        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.9        |\n","|    explained_variance   | 0.833        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0447      |\n","|    n_updates            | 1415         |\n","|    policy_gradient_loss | -0.00313     |\n","|    std                  | 3.74         |\n","|    value_loss           | 2.01         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 951     |\n","|    iterations      | 284     |\n","|    time_elapsed    | 3668    |\n","|    total_timesteps | 3489792 |\n","--------------------------------\n","Eval num_timesteps=3492000, episode_reward=4.43 +/- 0.36\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 4.43        |\n","| time/                   |             |\n","|    total_timesteps      | 3492000     |\n","| train/                  |             |\n","|    approx_kl            | 0.003151219 |\n","|    clip_fraction        | 0.12        |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11         |\n","|    explained_variance   | 0.641       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 0.238       |\n","|    n_updates            | 1420        |\n","|    policy_gradient_loss | -0.0039     |\n","|    std                  | 3.8         |\n","|    value_loss           | 4.75        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 952     |\n","|    iterations      | 285     |\n","|    time_elapsed    | 3678    |\n","|    total_timesteps | 3502080 |\n","--------------------------------\n","Eval num_timesteps=3504000, episode_reward=8.15 +/- 2.79\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 8.15         |\n","| time/                   |              |\n","|    total_timesteps      | 3504000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0026025309 |\n","|    clip_fraction        | 0.0913       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11          |\n","|    explained_variance   | 0.377        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.00535     |\n","|    n_updates            | 1425         |\n","|    policy_gradient_loss | -0.00373     |\n","|    std                  | 3.81         |\n","|    value_loss           | 4.47         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 951     |\n","|    iterations      | 286     |\n","|    time_elapsed    | 3693    |\n","|    total_timesteps | 3514368 |\n","--------------------------------\n","Eval num_timesteps=3516000, episode_reward=6.11 +/- 2.45\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 6.11        |\n","| time/                   |             |\n","|    total_timesteps      | 3516000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002652467 |\n","|    clip_fraction        | 0.128       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11         |\n","|    explained_variance   | 0.583       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 0.487       |\n","|    n_updates            | 1430        |\n","|    policy_gradient_loss | -0.00362    |\n","|    std                  | 3.82        |\n","|    value_loss           | 10.2        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 951     |\n","|    iterations      | 287     |\n","|    time_elapsed    | 3705    |\n","|    total_timesteps | 3526656 |\n","--------------------------------\n","Eval num_timesteps=3528000, episode_reward=4.38 +/- 0.61\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 4.38         |\n","| time/                   |              |\n","|    total_timesteps      | 3528000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021165458 |\n","|    clip_fraction        | 0.105        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11          |\n","|    explained_variance   | 0.0124       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.106       |\n","|    n_updates            | 1435         |\n","|    policy_gradient_loss | -0.00443     |\n","|    std                  | 3.82         |\n","|    value_loss           | 0.384        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 951     |\n","|    iterations      | 288     |\n","|    time_elapsed    | 3717    |\n","|    total_timesteps | 3538944 |\n","--------------------------------\n","Eval num_timesteps=3540000, episode_reward=5.44 +/- 1.44\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 5.44        |\n","| time/                   |             |\n","|    total_timesteps      | 3540000     |\n","| train/                  |             |\n","|    approx_kl            | 0.003090296 |\n","|    clip_fraction        | 0.143       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11         |\n","|    explained_variance   | 0.581       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 0.569       |\n","|    n_updates            | 1440        |\n","|    policy_gradient_loss | -0.0029     |\n","|    std                  | 3.85        |\n","|    value_loss           | 9.09        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 951     |\n","|    iterations      | 289     |\n","|    time_elapsed    | 3732    |\n","|    total_timesteps | 3551232 |\n","--------------------------------\n","Eval num_timesteps=3552000, episode_reward=5.40 +/- 0.80\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 5.4          |\n","| time/                   |              |\n","|    total_timesteps      | 3552000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0032400836 |\n","|    clip_fraction        | 0.162        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.1        |\n","|    explained_variance   | 0.457        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.162        |\n","|    n_updates            | 1445         |\n","|    policy_gradient_loss | -0.00293     |\n","|    std                  | 3.89         |\n","|    value_loss           | 8.73         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 952     |\n","|    iterations      | 290     |\n","|    time_elapsed    | 3743    |\n","|    total_timesteps | 3563520 |\n","--------------------------------\n","Eval num_timesteps=3564000, episode_reward=7.30 +/- 1.44\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 7.3          |\n","| time/                   |              |\n","|    total_timesteps      | 3564000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0030211017 |\n","|    clip_fraction        | 0.115        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.1        |\n","|    explained_variance   | 0.566        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0919      |\n","|    n_updates            | 1450         |\n","|    policy_gradient_loss | -0.00339     |\n","|    std                  | 3.95         |\n","|    value_loss           | 6.53         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 951     |\n","|    iterations      | 291     |\n","|    time_elapsed    | 3757    |\n","|    total_timesteps | 3575808 |\n","--------------------------------\n","Eval num_timesteps=3576000, episode_reward=8.93 +/- 0.04\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 8.93        |\n","| time/                   |             |\n","|    total_timesteps      | 3576000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002703517 |\n","|    clip_fraction        | 0.114       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11.2       |\n","|    explained_variance   | -0.00738    |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.1        |\n","|    n_updates            | 1455        |\n","|    policy_gradient_loss | -0.00422    |\n","|    std                  | 3.95        |\n","|    value_loss           | 0.676       |\n","-----------------------------------------\n","Eval num_timesteps=3588000, episode_reward=6.37 +/- 0.50\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.37     |\n","| time/              |          |\n","|    total_timesteps | 3588000  |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 950     |\n","|    iterations      | 292     |\n","|    time_elapsed    | 3776    |\n","|    total_timesteps | 3588096 |\n","--------------------------------\n","Eval num_timesteps=3600000, episode_reward=4.44 +/- 0.73\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 4.44         |\n","| time/                   |              |\n","|    total_timesteps      | 3600000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0028250718 |\n","|    clip_fraction        | 0.134        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.2        |\n","|    explained_variance   | 0.719        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.192        |\n","|    n_updates            | 1460         |\n","|    policy_gradient_loss | -0.00345     |\n","|    std                  | 3.99         |\n","|    value_loss           | 7.21         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 950     |\n","|    iterations      | 293     |\n","|    time_elapsed    | 3789    |\n","|    total_timesteps | 3600384 |\n","--------------------------------\n","Eval num_timesteps=3612000, episode_reward=4.84 +/- 0.30\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 4.84         |\n","| time/                   |              |\n","|    total_timesteps      | 3612000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0026893213 |\n","|    clip_fraction        | 0.157        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.2        |\n","|    explained_variance   | 0.636        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.167        |\n","|    n_updates            | 1465         |\n","|    policy_gradient_loss | -0.0031      |\n","|    std                  | 4.03         |\n","|    value_loss           | 8.61         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 950     |\n","|    iterations      | 294     |\n","|    time_elapsed    | 3801    |\n","|    total_timesteps | 3612672 |\n","--------------------------------\n","Eval num_timesteps=3624000, episode_reward=7.82 +/- 0.13\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 7.82        |\n","| time/                   |             |\n","|    total_timesteps      | 3624000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002438634 |\n","|    clip_fraction        | 0.0932      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11.3       |\n","|    explained_variance   | 0.053       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0977     |\n","|    n_updates            | 1470        |\n","|    policy_gradient_loss | -0.00384    |\n","|    std                  | 4.04        |\n","|    value_loss           | 0.472       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 949     |\n","|    iterations      | 295     |\n","|    time_elapsed    | 3816    |\n","|    total_timesteps | 3624960 |\n","--------------------------------\n","Eval num_timesteps=3636000, episode_reward=-24.70 +/- 41.44\n","Episode length: 364.80 +/- 165.59\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 365          |\n","|    mean_reward          | -24.7        |\n","| time/                   |              |\n","|    total_timesteps      | 3636000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018292209 |\n","|    clip_fraction        | 0.0944       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.3        |\n","|    explained_variance   | 0.58         |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0977      |\n","|    n_updates            | 1475         |\n","|    policy_gradient_loss | -0.00431     |\n","|    std                  | 4.06         |\n","|    value_loss           | 0.97         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 950     |\n","|    iterations      | 296     |\n","|    time_elapsed    | 3826    |\n","|    total_timesteps | 3637248 |\n","--------------------------------\n","Eval num_timesteps=3648000, episode_reward=-26.51 +/- 40.95\n","Episode length: 346.80 +/- 187.63\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 347          |\n","|    mean_reward          | -26.5        |\n","| time/                   |              |\n","|    total_timesteps      | 3648000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0023958613 |\n","|    clip_fraction        | 0.0911       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.3        |\n","|    explained_variance   | -0.436       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0894      |\n","|    n_updates            | 1480         |\n","|    policy_gradient_loss | -0.00342     |\n","|    std                  | 4.05         |\n","|    value_loss           | 1.15         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 950     |\n","|    iterations      | 297     |\n","|    time_elapsed    | 3840    |\n","|    total_timesteps | 3649536 |\n","--------------------------------\n","Eval num_timesteps=3660000, episode_reward=8.96 +/- 2.68\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 8.96         |\n","| time/                   |              |\n","|    total_timesteps      | 3660000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019001509 |\n","|    clip_fraction        | 0.0829       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.3        |\n","|    explained_variance   | 0.837        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.109       |\n","|    n_updates            | 1485         |\n","|    policy_gradient_loss | -0.00426     |\n","|    std                  | 4.05         |\n","|    value_loss           | 0.523        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 950     |\n","|    iterations      | 298     |\n","|    time_elapsed    | 3854    |\n","|    total_timesteps | 3661824 |\n","--------------------------------\n","Eval num_timesteps=3672000, episode_reward=12.04 +/- 3.84\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 12           |\n","| time/                   |              |\n","|    total_timesteps      | 3672000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0035624162 |\n","|    clip_fraction        | 0.149        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.3        |\n","|    explained_variance   | 0.551        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.848        |\n","|    n_updates            | 1490         |\n","|    policy_gradient_loss | -0.00292     |\n","|    std                  | 4.08         |\n","|    value_loss           | 8.82         |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 950     |\n","|    iterations      | 299     |\n","|    time_elapsed    | 3865    |\n","|    total_timesteps | 3674112 |\n","--------------------------------\n","Eval num_timesteps=3684000, episode_reward=8.30 +/- 2.65\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 8.3          |\n","| time/                   |              |\n","|    total_timesteps      | 3684000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0024708721 |\n","|    clip_fraction        | 0.112        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.3        |\n","|    explained_variance   | 0.391        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.628        |\n","|    n_updates            | 1495         |\n","|    policy_gradient_loss | -0.00316     |\n","|    std                  | 4.11         |\n","|    value_loss           | 22.5         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 949     |\n","|    iterations      | 300     |\n","|    time_elapsed    | 3883    |\n","|    total_timesteps | 3686400 |\n","--------------------------------\n","Eval num_timesteps=3696000, episode_reward=11.59 +/- 2.00\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 11.6         |\n","| time/                   |              |\n","|    total_timesteps      | 3696000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021340086 |\n","|    clip_fraction        | 0.0843       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.3        |\n","|    explained_variance   | -0.328       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.107       |\n","|    n_updates            | 1500         |\n","|    policy_gradient_loss | -0.00447     |\n","|    std                  | 4.12         |\n","|    value_loss           | 0.434        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 949     |\n","|    iterations      | 301     |\n","|    time_elapsed    | 3894    |\n","|    total_timesteps | 3698688 |\n","--------------------------------\n","Eval num_timesteps=3708000, episode_reward=6.80 +/- 1.97\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 6.8          |\n","| time/                   |              |\n","|    total_timesteps      | 3708000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019963284 |\n","|    clip_fraction        | 0.09         |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.3        |\n","|    explained_variance   | 0.203        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.115       |\n","|    n_updates            | 1505         |\n","|    policy_gradient_loss | -0.00443     |\n","|    std                  | 4.11         |\n","|    value_loss           | 0.217        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 949     |\n","|    iterations      | 302     |\n","|    time_elapsed    | 3908    |\n","|    total_timesteps | 3710976 |\n","--------------------------------\n","Eval num_timesteps=3720000, episode_reward=-43.61 +/- 41.31\n","Episode length: 261.20 +/- 194.98\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 261          |\n","|    mean_reward          | -43.6        |\n","| time/                   |              |\n","|    total_timesteps      | 3720000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021178278 |\n","|    clip_fraction        | 0.115        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.3        |\n","|    explained_variance   | 0.929        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.107       |\n","|    n_updates            | 1510         |\n","|    policy_gradient_loss | -0.00418     |\n","|    std                  | 4.13         |\n","|    value_loss           | 0.975        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 949     |\n","|    iterations      | 303     |\n","|    time_elapsed    | 3920    |\n","|    total_timesteps | 3723264 |\n","--------------------------------\n","Eval num_timesteps=3732000, episode_reward=8.51 +/- 4.44\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 8.51         |\n","| time/                   |              |\n","|    total_timesteps      | 3732000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0029030063 |\n","|    clip_fraction        | 0.138        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.4        |\n","|    explained_variance   | 0.631        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.0234       |\n","|    n_updates            | 1515         |\n","|    policy_gradient_loss | -0.00326     |\n","|    std                  | 4.16         |\n","|    value_loss           | 7.66         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 949     |\n","|    iterations      | 304     |\n","|    time_elapsed    | 3933    |\n","|    total_timesteps | 3735552 |\n","--------------------------------\n","Eval num_timesteps=3744000, episode_reward=14.90 +/- 0.78\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 14.9        |\n","| time/                   |             |\n","|    total_timesteps      | 3744000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002909974 |\n","|    clip_fraction        | 0.12        |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11.4       |\n","|    explained_variance   | 0.206       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 0.53        |\n","|    n_updates            | 1520        |\n","|    policy_gradient_loss | -0.00392    |\n","|    std                  | 4.19        |\n","|    value_loss           | 6.56        |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 949     |\n","|    iterations      | 305     |\n","|    time_elapsed    | 3947    |\n","|    total_timesteps | 3747840 |\n","--------------------------------\n","Eval num_timesteps=3756000, episode_reward=-41.79 +/- 42.07\n","Episode length: 243.20 +/- 209.68\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 243        |\n","|    mean_reward          | -41.8      |\n","| time/                   |            |\n","|    total_timesteps      | 3756000    |\n","| train/                  |            |\n","|    approx_kl            | 0.00257138 |\n","|    clip_fraction        | 0.117      |\n","|    clip_range           | 0.1        |\n","|    entropy_loss         | -11.4      |\n","|    explained_variance   | 0.66       |\n","|    learning_rate        | 0.000622   |\n","|    loss                 | 0.0814     |\n","|    n_updates            | 1525       |\n","|    policy_gradient_loss | -0.0037    |\n","|    std                  | 4.24       |\n","|    value_loss           | 9.53       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 949     |\n","|    iterations      | 306     |\n","|    time_elapsed    | 3958    |\n","|    total_timesteps | 3760128 |\n","--------------------------------\n","Eval num_timesteps=3768000, episode_reward=11.50 +/- 1.57\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 11.5         |\n","| time/                   |              |\n","|    total_timesteps      | 3768000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0024950814 |\n","|    clip_fraction        | 0.0927       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.5        |\n","|    explained_variance   | 0.144        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.104       |\n","|    n_updates            | 1530         |\n","|    policy_gradient_loss | -0.0035      |\n","|    std                  | 4.25         |\n","|    value_loss           | 0.767        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 949     |\n","|    iterations      | 307     |\n","|    time_elapsed    | 3972    |\n","|    total_timesteps | 3772416 |\n","--------------------------------\n","Eval num_timesteps=3780000, episode_reward=10.08 +/- 0.38\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 10.1        |\n","| time/                   |             |\n","|    total_timesteps      | 3780000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002120666 |\n","|    clip_fraction        | 0.104       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11.5       |\n","|    explained_variance   | 0.258       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.1        |\n","|    n_updates            | 1535        |\n","|    policy_gradient_loss | -0.00423    |\n","|    std                  | 4.27        |\n","|    value_loss           | 0.874       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 949     |\n","|    iterations      | 308     |\n","|    time_elapsed    | 3985    |\n","|    total_timesteps | 3784704 |\n","--------------------------------\n","Eval num_timesteps=3792000, episode_reward=11.28 +/- 5.03\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 11.3         |\n","| time/                   |              |\n","|    total_timesteps      | 3792000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019009864 |\n","|    clip_fraction        | 0.0957       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.5        |\n","|    explained_variance   | 0.281        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.108       |\n","|    n_updates            | 1540         |\n","|    policy_gradient_loss | -0.00366     |\n","|    std                  | 4.27         |\n","|    value_loss           | 0.184        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 949     |\n","|    iterations      | 309     |\n","|    time_elapsed    | 3997    |\n","|    total_timesteps | 3796992 |\n","--------------------------------\n","Eval num_timesteps=3804000, episode_reward=11.31 +/- 2.09\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 11.3         |\n","| time/                   |              |\n","|    total_timesteps      | 3804000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022558572 |\n","|    clip_fraction        | 0.0914       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.5        |\n","|    explained_variance   | 0.261        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.108       |\n","|    n_updates            | 1545         |\n","|    policy_gradient_loss | -0.00381     |\n","|    std                  | 4.25         |\n","|    value_loss           | 0.61         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 949     |\n","|    iterations      | 310     |\n","|    time_elapsed    | 4011    |\n","|    total_timesteps | 3809280 |\n","--------------------------------\n","Eval num_timesteps=3816000, episode_reward=7.93 +/- 2.04\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 7.93         |\n","| time/                   |              |\n","|    total_timesteps      | 3816000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0033809058 |\n","|    clip_fraction        | 0.168        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.5        |\n","|    explained_variance   | 0.549        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.56         |\n","|    n_updates            | 1550         |\n","|    policy_gradient_loss | -0.00293     |\n","|    std                  | 4.32         |\n","|    value_loss           | 9.54         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 949     |\n","|    iterations      | 311     |\n","|    time_elapsed    | 4022    |\n","|    total_timesteps | 3821568 |\n","--------------------------------\n","Eval num_timesteps=3828000, episode_reward=8.19 +/- 0.60\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 8.19        |\n","| time/                   |             |\n","|    total_timesteps      | 3828000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002770635 |\n","|    clip_fraction        | 0.113       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11.5       |\n","|    explained_variance   | 0.263       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.115      |\n","|    n_updates            | 1555        |\n","|    policy_gradient_loss | -0.00396    |\n","|    std                  | 4.34        |\n","|    value_loss           | 0.171       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 949     |\n","|    iterations      | 312     |\n","|    time_elapsed    | 4036    |\n","|    total_timesteps | 3833856 |\n","--------------------------------\n","Eval num_timesteps=3840000, episode_reward=10.57 +/- 2.24\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 10.6         |\n","| time/                   |              |\n","|    total_timesteps      | 3840000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0024531495 |\n","|    clip_fraction        | 0.117        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.6        |\n","|    explained_variance   | 0.495        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.109       |\n","|    n_updates            | 1560         |\n","|    policy_gradient_loss | -0.00412     |\n","|    std                  | 4.37         |\n","|    value_loss           | 0.186        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 949     |\n","|    iterations      | 313     |\n","|    time_elapsed    | 4050    |\n","|    total_timesteps | 3846144 |\n","--------------------------------\n","Eval num_timesteps=3852000, episode_reward=12.39 +/- 1.45\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 12.4         |\n","| time/                   |              |\n","|    total_timesteps      | 3852000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0027094411 |\n","|    clip_fraction        | 0.119        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.6        |\n","|    explained_variance   | 0.588        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.00586     |\n","|    n_updates            | 1565         |\n","|    policy_gradient_loss | -0.00306     |\n","|    std                  | 4.39         |\n","|    value_loss           | 9.38         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 950     |\n","|    iterations      | 314     |\n","|    time_elapsed    | 4061    |\n","|    total_timesteps | 3858432 |\n","--------------------------------\n","Eval num_timesteps=3864000, episode_reward=11.87 +/- 1.49\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 11.9         |\n","| time/                   |              |\n","|    total_timesteps      | 3864000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022084604 |\n","|    clip_fraction        | 0.0847       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.6        |\n","|    explained_variance   | 0.171        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.12        |\n","|    n_updates            | 1570         |\n","|    policy_gradient_loss | -0.00439     |\n","|    std                  | 4.4          |\n","|    value_loss           | 0.162        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 949     |\n","|    iterations      | 315     |\n","|    time_elapsed    | 4075    |\n","|    total_timesteps | 3870720 |\n","--------------------------------\n","Eval num_timesteps=3876000, episode_reward=16.26 +/- 2.78\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 16.3         |\n","| time/                   |              |\n","|    total_timesteps      | 3876000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022899595 |\n","|    clip_fraction        | 0.108        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.6        |\n","|    explained_variance   | 0.495        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.107       |\n","|    n_updates            | 1575         |\n","|    policy_gradient_loss | -0.00355     |\n","|    std                  | 4.39         |\n","|    value_loss           | 0.163        |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 949     |\n","|    iterations      | 316     |\n","|    time_elapsed    | 4087    |\n","|    total_timesteps | 3883008 |\n","--------------------------------\n","Eval num_timesteps=3888000, episode_reward=-21.33 +/- 42.39\n","Episode length: 357.60 +/- 174.40\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 358         |\n","|    mean_reward          | -21.3       |\n","| time/                   |             |\n","|    total_timesteps      | 3888000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002056215 |\n","|    clip_fraction        | 0.0763      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11.6       |\n","|    explained_variance   | 0.396       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.111      |\n","|    n_updates            | 1580        |\n","|    policy_gradient_loss | -0.00309    |\n","|    std                  | 4.41        |\n","|    value_loss           | 0.183       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 949     |\n","|    iterations      | 317     |\n","|    time_elapsed    | 4100    |\n","|    total_timesteps | 3895296 |\n","--------------------------------\n","Eval num_timesteps=3900000, episode_reward=12.38 +/- 2.68\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 12.4        |\n","| time/                   |             |\n","|    total_timesteps      | 3900000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002231448 |\n","|    clip_fraction        | 0.0882      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11.6       |\n","|    explained_variance   | 0.344       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.113      |\n","|    n_updates            | 1585        |\n","|    policy_gradient_loss | -0.00356    |\n","|    std                  | 4.44        |\n","|    value_loss           | 0.238       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 949     |\n","|    iterations      | 318     |\n","|    time_elapsed    | 4115    |\n","|    total_timesteps | 3907584 |\n","--------------------------------\n","Eval num_timesteps=3912000, episode_reward=17.53 +/- 0.06\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 17.5         |\n","| time/                   |              |\n","|    total_timesteps      | 3912000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019562554 |\n","|    clip_fraction        | 0.0844       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.7        |\n","|    explained_variance   | 0.505        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.115       |\n","|    n_updates            | 1590         |\n","|    policy_gradient_loss | -0.00351     |\n","|    std                  | 4.48         |\n","|    value_loss           | 0.176        |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 950     |\n","|    iterations      | 319     |\n","|    time_elapsed    | 4125    |\n","|    total_timesteps | 3919872 |\n","--------------------------------\n","Eval num_timesteps=3924000, episode_reward=13.21 +/- 2.28\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 13.2         |\n","| time/                   |              |\n","|    total_timesteps      | 3924000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0028531626 |\n","|    clip_fraction        | 0.121        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.7        |\n","|    explained_variance   | 0.0025       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.238        |\n","|    n_updates            | 1595         |\n","|    policy_gradient_loss | -0.00307     |\n","|    std                  | 4.54         |\n","|    value_loss           | 10.1         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 949     |\n","|    iterations      | 320     |\n","|    time_elapsed    | 4140    |\n","|    total_timesteps | 3932160 |\n","--------------------------------\n","Eval num_timesteps=3936000, episode_reward=14.77 +/- 2.82\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 14.8        |\n","| time/                   |             |\n","|    total_timesteps      | 3936000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002082574 |\n","|    clip_fraction        | 0.0953      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11.7       |\n","|    explained_variance   | 0.0839      |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.105      |\n","|    n_updates            | 1600        |\n","|    policy_gradient_loss | -0.00437    |\n","|    std                  | 4.54        |\n","|    value_loss           | 0.647       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 949     |\n","|    iterations      | 321     |\n","|    time_elapsed    | 4154    |\n","|    total_timesteps | 3944448 |\n","--------------------------------\n","Eval num_timesteps=3948000, episode_reward=12.87 +/- 0.45\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 12.9        |\n","| time/                   |             |\n","|    total_timesteps      | 3948000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002404745 |\n","|    clip_fraction        | 0.109       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11.7       |\n","|    explained_variance   | 0.238       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.116      |\n","|    n_updates            | 1605        |\n","|    policy_gradient_loss | -0.0043     |\n","|    std                  | 4.57        |\n","|    value_loss           | 0.239       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 949     |\n","|    iterations      | 322     |\n","|    time_elapsed    | 4168    |\n","|    total_timesteps | 3956736 |\n","--------------------------------\n","Eval num_timesteps=3960000, episode_reward=9.55 +/- 2.55\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 9.55         |\n","| time/                   |              |\n","|    total_timesteps      | 3960000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018058276 |\n","|    clip_fraction        | 0.0857       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.7        |\n","|    explained_variance   | 0.415        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.117       |\n","|    n_updates            | 1610         |\n","|    policy_gradient_loss | -0.00343     |\n","|    std                  | 4.58         |\n","|    value_loss           | 0.229        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 949     |\n","|    iterations      | 323     |\n","|    time_elapsed    | 4181    |\n","|    total_timesteps | 3969024 |\n","--------------------------------\n","Eval num_timesteps=3972000, episode_reward=15.72 +/- 2.70\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 15.7         |\n","| time/                   |              |\n","|    total_timesteps      | 3972000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0029481866 |\n","|    clip_fraction        | 0.115        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.8        |\n","|    explained_variance   | 0.522        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.306        |\n","|    n_updates            | 1615         |\n","|    policy_gradient_loss | -0.00318     |\n","|    std                  | 4.64         |\n","|    value_loss           | 7.42         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 949     |\n","|    iterations      | 324     |\n","|    time_elapsed    | 4193    |\n","|    total_timesteps | 3981312 |\n","--------------------------------\n","Eval num_timesteps=3984000, episode_reward=23.00 +/- 2.26\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 23           |\n","| time/                   |              |\n","|    total_timesteps      | 3984000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021820844 |\n","|    clip_fraction        | 0.0889       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.8        |\n","|    explained_variance   | -0.218       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.11        |\n","|    n_updates            | 1620         |\n","|    policy_gradient_loss | -0.00378     |\n","|    std                  | 4.67         |\n","|    value_loss           | 0.42         |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 949     |\n","|    iterations      | 325     |\n","|    time_elapsed    | 4207    |\n","|    total_timesteps | 3993600 |\n","--------------------------------\n","Eval num_timesteps=3996000, episode_reward=-19.33 +/- 44.93\n","Episode length: 357.60 +/- 174.40\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 358          |\n","|    mean_reward          | -19.3        |\n","| time/                   |              |\n","|    total_timesteps      | 3996000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0026609835 |\n","|    clip_fraction        | 0.111        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.8        |\n","|    explained_variance   | 0.0809       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.279        |\n","|    n_updates            | 1625         |\n","|    policy_gradient_loss | -0.00364     |\n","|    std                  | 4.72         |\n","|    value_loss           | 8.02         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 949     |\n","|    iterations      | 326     |\n","|    time_elapsed    | 4219    |\n","|    total_timesteps | 4005888 |\n","--------------------------------\n","Eval num_timesteps=4008000, episode_reward=19.18 +/- 0.55\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 19.2         |\n","| time/                   |              |\n","|    total_timesteps      | 4008000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021278644 |\n","|    clip_fraction        | 0.0929       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.9        |\n","|    explained_variance   | 0.153        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.113       |\n","|    n_updates            | 1630         |\n","|    policy_gradient_loss | -0.00297     |\n","|    std                  | 4.75         |\n","|    value_loss           | 0.327        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 949     |\n","|    iterations      | 327     |\n","|    time_elapsed    | 4232    |\n","|    total_timesteps | 4018176 |\n","--------------------------------\n","Eval num_timesteps=4020000, episode_reward=16.32 +/- 0.97\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 16.3         |\n","| time/                   |              |\n","|    total_timesteps      | 4020000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021411844 |\n","|    clip_fraction        | 0.0926       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.9        |\n","|    explained_variance   | 0.333        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.115       |\n","|    n_updates            | 1635         |\n","|    policy_gradient_loss | -0.00368     |\n","|    std                  | 4.75         |\n","|    value_loss           | 0.264        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 949     |\n","|    iterations      | 328     |\n","|    time_elapsed    | 4246    |\n","|    total_timesteps | 4030464 |\n","--------------------------------\n","Eval num_timesteps=4032000, episode_reward=21.40 +/- 1.40\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 21.4        |\n","| time/                   |             |\n","|    total_timesteps      | 4032000     |\n","| train/                  |             |\n","|    approx_kl            | 0.001880766 |\n","|    clip_fraction        | 0.0747      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11.9       |\n","|    explained_variance   | -0.0385     |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.115      |\n","|    n_updates            | 1640        |\n","|    policy_gradient_loss | -0.003      |\n","|    std                  | 4.76        |\n","|    value_loss           | 0.527       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 949     |\n","|    iterations      | 329     |\n","|    time_elapsed    | 4256    |\n","|    total_timesteps | 4042752 |\n","--------------------------------\n","Eval num_timesteps=4044000, episode_reward=18.68 +/- 0.98\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 18.7         |\n","| time/                   |              |\n","|    total_timesteps      | 4044000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020324262 |\n","|    clip_fraction        | 0.103        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.9        |\n","|    explained_variance   | 0.567        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.119       |\n","|    n_updates            | 1645         |\n","|    policy_gradient_loss | -0.00365     |\n","|    std                  | 4.76         |\n","|    value_loss           | 0.213        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 949     |\n","|    iterations      | 330     |\n","|    time_elapsed    | 4271    |\n","|    total_timesteps | 4055040 |\n","--------------------------------\n","Eval num_timesteps=4056000, episode_reward=20.00 +/- 0.44\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 20           |\n","| time/                   |              |\n","|    total_timesteps      | 4056000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0023703405 |\n","|    clip_fraction        | 0.0947       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.9        |\n","|    explained_variance   | 0.393        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.116       |\n","|    n_updates            | 1650         |\n","|    policy_gradient_loss | -0.00432     |\n","|    std                  | 4.78         |\n","|    value_loss           | 0.319        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 949     |\n","|    iterations      | 331     |\n","|    time_elapsed    | 4284    |\n","|    total_timesteps | 4067328 |\n","--------------------------------\n","Eval num_timesteps=4068000, episode_reward=21.71 +/- 4.84\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 21.7         |\n","| time/                   |              |\n","|    total_timesteps      | 4068000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0035472782 |\n","|    clip_fraction        | 0.136        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.9        |\n","|    explained_variance   | 0.537        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0521      |\n","|    n_updates            | 1655         |\n","|    policy_gradient_loss | -0.004       |\n","|    std                  | 4.8          |\n","|    value_loss           | 8.95         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 949     |\n","|    iterations      | 332     |\n","|    time_elapsed    | 4296    |\n","|    total_timesteps | 4079616 |\n","--------------------------------\n","Eval num_timesteps=4080000, episode_reward=21.04 +/- 0.14\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 21           |\n","| time/                   |              |\n","|    total_timesteps      | 4080000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0027400844 |\n","|    clip_fraction        | 0.117        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12          |\n","|    explained_variance   | 0.0273       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.397        |\n","|    n_updates            | 1660         |\n","|    policy_gradient_loss | -0.00385     |\n","|    std                  | 4.85         |\n","|    value_loss           | 12.1         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 949     |\n","|    iterations      | 333     |\n","|    time_elapsed    | 4310    |\n","|    total_timesteps | 4091904 |\n","--------------------------------\n","Eval num_timesteps=4092000, episode_reward=14.42 +/- 2.85\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 14.4         |\n","| time/                   |              |\n","|    total_timesteps      | 4092000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022351397 |\n","|    clip_fraction        | 0.0948       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12          |\n","|    explained_variance   | 0.101        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.113       |\n","|    n_updates            | 1665         |\n","|    policy_gradient_loss | -0.00417     |\n","|    std                  | 4.87         |\n","|    value_loss           | 0.369        |\n","------------------------------------------\n","Eval num_timesteps=4104000, episode_reward=20.92 +/- 3.29\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 20.9     |\n","| time/              |          |\n","|    total_timesteps | 4104000  |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 947     |\n","|    iterations      | 334     |\n","|    time_elapsed    | 4330    |\n","|    total_timesteps | 4104192 |\n","--------------------------------\n","Eval num_timesteps=4116000, episode_reward=19.22 +/- 0.02\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 19.2         |\n","| time/                   |              |\n","|    total_timesteps      | 4116000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0030304946 |\n","|    clip_fraction        | 0.116        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12          |\n","|    explained_variance   | 0.349        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.717        |\n","|    n_updates            | 1670         |\n","|    policy_gradient_loss | -0.00373     |\n","|    std                  | 4.9          |\n","|    value_loss           | 18.9         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 948     |\n","|    iterations      | 335     |\n","|    time_elapsed    | 4341    |\n","|    total_timesteps | 4116480 |\n","--------------------------------\n","Eval num_timesteps=4128000, episode_reward=18.87 +/- 2.16\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 18.9         |\n","| time/                   |              |\n","|    total_timesteps      | 4128000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0025505137 |\n","|    clip_fraction        | 0.109        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12          |\n","|    explained_variance   | 0.51         |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0166      |\n","|    n_updates            | 1675         |\n","|    policy_gradient_loss | -0.00299     |\n","|    std                  | 4.95         |\n","|    value_loss           | 8.87         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 947     |\n","|    iterations      | 336     |\n","|    time_elapsed    | 4355    |\n","|    total_timesteps | 4128768 |\n","--------------------------------\n","Eval num_timesteps=4140000, episode_reward=17.97 +/- 1.01\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 18           |\n","| time/                   |              |\n","|    total_timesteps      | 4140000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0025705036 |\n","|    clip_fraction        | 0.115        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.1        |\n","|    explained_variance   | 0.716        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.278        |\n","|    n_updates            | 1680         |\n","|    policy_gradient_loss | -0.00317     |\n","|    std                  | 4.97         |\n","|    value_loss           | 15.6         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 947     |\n","|    iterations      | 337     |\n","|    time_elapsed    | 4369    |\n","|    total_timesteps | 4141056 |\n","--------------------------------\n","Eval num_timesteps=4152000, episode_reward=20.91 +/- 0.80\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 20.9         |\n","| time/                   |              |\n","|    total_timesteps      | 4152000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0024318327 |\n","|    clip_fraction        | 0.101        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.1        |\n","|    explained_variance   | 0.736        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0234      |\n","|    n_updates            | 1685         |\n","|    policy_gradient_loss | -0.00385     |\n","|    std                  | 5            |\n","|    value_loss           | 3.2          |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 948     |\n","|    iterations      | 338     |\n","|    time_elapsed    | 4380    |\n","|    total_timesteps | 4153344 |\n","--------------------------------\n","Eval num_timesteps=4164000, episode_reward=18.65 +/- 3.27\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 18.7         |\n","| time/                   |              |\n","|    total_timesteps      | 4164000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022582572 |\n","|    clip_fraction        | 0.112        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.1        |\n","|    explained_variance   | 0.0786       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0744      |\n","|    n_updates            | 1690         |\n","|    policy_gradient_loss | -0.0038      |\n","|    std                  | 5.03         |\n","|    value_loss           | 1.49         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 947     |\n","|    iterations      | 339     |\n","|    time_elapsed    | 4395    |\n","|    total_timesteps | 4165632 |\n","--------------------------------\n","Eval num_timesteps=4176000, episode_reward=22.75 +/- 3.43\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 22.8        |\n","| time/                   |             |\n","|    total_timesteps      | 4176000     |\n","| train/                  |             |\n","|    approx_kl            | 0.001959847 |\n","|    clip_fraction        | 0.0763      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12.1       |\n","|    explained_variance   | 0.459       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.117      |\n","|    n_updates            | 1695        |\n","|    policy_gradient_loss | -0.00418    |\n","|    std                  | 5.03        |\n","|    value_loss           | 0.372       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 948     |\n","|    iterations      | 340     |\n","|    time_elapsed    | 4406    |\n","|    total_timesteps | 4177920 |\n","--------------------------------\n","Eval num_timesteps=4188000, episode_reward=19.07 +/- 5.05\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 19.1        |\n","| time/                   |             |\n","|    total_timesteps      | 4188000     |\n","| train/                  |             |\n","|    approx_kl            | 0.003203513 |\n","|    clip_fraction        | 0.135       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12.1       |\n","|    explained_variance   | 0.446       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0226     |\n","|    n_updates            | 1700        |\n","|    policy_gradient_loss | -0.00339    |\n","|    std                  | 5.05        |\n","|    value_loss           | 2.64        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 947     |\n","|    iterations      | 341     |\n","|    time_elapsed    | 4420    |\n","|    total_timesteps | 4190208 |\n","--------------------------------\n","Eval num_timesteps=4200000, episode_reward=25.50 +/- 0.51\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 25.5         |\n","| time/                   |              |\n","|    total_timesteps      | 4200000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0032508972 |\n","|    clip_fraction        | 0.147        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.2        |\n","|    explained_variance   | 0.557        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.469        |\n","|    n_updates            | 1705         |\n","|    policy_gradient_loss | -0.00366     |\n","|    std                  | 5.09         |\n","|    value_loss           | 9.47         |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 947     |\n","|    iterations      | 342     |\n","|    time_elapsed    | 4434    |\n","|    total_timesteps | 4202496 |\n","--------------------------------\n","Eval num_timesteps=4212000, episode_reward=22.10 +/- 3.15\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 22.1         |\n","| time/                   |              |\n","|    total_timesteps      | 4212000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021387346 |\n","|    clip_fraction        | 0.0931       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.2        |\n","|    explained_variance   | 0.463        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.121       |\n","|    n_updates            | 1710         |\n","|    policy_gradient_loss | -0.00452     |\n","|    std                  | 5.1          |\n","|    value_loss           | 0.205        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 947     |\n","|    iterations      | 343     |\n","|    time_elapsed    | 4449    |\n","|    total_timesteps | 4214784 |\n","--------------------------------\n","Eval num_timesteps=4224000, episode_reward=21.44 +/- 1.93\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 21.4         |\n","| time/                   |              |\n","|    total_timesteps      | 4224000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0028552988 |\n","|    clip_fraction        | 0.136        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.2        |\n","|    explained_variance   | 0.633        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0246      |\n","|    n_updates            | 1715         |\n","|    policy_gradient_loss | -0.00357     |\n","|    std                  | 5.12         |\n","|    value_loss           | 8.14         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 946     |\n","|    iterations      | 344     |\n","|    time_elapsed    | 4463    |\n","|    total_timesteps | 4227072 |\n","--------------------------------\n","Eval num_timesteps=4236000, episode_reward=30.92 +/- 0.33\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 30.9         |\n","| time/                   |              |\n","|    total_timesteps      | 4236000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0025393886 |\n","|    clip_fraction        | 0.107        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.2        |\n","|    explained_variance   | 0.554        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0497      |\n","|    n_updates            | 1720         |\n","|    policy_gradient_loss | -0.00373     |\n","|    std                  | 5.19         |\n","|    value_loss           | 7.45         |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 947     |\n","|    iterations      | 345     |\n","|    time_elapsed    | 4474    |\n","|    total_timesteps | 4239360 |\n","--------------------------------\n","Eval num_timesteps=4248000, episode_reward=27.48 +/- 1.57\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 27.5         |\n","| time/                   |              |\n","|    total_timesteps      | 4248000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0026641295 |\n","|    clip_fraction        | 0.112        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.3        |\n","|    explained_variance   | 0.641        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0129      |\n","|    n_updates            | 1725         |\n","|    policy_gradient_loss | -0.00369     |\n","|    std                  | 5.22         |\n","|    value_loss           | 8.99         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 947     |\n","|    iterations      | 346     |\n","|    time_elapsed    | 4488    |\n","|    total_timesteps | 4251648 |\n","--------------------------------\n","Eval num_timesteps=4260000, episode_reward=21.94 +/- 6.24\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 21.9         |\n","| time/                   |              |\n","|    total_timesteps      | 4260000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021919713 |\n","|    clip_fraction        | 0.102        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.3        |\n","|    explained_variance   | -0.0421      |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.109       |\n","|    n_updates            | 1730         |\n","|    policy_gradient_loss | -0.00415     |\n","|    std                  | 5.23         |\n","|    value_loss           | 0.699        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 947     |\n","|    iterations      | 347     |\n","|    time_elapsed    | 4501    |\n","|    total_timesteps | 4263936 |\n","--------------------------------\n","Eval num_timesteps=4272000, episode_reward=1.09 +/- 45.92\n","Episode length: 464.00 +/- 44.09\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 464          |\n","|    mean_reward          | 1.09         |\n","| time/                   |              |\n","|    total_timesteps      | 4272000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0026284636 |\n","|    clip_fraction        | 0.142        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.3        |\n","|    explained_variance   | 0.676        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0674      |\n","|    n_updates            | 1735         |\n","|    policy_gradient_loss | -0.00355     |\n","|    std                  | 5.27         |\n","|    value_loss           | 9.16         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 947     |\n","|    iterations      | 348     |\n","|    time_elapsed    | 4513    |\n","|    total_timesteps | 4276224 |\n","--------------------------------\n","Eval num_timesteps=4284000, episode_reward=32.56 +/- 0.18\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 32.6         |\n","| time/                   |              |\n","|    total_timesteps      | 4284000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0030056909 |\n","|    clip_fraction        | 0.145        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.3        |\n","|    explained_variance   | 0.436        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0719      |\n","|    n_updates            | 1740         |\n","|    policy_gradient_loss | -0.00334     |\n","|    std                  | 5.31         |\n","|    value_loss           | 8.11         |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 947     |\n","|    iterations      | 349     |\n","|    time_elapsed    | 4527    |\n","|    total_timesteps | 4288512 |\n","--------------------------------\n","Eval num_timesteps=4296000, episode_reward=36.92 +/- 0.54\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 36.9        |\n","| time/                   |             |\n","|    total_timesteps      | 4296000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002423555 |\n","|    clip_fraction        | 0.111       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12.4       |\n","|    explained_variance   | 0.0742      |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.113      |\n","|    n_updates            | 1745        |\n","|    policy_gradient_loss | -0.00399    |\n","|    std                  | 5.33        |\n","|    value_loss           | 0.603       |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 947     |\n","|    iterations      | 350     |\n","|    time_elapsed    | 4538    |\n","|    total_timesteps | 4300800 |\n","--------------------------------\n","Eval num_timesteps=4308000, episode_reward=29.66 +/- 0.21\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 29.7        |\n","| time/                   |             |\n","|    total_timesteps      | 4308000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002954417 |\n","|    clip_fraction        | 0.155       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12.4       |\n","|    explained_variance   | 0.684       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0544     |\n","|    n_updates            | 1750        |\n","|    policy_gradient_loss | -0.00365    |\n","|    std                  | 5.38        |\n","|    value_loss           | 9.74        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 947     |\n","|    iterations      | 351     |\n","|    time_elapsed    | 4552    |\n","|    total_timesteps | 4313088 |\n","--------------------------------\n","Eval num_timesteps=4320000, episode_reward=-30.76 +/- 51.27\n","Episode length: 267.20 +/- 190.08\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 267        |\n","|    mean_reward          | -30.8      |\n","| time/                   |            |\n","|    total_timesteps      | 4320000    |\n","| train/                  |            |\n","|    approx_kl            | 0.00195661 |\n","|    clip_fraction        | 0.0766     |\n","|    clip_range           | 0.1        |\n","|    entropy_loss         | -12.4      |\n","|    explained_variance   | 0.101      |\n","|    learning_rate        | 0.000622   |\n","|    loss                 | -0.121     |\n","|    n_updates            | 1755       |\n","|    policy_gradient_loss | -0.00458   |\n","|    std                  | 5.4        |\n","|    value_loss           | 0.239      |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 947     |\n","|    iterations      | 352     |\n","|    time_elapsed    | 4566    |\n","|    total_timesteps | 4325376 |\n","--------------------------------\n","Eval num_timesteps=4332000, episode_reward=-31.48 +/- 48.57\n","Episode length: 270.20 +/- 187.63\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 270         |\n","|    mean_reward          | -31.5       |\n","| time/                   |             |\n","|    total_timesteps      | 4332000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002326473 |\n","|    clip_fraction        | 0.124       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12.4       |\n","|    explained_variance   | 0.659       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.00218    |\n","|    n_updates            | 1760        |\n","|    policy_gradient_loss | -0.00348    |\n","|    std                  | 5.47        |\n","|    value_loss           | 7.34        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 947     |\n","|    iterations      | 353     |\n","|    time_elapsed    | 4576    |\n","|    total_timesteps | 4337664 |\n","--------------------------------\n","Eval num_timesteps=4344000, episode_reward=26.20 +/- 2.56\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 26.2         |\n","| time/                   |              |\n","|    total_timesteps      | 4344000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0038716437 |\n","|    clip_fraction        | 0.137        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.5        |\n","|    explained_variance   | 0.0624       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.363        |\n","|    n_updates            | 1765         |\n","|    policy_gradient_loss | -0.00347     |\n","|    std                  | 5.54         |\n","|    value_loss           | 13           |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 947     |\n","|    iterations      | 354     |\n","|    time_elapsed    | 4591    |\n","|    total_timesteps | 4349952 |\n","--------------------------------\n","Eval num_timesteps=4356000, episode_reward=28.21 +/- 0.79\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 28.2         |\n","| time/                   |              |\n","|    total_timesteps      | 4356000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0027544748 |\n","|    clip_fraction        | 0.125        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.5        |\n","|    explained_variance   | 0.0822       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.182        |\n","|    n_updates            | 1770         |\n","|    policy_gradient_loss | -0.00357     |\n","|    std                  | 5.61         |\n","|    value_loss           | 11.4         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 947     |\n","|    iterations      | 355     |\n","|    time_elapsed    | 4603    |\n","|    total_timesteps | 4362240 |\n","--------------------------------\n","Eval num_timesteps=4368000, episode_reward=14.63 +/- 10.78\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 14.6        |\n","| time/                   |             |\n","|    total_timesteps      | 4368000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002228931 |\n","|    clip_fraction        | 0.0892      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12.6       |\n","|    explained_variance   | 0.58        |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0953     |\n","|    n_updates            | 1775        |\n","|    policy_gradient_loss | -0.00268    |\n","|    std                  | 5.67        |\n","|    value_loss           | 8.42        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 947     |\n","|    iterations      | 356     |\n","|    time_elapsed    | 4615    |\n","|    total_timesteps | 4374528 |\n","--------------------------------\n","Eval num_timesteps=4380000, episode_reward=27.33 +/- 0.37\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 27.3         |\n","| time/                   |              |\n","|    total_timesteps      | 4380000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018016087 |\n","|    clip_fraction        | 0.0864       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.6        |\n","|    explained_variance   | 0.0109       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.119       |\n","|    n_updates            | 1780         |\n","|    policy_gradient_loss | -0.00396     |\n","|    std                  | 5.65         |\n","|    value_loss           | 0.484        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 947     |\n","|    iterations      | 357     |\n","|    time_elapsed    | 4630    |\n","|    total_timesteps | 4386816 |\n","--------------------------------\n","Eval num_timesteps=4392000, episode_reward=-44.79 +/- 35.72\n","Episode length: 255.20 +/- 199.88\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 255          |\n","|    mean_reward          | -44.8        |\n","| time/                   |              |\n","|    total_timesteps      | 4392000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019080952 |\n","|    clip_fraction        | 0.0839       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.6        |\n","|    explained_variance   | 0.418        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.121       |\n","|    n_updates            | 1785         |\n","|    policy_gradient_loss | -0.00343     |\n","|    std                  | 5.67         |\n","|    value_loss           | 0.311        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 947     |\n","|    iterations      | 358     |\n","|    time_elapsed    | 4640    |\n","|    total_timesteps | 4399104 |\n","--------------------------------\n","Eval num_timesteps=4404000, episode_reward=23.03 +/- 2.92\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 23           |\n","| time/                   |              |\n","|    total_timesteps      | 4404000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021970028 |\n","|    clip_fraction        | 0.1          |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.6        |\n","|    explained_variance   | 0.626        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.0331       |\n","|    n_updates            | 1790         |\n","|    policy_gradient_loss | -0.00324     |\n","|    std                  | 5.7          |\n","|    value_loss           | 9.05         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 947     |\n","|    iterations      | 359     |\n","|    time_elapsed    | 4654    |\n","|    total_timesteps | 4411392 |\n","--------------------------------\n","Eval num_timesteps=4416000, episode_reward=31.26 +/- 3.00\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 31.3         |\n","| time/                   |              |\n","|    total_timesteps      | 4416000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0023054814 |\n","|    clip_fraction        | 0.0907       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.6        |\n","|    explained_variance   | -0.134       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.117       |\n","|    n_updates            | 1795         |\n","|    policy_gradient_loss | -0.00467     |\n","|    std                  | 5.73         |\n","|    value_loss           | 0.512        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 947     |\n","|    iterations      | 360     |\n","|    time_elapsed    | 4667    |\n","|    total_timesteps | 4423680 |\n","--------------------------------\n","Eval num_timesteps=4428000, episode_reward=29.60 +/- 2.19\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 29.6         |\n","| time/                   |              |\n","|    total_timesteps      | 4428000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0028241186 |\n","|    clip_fraction        | 0.158        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.7        |\n","|    explained_variance   | 0.644        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.505        |\n","|    n_updates            | 1800         |\n","|    policy_gradient_loss | -0.00416     |\n","|    std                  | 5.77         |\n","|    value_loss           | 8.89         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 947     |\n","|    iterations      | 361     |\n","|    time_elapsed    | 4679    |\n","|    total_timesteps | 4435968 |\n","--------------------------------\n","Eval num_timesteps=4440000, episode_reward=25.48 +/- 1.15\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 25.5         |\n","| time/                   |              |\n","|    total_timesteps      | 4440000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0028888828 |\n","|    clip_fraction        | 0.118        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.7        |\n","|    explained_variance   | 0.733        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.269        |\n","|    n_updates            | 1805         |\n","|    policy_gradient_loss | -0.00475     |\n","|    std                  | 5.82         |\n","|    value_loss           | 10           |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 947     |\n","|    iterations      | 362     |\n","|    time_elapsed    | 4694    |\n","|    total_timesteps | 4448256 |\n","--------------------------------\n","Eval num_timesteps=4452000, episode_reward=24.18 +/- 1.09\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 24.2         |\n","| time/                   |              |\n","|    total_timesteps      | 4452000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022317562 |\n","|    clip_fraction        | 0.117        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.7        |\n","|    explained_variance   | 0.642        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.474        |\n","|    n_updates            | 1810         |\n","|    policy_gradient_loss | -0.00429     |\n","|    std                  | 5.85         |\n","|    value_loss           | 7.73         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 947     |\n","|    iterations      | 363     |\n","|    time_elapsed    | 4705    |\n","|    total_timesteps | 4460544 |\n","--------------------------------\n","Eval num_timesteps=4464000, episode_reward=27.33 +/- 2.20\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 27.3         |\n","| time/                   |              |\n","|    total_timesteps      | 4464000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0016355695 |\n","|    clip_fraction        | 0.0938       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.7        |\n","|    explained_variance   | -0.192       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.122       |\n","|    n_updates            | 1815         |\n","|    policy_gradient_loss | -0.00409     |\n","|    std                  | 5.83         |\n","|    value_loss           | 0.494        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 947     |\n","|    iterations      | 364     |\n","|    time_elapsed    | 4722    |\n","|    total_timesteps | 4472832 |\n","--------------------------------\n","Eval num_timesteps=4476000, episode_reward=-14.71 +/- 51.97\n","Episode length: 321.20 +/- 218.98\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 321         |\n","|    mean_reward          | -14.7       |\n","| time/                   |             |\n","|    total_timesteps      | 4476000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002243565 |\n","|    clip_fraction        | 0.0986      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12.7       |\n","|    explained_variance   | 0.101       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.126      |\n","|    n_updates            | 1820        |\n","|    policy_gradient_loss | -0.00503    |\n","|    std                  | 5.82        |\n","|    value_loss           | 0.263       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 947     |\n","|    iterations      | 365     |\n","|    time_elapsed    | 4735    |\n","|    total_timesteps | 4485120 |\n","--------------------------------\n","Eval num_timesteps=4488000, episode_reward=30.58 +/- 1.54\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 30.6         |\n","| time/                   |              |\n","|    total_timesteps      | 4488000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0017501753 |\n","|    clip_fraction        | 0.0756       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.7        |\n","|    explained_variance   | 0.479        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.122       |\n","|    n_updates            | 1825         |\n","|    policy_gradient_loss | -0.00376     |\n","|    std                  | 5.85         |\n","|    value_loss           | 0.223        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 947     |\n","|    iterations      | 366     |\n","|    time_elapsed    | 4747    |\n","|    total_timesteps | 4497408 |\n","--------------------------------\n","Eval num_timesteps=4500000, episode_reward=32.81 +/- 1.60\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 32.8         |\n","| time/                   |              |\n","|    total_timesteps      | 4500000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020516352 |\n","|    clip_fraction        | 0.0842       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.7        |\n","|    explained_variance   | 0.538        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.132       |\n","|    n_updates            | 1830         |\n","|    policy_gradient_loss | -0.00449     |\n","|    std                  | 5.84         |\n","|    value_loss           | 0.255        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 946     |\n","|    iterations      | 367     |\n","|    time_elapsed    | 4762    |\n","|    total_timesteps | 4509696 |\n","--------------------------------\n","Eval num_timesteps=4512000, episode_reward=32.61 +/- 2.69\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 32.6         |\n","| time/                   |              |\n","|    total_timesteps      | 4512000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021349106 |\n","|    clip_fraction        | 0.0825       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.7        |\n","|    explained_variance   | 0.518        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.125       |\n","|    n_updates            | 1835         |\n","|    policy_gradient_loss | -0.00404     |\n","|    std                  | 5.85         |\n","|    value_loss           | 0.19         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 947     |\n","|    iterations      | 368     |\n","|    time_elapsed    | 4772    |\n","|    total_timesteps | 4521984 |\n","--------------------------------\n","Eval num_timesteps=4524000, episode_reward=27.67 +/- 0.37\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 27.7        |\n","| time/                   |             |\n","|    total_timesteps      | 4524000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002282601 |\n","|    clip_fraction        | 0.0885      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12.7       |\n","|    explained_variance   | 0.475       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.123      |\n","|    n_updates            | 1840        |\n","|    policy_gradient_loss | -0.00397    |\n","|    std                  | 5.88        |\n","|    value_loss           | 0.195       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 947     |\n","|    iterations      | 369     |\n","|    time_elapsed    | 4786    |\n","|    total_timesteps | 4534272 |\n","--------------------------------\n","Eval num_timesteps=4536000, episode_reward=-3.40 +/- 42.60\n","Episode length: 479.20 +/- 25.47\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 479          |\n","|    mean_reward          | -3.4         |\n","| time/                   |              |\n","|    total_timesteps      | 4536000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021338882 |\n","|    clip_fraction        | 0.105        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.8        |\n","|    explained_variance   | 0.397        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.127       |\n","|    n_updates            | 1845         |\n","|    policy_gradient_loss | -0.00404     |\n","|    std                  | 5.89         |\n","|    value_loss           | 0.234        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 947     |\n","|    iterations      | 370     |\n","|    time_elapsed    | 4799    |\n","|    total_timesteps | 4546560 |\n","--------------------------------\n","Eval num_timesteps=4548000, episode_reward=32.09 +/- 2.05\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 32.1        |\n","| time/                   |             |\n","|    total_timesteps      | 4548000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002169586 |\n","|    clip_fraction        | 0.0907      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12.8       |\n","|    explained_variance   | 0.532       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.127      |\n","|    n_updates            | 1850        |\n","|    policy_gradient_loss | -0.0038     |\n","|    std                  | 5.91        |\n","|    value_loss           | 0.222       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 947     |\n","|    iterations      | 371     |\n","|    time_elapsed    | 4811    |\n","|    total_timesteps | 4558848 |\n","--------------------------------\n","Eval num_timesteps=4560000, episode_reward=34.63 +/- 1.86\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 34.6         |\n","| time/                   |              |\n","|    total_timesteps      | 4560000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019391729 |\n","|    clip_fraction        | 0.0817       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.8        |\n","|    explained_variance   | 0.461        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.127       |\n","|    n_updates            | 1855         |\n","|    policy_gradient_loss | -0.00432     |\n","|    std                  | 5.91         |\n","|    value_loss           | 0.213        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 947     |\n","|    iterations      | 372     |\n","|    time_elapsed    | 4825    |\n","|    total_timesteps | 4571136 |\n","--------------------------------\n","Eval num_timesteps=4572000, episode_reward=29.31 +/- 2.14\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 29.3         |\n","| time/                   |              |\n","|    total_timesteps      | 4572000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021848574 |\n","|    clip_fraction        | 0.0835       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.8        |\n","|    explained_variance   | 0.523        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.129       |\n","|    n_updates            | 1860         |\n","|    policy_gradient_loss | -0.00407     |\n","|    std                  | 5.93         |\n","|    value_loss           | 0.214        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 947     |\n","|    iterations      | 373     |\n","|    time_elapsed    | 4836    |\n","|    total_timesteps | 4583424 |\n","--------------------------------\n","Eval num_timesteps=4584000, episode_reward=-12.65 +/- 54.01\n","Episode length: 332.80 +/- 204.78\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 333          |\n","|    mean_reward          | -12.6        |\n","| time/                   |              |\n","|    total_timesteps      | 4584000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0028961145 |\n","|    clip_fraction        | 0.12         |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.8        |\n","|    explained_variance   | 0.579        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.214        |\n","|    n_updates            | 1865         |\n","|    policy_gradient_loss | -0.00345     |\n","|    std                  | 5.98         |\n","|    value_loss           | 6.73         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 947     |\n","|    iterations      | 374     |\n","|    time_elapsed    | 4850    |\n","|    total_timesteps | 4595712 |\n","--------------------------------\n","Eval num_timesteps=4596000, episode_reward=33.78 +/- 0.71\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 33.8        |\n","| time/                   |             |\n","|    total_timesteps      | 4596000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002002542 |\n","|    clip_fraction        | 0.0835      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12.8       |\n","|    explained_variance   | 0.25        |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.12       |\n","|    n_updates            | 1870        |\n","|    policy_gradient_loss | -0.00354    |\n","|    std                  | 5.98        |\n","|    value_loss           | 0.389       |\n","-----------------------------------------\n","Eval num_timesteps=4608000, episode_reward=35.18 +/- 0.31\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 35.2     |\n","| time/              |          |\n","|    total_timesteps | 4608000  |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 946     |\n","|    iterations      | 375     |\n","|    time_elapsed    | 4869    |\n","|    total_timesteps | 4608000 |\n","--------------------------------\n","Eval num_timesteps=4620000, episode_reward=30.80 +/- 0.10\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 30.8         |\n","| time/                   |              |\n","|    total_timesteps      | 4620000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0030718113 |\n","|    clip_fraction        | 0.119        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.8        |\n","|    explained_variance   | 0.544        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0862      |\n","|    n_updates            | 1875         |\n","|    policy_gradient_loss | -0.00344     |\n","|    std                  | 6.05         |\n","|    value_loss           | 6.93         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 946     |\n","|    iterations      | 376     |\n","|    time_elapsed    | 4882    |\n","|    total_timesteps | 4620288 |\n","--------------------------------\n","Eval num_timesteps=4632000, episode_reward=29.12 +/- 2.04\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 29.1         |\n","| time/                   |              |\n","|    total_timesteps      | 4632000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0024207167 |\n","|    clip_fraction        | 0.105        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.9        |\n","|    explained_variance   | 0.561        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.136        |\n","|    n_updates            | 1880         |\n","|    policy_gradient_loss | -0.00387     |\n","|    std                  | 6.11         |\n","|    value_loss           | 8.28         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 946     |\n","|    iterations      | 377     |\n","|    time_elapsed    | 4894    |\n","|    total_timesteps | 4632576 |\n","--------------------------------\n","Eval num_timesteps=4644000, episode_reward=32.99 +/- 1.42\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 33           |\n","| time/                   |              |\n","|    total_timesteps      | 4644000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020359193 |\n","|    clip_fraction        | 0.0888       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.9        |\n","|    explained_variance   | 0.342        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.12        |\n","|    n_updates            | 1885         |\n","|    policy_gradient_loss | -0.00424     |\n","|    std                  | 6.12         |\n","|    value_loss           | 0.586        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 946     |\n","|    iterations      | 378     |\n","|    time_elapsed    | 4909    |\n","|    total_timesteps | 4644864 |\n","--------------------------------\n","Eval num_timesteps=4656000, episode_reward=32.47 +/- 0.76\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 32.5         |\n","| time/                   |              |\n","|    total_timesteps      | 4656000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019857509 |\n","|    clip_fraction        | 0.0914       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.9        |\n","|    explained_variance   | 0.622        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.12        |\n","|    n_updates            | 1890         |\n","|    policy_gradient_loss | -0.00401     |\n","|    std                  | 6.12         |\n","|    value_loss           | 0.367        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 946     |\n","|    iterations      | 379     |\n","|    time_elapsed    | 4919    |\n","|    total_timesteps | 4657152 |\n","--------------------------------\n","Eval num_timesteps=4668000, episode_reward=31.95 +/- 0.37\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 31.9         |\n","| time/                   |              |\n","|    total_timesteps      | 4668000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018363545 |\n","|    clip_fraction        | 0.0847       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.9        |\n","|    explained_variance   | 0.225        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.124       |\n","|    n_updates            | 1895         |\n","|    policy_gradient_loss | -0.00475     |\n","|    std                  | 6.12         |\n","|    value_loss           | 0.381        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 946     |\n","|    iterations      | 380     |\n","|    time_elapsed    | 4933    |\n","|    total_timesteps | 4669440 |\n","--------------------------------\n","Eval num_timesteps=4680000, episode_reward=33.71 +/- 1.07\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 33.7         |\n","| time/                   |              |\n","|    total_timesteps      | 4680000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0023062367 |\n","|    clip_fraction        | 0.0985       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.9        |\n","|    explained_variance   | 0.514        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.128       |\n","|    n_updates            | 1900         |\n","|    policy_gradient_loss | -0.00476     |\n","|    std                  | 6.13         |\n","|    value_loss           | 0.229        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 946     |\n","|    iterations      | 381     |\n","|    time_elapsed    | 4947    |\n","|    total_timesteps | 4681728 |\n","--------------------------------\n","Eval num_timesteps=4692000, episode_reward=33.34 +/- 3.22\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 33.3         |\n","| time/                   |              |\n","|    total_timesteps      | 4692000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0026524512 |\n","|    clip_fraction        | 0.103        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.9        |\n","|    explained_variance   | 0.543        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.125       |\n","|    n_updates            | 1905         |\n","|    policy_gradient_loss | -0.00448     |\n","|    std                  | 6.13         |\n","|    value_loss           | 0.239        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 946     |\n","|    iterations      | 382     |\n","|    time_elapsed    | 4958    |\n","|    total_timesteps | 4694016 |\n","--------------------------------\n","Eval num_timesteps=4704000, episode_reward=33.66 +/- 0.30\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 33.7         |\n","| time/                   |              |\n","|    total_timesteps      | 4704000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020869458 |\n","|    clip_fraction        | 0.104        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.9        |\n","|    explained_variance   | 0.647        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.13        |\n","|    n_updates            | 1910         |\n","|    policy_gradient_loss | -0.00433     |\n","|    std                  | 6.16         |\n","|    value_loss           | 0.224        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 946     |\n","|    iterations      | 383     |\n","|    time_elapsed    | 4973    |\n","|    total_timesteps | 4706304 |\n","--------------------------------\n","Eval num_timesteps=4716000, episode_reward=36.88 +/- 0.96\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 36.9        |\n","| time/                   |             |\n","|    total_timesteps      | 4716000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002049278 |\n","|    clip_fraction        | 0.0929      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -13         |\n","|    explained_variance   | 0.544       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.126      |\n","|    n_updates            | 1915        |\n","|    policy_gradient_loss | -0.0038     |\n","|    std                  | 6.19        |\n","|    value_loss           | 0.255       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 946     |\n","|    iterations      | 384     |\n","|    time_elapsed    | 4985    |\n","|    total_timesteps | 4718592 |\n","--------------------------------\n","Eval num_timesteps=4728000, episode_reward=36.63 +/- 0.63\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 36.6         |\n","| time/                   |              |\n","|    total_timesteps      | 4728000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0024133346 |\n","|    clip_fraction        | 0.102        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13          |\n","|    explained_variance   | 0.584        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.127       |\n","|    n_updates            | 1920         |\n","|    policy_gradient_loss | -0.00446     |\n","|    std                  | 6.23         |\n","|    value_loss           | 0.206        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 945     |\n","|    iterations      | 385     |\n","|    time_elapsed    | 5002    |\n","|    total_timesteps | 4730880 |\n","--------------------------------\n","Eval num_timesteps=4740000, episode_reward=30.05 +/- 0.45\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 30.1         |\n","| time/                   |              |\n","|    total_timesteps      | 4740000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018274576 |\n","|    clip_fraction        | 0.0925       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13          |\n","|    explained_variance   | 0.461        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.122       |\n","|    n_updates            | 1925         |\n","|    policy_gradient_loss | -0.00404     |\n","|    std                  | 6.25         |\n","|    value_loss           | 0.267        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 945     |\n","|    iterations      | 386     |\n","|    time_elapsed    | 5015    |\n","|    total_timesteps | 4743168 |\n","--------------------------------\n","Eval num_timesteps=4752000, episode_reward=32.80 +/- 1.92\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 32.8         |\n","| time/                   |              |\n","|    total_timesteps      | 4752000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0026537522 |\n","|    clip_fraction        | 0.107        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13          |\n","|    explained_variance   | 0.523        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.121       |\n","|    n_updates            | 1930         |\n","|    policy_gradient_loss | -0.00425     |\n","|    std                  | 6.29         |\n","|    value_loss           | 0.209        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 946     |\n","|    iterations      | 387     |\n","|    time_elapsed    | 5026    |\n","|    total_timesteps | 4755456 |\n","--------------------------------\n","Eval num_timesteps=4764000, episode_reward=32.15 +/- 4.51\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 32.1        |\n","| time/                   |             |\n","|    total_timesteps      | 4764000     |\n","| train/                  |             |\n","|    approx_kl            | 0.003207242 |\n","|    clip_fraction        | 0.123       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -13         |\n","|    explained_variance   | 0.616       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 0.139       |\n","|    n_updates            | 1935        |\n","|    policy_gradient_loss | -0.0037     |\n","|    std                  | 6.37        |\n","|    value_loss           | 6.51        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 945     |\n","|    iterations      | 388     |\n","|    time_elapsed    | 5041    |\n","|    total_timesteps | 4767744 |\n","--------------------------------\n","Eval num_timesteps=4776000, episode_reward=32.75 +/- 0.16\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 32.8         |\n","| time/                   |              |\n","|    total_timesteps      | 4776000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0024648414 |\n","|    clip_fraction        | 0.098        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.1        |\n","|    explained_variance   | 0.0368       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.122       |\n","|    n_updates            | 1940         |\n","|    policy_gradient_loss | -0.00389     |\n","|    std                  | 6.38         |\n","|    value_loss           | 0.281        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 945     |\n","|    iterations      | 389     |\n","|    time_elapsed    | 5053    |\n","|    total_timesteps | 4780032 |\n","--------------------------------\n","Eval num_timesteps=4788000, episode_reward=31.97 +/- 1.01\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 32           |\n","| time/                   |              |\n","|    total_timesteps      | 4788000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021180091 |\n","|    clip_fraction        | 0.103        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.1        |\n","|    explained_variance   | 0.544        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.779        |\n","|    n_updates            | 1945         |\n","|    policy_gradient_loss | -0.00348     |\n","|    std                  | 6.44         |\n","|    value_loss           | 7.71         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 946     |\n","|    iterations      | 390     |\n","|    time_elapsed    | 5065    |\n","|    total_timesteps | 4792320 |\n","--------------------------------\n","Eval num_timesteps=4800000, episode_reward=30.87 +/- 1.28\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 30.9         |\n","| time/                   |              |\n","|    total_timesteps      | 4800000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021769947 |\n","|    clip_fraction        | 0.104        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.1        |\n","|    explained_variance   | -0.221       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.12        |\n","|    n_updates            | 1950         |\n","|    policy_gradient_loss | -0.00469     |\n","|    std                  | 6.44         |\n","|    value_loss           | 0.468        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 945     |\n","|    iterations      | 391     |\n","|    time_elapsed    | 5080    |\n","|    total_timesteps | 4804608 |\n","--------------------------------\n","Eval num_timesteps=4812000, episode_reward=35.02 +/- 0.24\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 35           |\n","| time/                   |              |\n","|    total_timesteps      | 4812000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021715732 |\n","|    clip_fraction        | 0.0814       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.1        |\n","|    explained_variance   | 0.374        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.129       |\n","|    n_updates            | 1955         |\n","|    policy_gradient_loss | -0.00387     |\n","|    std                  | 6.49         |\n","|    value_loss           | 0.217        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 946     |\n","|    iterations      | 392     |\n","|    time_elapsed    | 5090    |\n","|    total_timesteps | 4816896 |\n","--------------------------------\n","Eval num_timesteps=4824000, episode_reward=30.69 +/- 0.79\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 30.7         |\n","| time/                   |              |\n","|    total_timesteps      | 4824000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0027059999 |\n","|    clip_fraction        | 0.131        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.2        |\n","|    explained_variance   | 0.579        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.0996       |\n","|    n_updates            | 1960         |\n","|    policy_gradient_loss | -0.0033      |\n","|    std                  | 6.56         |\n","|    value_loss           | 4.26         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 945     |\n","|    iterations      | 393     |\n","|    time_elapsed    | 5105    |\n","|    total_timesteps | 4829184 |\n","--------------------------------\n","Eval num_timesteps=4836000, episode_reward=34.45 +/- 1.61\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 34.5        |\n","| time/                   |             |\n","|    total_timesteps      | 4836000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002043341 |\n","|    clip_fraction        | 0.0933      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -13.2       |\n","|    explained_variance   | -0.63       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0936     |\n","|    n_updates            | 1965        |\n","|    policy_gradient_loss | -0.00423    |\n","|    std                  | 6.56        |\n","|    value_loss           | 1.23        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 945     |\n","|    iterations      | 394     |\n","|    time_elapsed    | 5118    |\n","|    total_timesteps | 4841472 |\n","--------------------------------\n","Eval num_timesteps=4848000, episode_reward=30.85 +/- 0.34\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 30.9         |\n","| time/                   |              |\n","|    total_timesteps      | 4848000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018928958 |\n","|    clip_fraction        | 0.105        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.2        |\n","|    explained_variance   | 0.436        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.126       |\n","|    n_updates            | 1970         |\n","|    policy_gradient_loss | -0.00432     |\n","|    std                  | 6.56         |\n","|    value_loss           | 0.259        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 946     |\n","|    iterations      | 395     |\n","|    time_elapsed    | 5129    |\n","|    total_timesteps | 4853760 |\n","--------------------------------\n","Eval num_timesteps=4860000, episode_reward=36.85 +/- 1.07\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 36.9         |\n","| time/                   |              |\n","|    total_timesteps      | 4860000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0024601151 |\n","|    clip_fraction        | 0.108        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.2        |\n","|    explained_variance   | 0.543        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.128       |\n","|    n_updates            | 1975         |\n","|    policy_gradient_loss | -0.00425     |\n","|    std                  | 6.59         |\n","|    value_loss           | 0.22         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 945     |\n","|    iterations      | 396     |\n","|    time_elapsed    | 5144    |\n","|    total_timesteps | 4866048 |\n","--------------------------------\n","Eval num_timesteps=4872000, episode_reward=39.26 +/- 4.25\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 39.3       |\n","| time/                   |            |\n","|    total_timesteps      | 4872000    |\n","| train/                  |            |\n","|    approx_kl            | 0.00209058 |\n","|    clip_fraction        | 0.093      |\n","|    clip_range           | 0.1        |\n","|    entropy_loss         | -13.2      |\n","|    explained_variance   | 0.387      |\n","|    learning_rate        | 0.000622   |\n","|    loss                 | -0.119     |\n","|    n_updates            | 1980       |\n","|    policy_gradient_loss | -0.00365   |\n","|    std                  | 6.63       |\n","|    value_loss           | 0.289      |\n","----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 946     |\n","|    iterations      | 397     |\n","|    time_elapsed    | 5155    |\n","|    total_timesteps | 4878336 |\n","--------------------------------\n","Eval num_timesteps=4884000, episode_reward=37.41 +/- 0.50\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 37.4        |\n","| time/                   |             |\n","|    total_timesteps      | 4884000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002119491 |\n","|    clip_fraction        | 0.127       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -13.2       |\n","|    explained_variance   | 0.498       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 0.315       |\n","|    n_updates            | 1985        |\n","|    policy_gradient_loss | -0.0028     |\n","|    std                  | 6.68        |\n","|    value_loss           | 6.5         |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 946     |\n","|    iterations      | 398     |\n","|    time_elapsed    | 5169    |\n","|    total_timesteps | 4890624 |\n","--------------------------------\n","Eval num_timesteps=4896000, episode_reward=40.68 +/- 0.65\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 40.7         |\n","| time/                   |              |\n","|    total_timesteps      | 4896000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0026125845 |\n","|    clip_fraction        | 0.0938       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.3        |\n","|    explained_variance   | -0.0775      |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.129       |\n","|    n_updates            | 1990         |\n","|    policy_gradient_loss | -0.00407     |\n","|    std                  | 6.73         |\n","|    value_loss           | 0.253        |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 945     |\n","|    iterations      | 399     |\n","|    time_elapsed    | 5183    |\n","|    total_timesteps | 4902912 |\n","--------------------------------\n","Eval num_timesteps=4908000, episode_reward=35.02 +/- 2.83\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 35           |\n","| time/                   |              |\n","|    total_timesteps      | 4908000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0032276122 |\n","|    clip_fraction        | 0.117        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.3        |\n","|    explained_variance   | 0.645        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.111       |\n","|    n_updates            | 1995         |\n","|    policy_gradient_loss | -0.00363     |\n","|    std                  | 6.79         |\n","|    value_loss           | 4.27         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 946     |\n","|    iterations      | 400     |\n","|    time_elapsed    | 5194    |\n","|    total_timesteps | 4915200 |\n","--------------------------------\n","Eval num_timesteps=4920000, episode_reward=38.30 +/- 2.35\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 38.3         |\n","| time/                   |              |\n","|    total_timesteps      | 4920000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019205431 |\n","|    clip_fraction        | 0.0735       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.3        |\n","|    explained_variance   | 0.282        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.125       |\n","|    n_updates            | 2000         |\n","|    policy_gradient_loss | -0.00403     |\n","|    std                  | 6.79         |\n","|    value_loss           | 0.494        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 945     |\n","|    iterations      | 401     |\n","|    time_elapsed    | 5208    |\n","|    total_timesteps | 4927488 |\n","--------------------------------\n","Eval num_timesteps=4932000, episode_reward=39.37 +/- 2.50\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 39.4        |\n","| time/                   |             |\n","|    total_timesteps      | 4932000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002078817 |\n","|    clip_fraction        | 0.095       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -13.3       |\n","|    explained_variance   | 0.0491      |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.127      |\n","|    n_updates            | 2005        |\n","|    policy_gradient_loss | -0.00419    |\n","|    std                  | 6.79        |\n","|    value_loss           | 0.341       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 946     |\n","|    iterations      | 402     |\n","|    time_elapsed    | 5220    |\n","|    total_timesteps | 4939776 |\n","--------------------------------\n","Eval num_timesteps=4944000, episode_reward=-29.87 +/- 56.23\n","Episode length: 260.00 +/- 195.96\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 260          |\n","|    mean_reward          | -29.9        |\n","| time/                   |              |\n","|    total_timesteps      | 4944000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0016847131 |\n","|    clip_fraction        | 0.0766       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.3        |\n","|    explained_variance   | 0.45         |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.128       |\n","|    n_updates            | 2010         |\n","|    policy_gradient_loss | -0.00364     |\n","|    std                  | 6.82         |\n","|    value_loss           | 0.248        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 946     |\n","|    iterations      | 403     |\n","|    time_elapsed    | 5233    |\n","|    total_timesteps | 4952064 |\n","--------------------------------\n","Eval num_timesteps=4956000, episode_reward=38.86 +/- 0.28\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 38.9        |\n","| time/                   |             |\n","|    total_timesteps      | 4956000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002209983 |\n","|    clip_fraction        | 0.0889      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -13.4       |\n","|    explained_variance   | 0.546       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.132      |\n","|    n_updates            | 2015        |\n","|    policy_gradient_loss | -0.00449    |\n","|    std                  | 6.84        |\n","|    value_loss           | 0.26        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 945     |\n","|    iterations      | 404     |\n","|    time_elapsed    | 5248    |\n","|    total_timesteps | 4964352 |\n","--------------------------------\n","Eval num_timesteps=4968000, episode_reward=33.14 +/- 0.94\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 33.1         |\n","| time/                   |              |\n","|    total_timesteps      | 4968000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021118962 |\n","|    clip_fraction        | 0.101        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.4        |\n","|    explained_variance   | 0.635        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.135       |\n","|    n_updates            | 2020         |\n","|    policy_gradient_loss | -0.00429     |\n","|    std                  | 6.89         |\n","|    value_loss           | 0.188        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 945     |\n","|    iterations      | 405     |\n","|    time_elapsed    | 5260    |\n","|    total_timesteps | 4976640 |\n","--------------------------------\n","Eval num_timesteps=4980000, episode_reward=39.80 +/- 0.38\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 39.8         |\n","| time/                   |              |\n","|    total_timesteps      | 4980000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021857661 |\n","|    clip_fraction        | 0.0966       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.4        |\n","|    explained_variance   | 0.545        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.129       |\n","|    n_updates            | 2025         |\n","|    policy_gradient_loss | -0.00384     |\n","|    std                  | 6.93         |\n","|    value_loss           | 0.217        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 945     |\n","|    iterations      | 406     |\n","|    time_elapsed    | 5277    |\n","|    total_timesteps | 4988928 |\n","--------------------------------\n","Eval num_timesteps=4992000, episode_reward=38.99 +/- 2.33\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 39          |\n","| time/                   |             |\n","|    total_timesteps      | 4992000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002336695 |\n","|    clip_fraction        | 0.109       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -13.4       |\n","|    explained_variance   | 0.509       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 0.368       |\n","|    n_updates            | 2030        |\n","|    policy_gradient_loss | -0.0033     |\n","|    std                  | 6.98        |\n","|    value_loss           | 7.61        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 945     |\n","|    iterations      | 407     |\n","|    time_elapsed    | 5289    |\n","|    total_timesteps | 5001216 |\n","--------------------------------\n"]}],"source":["from stable_baselines3.common.logger import configure\n","\n","tmp_path = \"./multiwalker_ppo_rew_04_log_eval/\"\n","# set up logger\n","new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n","\n","model = PPO(\"MlpPolicy\", env, verbose=3, gamma=0.99, n_steps=512, ent_coef=0.01, learning_rate=0.00062211, vf_coef=0.042202, max_grad_norm=0.9, gae_lambda=0.95, n_epochs=5, clip_range=0.1, batch_size=512)\n","# model.set_logger()\n","model.set_logger(new_logger)\n","model.learn(total_timesteps=5000000,callback=eval_callback)\n","model.save(\"multiwalker_ppo_rew_04\")"]},{"cell_type":"code","source":["from stable_baselines3 import PPO\n","# from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n","from pettingzoo.sisl import multiwalker_v9\n","import supersuit as ss\n","from stable_baselines3.common.monitor import Monitor"],"metadata":{"id":"vD2MPicTHfDs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env = multiwalker_v9.parallel_env(render_mode=\"rgb_array\",shared_reward=0.4)\n","\n","env = ss.frame_stack_v1(env, 3)\n","env = ss.pettingzoo_env_to_vec_env_v1(env)\n","env = ss.concat_vec_envs_v1(env, 8, num_cpus=4, base_class='stable_baselines3')"],"metadata":{"id":"F_V_ANlAHfDt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3.common.callbacks import EvalCallback\n","eval_callback = EvalCallback(env, best_model_save_path=\"./multiwalker_ppo_04_2_log_eval/\",\n","                             log_path=\"./multiwalker_ppo_04_2_log_eval/\", eval_freq=500,\n","                             deterministic=True, render=False)"],"metadata":{"id":"Y6ecNWRdHfDu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3.common.logger import configure\n","\n","tmp_path = \"./multiwalker_ppo_04_2_log_eval/\"\n","# set up logger\n","new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n","\n","model = PPO(\"MlpPolicy\", env, verbose=3, n_steps=512,batch_size=512)\n","# model.set_logger()\n","model.set_logger(new_logger)\n","model.learn(total_timesteps=5000000,callback=eval_callback)\n","model.save(\"multiwalker_ppo_04_2\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698925973801,"user_tz":-60,"elapsed":5709176,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"826314ed-f2d9-4f50-c61b-e87f9ca57275","id":"_mTI5qKDHfDv"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n","|    mean_reward          | 7.06        |\n","| time/                   |             |\n","|    total_timesteps      | 2832000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010968395 |\n","|    clip_fraction        | 0.122       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.04       |\n","|    explained_variance   | 0.722       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0813      |\n","|    n_updates            | 2300        |\n","|    policy_gradient_loss | -0.00906    |\n","|    std                  | 0.52        |\n","|    value_loss           | 0.181       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 889     |\n","|    iterations      | 231     |\n","|    time_elapsed    | 3192    |\n","|    total_timesteps | 2838528 |\n","--------------------------------\n","Eval num_timesteps=2844000, episode_reward=18.67 +/- 2.35\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 18.7        |\n","| time/                   |             |\n","|    total_timesteps      | 2844000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010694501 |\n","|    clip_fraction        | 0.141       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.04       |\n","|    explained_variance   | 0.75        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0889      |\n","|    n_updates            | 2310        |\n","|    policy_gradient_loss | -0.00733    |\n","|    std                  | 0.52        |\n","|    value_loss           | 0.184       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 889     |\n","|    iterations      | 232     |\n","|    time_elapsed    | 3205    |\n","|    total_timesteps | 2850816 |\n","--------------------------------\n","Eval num_timesteps=2856000, episode_reward=21.02 +/- 2.22\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 21          |\n","| time/                   |             |\n","|    total_timesteps      | 2856000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009852433 |\n","|    clip_fraction        | 0.116       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.04       |\n","|    explained_variance   | 0.783       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0938      |\n","|    n_updates            | 2320        |\n","|    policy_gradient_loss | -0.00714    |\n","|    std                  | 0.52        |\n","|    value_loss           | 0.176       |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 889     |\n","|    iterations      | 233     |\n","|    time_elapsed    | 3218    |\n","|    total_timesteps | 2863104 |\n","--------------------------------\n","Eval num_timesteps=2868000, episode_reward=16.53 +/- 10.87\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 16.5         |\n","| time/                   |              |\n","|    total_timesteps      | 2868000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0077494998 |\n","|    clip_fraction        | 0.102        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -3.04        |\n","|    explained_variance   | 0.885        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.0709       |\n","|    n_updates            | 2330         |\n","|    policy_gradient_loss | -0.00746     |\n","|    std                  | 0.519        |\n","|    value_loss           | 0.221        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 889     |\n","|    iterations      | 234     |\n","|    time_elapsed    | 3234    |\n","|    total_timesteps | 2875392 |\n","--------------------------------\n","Eval num_timesteps=2880000, episode_reward=5.24 +/- 0.73\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 5.24        |\n","| time/                   |             |\n","|    total_timesteps      | 2880000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008852356 |\n","|    clip_fraction        | 0.114       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.04       |\n","|    explained_variance   | 0.562       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.324       |\n","|    n_updates            | 2340        |\n","|    policy_gradient_loss | -0.00348    |\n","|    std                  | 0.52        |\n","|    value_loss           | 7.38        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 889     |\n","|    iterations      | 235     |\n","|    time_elapsed    | 3246    |\n","|    total_timesteps | 2887680 |\n","--------------------------------\n","Eval num_timesteps=2892000, episode_reward=6.00 +/- 0.34\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 6           |\n","| time/                   |             |\n","|    total_timesteps      | 2892000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011695706 |\n","|    clip_fraction        | 0.125       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.05       |\n","|    explained_variance   | 0.68        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0924      |\n","|    n_updates            | 2350        |\n","|    policy_gradient_loss | -0.00756    |\n","|    std                  | 0.52        |\n","|    value_loss           | 0.227       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 889     |\n","|    iterations      | 236     |\n","|    time_elapsed    | 3260    |\n","|    total_timesteps | 2899968 |\n","--------------------------------\n","Eval num_timesteps=2904000, episode_reward=10.25 +/- 6.44\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 10.3        |\n","| time/                   |             |\n","|    total_timesteps      | 2904000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010881799 |\n","|    clip_fraction        | 0.127       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.01       |\n","|    explained_variance   | 0.855       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0584      |\n","|    n_updates            | 2360        |\n","|    policy_gradient_loss | -0.0078     |\n","|    std                  | 0.513       |\n","|    value_loss           | 0.147       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 888     |\n","|    iterations      | 237     |\n","|    time_elapsed    | 3276    |\n","|    total_timesteps | 2912256 |\n","--------------------------------\n","Eval num_timesteps=2916000, episode_reward=8.89 +/- 7.14\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 8.89        |\n","| time/                   |             |\n","|    total_timesteps      | 2916000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011296044 |\n","|    clip_fraction        | 0.141       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.98       |\n","|    explained_variance   | 0.848       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0504      |\n","|    n_updates            | 2370        |\n","|    policy_gradient_loss | -0.00853    |\n","|    std                  | 0.511       |\n","|    value_loss           | 0.147       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 889     |\n","|    iterations      | 238     |\n","|    time_elapsed    | 3288    |\n","|    total_timesteps | 2924544 |\n","--------------------------------\n","Eval num_timesteps=2928000, episode_reward=5.36 +/- 0.36\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 5.36       |\n","| time/                   |            |\n","|    total_timesteps      | 2928000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01016995 |\n","|    clip_fraction        | 0.131      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.97      |\n","|    explained_variance   | 0.816      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.0663     |\n","|    n_updates            | 2380       |\n","|    policy_gradient_loss | -0.00904   |\n","|    std                  | 0.511      |\n","|    value_loss           | 0.16       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 889     |\n","|    iterations      | 239     |\n","|    time_elapsed    | 3302    |\n","|    total_timesteps | 2936832 |\n","--------------------------------\n","Eval num_timesteps=2940000, episode_reward=-17.27 +/- 33.25\n","Episode length: 482.00 +/- 22.05\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 482         |\n","|    mean_reward          | -17.3       |\n","| time/                   |             |\n","|    total_timesteps      | 2940000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011522837 |\n","|    clip_fraction        | 0.122       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.97       |\n","|    explained_variance   | 0.754       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.075       |\n","|    n_updates            | 2390        |\n","|    policy_gradient_loss | -0.00726    |\n","|    std                  | 0.51        |\n","|    value_loss           | 0.176       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 888     |\n","|    iterations      | 240     |\n","|    time_elapsed    | 3317    |\n","|    total_timesteps | 2949120 |\n","--------------------------------\n","Eval num_timesteps=2952000, episode_reward=-33.86 +/- 33.00\n","Episode length: 415.40 +/- 69.08\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 415         |\n","|    mean_reward          | -33.9       |\n","| time/                   |             |\n","|    total_timesteps      | 2952000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011280161 |\n","|    clip_fraction        | 0.123       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.97       |\n","|    explained_variance   | 0.779       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0609      |\n","|    n_updates            | 2400        |\n","|    policy_gradient_loss | -0.0107     |\n","|    std                  | 0.511       |\n","|    value_loss           | 0.166       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 889     |\n","|    iterations      | 241     |\n","|    time_elapsed    | 3329    |\n","|    total_timesteps | 2961408 |\n","--------------------------------\n","Eval num_timesteps=2964000, episode_reward=13.10 +/- 9.15\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 13.1        |\n","| time/                   |             |\n","|    total_timesteps      | 2964000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009908602 |\n","|    clip_fraction        | 0.121       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.96       |\n","|    explained_variance   | 0.782       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0588      |\n","|    n_updates            | 2410        |\n","|    policy_gradient_loss | -0.00794    |\n","|    std                  | 0.509       |\n","|    value_loss           | 0.207       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 888     |\n","|    iterations      | 242     |\n","|    time_elapsed    | 3347    |\n","|    total_timesteps | 2973696 |\n","--------------------------------\n","Eval num_timesteps=2976000, episode_reward=9.45 +/- 4.38\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 9.45        |\n","| time/                   |             |\n","|    total_timesteps      | 2976000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010552066 |\n","|    clip_fraction        | 0.137       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.96       |\n","|    explained_variance   | 0.693       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0646      |\n","|    n_updates            | 2420        |\n","|    policy_gradient_loss | -0.0108     |\n","|    std                  | 0.511       |\n","|    value_loss           | 0.221       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 887     |\n","|    iterations      | 243     |\n","|    time_elapsed    | 3364    |\n","|    total_timesteps | 2985984 |\n","--------------------------------\n","Eval num_timesteps=2988000, episode_reward=17.77 +/- 8.04\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 17.8        |\n","| time/                   |             |\n","|    total_timesteps      | 2988000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011372426 |\n","|    clip_fraction        | 0.114       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.98       |\n","|    explained_variance   | 0.683       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0774      |\n","|    n_updates            | 2430        |\n","|    policy_gradient_loss | -0.00948    |\n","|    std                  | 0.513       |\n","|    value_loss           | 0.229       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 888     |\n","|    iterations      | 244     |\n","|    time_elapsed    | 3376    |\n","|    total_timesteps | 2998272 |\n","--------------------------------\n","Eval num_timesteps=3000000, episode_reward=22.59 +/- 4.21\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 22.6        |\n","| time/                   |             |\n","|    total_timesteps      | 3000000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010817293 |\n","|    clip_fraction        | 0.135       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.97       |\n","|    explained_variance   | 0.731       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0915      |\n","|    n_updates            | 2440        |\n","|    policy_gradient_loss | -0.00874    |\n","|    std                  | 0.51        |\n","|    value_loss           | 0.23        |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 887     |\n","|    iterations      | 245     |\n","|    time_elapsed    | 3390    |\n","|    total_timesteps | 3010560 |\n","--------------------------------\n","Eval num_timesteps=3012000, episode_reward=15.19 +/- 11.25\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 15.2        |\n","| time/                   |             |\n","|    total_timesteps      | 3012000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011864646 |\n","|    clip_fraction        | 0.144       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.98       |\n","|    explained_variance   | 0.788       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0652      |\n","|    n_updates            | 2450        |\n","|    policy_gradient_loss | -0.00914    |\n","|    std                  | 0.513       |\n","|    value_loss           | 0.174       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 887     |\n","|    iterations      | 246     |\n","|    time_elapsed    | 3406    |\n","|    total_timesteps | 3022848 |\n","--------------------------------\n","Eval num_timesteps=3024000, episode_reward=23.60 +/- 1.44\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 23.6        |\n","| time/                   |             |\n","|    total_timesteps      | 3024000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009760839 |\n","|    clip_fraction        | 0.121       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.98       |\n","|    explained_variance   | 0.783       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.074       |\n","|    n_updates            | 2460        |\n","|    policy_gradient_loss | -0.00783    |\n","|    std                  | 0.511       |\n","|    value_loss           | 0.18        |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 887     |\n","|    iterations      | 247     |\n","|    time_elapsed    | 3418    |\n","|    total_timesteps | 3035136 |\n","--------------------------------\n","Eval num_timesteps=3036000, episode_reward=14.65 +/- 6.97\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 14.7        |\n","| time/                   |             |\n","|    total_timesteps      | 3036000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010124199 |\n","|    clip_fraction        | 0.123       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.95       |\n","|    explained_variance   | 0.84        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0939      |\n","|    n_updates            | 2470        |\n","|    policy_gradient_loss | -0.00932    |\n","|    std                  | 0.507       |\n","|    value_loss           | 0.175       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 887     |\n","|    iterations      | 248     |\n","|    time_elapsed    | 3432    |\n","|    total_timesteps | 3047424 |\n","--------------------------------\n","Eval num_timesteps=3048000, episode_reward=-20.73 +/- 32.24\n","Episode length: 472.80 +/- 33.31\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 473         |\n","|    mean_reward          | -20.7       |\n","| time/                   |             |\n","|    total_timesteps      | 3048000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009670582 |\n","|    clip_fraction        | 0.114       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.94       |\n","|    explained_variance   | 0.818       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0849      |\n","|    n_updates            | 2480        |\n","|    policy_gradient_loss | -0.00922    |\n","|    std                  | 0.507       |\n","|    value_loss           | 0.207       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 887     |\n","|    iterations      | 249     |\n","|    time_elapsed    | 3448    |\n","|    total_timesteps | 3059712 |\n","--------------------------------\n","Eval num_timesteps=3060000, episode_reward=-14.46 +/- 45.22\n","Episode length: 437.60 +/- 76.42\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 438         |\n","|    mean_reward          | -14.5       |\n","| time/                   |             |\n","|    total_timesteps      | 3060000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012484784 |\n","|    clip_fraction        | 0.149       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.94       |\n","|    explained_variance   | 0.804       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.1         |\n","|    n_updates            | 2490        |\n","|    policy_gradient_loss | -0.00999    |\n","|    std                  | 0.507       |\n","|    value_loss           | 0.191       |\n","-----------------------------------------\n","Eval num_timesteps=3072000, episode_reward=14.06 +/- 8.73\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 14.1     |\n","| time/              |          |\n","|    total_timesteps | 3072000  |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 250     |\n","|    time_elapsed    | 3468    |\n","|    total_timesteps | 3072000 |\n","--------------------------------\n","Eval num_timesteps=3084000, episode_reward=24.38 +/- 0.30\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 24.4        |\n","| time/                   |             |\n","|    total_timesteps      | 3084000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008854077 |\n","|    clip_fraction        | 0.117       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.94       |\n","|    explained_variance   | 0.616       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 8.25        |\n","|    n_updates            | 2500        |\n","|    policy_gradient_loss | -0.00517    |\n","|    std                  | 0.508       |\n","|    value_loss           | 8.68        |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 251     |\n","|    time_elapsed    | 3479    |\n","|    total_timesteps | 3084288 |\n","--------------------------------\n","Eval num_timesteps=3096000, episode_reward=6.77 +/- 0.90\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 6.77        |\n","| time/                   |             |\n","|    total_timesteps      | 3096000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011896557 |\n","|    clip_fraction        | 0.121       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.96       |\n","|    explained_variance   | 0.849       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0645      |\n","|    n_updates            | 2510        |\n","|    policy_gradient_loss | -0.0108     |\n","|    std                  | 0.51        |\n","|    value_loss           | 0.164       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 252     |\n","|    time_elapsed    | 3495    |\n","|    total_timesteps | 3096576 |\n","--------------------------------\n","Eval num_timesteps=3108000, episode_reward=18.92 +/- 5.20\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 18.9         |\n","| time/                   |              |\n","|    total_timesteps      | 3108000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0106566325 |\n","|    clip_fraction        | 0.137        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.94        |\n","|    explained_variance   | 0.848        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.0865       |\n","|    n_updates            | 2520         |\n","|    policy_gradient_loss | -0.00998     |\n","|    std                  | 0.507        |\n","|    value_loss           | 0.17         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 253     |\n","|    time_elapsed    | 3509    |\n","|    total_timesteps | 3108864 |\n","--------------------------------\n","Eval num_timesteps=3120000, episode_reward=6.42 +/- 2.31\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 6.42        |\n","| time/                   |             |\n","|    total_timesteps      | 3120000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012509465 |\n","|    clip_fraction        | 0.124       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.93       |\n","|    explained_variance   | 0.877       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0551      |\n","|    n_updates            | 2530        |\n","|    policy_gradient_loss | -0.00958    |\n","|    std                  | 0.505       |\n","|    value_loss           | 0.16        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 886     |\n","|    iterations      | 254     |\n","|    time_elapsed    | 3521    |\n","|    total_timesteps | 3121152 |\n","--------------------------------\n","Eval num_timesteps=3132000, episode_reward=19.88 +/- 10.80\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 19.9        |\n","| time/                   |             |\n","|    total_timesteps      | 3132000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012818868 |\n","|    clip_fraction        | 0.14        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.92       |\n","|    explained_variance   | 0.746       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0909      |\n","|    n_updates            | 2540        |\n","|    policy_gradient_loss | -0.00912    |\n","|    std                  | 0.506       |\n","|    value_loss           | 0.489       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 255     |\n","|    time_elapsed    | 3537    |\n","|    total_timesteps | 3133440 |\n","--------------------------------\n","Eval num_timesteps=3144000, episode_reward=20.49 +/- 6.98\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 20.5        |\n","| time/                   |             |\n","|    total_timesteps      | 3144000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008987162 |\n","|    clip_fraction        | 0.107       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.92       |\n","|    explained_variance   | 0.581       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 5.95        |\n","|    n_updates            | 2550        |\n","|    policy_gradient_loss | -0.00751    |\n","|    std                  | 0.506       |\n","|    value_loss           | 12.6        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 256     |\n","|    time_elapsed    | 3551    |\n","|    total_timesteps | 3145728 |\n","--------------------------------\n","Eval num_timesteps=3156000, episode_reward=-15.42 +/- 42.34\n","Episode length: 484.40 +/- 19.11\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 484          |\n","|    mean_reward          | -15.4        |\n","| time/                   |              |\n","|    total_timesteps      | 3156000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0064073415 |\n","|    clip_fraction        | 0.05         |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.92        |\n","|    explained_variance   | 0.496        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 35.8         |\n","|    n_updates            | 2560         |\n","|    policy_gradient_loss | -0.0071      |\n","|    std                  | 0.506        |\n","|    value_loss           | 44.7         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 257     |\n","|    time_elapsed    | 3564    |\n","|    total_timesteps | 3158016 |\n","--------------------------------\n","Eval num_timesteps=3168000, episode_reward=-19.94 +/- 47.55\n","Episode length: 317.20 +/- 223.88\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 317         |\n","|    mean_reward          | -19.9       |\n","| time/                   |             |\n","|    total_timesteps      | 3168000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009267018 |\n","|    clip_fraction        | 0.0732      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.93       |\n","|    explained_variance   | 0.465       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 19          |\n","|    n_updates            | 2570        |\n","|    policy_gradient_loss | -0.0076     |\n","|    std                  | 0.506       |\n","|    value_loss           | 23.8        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 258     |\n","|    time_elapsed    | 3580    |\n","|    total_timesteps | 3170304 |\n","--------------------------------\n","Eval num_timesteps=3180000, episode_reward=-33.95 +/- 49.20\n","Episode length: 469.40 +/- 24.98\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 469         |\n","|    mean_reward          | -34         |\n","| time/                   |             |\n","|    total_timesteps      | 3180000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012050644 |\n","|    clip_fraction        | 0.136       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.91       |\n","|    explained_variance   | 0.299       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.106       |\n","|    n_updates            | 2580        |\n","|    policy_gradient_loss | -0.0124     |\n","|    std                  | 0.503       |\n","|    value_loss           | 0.313       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 259     |\n","|    time_elapsed    | 3594    |\n","|    total_timesteps | 3182592 |\n","--------------------------------\n","Eval num_timesteps=3192000, episode_reward=26.27 +/- 2.29\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 26.3        |\n","| time/                   |             |\n","|    total_timesteps      | 3192000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012339026 |\n","|    clip_fraction        | 0.152       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.88       |\n","|    explained_variance   | 0.551       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0703      |\n","|    n_updates            | 2590        |\n","|    policy_gradient_loss | -0.00883    |\n","|    std                  | 0.499       |\n","|    value_loss           | 0.237       |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 260     |\n","|    time_elapsed    | 3611    |\n","|    total_timesteps | 3194880 |\n","--------------------------------\n","Eval num_timesteps=3204000, episode_reward=19.24 +/- 10.78\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 19.2        |\n","| time/                   |             |\n","|    total_timesteps      | 3204000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011258033 |\n","|    clip_fraction        | 0.146       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.87       |\n","|    explained_variance   | 0.727       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0965      |\n","|    n_updates            | 2600        |\n","|    policy_gradient_loss | -0.00954    |\n","|    std                  | 0.499       |\n","|    value_loss           | 0.203       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 261     |\n","|    time_elapsed    | 3626    |\n","|    total_timesteps | 3207168 |\n","--------------------------------\n","Eval num_timesteps=3216000, episode_reward=21.70 +/- 2.16\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 21.7        |\n","| time/                   |             |\n","|    total_timesteps      | 3216000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010599419 |\n","|    clip_fraction        | 0.124       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.87       |\n","|    explained_variance   | 0.65        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 11.6        |\n","|    n_updates            | 2610        |\n","|    policy_gradient_loss | -0.00589    |\n","|    std                  | 0.5         |\n","|    value_loss           | 9.13        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 262     |\n","|    time_elapsed    | 3638    |\n","|    total_timesteps | 3219456 |\n","--------------------------------\n","Eval num_timesteps=3228000, episode_reward=-43.50 +/- 38.13\n","Episode length: 457.40 +/- 34.78\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 457         |\n","|    mean_reward          | -43.5       |\n","| time/                   |             |\n","|    total_timesteps      | 3228000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009318815 |\n","|    clip_fraction        | 0.128       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.88       |\n","|    explained_variance   | 0.663       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 5.7         |\n","|    n_updates            | 2620        |\n","|    policy_gradient_loss | -0.0077     |\n","|    std                  | 0.501       |\n","|    value_loss           | 16.4        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 263     |\n","|    time_elapsed    | 3653    |\n","|    total_timesteps | 3231744 |\n","--------------------------------\n","Eval num_timesteps=3240000, episode_reward=25.87 +/- 3.31\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 25.9        |\n","| time/                   |             |\n","|    total_timesteps      | 3240000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010076662 |\n","|    clip_fraction        | 0.108       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.88       |\n","|    explained_variance   | 0.748       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.15        |\n","|    n_updates            | 2630        |\n","|    policy_gradient_loss | -0.00746    |\n","|    std                  | 0.499       |\n","|    value_loss           | 9.4         |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 264     |\n","|    time_elapsed    | 3667    |\n","|    total_timesteps | 3244032 |\n","--------------------------------\n","Eval num_timesteps=3252000, episode_reward=-20.34 +/- 32.36\n","Episode length: 459.20 +/- 33.31\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 459         |\n","|    mean_reward          | -20.3       |\n","| time/                   |             |\n","|    total_timesteps      | 3252000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009202903 |\n","|    clip_fraction        | 0.111       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.88       |\n","|    explained_variance   | 0.556       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.81        |\n","|    n_updates            | 2640        |\n","|    policy_gradient_loss | -0.00716    |\n","|    std                  | 0.499       |\n","|    value_loss           | 17.2        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 265     |\n","|    time_elapsed    | 3679    |\n","|    total_timesteps | 3256320 |\n","--------------------------------\n","Eval num_timesteps=3264000, episode_reward=22.32 +/- 6.80\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 22.3        |\n","| time/                   |             |\n","|    total_timesteps      | 3264000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013598922 |\n","|    clip_fraction        | 0.146       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.86       |\n","|    explained_variance   | 0.243       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.129       |\n","|    n_updates            | 2650        |\n","|    policy_gradient_loss | -0.0102     |\n","|    std                  | 0.495       |\n","|    value_loss           | 0.325       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 266     |\n","|    time_elapsed    | 3694    |\n","|    total_timesteps | 3268608 |\n","--------------------------------\n","Eval num_timesteps=3276000, episode_reward=11.69 +/- 5.61\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 11.7        |\n","| time/                   |             |\n","|    total_timesteps      | 3276000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012730229 |\n","|    clip_fraction        | 0.158       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.83       |\n","|    explained_variance   | 0.636       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0958      |\n","|    n_updates            | 2660        |\n","|    policy_gradient_loss | -0.0104     |\n","|    std                  | 0.493       |\n","|    value_loss           | 0.254       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 267     |\n","|    time_elapsed    | 3708    |\n","|    total_timesteps | 3280896 |\n","--------------------------------\n","Eval num_timesteps=3288000, episode_reward=-70.03 +/- 6.35\n","Episode length: 460.60 +/- 15.19\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 461         |\n","|    mean_reward          | -70         |\n","| time/                   |             |\n","|    total_timesteps      | 3288000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013544465 |\n","|    clip_fraction        | 0.158       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.81       |\n","|    explained_variance   | 0.743       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0647      |\n","|    n_updates            | 2670        |\n","|    policy_gradient_loss | -0.00863    |\n","|    std                  | 0.49        |\n","|    value_loss           | 0.189       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 268     |\n","|    time_elapsed    | 3720    |\n","|    total_timesteps | 3293184 |\n","--------------------------------\n","Eval num_timesteps=3300000, episode_reward=-18.87 +/- 48.74\n","Episode length: 487.20 +/- 15.68\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 487         |\n","|    mean_reward          | -18.9       |\n","| time/                   |             |\n","|    total_timesteps      | 3300000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013827998 |\n","|    clip_fraction        | 0.157       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.79       |\n","|    explained_variance   | 0.669       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.234       |\n","|    n_updates            | 2680        |\n","|    policy_gradient_loss | -0.0108     |\n","|    std                  | 0.489       |\n","|    value_loss           | 0.404       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 269     |\n","|    time_elapsed    | 3736    |\n","|    total_timesteps | 3305472 |\n","--------------------------------\n","Eval num_timesteps=3312000, episode_reward=-15.84 +/- 51.04\n","Episode length: 440.00 +/- 73.48\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 440         |\n","|    mean_reward          | -15.8       |\n","| time/                   |             |\n","|    total_timesteps      | 3312000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011503416 |\n","|    clip_fraction        | 0.146       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.8        |\n","|    explained_variance   | 0.65        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.21        |\n","|    n_updates            | 2690        |\n","|    policy_gradient_loss | -0.00773    |\n","|    std                  | 0.491       |\n","|    value_loss           | 9.17        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 270     |\n","|    time_elapsed    | 3749    |\n","|    total_timesteps | 3317760 |\n","--------------------------------\n","Eval num_timesteps=3324000, episode_reward=13.00 +/- 6.51\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 13          |\n","| time/                   |             |\n","|    total_timesteps      | 3324000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009122752 |\n","|    clip_fraction        | 0.0891      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.81       |\n","|    explained_variance   | 0.59        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.21        |\n","|    n_updates            | 2700        |\n","|    policy_gradient_loss | -0.0076     |\n","|    std                  | 0.493       |\n","|    value_loss           | 17.2        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 271     |\n","|    time_elapsed    | 3761    |\n","|    total_timesteps | 3330048 |\n","--------------------------------\n","Eval num_timesteps=3336000, episode_reward=15.77 +/- 12.21\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 15.8        |\n","| time/                   |             |\n","|    total_timesteps      | 3336000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014840239 |\n","|    clip_fraction        | 0.174       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.83       |\n","|    explained_variance   | 0.579       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 6.23        |\n","|    n_updates            | 2710        |\n","|    policy_gradient_loss | -0.00527    |\n","|    std                  | 0.494       |\n","|    value_loss           | 8.73        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 272     |\n","|    time_elapsed    | 3777    |\n","|    total_timesteps | 3342336 |\n","--------------------------------\n","Eval num_timesteps=3348000, episode_reward=-32.67 +/- 48.70\n","Episode length: 389.60 +/- 90.14\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 390         |\n","|    mean_reward          | -32.7       |\n","| time/                   |             |\n","|    total_timesteps      | 3348000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008536678 |\n","|    clip_fraction        | 0.121       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.83       |\n","|    explained_variance   | 0.643       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.791       |\n","|    n_updates            | 2720        |\n","|    policy_gradient_loss | -0.008      |\n","|    std                  | 0.495       |\n","|    value_loss           | 11          |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 273     |\n","|    time_elapsed    | 3789    |\n","|    total_timesteps | 3354624 |\n","--------------------------------\n","Eval num_timesteps=3360000, episode_reward=27.86 +/- 1.01\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 27.9       |\n","| time/                   |            |\n","|    total_timesteps      | 3360000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01090492 |\n","|    clip_fraction        | 0.163      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.83      |\n","|    explained_variance   | 0.696      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 7.01       |\n","|    n_updates            | 2730       |\n","|    policy_gradient_loss | -0.00693   |\n","|    std                  | 0.494      |\n","|    value_loss           | 8.59       |\n","----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 274     |\n","|    time_elapsed    | 3803    |\n","|    total_timesteps | 3366912 |\n","--------------------------------\n","Eval num_timesteps=3372000, episode_reward=-31.21 +/- 46.84\n","Episode length: 476.60 +/- 19.11\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 477         |\n","|    mean_reward          | -31.2       |\n","| time/                   |             |\n","|    total_timesteps      | 3372000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015898146 |\n","|    clip_fraction        | 0.142       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.82       |\n","|    explained_variance   | 0.735       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.78        |\n","|    n_updates            | 2740        |\n","|    policy_gradient_loss | -0.00602    |\n","|    std                  | 0.493       |\n","|    value_loss           | 8.41        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 275     |\n","|    time_elapsed    | 3819    |\n","|    total_timesteps | 3379200 |\n","--------------------------------\n","Eval num_timesteps=3384000, episode_reward=-37.49 +/- 40.85\n","Episode length: 378.20 +/- 99.45\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 378          |\n","|    mean_reward          | -37.5        |\n","| time/                   |              |\n","|    total_timesteps      | 3384000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0078185415 |\n","|    clip_fraction        | 0.103        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.82        |\n","|    explained_variance   | 0.533        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 8.07         |\n","|    n_updates            | 2750         |\n","|    policy_gradient_loss | -0.00649     |\n","|    std                  | 0.494        |\n","|    value_loss           | 14.4         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 885     |\n","|    iterations      | 276     |\n","|    time_elapsed    | 3830    |\n","|    total_timesteps | 3391488 |\n","--------------------------------\n","Eval num_timesteps=3396000, episode_reward=22.05 +/- 4.88\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 22.1       |\n","| time/                   |            |\n","|    total_timesteps      | 3396000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01603498 |\n","|    clip_fraction        | 0.175      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.82      |\n","|    explained_variance   | 0.659      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.464      |\n","|    n_updates            | 2760       |\n","|    policy_gradient_loss | -0.0069    |\n","|    std                  | 0.491      |\n","|    value_loss           | 8.88       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 884     |\n","|    iterations      | 277     |\n","|    time_elapsed    | 3848    |\n","|    total_timesteps | 3403776 |\n","--------------------------------\n","Eval num_timesteps=3408000, episode_reward=-27.96 +/- 38.66\n","Episode length: 439.40 +/- 49.48\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 439         |\n","|    mean_reward          | -28         |\n","| time/                   |             |\n","|    total_timesteps      | 3408000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014107965 |\n","|    clip_fraction        | 0.186       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.78       |\n","|    explained_variance   | 0.63        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0938      |\n","|    n_updates            | 2770        |\n","|    policy_gradient_loss | -0.0109     |\n","|    std                  | 0.485       |\n","|    value_loss           | 0.303       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 883     |\n","|    iterations      | 278     |\n","|    time_elapsed    | 3865    |\n","|    total_timesteps | 3416064 |\n","--------------------------------\n","Eval num_timesteps=3420000, episode_reward=21.79 +/- 0.08\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 21.8        |\n","| time/                   |             |\n","|    total_timesteps      | 3420000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015182257 |\n","|    clip_fraction        | 0.187       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.77       |\n","|    explained_variance   | 0.785       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.128       |\n","|    n_updates            | 2780        |\n","|    policy_gradient_loss | -0.00928    |\n","|    std                  | 0.486       |\n","|    value_loss           | 0.243       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 883     |\n","|    iterations      | 279     |\n","|    time_elapsed    | 3879    |\n","|    total_timesteps | 3428352 |\n","--------------------------------\n","Eval num_timesteps=3432000, episode_reward=21.28 +/- 6.91\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 21.3        |\n","| time/                   |             |\n","|    total_timesteps      | 3432000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014191466 |\n","|    clip_fraction        | 0.153       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.75       |\n","|    explained_variance   | 0.842       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0564      |\n","|    n_updates            | 2790        |\n","|    policy_gradient_loss | -0.00907    |\n","|    std                  | 0.483       |\n","|    value_loss           | 0.186       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 883     |\n","|    iterations      | 280     |\n","|    time_elapsed    | 3892    |\n","|    total_timesteps | 3440640 |\n","--------------------------------\n","Eval num_timesteps=3444000, episode_reward=20.16 +/- 6.81\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 20.2        |\n","| time/                   |             |\n","|    total_timesteps      | 3444000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013254385 |\n","|    clip_fraction        | 0.166       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.73       |\n","|    explained_variance   | 0.854       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0789      |\n","|    n_updates            | 2800        |\n","|    policy_gradient_loss | -0.0111     |\n","|    std                  | 0.48        |\n","|    value_loss           | 0.198       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 883     |\n","|    iterations      | 281     |\n","|    time_elapsed    | 3908    |\n","|    total_timesteps | 3452928 |\n","--------------------------------\n","Eval num_timesteps=3456000, episode_reward=-32.37 +/- 47.55\n","Episode length: 486.80 +/- 10.78\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 487         |\n","|    mean_reward          | -32.4       |\n","| time/                   |             |\n","|    total_timesteps      | 3456000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009690691 |\n","|    clip_fraction        | 0.136       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.7        |\n","|    explained_variance   | 0.8         |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0354      |\n","|    n_updates            | 2810        |\n","|    policy_gradient_loss | -0.0098     |\n","|    std                  | 0.478       |\n","|    value_loss           | 0.172       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 883     |\n","|    iterations      | 282     |\n","|    time_elapsed    | 3921    |\n","|    total_timesteps | 3465216 |\n","--------------------------------\n","Eval num_timesteps=3468000, episode_reward=21.93 +/- 1.27\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 21.9        |\n","| time/                   |             |\n","|    total_timesteps      | 3468000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009162332 |\n","|    clip_fraction        | 0.118       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.71       |\n","|    explained_variance   | 0.377       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 7.55        |\n","|    n_updates            | 2820        |\n","|    policy_gradient_loss | -0.00837    |\n","|    std                  | 0.479       |\n","|    value_loss           | 8.33        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 883     |\n","|    iterations      | 283     |\n","|    time_elapsed    | 3934    |\n","|    total_timesteps | 3477504 |\n","--------------------------------\n","Eval num_timesteps=3480000, episode_reward=-17.63 +/- 36.24\n","Episode length: 482.80 +/- 21.07\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 483         |\n","|    mean_reward          | -17.6       |\n","| time/                   |             |\n","|    total_timesteps      | 3480000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009257435 |\n","|    clip_fraction        | 0.0897      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.71       |\n","|    explained_variance   | 0.716       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.337       |\n","|    n_updates            | 2830        |\n","|    policy_gradient_loss | -0.00855    |\n","|    std                  | 0.479       |\n","|    value_loss           | 2.12        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 883     |\n","|    iterations      | 284     |\n","|    time_elapsed    | 3950    |\n","|    total_timesteps | 3489792 |\n","--------------------------------\n","Eval num_timesteps=3492000, episode_reward=23.81 +/- 2.38\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 23.8        |\n","| time/                   |             |\n","|    total_timesteps      | 3492000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008087723 |\n","|    clip_fraction        | 0.0689      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.71       |\n","|    explained_variance   | 0.536       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 6.42        |\n","|    n_updates            | 2840        |\n","|    policy_gradient_loss | -0.00478    |\n","|    std                  | 0.48        |\n","|    value_loss           | 16.6        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 883     |\n","|    iterations      | 285     |\n","|    time_elapsed    | 3963    |\n","|    total_timesteps | 3502080 |\n","--------------------------------\n","Eval num_timesteps=3504000, episode_reward=-6.98 +/- 40.80\n","Episode length: 476.80 +/- 28.41\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 477         |\n","|    mean_reward          | -6.98       |\n","| time/                   |             |\n","|    total_timesteps      | 3504000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009879212 |\n","|    clip_fraction        | 0.0861      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.71       |\n","|    explained_variance   | 0.685       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.394       |\n","|    n_updates            | 2850        |\n","|    policy_gradient_loss | -0.00629    |\n","|    std                  | 0.479       |\n","|    value_loss           | 3.3         |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 883     |\n","|    iterations      | 286     |\n","|    time_elapsed    | 3977    |\n","|    total_timesteps | 3514368 |\n","--------------------------------\n","Eval num_timesteps=3516000, episode_reward=14.84 +/- 6.89\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 14.8        |\n","| time/                   |             |\n","|    total_timesteps      | 3516000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008874359 |\n","|    clip_fraction        | 0.0834      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.71       |\n","|    explained_variance   | 0.748       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.22        |\n","|    n_updates            | 2860        |\n","|    policy_gradient_loss | -0.00647    |\n","|    std                  | 0.479       |\n","|    value_loss           | 8.91        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 883     |\n","|    iterations      | 287     |\n","|    time_elapsed    | 3993    |\n","|    total_timesteps | 3526656 |\n","--------------------------------\n","Eval num_timesteps=3528000, episode_reward=16.11 +/- 10.98\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 16.1         |\n","| time/                   |              |\n","|    total_timesteps      | 3528000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0122234775 |\n","|    clip_fraction        | 0.164        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.71        |\n","|    explained_variance   | 0.611        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.131        |\n","|    n_updates            | 2870         |\n","|    policy_gradient_loss | -0.00678     |\n","|    std                  | 0.479        |\n","|    value_loss           | 0.442        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 883     |\n","|    iterations      | 288     |\n","|    time_elapsed    | 4007    |\n","|    total_timesteps | 3538944 |\n","--------------------------------\n","Eval num_timesteps=3540000, episode_reward=24.80 +/- 1.42\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 24.8        |\n","| time/                   |             |\n","|    total_timesteps      | 3540000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010768571 |\n","|    clip_fraction        | 0.154       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.71       |\n","|    explained_variance   | 0.77        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.52        |\n","|    n_updates            | 2880        |\n","|    policy_gradient_loss | -0.00547    |\n","|    std                  | 0.479       |\n","|    value_loss           | 7.36        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 883     |\n","|    iterations      | 289     |\n","|    time_elapsed    | 4020    |\n","|    total_timesteps | 3551232 |\n","--------------------------------\n","Eval num_timesteps=3552000, episode_reward=14.83 +/- 4.06\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 14.8        |\n","| time/                   |             |\n","|    total_timesteps      | 3552000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014796726 |\n","|    clip_fraction        | 0.152       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.71       |\n","|    explained_variance   | 0.707       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.191       |\n","|    n_updates            | 2890        |\n","|    policy_gradient_loss | -0.00619    |\n","|    std                  | 0.479       |\n","|    value_loss           | 0.739       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 882     |\n","|    iterations      | 290     |\n","|    time_elapsed    | 4036    |\n","|    total_timesteps | 3563520 |\n","--------------------------------\n","Eval num_timesteps=3564000, episode_reward=-11.74 +/- 39.42\n","Episode length: 489.20 +/- 13.23\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 489        |\n","|    mean_reward          | -11.7      |\n","| time/                   |            |\n","|    total_timesteps      | 3564000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01406946 |\n","|    clip_fraction        | 0.165      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.68      |\n","|    explained_variance   | 0.876      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.0758     |\n","|    n_updates            | 2900       |\n","|    policy_gradient_loss | -0.00816   |\n","|    std                  | 0.474      |\n","|    value_loss           | 0.249      |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 883     |\n","|    iterations      | 291     |\n","|    time_elapsed    | 4049    |\n","|    total_timesteps | 3575808 |\n","--------------------------------\n","Eval num_timesteps=3576000, episode_reward=27.83 +/- 1.03\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 27.8        |\n","| time/                   |             |\n","|    total_timesteps      | 3576000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008576375 |\n","|    clip_fraction        | 0.0938      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.67       |\n","|    explained_variance   | 0.814       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.37        |\n","|    n_updates            | 2910        |\n","|    policy_gradient_loss | -0.00454    |\n","|    std                  | 0.473       |\n","|    value_loss           | 6.06        |\n","-----------------------------------------\n","Eval num_timesteps=3588000, episode_reward=21.92 +/- 5.70\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 21.9     |\n","| time/              |          |\n","|    total_timesteps | 3588000  |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 881     |\n","|    iterations      | 292     |\n","|    time_elapsed    | 4070    |\n","|    total_timesteps | 3588096 |\n","--------------------------------\n","Eval num_timesteps=3600000, episode_reward=11.15 +/- 4.36\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 11.2        |\n","| time/                   |             |\n","|    total_timesteps      | 3600000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010103739 |\n","|    clip_fraction        | 0.123       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.65       |\n","|    explained_variance   | 0.727       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.24        |\n","|    n_updates            | 2920        |\n","|    policy_gradient_loss | -0.00786    |\n","|    std                  | 0.471       |\n","|    value_loss           | 3.12        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 881     |\n","|    iterations      | 293     |\n","|    time_elapsed    | 4084    |\n","|    total_timesteps | 3600384 |\n","--------------------------------\n","Eval num_timesteps=3612000, episode_reward=-31.48 +/- 48.98\n","Episode length: 499.40 +/- 0.49\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 499         |\n","|    mean_reward          | -31.5       |\n","| time/                   |             |\n","|    total_timesteps      | 3612000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012818431 |\n","|    clip_fraction        | 0.165       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.64       |\n","|    explained_variance   | 0.562       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 6.04        |\n","|    n_updates            | 2930        |\n","|    policy_gradient_loss | -0.00864    |\n","|    std                  | 0.47        |\n","|    value_loss           | 9.74        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 880     |\n","|    iterations      | 294     |\n","|    time_elapsed    | 4104    |\n","|    total_timesteps | 3612672 |\n","--------------------------------\n","Eval num_timesteps=3624000, episode_reward=27.34 +/- 0.09\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 27.3        |\n","| time/                   |             |\n","|    total_timesteps      | 3624000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011923355 |\n","|    clip_fraction        | 0.15        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.63       |\n","|    explained_variance   | 0.648       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.43        |\n","|    n_updates            | 2940        |\n","|    policy_gradient_loss | -0.00814    |\n","|    std                  | 0.469       |\n","|    value_loss           | 8.63        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 879     |\n","|    iterations      | 295     |\n","|    time_elapsed    | 4119    |\n","|    total_timesteps | 3624960 |\n","--------------------------------\n","Eval num_timesteps=3636000, episode_reward=21.73 +/- 2.23\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 21.7       |\n","| time/                   |            |\n","|    total_timesteps      | 3636000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01404861 |\n","|    clip_fraction        | 0.178      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.63      |\n","|    explained_variance   | 0.681      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.143      |\n","|    n_updates            | 2950       |\n","|    policy_gradient_loss | -0.00962   |\n","|    std                  | 0.468      |\n","|    value_loss           | 0.481      |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 880     |\n","|    iterations      | 296     |\n","|    time_elapsed    | 4132    |\n","|    total_timesteps | 3637248 |\n","--------------------------------\n","Eval num_timesteps=3648000, episode_reward=17.61 +/- 11.15\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 17.6        |\n","| time/                   |             |\n","|    total_timesteps      | 3648000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011811239 |\n","|    clip_fraction        | 0.18        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.63       |\n","|    explained_variance   | 0.451       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 8.56        |\n","|    n_updates            | 2960        |\n","|    policy_gradient_loss | -0.00964    |\n","|    std                  | 0.471       |\n","|    value_loss           | 9.51        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 879     |\n","|    iterations      | 297     |\n","|    time_elapsed    | 4148    |\n","|    total_timesteps | 3649536 |\n","--------------------------------\n","Eval num_timesteps=3660000, episode_reward=-32.23 +/- 45.83\n","Episode length: 483.20 +/- 13.72\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 483         |\n","|    mean_reward          | -32.2       |\n","| time/                   |             |\n","|    total_timesteps      | 3660000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014865697 |\n","|    clip_fraction        | 0.172       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.64       |\n","|    explained_variance   | 0.756       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0595      |\n","|    n_updates            | 2970        |\n","|    policy_gradient_loss | -0.00963    |\n","|    std                  | 0.469       |\n","|    value_loss           | 0.189       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 879     |\n","|    iterations      | 298     |\n","|    time_elapsed    | 4161    |\n","|    total_timesteps | 3661824 |\n","--------------------------------\n","Eval num_timesteps=3672000, episode_reward=-35.19 +/- 41.01\n","Episode length: 464.00 +/- 29.39\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 464         |\n","|    mean_reward          | -35.2       |\n","| time/                   |             |\n","|    total_timesteps      | 3672000     |\n","| train/                  |             |\n","|    approx_kl            | 0.016499503 |\n","|    clip_fraction        | 0.173       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.61       |\n","|    explained_variance   | 0.904       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0621      |\n","|    n_updates            | 2980        |\n","|    policy_gradient_loss | -0.01       |\n","|    std                  | 0.466       |\n","|    value_loss           | 0.138       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 880     |\n","|    iterations      | 299     |\n","|    time_elapsed    | 4174    |\n","|    total_timesteps | 3674112 |\n","--------------------------------\n","Eval num_timesteps=3684000, episode_reward=26.86 +/- 0.34\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 26.9        |\n","| time/                   |             |\n","|    total_timesteps      | 3684000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013400483 |\n","|    clip_fraction        | 0.158       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.58       |\n","|    explained_variance   | 0.82        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.107       |\n","|    n_updates            | 2990        |\n","|    policy_gradient_loss | -0.0103     |\n","|    std                  | 0.463       |\n","|    value_loss           | 0.171       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 879     |\n","|    iterations      | 300     |\n","|    time_elapsed    | 4190    |\n","|    total_timesteps | 3686400 |\n","--------------------------------\n","Eval num_timesteps=3696000, episode_reward=15.64 +/- 10.64\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 15.6        |\n","| time/                   |             |\n","|    total_timesteps      | 3696000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013474715 |\n","|    clip_fraction        | 0.163       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.59       |\n","|    explained_variance   | 0.768       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0758      |\n","|    n_updates            | 3000        |\n","|    policy_gradient_loss | -0.00953    |\n","|    std                  | 0.466       |\n","|    value_loss           | 0.21        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 879     |\n","|    iterations      | 301     |\n","|    time_elapsed    | 4204    |\n","|    total_timesteps | 3698688 |\n","--------------------------------\n","Eval num_timesteps=3708000, episode_reward=20.34 +/- 5.52\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 20.3        |\n","| time/                   |             |\n","|    total_timesteps      | 3708000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011040307 |\n","|    clip_fraction        | 0.141       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.58       |\n","|    explained_variance   | 0.914       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0314      |\n","|    n_updates            | 3010        |\n","|    policy_gradient_loss | -0.00966    |\n","|    std                  | 0.463       |\n","|    value_loss           | 0.109       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 879     |\n","|    iterations      | 302     |\n","|    time_elapsed    | 4217    |\n","|    total_timesteps | 3710976 |\n","--------------------------------\n","Eval num_timesteps=3720000, episode_reward=27.62 +/- 0.20\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 27.6        |\n","| time/                   |             |\n","|    total_timesteps      | 3720000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012202676 |\n","|    clip_fraction        | 0.135       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.57       |\n","|    explained_variance   | 0.708       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.486       |\n","|    n_updates            | 3020        |\n","|    policy_gradient_loss | -0.00604    |\n","|    std                  | 0.463       |\n","|    value_loss           | 5.42        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 879     |\n","|    iterations      | 303     |\n","|    time_elapsed    | 4233    |\n","|    total_timesteps | 3723264 |\n","--------------------------------\n","Eval num_timesteps=3732000, episode_reward=24.40 +/- 8.72\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 24.4        |\n","| time/                   |             |\n","|    total_timesteps      | 3732000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011633505 |\n","|    clip_fraction        | 0.143       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.57       |\n","|    explained_variance   | 0.743       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0936      |\n","|    n_updates            | 3030        |\n","|    policy_gradient_loss | -0.00756    |\n","|    std                  | 0.462       |\n","|    value_loss           | 0.238       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 879     |\n","|    iterations      | 304     |\n","|    time_elapsed    | 4246    |\n","|    total_timesteps | 3735552 |\n","--------------------------------\n","Eval num_timesteps=3744000, episode_reward=12.86 +/- 10.44\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 12.9        |\n","| time/                   |             |\n","|    total_timesteps      | 3744000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011972583 |\n","|    clip_fraction        | 0.153       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.57       |\n","|    explained_variance   | 0.821       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.106       |\n","|    n_updates            | 3040        |\n","|    policy_gradient_loss | -0.00866    |\n","|    std                  | 0.462       |\n","|    value_loss           | 0.272       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 879     |\n","|    iterations      | 305     |\n","|    time_elapsed    | 4259    |\n","|    total_timesteps | 3747840 |\n","--------------------------------\n","Eval num_timesteps=3756000, episode_reward=26.65 +/- 3.06\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 26.6        |\n","| time/                   |             |\n","|    total_timesteps      | 3756000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012578794 |\n","|    clip_fraction        | 0.175       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.57       |\n","|    explained_variance   | 0.668       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 11.8        |\n","|    n_updates            | 3050        |\n","|    policy_gradient_loss | -0.00689    |\n","|    std                  | 0.463       |\n","|    value_loss           | 8.27        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 879     |\n","|    iterations      | 306     |\n","|    time_elapsed    | 4275    |\n","|    total_timesteps | 3760128 |\n","--------------------------------\n","Eval num_timesteps=3768000, episode_reward=30.63 +/- 0.48\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 30.6        |\n","| time/                   |             |\n","|    total_timesteps      | 3768000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011555058 |\n","|    clip_fraction        | 0.139       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.55       |\n","|    explained_variance   | 0.797       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0616      |\n","|    n_updates            | 3060        |\n","|    policy_gradient_loss | -0.00924    |\n","|    std                  | 0.459       |\n","|    value_loss           | 0.212       |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 879     |\n","|    iterations      | 307     |\n","|    time_elapsed    | 4288    |\n","|    total_timesteps | 3772416 |\n","--------------------------------\n","Eval num_timesteps=3780000, episode_reward=28.60 +/- 0.95\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 28.6       |\n","| time/                   |            |\n","|    total_timesteps      | 3780000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01451996 |\n","|    clip_fraction        | 0.144      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.53      |\n","|    explained_variance   | 0.432      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.0768     |\n","|    n_updates            | 3070       |\n","|    policy_gradient_loss | -0.0075    |\n","|    std                  | 0.458      |\n","|    value_loss           | 0.63       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 879     |\n","|    iterations      | 308     |\n","|    time_elapsed    | 4301    |\n","|    total_timesteps | 3784704 |\n","--------------------------------\n","Eval num_timesteps=3792000, episode_reward=22.25 +/- 6.78\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 22.2        |\n","| time/                   |             |\n","|    total_timesteps      | 3792000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011197808 |\n","|    clip_fraction        | 0.131       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.53       |\n","|    explained_variance   | 0.393       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.51        |\n","|    n_updates            | 3080        |\n","|    policy_gradient_loss | -0.0069     |\n","|    std                  | 0.458       |\n","|    value_loss           | 9.61        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 879     |\n","|    iterations      | 309     |\n","|    time_elapsed    | 4317    |\n","|    total_timesteps | 3796992 |\n","--------------------------------\n","Eval num_timesteps=3804000, episode_reward=15.36 +/- 0.83\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 15.4         |\n","| time/                   |              |\n","|    total_timesteps      | 3804000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0066982694 |\n","|    clip_fraction        | 0.0642       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.53        |\n","|    explained_variance   | 0.675        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 3.1          |\n","|    n_updates            | 3090         |\n","|    policy_gradient_loss | -0.00601     |\n","|    std                  | 0.458        |\n","|    value_loss           | 5.57         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 879     |\n","|    iterations      | 310     |\n","|    time_elapsed    | 4329    |\n","|    total_timesteps | 3809280 |\n","--------------------------------\n","Eval num_timesteps=3816000, episode_reward=23.13 +/- 10.04\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 23.1        |\n","| time/                   |             |\n","|    total_timesteps      | 3816000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012813956 |\n","|    clip_fraction        | 0.168       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.52       |\n","|    explained_variance   | 0.833       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0625      |\n","|    n_updates            | 3100        |\n","|    policy_gradient_loss | -0.0106     |\n","|    std                  | 0.455       |\n","|    value_loss           | 0.26        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 879     |\n","|    iterations      | 311     |\n","|    time_elapsed    | 4345    |\n","|    total_timesteps | 3821568 |\n","--------------------------------\n","Eval num_timesteps=3828000, episode_reward=-34.26 +/- 44.42\n","Episode length: 483.80 +/- 13.23\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 484         |\n","|    mean_reward          | -34.3       |\n","| time/                   |             |\n","|    total_timesteps      | 3828000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008255922 |\n","|    clip_fraction        | 0.0878      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.51       |\n","|    explained_variance   | 0.783       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.49        |\n","|    n_updates            | 3110        |\n","|    policy_gradient_loss | -0.00693    |\n","|    std                  | 0.455       |\n","|    value_loss           | 5.86        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 878     |\n","|    iterations      | 312     |\n","|    time_elapsed    | 4363    |\n","|    total_timesteps | 3833856 |\n","--------------------------------\n","Eval num_timesteps=3840000, episode_reward=-24.23 +/- 44.24\n","Episode length: 468.20 +/- 25.96\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 468         |\n","|    mean_reward          | -24.2       |\n","| time/                   |             |\n","|    total_timesteps      | 3840000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012150761 |\n","|    clip_fraction        | 0.148       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.51       |\n","|    explained_variance   | 0.576       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.96        |\n","|    n_updates            | 3120        |\n","|    policy_gradient_loss | -0.00701    |\n","|    std                  | 0.455       |\n","|    value_loss           | 9.62        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 878     |\n","|    iterations      | 313     |\n","|    time_elapsed    | 4376    |\n","|    total_timesteps | 3846144 |\n","--------------------------------\n","Eval num_timesteps=3852000, episode_reward=28.90 +/- 10.14\n","Episode length: 500.00 +/- 0.00\n","---------------------------------------\n","| eval/                   |           |\n","|    mean_ep_length       | 500       |\n","|    mean_reward          | 28.9      |\n","| time/                   |           |\n","|    total_timesteps      | 3852000   |\n","| train/                  |           |\n","|    approx_kl            | 0.0110787 |\n","|    clip_fraction        | 0.136     |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -2.51     |\n","|    explained_variance   | 0.639     |\n","|    learning_rate        | 0.0003    |\n","|    loss                 | 11.5      |\n","|    n_updates            | 3130      |\n","|    policy_gradient_loss | -0.00677  |\n","|    std                  | 0.456     |\n","|    value_loss           | 16.8      |\n","---------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 878     |\n","|    iterations      | 314     |\n","|    time_elapsed    | 4390    |\n","|    total_timesteps | 3858432 |\n","--------------------------------\n","Eval num_timesteps=3864000, episode_reward=-27.78 +/- 42.11\n","Episode length: 435.20 +/- 52.91\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 435        |\n","|    mean_reward          | -27.8      |\n","| time/                   |            |\n","|    total_timesteps      | 3864000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01236625 |\n","|    clip_fraction        | 0.129      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.52      |\n","|    explained_variance   | 0.719      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 9.75       |\n","|    n_updates            | 3140       |\n","|    policy_gradient_loss | -0.00832   |\n","|    std                  | 0.456      |\n","|    value_loss           | 14.2       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 878     |\n","|    iterations      | 315     |\n","|    time_elapsed    | 4406    |\n","|    total_timesteps | 3870720 |\n","--------------------------------\n","Eval num_timesteps=3876000, episode_reward=28.12 +/- 3.59\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 28.1         |\n","| time/                   |              |\n","|    total_timesteps      | 3876000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0127477795 |\n","|    clip_fraction        | 0.147        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.51        |\n","|    explained_variance   | 0.758        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.164        |\n","|    n_updates            | 3150         |\n","|    policy_gradient_loss | -0.00732     |\n","|    std                  | 0.454        |\n","|    value_loss           | 0.421        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 878     |\n","|    iterations      | 316     |\n","|    time_elapsed    | 4418    |\n","|    total_timesteps | 3883008 |\n","--------------------------------\n","Eval num_timesteps=3888000, episode_reward=27.15 +/- 0.61\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 27.1        |\n","| time/                   |             |\n","|    total_timesteps      | 3888000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010925267 |\n","|    clip_fraction        | 0.146       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.5        |\n","|    explained_variance   | 0.686       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.685       |\n","|    n_updates            | 3160        |\n","|    policy_gradient_loss | -0.00705    |\n","|    std                  | 0.455       |\n","|    value_loss           | 8.87        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 878     |\n","|    iterations      | 317     |\n","|    time_elapsed    | 4432    |\n","|    total_timesteps | 3895296 |\n","--------------------------------\n","Eval num_timesteps=3900000, episode_reward=22.39 +/- 0.43\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 22.4        |\n","| time/                   |             |\n","|    total_timesteps      | 3900000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012755963 |\n","|    clip_fraction        | 0.159       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.48       |\n","|    explained_variance   | 0.839       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0541      |\n","|    n_updates            | 3170        |\n","|    policy_gradient_loss | -0.00978    |\n","|    std                  | 0.45        |\n","|    value_loss           | 0.199       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 878     |\n","|    iterations      | 318     |\n","|    time_elapsed    | 4448    |\n","|    total_timesteps | 3907584 |\n","--------------------------------\n","Eval num_timesteps=3912000, episode_reward=21.28 +/- 5.88\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 21.3        |\n","| time/                   |             |\n","|    total_timesteps      | 3912000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009375971 |\n","|    clip_fraction        | 0.154       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.46       |\n","|    explained_variance   | 0.551       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.13        |\n","|    n_updates            | 3180        |\n","|    policy_gradient_loss | -0.0076     |\n","|    std                  | 0.449       |\n","|    value_loss           | 7.44        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 878     |\n","|    iterations      | 319     |\n","|    time_elapsed    | 4459    |\n","|    total_timesteps | 3919872 |\n","--------------------------------\n","Eval num_timesteps=3924000, episode_reward=27.05 +/- 0.37\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 27          |\n","| time/                   |             |\n","|    total_timesteps      | 3924000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011053175 |\n","|    clip_fraction        | 0.115       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.45       |\n","|    explained_variance   | 0.743       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.447       |\n","|    n_updates            | 3190        |\n","|    policy_gradient_loss | -0.00744    |\n","|    std                  | 0.45        |\n","|    value_loss           | 2.63        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 878     |\n","|    iterations      | 320     |\n","|    time_elapsed    | 4474    |\n","|    total_timesteps | 3932160 |\n","--------------------------------\n","Eval num_timesteps=3936000, episode_reward=14.01 +/- 0.18\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 14          |\n","| time/                   |             |\n","|    total_timesteps      | 3936000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015054802 |\n","|    clip_fraction        | 0.187       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.42       |\n","|    explained_variance   | 0.327       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.211       |\n","|    n_updates            | 3200        |\n","|    policy_gradient_loss | -0.00969    |\n","|    std                  | 0.444       |\n","|    value_loss           | 0.428       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 878     |\n","|    iterations      | 321     |\n","|    time_elapsed    | 4489    |\n","|    total_timesteps | 3944448 |\n","--------------------------------\n","Eval num_timesteps=3948000, episode_reward=30.74 +/- 0.43\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 30.7        |\n","| time/                   |             |\n","|    total_timesteps      | 3948000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012436579 |\n","|    clip_fraction        | 0.153       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.41       |\n","|    explained_variance   | 0.928       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.339       |\n","|    n_updates            | 3210        |\n","|    policy_gradient_loss | -0.00804    |\n","|    std                  | 0.446       |\n","|    value_loss           | 0.469       |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 879     |\n","|    iterations      | 322     |\n","|    time_elapsed    | 4501    |\n","|    total_timesteps | 3956736 |\n","--------------------------------\n","Eval num_timesteps=3960000, episode_reward=26.78 +/- 2.50\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 26.8        |\n","| time/                   |             |\n","|    total_timesteps      | 3960000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008092351 |\n","|    clip_fraction        | 0.108       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.42       |\n","|    explained_variance   | 0.732       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.14        |\n","|    n_updates            | 3220        |\n","|    policy_gradient_loss | -0.00479    |\n","|    std                  | 0.446       |\n","|    value_loss           | 11.2        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 878     |\n","|    iterations      | 323     |\n","|    time_elapsed    | 4516    |\n","|    total_timesteps | 3969024 |\n","--------------------------------\n","Eval num_timesteps=3972000, episode_reward=26.72 +/- 2.94\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 26.7        |\n","| time/                   |             |\n","|    total_timesteps      | 3972000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015312071 |\n","|    clip_fraction        | 0.184       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.4        |\n","|    explained_variance   | 0.499       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.152       |\n","|    n_updates            | 3230        |\n","|    policy_gradient_loss | -0.00724    |\n","|    std                  | 0.443       |\n","|    value_loss           | 0.362       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 878     |\n","|    iterations      | 324     |\n","|    time_elapsed    | 4530    |\n","|    total_timesteps | 3981312 |\n","--------------------------------\n","Eval num_timesteps=3984000, episode_reward=19.93 +/- 7.05\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 19.9        |\n","| time/                   |             |\n","|    total_timesteps      | 3984000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014840347 |\n","|    clip_fraction        | 0.172       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.37       |\n","|    explained_variance   | 0.764       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0853      |\n","|    n_updates            | 3240        |\n","|    policy_gradient_loss | -0.0115     |\n","|    std                  | 0.439       |\n","|    value_loss           | 0.191       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 879     |\n","|    iterations      | 325     |\n","|    time_elapsed    | 4542    |\n","|    total_timesteps | 3993600 |\n","--------------------------------\n","Eval num_timesteps=3996000, episode_reward=12.84 +/- 0.10\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 12.8        |\n","| time/                   |             |\n","|    total_timesteps      | 3996000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011563276 |\n","|    clip_fraction        | 0.146       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.35       |\n","|    explained_variance   | 0.902       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.149       |\n","|    n_updates            | 3250        |\n","|    policy_gradient_loss | -0.0072     |\n","|    std                  | 0.438       |\n","|    value_loss           | 0.435       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 878     |\n","|    iterations      | 326     |\n","|    time_elapsed    | 4558    |\n","|    total_timesteps | 4005888 |\n","--------------------------------\n","Eval num_timesteps=4008000, episode_reward=19.00 +/- 8.42\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 19          |\n","| time/                   |             |\n","|    total_timesteps      | 4008000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013901915 |\n","|    clip_fraction        | 0.17        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.34       |\n","|    explained_variance   | 0.521       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.243       |\n","|    n_updates            | 3260        |\n","|    policy_gradient_loss | -0.00524    |\n","|    std                  | 0.436       |\n","|    value_loss           | 0.973       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 878     |\n","|    iterations      | 327     |\n","|    time_elapsed    | 4572    |\n","|    total_timesteps | 4018176 |\n","--------------------------------\n","Eval num_timesteps=4020000, episode_reward=26.17 +/- 5.33\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 26.2        |\n","| time/                   |             |\n","|    total_timesteps      | 4020000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014870827 |\n","|    clip_fraction        | 0.16        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.34       |\n","|    explained_variance   | 0.868       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.08        |\n","|    n_updates            | 3270        |\n","|    policy_gradient_loss | -0.00961    |\n","|    std                  | 0.436       |\n","|    value_loss           | 0.211       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 879     |\n","|    iterations      | 328     |\n","|    time_elapsed    | 4584    |\n","|    total_timesteps | 4030464 |\n","--------------------------------\n","Eval num_timesteps=4032000, episode_reward=21.95 +/- 4.31\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 21.9        |\n","| time/                   |             |\n","|    total_timesteps      | 4032000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014884855 |\n","|    clip_fraction        | 0.177       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.33       |\n","|    explained_variance   | 0.902       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.101       |\n","|    n_updates            | 3280        |\n","|    policy_gradient_loss | -0.00797    |\n","|    std                  | 0.435       |\n","|    value_loss           | 0.398       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 878     |\n","|    iterations      | 329     |\n","|    time_elapsed    | 4603    |\n","|    total_timesteps | 4042752 |\n","--------------------------------\n","Eval num_timesteps=4044000, episode_reward=28.23 +/- 0.59\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 28.2        |\n","| time/                   |             |\n","|    total_timesteps      | 4044000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014922879 |\n","|    clip_fraction        | 0.17        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.33       |\n","|    explained_variance   | 0.852       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0848      |\n","|    n_updates            | 3290        |\n","|    policy_gradient_loss | -0.00627    |\n","|    std                  | 0.435       |\n","|    value_loss           | 0.26        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 878     |\n","|    iterations      | 330     |\n","|    time_elapsed    | 4617    |\n","|    total_timesteps | 4055040 |\n","--------------------------------\n","Eval num_timesteps=4056000, episode_reward=-30.02 +/- 44.42\n","Episode length: 341.60 +/- 129.33\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 342         |\n","|    mean_reward          | -30         |\n","| time/                   |             |\n","|    total_timesteps      | 4056000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012807573 |\n","|    clip_fraction        | 0.156       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.31       |\n","|    explained_variance   | 0.917       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0818      |\n","|    n_updates            | 3300        |\n","|    policy_gradient_loss | -0.00989    |\n","|    std                  | 0.432       |\n","|    value_loss           | 0.147       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 878     |\n","|    iterations      | 331     |\n","|    time_elapsed    | 4630    |\n","|    total_timesteps | 4067328 |\n","--------------------------------\n","Eval num_timesteps=4068000, episode_reward=25.23 +/- 3.58\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 25.2        |\n","| time/                   |             |\n","|    total_timesteps      | 4068000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014849581 |\n","|    clip_fraction        | 0.167       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.29       |\n","|    explained_variance   | 0.872       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0851      |\n","|    n_updates            | 3310        |\n","|    policy_gradient_loss | -0.0122     |\n","|    std                  | 0.43        |\n","|    value_loss           | 0.189       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 878     |\n","|    iterations      | 332     |\n","|    time_elapsed    | 4646    |\n","|    total_timesteps | 4079616 |\n","--------------------------------\n","Eval num_timesteps=4080000, episode_reward=17.39 +/- 9.39\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 17.4         |\n","| time/                   |              |\n","|    total_timesteps      | 4080000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0085809985 |\n","|    clip_fraction        | 0.128        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.28        |\n","|    explained_variance   | 0.469        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 1.35         |\n","|    n_updates            | 3320         |\n","|    policy_gradient_loss | -0.00839     |\n","|    std                  | 0.431        |\n","|    value_loss           | 8.75         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 878     |\n","|    iterations      | 333     |\n","|    time_elapsed    | 4658    |\n","|    total_timesteps | 4091904 |\n","--------------------------------\n","Eval num_timesteps=4092000, episode_reward=19.22 +/- 5.88\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 19.2        |\n","| time/                   |             |\n","|    total_timesteps      | 4092000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013575796 |\n","|    clip_fraction        | 0.158       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.29       |\n","|    explained_variance   | 0.713       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0841      |\n","|    n_updates            | 3330        |\n","|    policy_gradient_loss | -0.011      |\n","|    std                  | 0.43        |\n","|    value_loss           | 0.324       |\n","-----------------------------------------\n","Eval num_timesteps=4104000, episode_reward=21.44 +/- 6.12\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 21.4     |\n","| time/              |          |\n","|    total_timesteps | 4104000  |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 334     |\n","|    time_elapsed    | 4678    |\n","|    total_timesteps | 4104192 |\n","--------------------------------\n","Eval num_timesteps=4116000, episode_reward=-34.41 +/- 40.53\n","Episode length: 452.60 +/- 38.70\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 453         |\n","|    mean_reward          | -34.4       |\n","| time/                   |             |\n","|    total_timesteps      | 4116000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012326497 |\n","|    clip_fraction        | 0.133       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.28       |\n","|    explained_variance   | 0.678       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.27        |\n","|    n_updates            | 3340        |\n","|    policy_gradient_loss | -0.00603    |\n","|    std                  | 0.431       |\n","|    value_loss           | 5.73        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 335     |\n","|    time_elapsed    | 4692    |\n","|    total_timesteps | 4116480 |\n","--------------------------------\n","Eval num_timesteps=4128000, episode_reward=27.70 +/- 2.75\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 27.7        |\n","| time/                   |             |\n","|    total_timesteps      | 4128000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009217926 |\n","|    clip_fraction        | 0.122       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.29       |\n","|    explained_variance   | 0.516       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.63        |\n","|    n_updates            | 3350        |\n","|    policy_gradient_loss | -0.00684    |\n","|    std                  | 0.43        |\n","|    value_loss           | 9.19        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 336     |\n","|    time_elapsed    | 4707    |\n","|    total_timesteps | 4128768 |\n","--------------------------------\n","Eval num_timesteps=4140000, episode_reward=23.81 +/- 1.22\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 23.8        |\n","| time/                   |             |\n","|    total_timesteps      | 4140000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014223382 |\n","|    clip_fraction        | 0.162       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.28       |\n","|    explained_variance   | 0.515       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.179       |\n","|    n_updates            | 3360        |\n","|    policy_gradient_loss | -0.0113     |\n","|    std                  | 0.429       |\n","|    value_loss           | 0.488       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 337     |\n","|    time_elapsed    | 4719    |\n","|    total_timesteps | 4141056 |\n","--------------------------------\n","Eval num_timesteps=4152000, episode_reward=-23.99 +/- 45.27\n","Episode length: 450.80 +/- 40.17\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 451         |\n","|    mean_reward          | -24         |\n","| time/                   |             |\n","|    total_timesteps      | 4152000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014090746 |\n","|    clip_fraction        | 0.18        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.25       |\n","|    explained_variance   | 0.754       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.132       |\n","|    n_updates            | 3370        |\n","|    policy_gradient_loss | -0.00937    |\n","|    std                  | 0.426       |\n","|    value_loss           | 0.239       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 338     |\n","|    time_elapsed    | 4733    |\n","|    total_timesteps | 4153344 |\n","--------------------------------\n","Eval num_timesteps=4164000, episode_reward=30.63 +/- 1.43\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 30.6        |\n","| time/                   |             |\n","|    total_timesteps      | 4164000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010330594 |\n","|    clip_fraction        | 0.161       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.25       |\n","|    explained_variance   | 0.722       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 6.38        |\n","|    n_updates            | 3380        |\n","|    policy_gradient_loss | -0.00568    |\n","|    std                  | 0.428       |\n","|    value_loss           | 8.02        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 339     |\n","|    time_elapsed    | 4748    |\n","|    total_timesteps | 4165632 |\n","--------------------------------\n","Eval num_timesteps=4176000, episode_reward=25.42 +/- 8.17\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 25.4        |\n","| time/                   |             |\n","|    total_timesteps      | 4176000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012505048 |\n","|    clip_fraction        | 0.155       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.24       |\n","|    explained_variance   | 0.886       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.226       |\n","|    n_updates            | 3390        |\n","|    policy_gradient_loss | -0.00881    |\n","|    std                  | 0.424       |\n","|    value_loss           | 0.641       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 340     |\n","|    time_elapsed    | 4760    |\n","|    total_timesteps | 4177920 |\n","--------------------------------\n","Eval num_timesteps=4188000, episode_reward=27.96 +/- 2.19\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 28          |\n","| time/                   |             |\n","|    total_timesteps      | 4188000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014073747 |\n","|    clip_fraction        | 0.183       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.22       |\n","|    explained_variance   | 0.9         |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0761      |\n","|    n_updates            | 3400        |\n","|    policy_gradient_loss | -0.00977    |\n","|    std                  | 0.423       |\n","|    value_loss           | 0.21        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 341     |\n","|    time_elapsed    | 4775    |\n","|    total_timesteps | 4190208 |\n","--------------------------------\n","Eval num_timesteps=4200000, episode_reward=26.39 +/- 3.37\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 26.4        |\n","| time/                   |             |\n","|    total_timesteps      | 4200000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015751673 |\n","|    clip_fraction        | 0.155       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.19       |\n","|    explained_variance   | 0.93        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0917      |\n","|    n_updates            | 3410        |\n","|    policy_gradient_loss | -0.00405    |\n","|    std                  | 0.421       |\n","|    value_loss           | 0.466       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 342     |\n","|    time_elapsed    | 4789    |\n","|    total_timesteps | 4202496 |\n","--------------------------------\n","Eval num_timesteps=4212000, episode_reward=30.17 +/- 1.67\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 30.2       |\n","| time/                   |            |\n","|    total_timesteps      | 4212000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01258394 |\n","|    clip_fraction        | 0.166      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.16      |\n","|    explained_variance   | 0.9        |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.0592     |\n","|    n_updates            | 3420       |\n","|    policy_gradient_loss | -0.00934   |\n","|    std                  | 0.417      |\n","|    value_loss           | 0.196      |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 343     |\n","|    time_elapsed    | 4801    |\n","|    total_timesteps | 4214784 |\n","--------------------------------\n","Eval num_timesteps=4224000, episode_reward=29.73 +/- 0.83\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 29.7        |\n","| time/                   |             |\n","|    total_timesteps      | 4224000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014640623 |\n","|    clip_fraction        | 0.178       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.15       |\n","|    explained_variance   | 0.888       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0723      |\n","|    n_updates            | 3430        |\n","|    policy_gradient_loss | -0.011      |\n","|    std                  | 0.416       |\n","|    value_loss           | 0.205       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 344     |\n","|    time_elapsed    | 4817    |\n","|    total_timesteps | 4227072 |\n","--------------------------------\n","Eval num_timesteps=4236000, episode_reward=30.39 +/- 2.54\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 30.4        |\n","| time/                   |             |\n","|    total_timesteps      | 4236000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014186946 |\n","|    clip_fraction        | 0.164       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.15       |\n","|    explained_variance   | 0.92        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0323      |\n","|    n_updates            | 3440        |\n","|    policy_gradient_loss | -0.0115     |\n","|    std                  | 0.417       |\n","|    value_loss           | 0.147       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 345     |\n","|    time_elapsed    | 4830    |\n","|    total_timesteps | 4239360 |\n","--------------------------------\n","Eval num_timesteps=4248000, episode_reward=19.76 +/- 8.34\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 19.8        |\n","| time/                   |             |\n","|    total_timesteps      | 4248000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013477948 |\n","|    clip_fraction        | 0.162       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.16       |\n","|    explained_variance   | 0.852       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0795      |\n","|    n_updates            | 3450        |\n","|    policy_gradient_loss | -0.0119     |\n","|    std                  | 0.419       |\n","|    value_loss           | 0.245       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 346     |\n","|    time_elapsed    | 4844    |\n","|    total_timesteps | 4251648 |\n","--------------------------------\n","Eval num_timesteps=4260000, episode_reward=-19.37 +/- 33.44\n","Episode length: 494.40 +/- 6.86\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 494         |\n","|    mean_reward          | -19.4       |\n","| time/                   |             |\n","|    total_timesteps      | 4260000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009056366 |\n","|    clip_fraction        | 0.107       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.17       |\n","|    explained_variance   | 0.381       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.3         |\n","|    n_updates            | 3460        |\n","|    policy_gradient_loss | -0.00394    |\n","|    std                  | 0.419       |\n","|    value_loss           | 8.64        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 876     |\n","|    iterations      | 347     |\n","|    time_elapsed    | 4862    |\n","|    total_timesteps | 4263936 |\n","--------------------------------\n","Eval num_timesteps=4272000, episode_reward=25.88 +/- 1.41\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 25.9        |\n","| time/                   |             |\n","|    total_timesteps      | 4272000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014578051 |\n","|    clip_fraction        | 0.158       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.15       |\n","|    explained_variance   | 0.665       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.101       |\n","|    n_updates            | 3470        |\n","|    policy_gradient_loss | -0.00981    |\n","|    std                  | 0.415       |\n","|    value_loss           | 0.372       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 348     |\n","|    time_elapsed    | 4874    |\n","|    total_timesteps | 4276224 |\n","--------------------------------\n","Eval num_timesteps=4284000, episode_reward=23.89 +/- 5.90\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 23.9        |\n","| time/                   |             |\n","|    total_timesteps      | 4284000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014237856 |\n","|    clip_fraction        | 0.183       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.12       |\n","|    explained_variance   | 0.87        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0868      |\n","|    n_updates            | 3480        |\n","|    policy_gradient_loss | -0.0122     |\n","|    std                  | 0.414       |\n","|    value_loss           | 0.254       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 349     |\n","|    time_elapsed    | 4888    |\n","|    total_timesteps | 4288512 |\n","--------------------------------\n","Eval num_timesteps=4296000, episode_reward=-6.58 +/- 43.68\n","Episode length: 466.00 +/- 41.64\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 466        |\n","|    mean_reward          | -6.58      |\n","| time/                   |            |\n","|    total_timesteps      | 4296000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01241253 |\n","|    clip_fraction        | 0.16       |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.1       |\n","|    explained_variance   | 0.924      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.184      |\n","|    n_updates            | 3490       |\n","|    policy_gradient_loss | -0.00731   |\n","|    std                  | 0.411      |\n","|    value_loss           | 0.3        |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 350     |\n","|    time_elapsed    | 4903    |\n","|    total_timesteps | 4300800 |\n","--------------------------------\n","Eval num_timesteps=4308000, episode_reward=28.80 +/- 1.28\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 28.8         |\n","| time/                   |              |\n","|    total_timesteps      | 4308000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0133157065 |\n","|    clip_fraction        | 0.156        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.1         |\n","|    explained_variance   | 0.84         |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.0891       |\n","|    n_updates            | 3500         |\n","|    policy_gradient_loss | -0.0104      |\n","|    std                  | 0.412        |\n","|    value_loss           | 0.23         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 351     |\n","|    time_elapsed    | 4914    |\n","|    total_timesteps | 4313088 |\n","--------------------------------\n","Eval num_timesteps=4320000, episode_reward=-25.43 +/- 44.52\n","Episode length: 449.00 +/- 41.64\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 449         |\n","|    mean_reward          | -25.4       |\n","| time/                   |             |\n","|    total_timesteps      | 4320000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014760799 |\n","|    clip_fraction        | 0.161       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.11       |\n","|    explained_variance   | 0.895       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.103       |\n","|    n_updates            | 3510        |\n","|    policy_gradient_loss | -0.0112     |\n","|    std                  | 0.413       |\n","|    value_loss           | 0.197       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 352     |\n","|    time_elapsed    | 4929    |\n","|    total_timesteps | 4325376 |\n","--------------------------------\n","Eval num_timesteps=4332000, episode_reward=33.47 +/- 2.73\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 33.5        |\n","| time/                   |             |\n","|    total_timesteps      | 4332000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013798416 |\n","|    clip_fraction        | 0.162       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.12       |\n","|    explained_variance   | 0.853       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0892      |\n","|    n_updates            | 3520        |\n","|    policy_gradient_loss | -0.0102     |\n","|    std                  | 0.415       |\n","|    value_loss           | 0.232       |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 353     |\n","|    time_elapsed    | 4944    |\n","|    total_timesteps | 4337664 |\n","--------------------------------\n","Eval num_timesteps=4344000, episode_reward=31.92 +/- 0.63\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 31.9        |\n","| time/                   |             |\n","|    total_timesteps      | 4344000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012815104 |\n","|    clip_fraction        | 0.163       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.12       |\n","|    explained_variance   | 0.938       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0607      |\n","|    n_updates            | 3530        |\n","|    policy_gradient_loss | -0.0138     |\n","|    std                  | 0.414       |\n","|    value_loss           | 0.165       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 354     |\n","|    time_elapsed    | 4956    |\n","|    total_timesteps | 4349952 |\n","--------------------------------\n","Eval num_timesteps=4356000, episode_reward=31.16 +/- 0.47\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 31.2        |\n","| time/                   |             |\n","|    total_timesteps      | 4356000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013856542 |\n","|    clip_fraction        | 0.152       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.12       |\n","|    explained_variance   | 0.908       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0821      |\n","|    n_updates            | 3540        |\n","|    policy_gradient_loss | -0.0119     |\n","|    std                  | 0.414       |\n","|    value_loss           | 0.193       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 355     |\n","|    time_elapsed    | 4971    |\n","|    total_timesteps | 4362240 |\n","--------------------------------\n","Eval num_timesteps=4368000, episode_reward=-7.55 +/- 49.85\n","Episode length: 471.60 +/- 34.78\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 472          |\n","|    mean_reward          | -7.55        |\n","| time/                   |              |\n","|    total_timesteps      | 4368000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0116892755 |\n","|    clip_fraction        | 0.156        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.12        |\n","|    explained_variance   | 0.912        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.124        |\n","|    n_updates            | 3550         |\n","|    policy_gradient_loss | -0.0108      |\n","|    std                  | 0.415        |\n","|    value_loss           | 0.238        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 356     |\n","|    time_elapsed    | 4985    |\n","|    total_timesteps | 4374528 |\n","--------------------------------\n","Eval num_timesteps=4380000, episode_reward=35.78 +/- 1.98\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 35.8        |\n","| time/                   |             |\n","|    total_timesteps      | 4380000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013207977 |\n","|    clip_fraction        | 0.147       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.12       |\n","|    explained_variance   | 0.923       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0971      |\n","|    n_updates            | 3560        |\n","|    policy_gradient_loss | -0.00938    |\n","|    std                  | 0.415       |\n","|    value_loss           | 0.191       |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 357     |\n","|    time_elapsed    | 4997    |\n","|    total_timesteps | 4386816 |\n","--------------------------------\n","Eval num_timesteps=4392000, episode_reward=32.90 +/- 0.16\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 32.9        |\n","| time/                   |             |\n","|    total_timesteps      | 4392000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015610829 |\n","|    clip_fraction        | 0.158       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.1        |\n","|    explained_variance   | 0.933       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0733      |\n","|    n_updates            | 3570        |\n","|    policy_gradient_loss | -0.0118     |\n","|    std                  | 0.41        |\n","|    value_loss           | 0.162       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 358     |\n","|    time_elapsed    | 5013    |\n","|    total_timesteps | 4399104 |\n","--------------------------------\n","Eval num_timesteps=4404000, episode_reward=34.19 +/- 0.81\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 34.2        |\n","| time/                   |             |\n","|    total_timesteps      | 4404000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011149523 |\n","|    clip_fraction        | 0.118       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.08       |\n","|    explained_variance   | 0.205       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.05        |\n","|    n_updates            | 3580        |\n","|    policy_gradient_loss | -0.00684    |\n","|    std                  | 0.411       |\n","|    value_loss           | 9.36        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 359     |\n","|    time_elapsed    | 5026    |\n","|    total_timesteps | 4411392 |\n","--------------------------------\n","Eval num_timesteps=4416000, episode_reward=27.51 +/- 3.07\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 27.5        |\n","| time/                   |             |\n","|    total_timesteps      | 4416000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012833255 |\n","|    clip_fraction        | 0.157       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.06       |\n","|    explained_variance   | 0.816       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.134       |\n","|    n_updates            | 3590        |\n","|    policy_gradient_loss | -0.012      |\n","|    std                  | 0.408       |\n","|    value_loss           | 0.271       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 360     |\n","|    time_elapsed    | 5039    |\n","|    total_timesteps | 4423680 |\n","--------------------------------\n","Eval num_timesteps=4428000, episode_reward=33.03 +/- 1.68\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 33         |\n","| time/                   |            |\n","|    total_timesteps      | 4428000    |\n","| train/                  |            |\n","|    approx_kl            | 0.00928715 |\n","|    clip_fraction        | 0.14       |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.05      |\n","|    explained_variance   | 0.613      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 3.86       |\n","|    n_updates            | 3600       |\n","|    policy_gradient_loss | -0.00755   |\n","|    std                  | 0.407      |\n","|    value_loss           | 8.38       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 361     |\n","|    time_elapsed    | 5055    |\n","|    total_timesteps | 4435968 |\n","--------------------------------\n","Eval num_timesteps=4440000, episode_reward=-14.81 +/- 38.64\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | -14.8       |\n","| time/                   |             |\n","|    total_timesteps      | 4440000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012349107 |\n","|    clip_fraction        | 0.142       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.04       |\n","|    explained_variance   | 0.874       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0926      |\n","|    n_updates            | 3610        |\n","|    policy_gradient_loss | -0.0113     |\n","|    std                  | 0.405       |\n","|    value_loss           | 0.327       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 362     |\n","|    time_elapsed    | 5067    |\n","|    total_timesteps | 4448256 |\n","--------------------------------\n","Eval num_timesteps=4452000, episode_reward=29.67 +/- 3.41\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 29.7         |\n","| time/                   |              |\n","|    total_timesteps      | 4452000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0084547475 |\n","|    clip_fraction        | 0.0867       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.03        |\n","|    explained_variance   | 0.329        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 1.83         |\n","|    n_updates            | 3620         |\n","|    policy_gradient_loss | -0.00631     |\n","|    std                  | 0.406        |\n","|    value_loss           | 7.49         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 363     |\n","|    time_elapsed    | 5080    |\n","|    total_timesteps | 4460544 |\n","--------------------------------\n","Eval num_timesteps=4464000, episode_reward=-8.76 +/- 53.59\n","Episode length: 347.20 +/- 187.14\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 347         |\n","|    mean_reward          | -8.76       |\n","| time/                   |             |\n","|    total_timesteps      | 4464000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013888764 |\n","|    clip_fraction        | 0.171       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.03       |\n","|    explained_variance   | 0.822       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.136       |\n","|    n_updates            | 3630        |\n","|    policy_gradient_loss | -0.0111     |\n","|    std                  | 0.405       |\n","|    value_loss           | 0.286       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 364     |\n","|    time_elapsed    | 5097    |\n","|    total_timesteps | 4472832 |\n","--------------------------------\n","Eval num_timesteps=4476000, episode_reward=26.83 +/- 10.51\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 26.8        |\n","| time/                   |             |\n","|    total_timesteps      | 4476000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015877651 |\n","|    clip_fraction        | 0.173       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.02       |\n","|    explained_variance   | 0.334       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.165       |\n","|    n_updates            | 3640        |\n","|    policy_gradient_loss | -0.00987    |\n","|    std                  | 0.402       |\n","|    value_loss           | 0.639       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 365     |\n","|    time_elapsed    | 5114    |\n","|    total_timesteps | 4485120 |\n","--------------------------------\n","Eval num_timesteps=4488000, episode_reward=34.73 +/- 1.42\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 34.7        |\n","| time/                   |             |\n","|    total_timesteps      | 4488000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011188212 |\n","|    clip_fraction        | 0.15        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2          |\n","|    explained_variance   | 0.339       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.68        |\n","|    n_updates            | 3650        |\n","|    policy_gradient_loss | -0.00836    |\n","|    std                  | 0.402       |\n","|    value_loss           | 12.9        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 366     |\n","|    time_elapsed    | 5127    |\n","|    total_timesteps | 4497408 |\n","--------------------------------\n","Eval num_timesteps=4500000, episode_reward=34.95 +/- 2.63\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 35          |\n","| time/                   |             |\n","|    total_timesteps      | 4500000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011102024 |\n","|    clip_fraction        | 0.111       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2          |\n","|    explained_variance   | 0.606       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 6.38        |\n","|    n_updates            | 3660        |\n","|    policy_gradient_loss | -0.00955    |\n","|    std                  | 0.401       |\n","|    value_loss           | 13.9        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 876     |\n","|    iterations      | 367     |\n","|    time_elapsed    | 5142    |\n","|    total_timesteps | 4509696 |\n","--------------------------------\n","Eval num_timesteps=4512000, episode_reward=29.76 +/- 1.96\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 29.8        |\n","| time/                   |             |\n","|    total_timesteps      | 4512000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010118057 |\n","|    clip_fraction        | 0.117       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.99       |\n","|    explained_variance   | 0.79        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.21        |\n","|    n_updates            | 3670        |\n","|    policy_gradient_loss | -0.00716    |\n","|    std                  | 0.4         |\n","|    value_loss           | 2.98        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 368     |\n","|    time_elapsed    | 5154    |\n","|    total_timesteps | 4521984 |\n","--------------------------------\n","Eval num_timesteps=4524000, episode_reward=30.16 +/- 1.42\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 30.2       |\n","| time/                   |            |\n","|    total_timesteps      | 4524000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01652053 |\n","|    clip_fraction        | 0.197      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.98      |\n","|    explained_variance   | 0.816      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.0781     |\n","|    n_updates            | 3680       |\n","|    policy_gradient_loss | -0.0109    |\n","|    std                  | 0.4        |\n","|    value_loss           | 0.253      |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 369     |\n","|    time_elapsed    | 5168    |\n","|    total_timesteps | 4534272 |\n","--------------------------------\n","Eval num_timesteps=4536000, episode_reward=-23.32 +/- 47.51\n","Episode length: 429.80 +/- 57.32\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 430         |\n","|    mean_reward          | -23.3       |\n","| time/                   |             |\n","|    total_timesteps      | 4536000     |\n","| train/                  |             |\n","|    approx_kl            | 0.016130157 |\n","|    clip_fraction        | 0.184       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.98       |\n","|    explained_variance   | 0.865       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.102       |\n","|    n_updates            | 3690        |\n","|    policy_gradient_loss | -0.00951    |\n","|    std                  | 0.399       |\n","|    value_loss           | 0.236       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 370     |\n","|    time_elapsed    | 5184    |\n","|    total_timesteps | 4546560 |\n","--------------------------------\n","Eval num_timesteps=4548000, episode_reward=29.96 +/- 11.08\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 30         |\n","| time/                   |            |\n","|    total_timesteps      | 4548000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01274684 |\n","|    clip_fraction        | 0.187      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.98      |\n","|    explained_variance   | 0.363      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 4.36       |\n","|    n_updates            | 3700       |\n","|    policy_gradient_loss | -0.00891   |\n","|    std                  | 0.399      |\n","|    value_loss           | 10.1       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 371     |\n","|    time_elapsed    | 5195    |\n","|    total_timesteps | 4558848 |\n","--------------------------------\n","Eval num_timesteps=4560000, episode_reward=-3.17 +/- 47.65\n","Episode length: 422.00 +/- 95.53\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 422         |\n","|    mean_reward          | -3.17       |\n","| time/                   |             |\n","|    total_timesteps      | 4560000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010106298 |\n","|    clip_fraction        | 0.126       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.98       |\n","|    explained_variance   | 0.271       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.98        |\n","|    n_updates            | 3710        |\n","|    policy_gradient_loss | -0.00608    |\n","|    std                  | 0.4         |\n","|    value_loss           | 5.88        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 372     |\n","|    time_elapsed    | 5210    |\n","|    total_timesteps | 4571136 |\n","--------------------------------\n","Eval num_timesteps=4572000, episode_reward=32.39 +/- 0.63\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 32.4        |\n","| time/                   |             |\n","|    total_timesteps      | 4572000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008531876 |\n","|    clip_fraction        | 0.0694      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.98       |\n","|    explained_variance   | 0.644       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 5.05        |\n","|    n_updates            | 3720        |\n","|    policy_gradient_loss | -0.00555    |\n","|    std                  | 0.4         |\n","|    value_loss           | 17.4        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 373     |\n","|    time_elapsed    | 5225    |\n","|    total_timesteps | 4583424 |\n","--------------------------------\n","Eval num_timesteps=4584000, episode_reward=30.01 +/- 0.40\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 30          |\n","| time/                   |             |\n","|    total_timesteps      | 4584000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014908171 |\n","|    clip_fraction        | 0.137       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.98       |\n","|    explained_variance   | 0.854       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.438       |\n","|    n_updates            | 3730        |\n","|    policy_gradient_loss | -0.00847    |\n","|    std                  | 0.398       |\n","|    value_loss           | 1.53        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 877     |\n","|    iterations      | 374     |\n","|    time_elapsed    | 5236    |\n","|    total_timesteps | 4595712 |\n","--------------------------------\n","Eval num_timesteps=4596000, episode_reward=35.42 +/- 0.39\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 35.4        |\n","| time/                   |             |\n","|    total_timesteps      | 4596000     |\n","| train/                  |             |\n","|    approx_kl            | 0.016101455 |\n","|    clip_fraction        | 0.185       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.95       |\n","|    explained_variance   | 0.277       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.677       |\n","|    n_updates            | 3740        |\n","|    policy_gradient_loss | -0.00865    |\n","|    std                  | 0.396       |\n","|    value_loss           | 1.1         |\n","-----------------------------------------\n","Eval num_timesteps=4608000, episode_reward=34.33 +/- 0.84\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 34.3     |\n","| time/              |          |\n","|    total_timesteps | 4608000  |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 876     |\n","|    iterations      | 375     |\n","|    time_elapsed    | 5256    |\n","|    total_timesteps | 4608000 |\n","--------------------------------\n","Eval num_timesteps=4620000, episode_reward=36.20 +/- 0.06\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 36.2         |\n","| time/                   |              |\n","|    total_timesteps      | 4620000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0086292215 |\n","|    clip_fraction        | 0.116        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -1.95        |\n","|    explained_variance   | 0.297        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 5.75         |\n","|    n_updates            | 3750         |\n","|    policy_gradient_loss | -0.00603     |\n","|    std                  | 0.396        |\n","|    value_loss           | 14.1         |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 876     |\n","|    iterations      | 376     |\n","|    time_elapsed    | 5272    |\n","|    total_timesteps | 4620288 |\n","--------------------------------\n","Eval num_timesteps=4632000, episode_reward=34.58 +/- 1.30\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 34.6       |\n","| time/                   |            |\n","|    total_timesteps      | 4632000    |\n","| train/                  |            |\n","|    approx_kl            | 0.02127481 |\n","|    clip_fraction        | 0.206      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.93      |\n","|    explained_variance   | 0.612      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.197      |\n","|    n_updates            | 3760       |\n","|    policy_gradient_loss | -0.00889   |\n","|    std                  | 0.393      |\n","|    value_loss           | 0.56       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 876     |\n","|    iterations      | 377     |\n","|    time_elapsed    | 5285    |\n","|    total_timesteps | 4632576 |\n","--------------------------------\n","Eval num_timesteps=4644000, episode_reward=32.61 +/- 0.81\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 32.6        |\n","| time/                   |             |\n","|    total_timesteps      | 4644000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015192471 |\n","|    clip_fraction        | 0.162       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.91       |\n","|    explained_variance   | 0.846       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.23        |\n","|    n_updates            | 3770        |\n","|    policy_gradient_loss | -0.00794    |\n","|    std                  | 0.393       |\n","|    value_loss           | 3.15        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 876     |\n","|    iterations      | 378     |\n","|    time_elapsed    | 5298    |\n","|    total_timesteps | 4644864 |\n","--------------------------------\n","Eval num_timesteps=4656000, episode_reward=33.89 +/- 1.95\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 33.9        |\n","| time/                   |             |\n","|    total_timesteps      | 4656000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010764855 |\n","|    clip_fraction        | 0.122       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.9        |\n","|    explained_variance   | 0.758       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.24        |\n","|    n_updates            | 3780        |\n","|    policy_gradient_loss | -0.00925    |\n","|    std                  | 0.392       |\n","|    value_loss           | 14.1        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 876     |\n","|    iterations      | 379     |\n","|    time_elapsed    | 5314    |\n","|    total_timesteps | 4657152 |\n","--------------------------------\n","Eval num_timesteps=4668000, episode_reward=33.78 +/- 5.53\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 33.8        |\n","| time/                   |             |\n","|    total_timesteps      | 4668000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012919999 |\n","|    clip_fraction        | 0.146       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.9        |\n","|    explained_variance   | 0.414       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 6.45        |\n","|    n_updates            | 3790        |\n","|    policy_gradient_loss | -0.01       |\n","|    std                  | 0.393       |\n","|    value_loss           | 18.8        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 876     |\n","|    iterations      | 380     |\n","|    time_elapsed    | 5326    |\n","|    total_timesteps | 4669440 |\n","--------------------------------\n","Eval num_timesteps=4680000, episode_reward=32.73 +/- 1.85\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 32.7        |\n","| time/                   |             |\n","|    total_timesteps      | 4680000     |\n","| train/                  |             |\n","|    approx_kl            | 0.017085545 |\n","|    clip_fraction        | 0.206       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.91       |\n","|    explained_variance   | 0.734       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0915      |\n","|    n_updates            | 3800        |\n","|    policy_gradient_loss | -0.0119     |\n","|    std                  | 0.393       |\n","|    value_loss           | 0.301       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 876     |\n","|    iterations      | 381     |\n","|    time_elapsed    | 5340    |\n","|    total_timesteps | 4681728 |\n","--------------------------------\n","Eval num_timesteps=4692000, episode_reward=26.33 +/- 5.82\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 26.3        |\n","| time/                   |             |\n","|    total_timesteps      | 4692000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015860787 |\n","|    clip_fraction        | 0.199       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.9        |\n","|    explained_variance   | 0.882       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0958      |\n","|    n_updates            | 3810        |\n","|    policy_gradient_loss | -0.0103     |\n","|    std                  | 0.391       |\n","|    value_loss           | 0.244       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 876     |\n","|    iterations      | 382     |\n","|    time_elapsed    | 5358    |\n","|    total_timesteps | 4694016 |\n","--------------------------------\n","Eval num_timesteps=4704000, episode_reward=34.72 +/- 0.65\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 34.7        |\n","| time/                   |             |\n","|    total_timesteps      | 4704000     |\n","| train/                  |             |\n","|    approx_kl            | 0.017612897 |\n","|    clip_fraction        | 0.194       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.87       |\n","|    explained_variance   | 0.866       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.117       |\n","|    n_updates            | 3820        |\n","|    policy_gradient_loss | -0.0121     |\n","|    std                  | 0.389       |\n","|    value_loss           | 0.228       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 876     |\n","|    iterations      | 383     |\n","|    time_elapsed    | 5370    |\n","|    total_timesteps | 4706304 |\n","--------------------------------\n","Eval num_timesteps=4716000, episode_reward=33.66 +/- 0.08\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 33.7        |\n","| time/                   |             |\n","|    total_timesteps      | 4716000     |\n","| train/                  |             |\n","|    approx_kl            | 0.007775992 |\n","|    clip_fraction        | 0.103       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.87       |\n","|    explained_variance   | 0.656       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.07        |\n","|    n_updates            | 3830        |\n","|    policy_gradient_loss | -0.00609    |\n","|    std                  | 0.389       |\n","|    value_loss           | 7.93        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 876     |\n","|    iterations      | 384     |\n","|    time_elapsed    | 5386    |\n","|    total_timesteps | 4718592 |\n","--------------------------------\n","Eval num_timesteps=4728000, episode_reward=34.83 +/- 0.75\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 34.8        |\n","| time/                   |             |\n","|    total_timesteps      | 4728000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015899917 |\n","|    clip_fraction        | 0.176       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.88       |\n","|    explained_variance   | 0.839       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.136       |\n","|    n_updates            | 3840        |\n","|    policy_gradient_loss | -0.0111     |\n","|    std                  | 0.389       |\n","|    value_loss           | 0.318       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 876     |\n","|    iterations      | 385     |\n","|    time_elapsed    | 5399    |\n","|    total_timesteps | 4730880 |\n","--------------------------------\n","Eval num_timesteps=4740000, episode_reward=33.28 +/- 0.60\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 33.3        |\n","| time/                   |             |\n","|    total_timesteps      | 4740000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015788836 |\n","|    clip_fraction        | 0.198       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.87       |\n","|    explained_variance   | 0.781       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.165       |\n","|    n_updates            | 3850        |\n","|    policy_gradient_loss | -0.0118     |\n","|    std                  | 0.39        |\n","|    value_loss           | 0.338       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 876     |\n","|    iterations      | 386     |\n","|    time_elapsed    | 5412    |\n","|    total_timesteps | 4743168 |\n","--------------------------------\n","Eval num_timesteps=4752000, episode_reward=34.71 +/- 0.12\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 34.7        |\n","| time/                   |             |\n","|    total_timesteps      | 4752000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013311923 |\n","|    clip_fraction        | 0.155       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.88       |\n","|    explained_variance   | 0.436       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 5.87        |\n","|    n_updates            | 3860        |\n","|    policy_gradient_loss | -0.0103     |\n","|    std                  | 0.39        |\n","|    value_loss           | 10.2        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 876     |\n","|    iterations      | 387     |\n","|    time_elapsed    | 5427    |\n","|    total_timesteps | 4755456 |\n","--------------------------------\n","Eval num_timesteps=4764000, episode_reward=32.07 +/- 3.83\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 32.1        |\n","| time/                   |             |\n","|    total_timesteps      | 4764000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009000258 |\n","|    clip_fraction        | 0.102       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.88       |\n","|    explained_variance   | 0.388       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.5         |\n","|    n_updates            | 3870        |\n","|    policy_gradient_loss | -0.0071     |\n","|    std                  | 0.39        |\n","|    value_loss           | 5.83        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 876     |\n","|    iterations      | 388     |\n","|    time_elapsed    | 5440    |\n","|    total_timesteps | 4767744 |\n","--------------------------------\n","Eval num_timesteps=4776000, episode_reward=34.56 +/- 1.07\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 34.6       |\n","| time/                   |            |\n","|    total_timesteps      | 4776000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01933019 |\n","|    clip_fraction        | 0.195      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.86      |\n","|    explained_variance   | 0.593      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.283      |\n","|    n_updates            | 3880       |\n","|    policy_gradient_loss | -0.00707   |\n","|    std                  | 0.387      |\n","|    value_loss           | 0.756      |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 876     |\n","|    iterations      | 389     |\n","|    time_elapsed    | 5453    |\n","|    total_timesteps | 4780032 |\n","--------------------------------\n","Eval num_timesteps=4788000, episode_reward=31.25 +/- 0.63\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 31.3        |\n","| time/                   |             |\n","|    total_timesteps      | 4788000     |\n","| train/                  |             |\n","|    approx_kl            | 0.016028075 |\n","|    clip_fraction        | 0.207       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.84       |\n","|    explained_variance   | 0.848       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.139       |\n","|    n_updates            | 3890        |\n","|    policy_gradient_loss | -0.0123     |\n","|    std                  | 0.386       |\n","|    value_loss           | 0.284       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 876     |\n","|    iterations      | 390     |\n","|    time_elapsed    | 5469    |\n","|    total_timesteps | 4792320 |\n","--------------------------------\n","Eval num_timesteps=4800000, episode_reward=33.97 +/- 1.11\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 34          |\n","| time/                   |             |\n","|    total_timesteps      | 4800000     |\n","| train/                  |             |\n","|    approx_kl            | 0.016341975 |\n","|    clip_fraction        | 0.18        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.84       |\n","|    explained_variance   | 0.879       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.115       |\n","|    n_updates            | 3900        |\n","|    policy_gradient_loss | -0.00946    |\n","|    std                  | 0.386       |\n","|    value_loss           | 0.261       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 876     |\n","|    iterations      | 391     |\n","|    time_elapsed    | 5481    |\n","|    total_timesteps | 4804608 |\n","--------------------------------\n","Eval num_timesteps=4812000, episode_reward=31.63 +/- 0.51\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 31.6         |\n","| time/                   |              |\n","|    total_timesteps      | 4812000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0155012235 |\n","|    clip_fraction        | 0.2          |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -1.83        |\n","|    explained_variance   | 0.854        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.0801       |\n","|    n_updates            | 3910         |\n","|    policy_gradient_loss | -0.0123      |\n","|    std                  | 0.385        |\n","|    value_loss           | 0.251        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 876     |\n","|    iterations      | 392     |\n","|    time_elapsed    | 5495    |\n","|    total_timesteps | 4816896 |\n","--------------------------------\n","Eval num_timesteps=4824000, episode_reward=38.04 +/- 0.91\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 38          |\n","| time/                   |             |\n","|    total_timesteps      | 4824000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014096685 |\n","|    clip_fraction        | 0.177       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.83       |\n","|    explained_variance   | 0.868       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0712      |\n","|    n_updates            | 3920        |\n","|    policy_gradient_loss | -0.0111     |\n","|    std                  | 0.386       |\n","|    value_loss           | 0.242       |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 876     |\n","|    iterations      | 393     |\n","|    time_elapsed    | 5510    |\n","|    total_timesteps | 4829184 |\n","--------------------------------\n","Eval num_timesteps=4836000, episode_reward=36.34 +/- 2.18\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 36.3        |\n","| time/                   |             |\n","|    total_timesteps      | 4836000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011555831 |\n","|    clip_fraction        | 0.157       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.84       |\n","|    explained_variance   | 0.426       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.82        |\n","|    n_updates            | 3930        |\n","|    policy_gradient_loss | -0.0108     |\n","|    std                  | 0.387       |\n","|    value_loss           | 8.48        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 876     |\n","|    iterations      | 394     |\n","|    time_elapsed    | 5521    |\n","|    total_timesteps | 4841472 |\n","--------------------------------\n","Eval num_timesteps=4848000, episode_reward=32.00 +/- 2.05\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 32          |\n","| time/                   |             |\n","|    total_timesteps      | 4848000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014737837 |\n","|    clip_fraction        | 0.175       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.83       |\n","|    explained_variance   | 0.843       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0934      |\n","|    n_updates            | 3940        |\n","|    policy_gradient_loss | -0.0101     |\n","|    std                  | 0.385       |\n","|    value_loss           | 0.243       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 876     |\n","|    iterations      | 395     |\n","|    time_elapsed    | 5537    |\n","|    total_timesteps | 4853760 |\n","--------------------------------\n","Eval num_timesteps=4860000, episode_reward=37.62 +/- 1.16\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 37.6        |\n","| time/                   |             |\n","|    total_timesteps      | 4860000     |\n","| train/                  |             |\n","|    approx_kl            | 0.016381701 |\n","|    clip_fraction        | 0.184       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.81       |\n","|    explained_variance   | 0.771       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.126       |\n","|    n_updates            | 3950        |\n","|    policy_gradient_loss | -0.013      |\n","|    std                  | 0.383       |\n","|    value_loss           | 0.324       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 876     |\n","|    iterations      | 396     |\n","|    time_elapsed    | 5551    |\n","|    total_timesteps | 4866048 |\n","--------------------------------\n","Eval num_timesteps=4872000, episode_reward=33.98 +/- 1.86\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 34          |\n","| time/                   |             |\n","|    total_timesteps      | 4872000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013237443 |\n","|    clip_fraction        | 0.171       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.79       |\n","|    explained_variance   | 0.898       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0558      |\n","|    n_updates            | 3960        |\n","|    policy_gradient_loss | -0.0124     |\n","|    std                  | 0.38        |\n","|    value_loss           | 0.187       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 876     |\n","|    iterations      | 397     |\n","|    time_elapsed    | 5563    |\n","|    total_timesteps | 4878336 |\n","--------------------------------\n","Eval num_timesteps=4884000, episode_reward=43.46 +/- 2.32\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 43.5        |\n","| time/                   |             |\n","|    total_timesteps      | 4884000     |\n","| train/                  |             |\n","|    approx_kl            | 0.016126718 |\n","|    clip_fraction        | 0.176       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.77       |\n","|    explained_variance   | 0.866       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.132       |\n","|    n_updates            | 3970        |\n","|    policy_gradient_loss | -0.0124     |\n","|    std                  | 0.379       |\n","|    value_loss           | 0.268       |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 876     |\n","|    iterations      | 398     |\n","|    time_elapsed    | 5578    |\n","|    total_timesteps | 4890624 |\n","--------------------------------\n","Eval num_timesteps=4896000, episode_reward=37.03 +/- 1.04\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 37         |\n","| time/                   |            |\n","|    total_timesteps      | 4896000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01389946 |\n","|    clip_fraction        | 0.163      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.76      |\n","|    explained_variance   | 0.852      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.136      |\n","|    n_updates            | 3980       |\n","|    policy_gradient_loss | -0.0116    |\n","|    std                  | 0.379      |\n","|    value_loss           | 0.294      |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 876     |\n","|    iterations      | 399     |\n","|    time_elapsed    | 5592    |\n","|    total_timesteps | 4902912 |\n","--------------------------------\n","Eval num_timesteps=4908000, episode_reward=38.28 +/- 0.49\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 38.3       |\n","| time/                   |            |\n","|    total_timesteps      | 4908000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01347706 |\n","|    clip_fraction        | 0.166      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.75      |\n","|    explained_variance   | 0.864      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.122      |\n","|    n_updates            | 3990       |\n","|    policy_gradient_loss | -0.0117    |\n","|    std                  | 0.378      |\n","|    value_loss           | 0.277      |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 876     |\n","|    iterations      | 400     |\n","|    time_elapsed    | 5610    |\n","|    total_timesteps | 4915200 |\n","--------------------------------\n","Eval num_timesteps=4920000, episode_reward=32.04 +/- 2.21\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 32          |\n","| time/                   |             |\n","|    total_timesteps      | 4920000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014197938 |\n","|    clip_fraction        | 0.16        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.74       |\n","|    explained_variance   | 0.902       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.166       |\n","|    n_updates            | 4000        |\n","|    policy_gradient_loss | -0.0118     |\n","|    std                  | 0.376       |\n","|    value_loss           | 0.301       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 876     |\n","|    iterations      | 401     |\n","|    time_elapsed    | 5624    |\n","|    total_timesteps | 4927488 |\n","--------------------------------\n","Eval num_timesteps=4932000, episode_reward=41.34 +/- 0.80\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 41.3        |\n","| time/                   |             |\n","|    total_timesteps      | 4932000     |\n","| train/                  |             |\n","|    approx_kl            | 0.006668814 |\n","|    clip_fraction        | 0.0937      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.74       |\n","|    explained_variance   | 0.405       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.67        |\n","|    n_updates            | 4010        |\n","|    policy_gradient_loss | -0.00685    |\n","|    std                  | 0.377       |\n","|    value_loss           | 6.48        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 875     |\n","|    iterations      | 402     |\n","|    time_elapsed    | 5640    |\n","|    total_timesteps | 4939776 |\n","--------------------------------\n","Eval num_timesteps=4944000, episode_reward=38.57 +/- 0.36\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 38.6         |\n","| time/                   |              |\n","|    total_timesteps      | 4944000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0110247135 |\n","|    clip_fraction        | 0.12         |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -1.74        |\n","|    explained_variance   | 0.717        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.534        |\n","|    n_updates            | 4020         |\n","|    policy_gradient_loss | -0.00754     |\n","|    std                  | 0.377        |\n","|    value_loss           | 2.17         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 876     |\n","|    iterations      | 403     |\n","|    time_elapsed    | 5651    |\n","|    total_timesteps | 4952064 |\n","--------------------------------\n","Eval num_timesteps=4956000, episode_reward=13.05 +/- 3.85\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 13.1        |\n","| time/                   |             |\n","|    total_timesteps      | 4956000     |\n","| train/                  |             |\n","|    approx_kl            | 0.016523847 |\n","|    clip_fraction        | 0.181       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.73       |\n","|    explained_variance   | 0.856       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.119       |\n","|    n_updates            | 4030        |\n","|    policy_gradient_loss | -0.00918    |\n","|    std                  | 0.375       |\n","|    value_loss           | 0.301       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 876     |\n","|    iterations      | 404     |\n","|    time_elapsed    | 5666    |\n","|    total_timesteps | 4964352 |\n","--------------------------------\n","Eval num_timesteps=4968000, episode_reward=38.60 +/- 0.15\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 38.6        |\n","| time/                   |             |\n","|    total_timesteps      | 4968000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015734844 |\n","|    clip_fraction        | 0.176       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.7        |\n","|    explained_variance   | 0.91        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.12        |\n","|    n_updates            | 4040        |\n","|    policy_gradient_loss | -0.0116     |\n","|    std                  | 0.372       |\n","|    value_loss           | 0.269       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 876     |\n","|    iterations      | 405     |\n","|    time_elapsed    | 5680    |\n","|    total_timesteps | 4976640 |\n","--------------------------------\n","Eval num_timesteps=4980000, episode_reward=33.78 +/- 0.29\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 33.8        |\n","| time/                   |             |\n","|    total_timesteps      | 4980000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014742355 |\n","|    clip_fraction        | 0.187       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.68       |\n","|    explained_variance   | 0.639       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.132       |\n","|    n_updates            | 4050        |\n","|    policy_gradient_loss | -0.00901    |\n","|    std                  | 0.371       |\n","|    value_loss           | 0.585       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 876     |\n","|    iterations      | 406     |\n","|    time_elapsed    | 5692    |\n","|    total_timesteps | 4988928 |\n","--------------------------------\n","Eval num_timesteps=4992000, episode_reward=38.43 +/- 6.19\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 38.4        |\n","| time/                   |             |\n","|    total_timesteps      | 4992000     |\n","| train/                  |             |\n","|    approx_kl            | 0.017717442 |\n","|    clip_fraction        | 0.194       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.69       |\n","|    explained_variance   | 0.836       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.235       |\n","|    n_updates            | 4060        |\n","|    policy_gradient_loss | -0.012      |\n","|    std                  | 0.372       |\n","|    value_loss           | 0.404       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 876     |\n","|    iterations      | 407     |\n","|    time_elapsed    | 5707    |\n","|    total_timesteps | 5001216 |\n","--------------------------------\n"]}]},{"cell_type":"code","source":["############ Por último 0.0"],"metadata":{"id":"gcIqtBEmWjvr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3 import PPO\n","# from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n","from pettingzoo.sisl import multiwalker_v9\n","import supersuit as ss\n","from stable_baselines3.common.monitor import Monitor"],"metadata":{"id":"d8WlQgh4Wuaj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env = multiwalker_v9.parallel_env(render_mode=\"rgb_array\",shared_reward=0)\n","\n","env = ss.frame_stack_v1(env, 3)\n","env = ss.pettingzoo_env_to_vec_env_v1(env)\n","env = ss.concat_vec_envs_v1(env, 8, num_cpus=4, base_class='stable_baselines3')"],"metadata":{"id":"qBu7M_KhWubl"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ihpR0bGHWubm"},"outputs":[],"source":["from stable_baselines3.common.callbacks import EvalCallback\n","eval_callback = EvalCallback(env, best_model_save_path=\"./multiwalker_ppo_rew_0_log_eval/\",\n","                             log_path=\"./multiwalker_ppo_rew_0_log_eval/\", eval_freq=500,\n","                             deterministic=True, render=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6e020f20-b6a8-4013-ad74-6caf071d32e6","executionInfo":{"status":"ok","timestamp":1698939879071,"user_tz":-60,"elapsed":5416171,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"id":"gkglMrxMWubn"},"outputs":[{"output_type":"stream","name":"stdout","text":["Logging to ./multiwalker_ppo_rew_0_log_eval/\n","Using cuda device\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 972     |\n","|    iterations      | 231     |\n","|    time_elapsed    | 2918    |\n","|    total_timesteps | 2838528 |\n","--------------------------------\n","Eval num_timesteps=2844000, episode_reward=8.01 +/- 0.22\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 8.01         |\n","| time/                   |              |\n","|    total_timesteps      | 2844000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0029453498 |\n","|    clip_fraction        | 0.131        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.1        |\n","|    explained_variance   | 0.586        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.538        |\n","|    n_updates            | 1155         |\n","|    policy_gradient_loss | -0.00319     |\n","|    std                  | 3.02         |\n","|    value_loss           | 15.2         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 971     |\n","|    iterations      | 232     |\n","|    time_elapsed    | 2933    |\n","|    total_timesteps | 2850816 |\n","--------------------------------\n","Eval num_timesteps=2856000, episode_reward=9.50 +/- 0.12\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 9.5          |\n","| time/                   |              |\n","|    total_timesteps      | 2856000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021955322 |\n","|    clip_fraction        | 0.0955       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.1        |\n","|    explained_variance   | 0.163        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0895      |\n","|    n_updates            | 1160         |\n","|    policy_gradient_loss | -0.00489     |\n","|    std                  | 3.02         |\n","|    value_loss           | 0.302        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 971     |\n","|    iterations      | 233     |\n","|    time_elapsed    | 2946    |\n","|    total_timesteps | 2863104 |\n","--------------------------------\n","Eval num_timesteps=2868000, episode_reward=9.71 +/- 1.12\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 9.71         |\n","| time/                   |              |\n","|    total_timesteps      | 2868000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0036923669 |\n","|    clip_fraction        | 0.182        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.1        |\n","|    explained_variance   | 0.52         |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.351        |\n","|    n_updates            | 1165         |\n","|    policy_gradient_loss | -0.00371     |\n","|    std                  | 3.05         |\n","|    value_loss           | 15.6         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 970     |\n","|    iterations      | 234     |\n","|    time_elapsed    | 2962    |\n","|    total_timesteps | 2875392 |\n","--------------------------------\n","Eval num_timesteps=2880000, episode_reward=6.22 +/- 1.64\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 6.22         |\n","| time/                   |              |\n","|    total_timesteps      | 2880000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018316702 |\n","|    clip_fraction        | 0.0786       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.1        |\n","|    explained_variance   | -0.0934      |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.1         |\n","|    n_updates            | 1170         |\n","|    policy_gradient_loss | -0.004       |\n","|    std                  | 3.06         |\n","|    value_loss           | 0.269        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 971     |\n","|    iterations      | 235     |\n","|    time_elapsed    | 2973    |\n","|    total_timesteps | 2887680 |\n","--------------------------------\n","Eval num_timesteps=2892000, episode_reward=7.46 +/- 1.06\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 7.46         |\n","| time/                   |              |\n","|    total_timesteps      | 2892000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0028921699 |\n","|    clip_fraction        | 0.133        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.2        |\n","|    explained_variance   | 0.656        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.235        |\n","|    n_updates            | 1175         |\n","|    policy_gradient_loss | -0.0031      |\n","|    std                  | 3.07         |\n","|    value_loss           | 14.2         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 970     |\n","|    iterations      | 236     |\n","|    time_elapsed    | 2987    |\n","|    total_timesteps | 2899968 |\n","--------------------------------\n","Eval num_timesteps=2904000, episode_reward=8.53 +/- 1.19\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 8.53         |\n","| time/                   |              |\n","|    total_timesteps      | 2904000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0029066317 |\n","|    clip_fraction        | 0.121        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.2        |\n","|    explained_variance   | 0.518        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.0397       |\n","|    n_updates            | 1180         |\n","|    policy_gradient_loss | -0.00402     |\n","|    std                  | 3.11         |\n","|    value_loss           | 12.4         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 970     |\n","|    iterations      | 237     |\n","|    time_elapsed    | 3002    |\n","|    total_timesteps | 2912256 |\n","--------------------------------\n","Eval num_timesteps=2916000, episode_reward=11.11 +/- 0.42\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 11.1         |\n","| time/                   |              |\n","|    total_timesteps      | 2916000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019954548 |\n","|    clip_fraction        | 0.108        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.2        |\n","|    explained_variance   | 0.281        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.0222       |\n","|    n_updates            | 1185         |\n","|    policy_gradient_loss | -0.00359     |\n","|    std                  | 3.12         |\n","|    value_loss           | 26.5         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 970     |\n","|    iterations      | 238     |\n","|    time_elapsed    | 3013    |\n","|    total_timesteps | 2924544 |\n","--------------------------------\n","Eval num_timesteps=2928000, episode_reward=8.64 +/- 1.84\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 8.64         |\n","| time/                   |              |\n","|    total_timesteps      | 2928000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022764734 |\n","|    clip_fraction        | 0.101        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.2        |\n","|    explained_variance   | -0.353       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0831      |\n","|    n_updates            | 1190         |\n","|    policy_gradient_loss | -0.00453     |\n","|    std                  | 3.13         |\n","|    value_loss           | 1.3          |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 969     |\n","|    iterations      | 239     |\n","|    time_elapsed    | 3029    |\n","|    total_timesteps | 2936832 |\n","--------------------------------\n","Eval num_timesteps=2940000, episode_reward=5.36 +/- 1.37\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 5.36         |\n","| time/                   |              |\n","|    total_timesteps      | 2940000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021740703 |\n","|    clip_fraction        | 0.103        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.2        |\n","|    explained_variance   | -0.00532     |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.102       |\n","|    n_updates            | 1195         |\n","|    policy_gradient_loss | -0.00428     |\n","|    std                  | 3.13         |\n","|    value_loss           | 0.266        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 969     |\n","|    iterations      | 240     |\n","|    time_elapsed    | 3043    |\n","|    total_timesteps | 2949120 |\n","--------------------------------\n","Eval num_timesteps=2952000, episode_reward=8.03 +/- 0.71\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 8.03         |\n","| time/                   |              |\n","|    total_timesteps      | 2952000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0025914817 |\n","|    clip_fraction        | 0.117        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.2        |\n","|    explained_variance   | 0.0701       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0935      |\n","|    n_updates            | 1200         |\n","|    policy_gradient_loss | -0.00377     |\n","|    std                  | 3.14         |\n","|    value_loss           | 0.258        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 969     |\n","|    iterations      | 241     |\n","|    time_elapsed    | 3055    |\n","|    total_timesteps | 2961408 |\n","--------------------------------\n","Eval num_timesteps=2964000, episode_reward=7.80 +/- 0.16\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 7.8          |\n","| time/                   |              |\n","|    total_timesteps      | 2964000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0027052213 |\n","|    clip_fraction        | 0.11         |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.3        |\n","|    explained_variance   | 0.0553       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.102       |\n","|    n_updates            | 1205         |\n","|    policy_gradient_loss | -0.00312     |\n","|    std                  | 3.15         |\n","|    value_loss           | 0.238        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 968     |\n","|    iterations      | 242     |\n","|    time_elapsed    | 3071    |\n","|    total_timesteps | 2973696 |\n","--------------------------------\n","Eval num_timesteps=2976000, episode_reward=8.29 +/- 0.96\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 8.29         |\n","| time/                   |              |\n","|    total_timesteps      | 2976000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020338304 |\n","|    clip_fraction        | 0.109        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.3        |\n","|    explained_variance   | 0.0877       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0994      |\n","|    n_updates            | 1210         |\n","|    policy_gradient_loss | -0.00386     |\n","|    std                  | 3.16         |\n","|    value_loss           | 0.243        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 968     |\n","|    iterations      | 243     |\n","|    time_elapsed    | 3083    |\n","|    total_timesteps | 2985984 |\n","--------------------------------\n","Eval num_timesteps=2988000, episode_reward=7.75 +/- 1.71\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 7.75         |\n","| time/                   |              |\n","|    total_timesteps      | 2988000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0023231574 |\n","|    clip_fraction        | 0.116        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.3        |\n","|    explained_variance   | 0.101        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0979      |\n","|    n_updates            | 1215         |\n","|    policy_gradient_loss | -0.00391     |\n","|    std                  | 3.16         |\n","|    value_loss           | 0.252        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 968     |\n","|    iterations      | 244     |\n","|    time_elapsed    | 3097    |\n","|    total_timesteps | 2998272 |\n","--------------------------------\n","Eval num_timesteps=3000000, episode_reward=9.19 +/- 0.48\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 9.19         |\n","| time/                   |              |\n","|    total_timesteps      | 3000000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020198242 |\n","|    clip_fraction        | 0.103        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.3        |\n","|    explained_variance   | 0.0986       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.101       |\n","|    n_updates            | 1220         |\n","|    policy_gradient_loss | -0.0037      |\n","|    std                  | 3.16         |\n","|    value_loss           | 0.234        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 967     |\n","|    iterations      | 245     |\n","|    time_elapsed    | 3112    |\n","|    total_timesteps | 3010560 |\n","--------------------------------\n","Eval num_timesteps=3012000, episode_reward=9.56 +/- 0.83\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 9.56         |\n","| time/                   |              |\n","|    total_timesteps      | 3012000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0027362613 |\n","|    clip_fraction        | 0.112        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.3        |\n","|    explained_variance   | 0.0631       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0994      |\n","|    n_updates            | 1225         |\n","|    policy_gradient_loss | -0.00396     |\n","|    std                  | 3.17         |\n","|    value_loss           | 0.239        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 967     |\n","|    iterations      | 246     |\n","|    time_elapsed    | 3124    |\n","|    total_timesteps | 3022848 |\n","--------------------------------\n","Eval num_timesteps=3024000, episode_reward=10.59 +/- 2.08\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 10.6        |\n","| time/                   |             |\n","|    total_timesteps      | 3024000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002736864 |\n","|    clip_fraction        | 0.126       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10.3       |\n","|    explained_variance   | 0.103       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0985     |\n","|    n_updates            | 1230        |\n","|    policy_gradient_loss | -0.00325    |\n","|    std                  | 3.18        |\n","|    value_loss           | 0.242       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 966     |\n","|    iterations      | 247     |\n","|    time_elapsed    | 3138    |\n","|    total_timesteps | 3035136 |\n","--------------------------------\n","Eval num_timesteps=3036000, episode_reward=12.18 +/- 0.40\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 12.2        |\n","| time/                   |             |\n","|    total_timesteps      | 3036000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002473098 |\n","|    clip_fraction        | 0.0938      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10.3       |\n","|    explained_variance   | 0.0904      |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.101      |\n","|    n_updates            | 1235        |\n","|    policy_gradient_loss | -0.00328    |\n","|    std                  | 3.19        |\n","|    value_loss           | 0.237       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 966     |\n","|    iterations      | 248     |\n","|    time_elapsed    | 3153    |\n","|    total_timesteps | 3047424 |\n","--------------------------------\n","Eval num_timesteps=3048000, episode_reward=11.52 +/- 1.92\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 11.5         |\n","| time/                   |              |\n","|    total_timesteps      | 3048000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0024507067 |\n","|    clip_fraction        | 0.113        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.3        |\n","|    explained_variance   | 0.4          |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.232        |\n","|    n_updates            | 1240         |\n","|    policy_gradient_loss | -0.00362     |\n","|    std                  | 3.23         |\n","|    value_loss           | 15.7         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 966     |\n","|    iterations      | 249     |\n","|    time_elapsed    | 3165    |\n","|    total_timesteps | 3059712 |\n","--------------------------------\n","Eval num_timesteps=3060000, episode_reward=11.23 +/- 0.04\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 11.2         |\n","| time/                   |              |\n","|    total_timesteps      | 3060000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020460864 |\n","|    clip_fraction        | 0.0959       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.4        |\n","|    explained_variance   | 0.141        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.105       |\n","|    n_updates            | 1245         |\n","|    policy_gradient_loss | -0.00381     |\n","|    std                  | 3.23         |\n","|    value_loss           | 0.438        |\n","------------------------------------------\n","Eval num_timesteps=3072000, episode_reward=11.79 +/- 1.34\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 11.8     |\n","| time/              |          |\n","|    total_timesteps | 3072000  |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 963     |\n","|    iterations      | 250     |\n","|    time_elapsed    | 3186    |\n","|    total_timesteps | 3072000 |\n","--------------------------------\n","Eval num_timesteps=3084000, episode_reward=11.71 +/- 1.19\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 11.7         |\n","| time/                   |              |\n","|    total_timesteps      | 3084000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0025048924 |\n","|    clip_fraction        | 0.0981       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.4        |\n","|    explained_variance   | -0.041       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0956      |\n","|    n_updates            | 1250         |\n","|    policy_gradient_loss | -0.00267     |\n","|    std                  | 3.25         |\n","|    value_loss           | 0.413        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 962     |\n","|    iterations      | 251     |\n","|    time_elapsed    | 3203    |\n","|    total_timesteps | 3084288 |\n","--------------------------------\n","Eval num_timesteps=3096000, episode_reward=10.98 +/- 0.15\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 11           |\n","| time/                   |              |\n","|    total_timesteps      | 3096000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0029367954 |\n","|    clip_fraction        | 0.13         |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.4        |\n","|    explained_variance   | 0.63         |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.774        |\n","|    n_updates            | 1255         |\n","|    policy_gradient_loss | -0.00369     |\n","|    std                  | 3.3          |\n","|    value_loss           | 14.8         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 962     |\n","|    iterations      | 252     |\n","|    time_elapsed    | 3216    |\n","|    total_timesteps | 3096576 |\n","--------------------------------\n","Eval num_timesteps=3108000, episode_reward=12.72 +/- 0.14\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 12.7        |\n","| time/                   |             |\n","|    total_timesteps      | 3108000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002417993 |\n","|    clip_fraction        | 0.106       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10.4       |\n","|    explained_variance   | 0.108       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0948     |\n","|    n_updates            | 1260        |\n","|    policy_gradient_loss | -0.00428    |\n","|    std                  | 3.29        |\n","|    value_loss           | 0.326       |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 962     |\n","|    iterations      | 253     |\n","|    time_elapsed    | 3230    |\n","|    total_timesteps | 3108864 |\n","--------------------------------\n","Eval num_timesteps=3120000, episode_reward=12.73 +/- 0.49\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 12.7         |\n","| time/                   |              |\n","|    total_timesteps      | 3120000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0023406763 |\n","|    clip_fraction        | 0.093        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.5        |\n","|    explained_variance   | 0.103        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0966      |\n","|    n_updates            | 1265         |\n","|    policy_gradient_loss | -0.00309     |\n","|    std                  | 3.31         |\n","|    value_loss           | 0.277        |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 961     |\n","|    iterations      | 254     |\n","|    time_elapsed    | 3245    |\n","|    total_timesteps | 3121152 |\n","--------------------------------\n","Eval num_timesteps=3132000, episode_reward=11.99 +/- 0.57\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 12           |\n","| time/                   |              |\n","|    total_timesteps      | 3132000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021761346 |\n","|    clip_fraction        | 0.081        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.5        |\n","|    explained_variance   | 0.561        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0857      |\n","|    n_updates            | 1270         |\n","|    policy_gradient_loss | -0.00305     |\n","|    std                  | 3.34         |\n","|    value_loss           | 0.989        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 962     |\n","|    iterations      | 255     |\n","|    time_elapsed    | 3257    |\n","|    total_timesteps | 3133440 |\n","--------------------------------\n","Eval num_timesteps=3144000, episode_reward=11.45 +/- 1.24\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 11.4         |\n","| time/                   |              |\n","|    total_timesteps      | 3144000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0027717445 |\n","|    clip_fraction        | 0.166        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.5        |\n","|    explained_variance   | 0.517        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0179      |\n","|    n_updates            | 1275         |\n","|    policy_gradient_loss | -0.00174     |\n","|    std                  | 3.37         |\n","|    value_loss           | 7.02         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 961     |\n","|    iterations      | 256     |\n","|    time_elapsed    | 3272    |\n","|    total_timesteps | 3145728 |\n","--------------------------------\n","Eval num_timesteps=3156000, episode_reward=12.38 +/- 2.37\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 12.4         |\n","| time/                   |              |\n","|    total_timesteps      | 3156000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0026868004 |\n","|    clip_fraction        | 0.103        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.5        |\n","|    explained_variance   | -0.156       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.105       |\n","|    n_updates            | 1280         |\n","|    policy_gradient_loss | -0.00371     |\n","|    std                  | 3.38         |\n","|    value_loss           | 0.269        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 960     |\n","|    iterations      | 257     |\n","|    time_elapsed    | 3287    |\n","|    total_timesteps | 3158016 |\n","--------------------------------\n","Eval num_timesteps=3168000, episode_reward=12.48 +/- 0.09\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 12.5         |\n","| time/                   |              |\n","|    total_timesteps      | 3168000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0026737556 |\n","|    clip_fraction        | 0.137        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.5        |\n","|    explained_variance   | 0.027        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.104       |\n","|    n_updates            | 1285         |\n","|    policy_gradient_loss | -0.00333     |\n","|    std                  | 3.38         |\n","|    value_loss           | 0.281        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 961     |\n","|    iterations      | 258     |\n","|    time_elapsed    | 3298    |\n","|    total_timesteps | 3170304 |\n","--------------------------------\n","Eval num_timesteps=3180000, episode_reward=11.58 +/- 1.85\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 11.6         |\n","| time/                   |              |\n","|    total_timesteps      | 3180000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021992184 |\n","|    clip_fraction        | 0.107        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.6        |\n","|    explained_variance   | 0.048        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.101       |\n","|    n_updates            | 1290         |\n","|    policy_gradient_loss | -0.00404     |\n","|    std                  | 3.39         |\n","|    value_loss           | 0.406        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 960     |\n","|    iterations      | 259     |\n","|    time_elapsed    | 3313    |\n","|    total_timesteps | 3182592 |\n","--------------------------------\n","Eval num_timesteps=3192000, episode_reward=13.85 +/- 0.60\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 13.9        |\n","| time/                   |             |\n","|    total_timesteps      | 3192000     |\n","| train/                  |             |\n","|    approx_kl            | 0.001911305 |\n","|    clip_fraction        | 0.144       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10.6       |\n","|    explained_variance   | 0.563       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 0.209       |\n","|    n_updates            | 1295        |\n","|    policy_gradient_loss | -0.00249    |\n","|    std                  | 3.42        |\n","|    value_loss           | 11          |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 960     |\n","|    iterations      | 260     |\n","|    time_elapsed    | 3327    |\n","|    total_timesteps | 3194880 |\n","--------------------------------\n","Eval num_timesteps=3204000, episode_reward=10.06 +/- 1.07\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 10.1        |\n","| time/                   |             |\n","|    total_timesteps      | 3204000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002151589 |\n","|    clip_fraction        | 0.101       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10.6       |\n","|    explained_variance   | -0.826      |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0857     |\n","|    n_updates            | 1300        |\n","|    policy_gradient_loss | -0.00389    |\n","|    std                  | 3.42        |\n","|    value_loss           | 0.81        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 960     |\n","|    iterations      | 261     |\n","|    time_elapsed    | 3339    |\n","|    total_timesteps | 3207168 |\n","--------------------------------\n","Eval num_timesteps=3216000, episode_reward=13.70 +/- 0.91\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 13.7        |\n","| time/                   |             |\n","|    total_timesteps      | 3216000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002409323 |\n","|    clip_fraction        | 0.0956      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10.6       |\n","|    explained_variance   | -0.187      |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.102      |\n","|    n_updates            | 1305        |\n","|    policy_gradient_loss | -0.00418    |\n","|    std                  | 3.43        |\n","|    value_loss           | 0.312       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 959     |\n","|    iterations      | 262     |\n","|    time_elapsed    | 3355    |\n","|    total_timesteps | 3219456 |\n","--------------------------------\n","Eval num_timesteps=3228000, episode_reward=8.93 +/- 2.30\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 8.93         |\n","| time/                   |              |\n","|    total_timesteps      | 3228000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021312414 |\n","|    clip_fraction        | 0.105        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.6        |\n","|    explained_variance   | -0.16        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0983      |\n","|    n_updates            | 1310         |\n","|    policy_gradient_loss | -0.00334     |\n","|    std                  | 3.43         |\n","|    value_loss           | 0.259        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 959     |\n","|    iterations      | 263     |\n","|    time_elapsed    | 3368    |\n","|    total_timesteps | 3231744 |\n","--------------------------------\n","Eval num_timesteps=3240000, episode_reward=10.64 +/- 0.34\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 10.6        |\n","| time/                   |             |\n","|    total_timesteps      | 3240000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002353462 |\n","|    clip_fraction        | 0.138       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10.6       |\n","|    explained_variance   | 0.646       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 0.317       |\n","|    n_updates            | 1315        |\n","|    policy_gradient_loss | -0.00305    |\n","|    std                  | 3.44        |\n","|    value_loss           | 14          |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 959     |\n","|    iterations      | 264     |\n","|    time_elapsed    | 3381    |\n","|    total_timesteps | 3244032 |\n","--------------------------------\n","Eval num_timesteps=3252000, episode_reward=10.36 +/- 0.16\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 10.4         |\n","| time/                   |              |\n","|    total_timesteps      | 3252000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021215016 |\n","|    clip_fraction        | 0.103        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.6        |\n","|    explained_variance   | -0.368       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0879      |\n","|    n_updates            | 1320         |\n","|    policy_gradient_loss | -0.0037      |\n","|    std                  | 3.45         |\n","|    value_loss           | 0.615        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 958     |\n","|    iterations      | 265     |\n","|    time_elapsed    | 3397    |\n","|    total_timesteps | 3256320 |\n","--------------------------------\n","Eval num_timesteps=3264000, episode_reward=8.14 +/- 3.43\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 8.14         |\n","| time/                   |              |\n","|    total_timesteps      | 3264000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022513052 |\n","|    clip_fraction        | 0.108        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.6        |\n","|    explained_variance   | 0.123        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.101       |\n","|    n_updates            | 1325         |\n","|    policy_gradient_loss | -0.00321     |\n","|    std                  | 3.45         |\n","|    value_loss           | 0.259        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 958     |\n","|    iterations      | 266     |\n","|    time_elapsed    | 3408    |\n","|    total_timesteps | 3268608 |\n","--------------------------------\n","Eval num_timesteps=3276000, episode_reward=7.40 +/- 2.43\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 7.4          |\n","| time/                   |              |\n","|    total_timesteps      | 3276000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021260446 |\n","|    clip_fraction        | 0.0975       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.6        |\n","|    explained_variance   | 0.0515       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.103       |\n","|    n_updates            | 1330         |\n","|    policy_gradient_loss | -0.00332     |\n","|    std                  | 3.47         |\n","|    value_loss           | 0.249        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 958     |\n","|    iterations      | 267     |\n","|    time_elapsed    | 3423    |\n","|    total_timesteps | 3280896 |\n","--------------------------------\n","Eval num_timesteps=3288000, episode_reward=12.25 +/- 1.35\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 12.2         |\n","| time/                   |              |\n","|    total_timesteps      | 3288000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021019147 |\n","|    clip_fraction        | 0.112        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.6        |\n","|    explained_variance   | -0.398       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0995      |\n","|    n_updates            | 1335         |\n","|    policy_gradient_loss | -0.0033      |\n","|    std                  | 3.47         |\n","|    value_loss           | 0.396        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 957     |\n","|    iterations      | 268     |\n","|    time_elapsed    | 3437    |\n","|    total_timesteps | 3293184 |\n","--------------------------------\n","Eval num_timesteps=3300000, episode_reward=9.24 +/- 3.42\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 9.24         |\n","| time/                   |              |\n","|    total_timesteps      | 3300000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0024174845 |\n","|    clip_fraction        | 0.111        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.7        |\n","|    explained_variance   | 0.145        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.104       |\n","|    n_updates            | 1340         |\n","|    policy_gradient_loss | -0.00393     |\n","|    std                  | 3.49         |\n","|    value_loss           | 0.249        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 958     |\n","|    iterations      | 269     |\n","|    time_elapsed    | 3449    |\n","|    total_timesteps | 3305472 |\n","--------------------------------\n","Eval num_timesteps=3312000, episode_reward=8.35 +/- 2.95\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 8.35         |\n","| time/                   |              |\n","|    total_timesteps      | 3312000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019912247 |\n","|    clip_fraction        | 0.108        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.7        |\n","|    explained_variance   | 0.136        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.104       |\n","|    n_updates            | 1345         |\n","|    policy_gradient_loss | -0.00272     |\n","|    std                  | 3.51         |\n","|    value_loss           | 0.212        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 957     |\n","|    iterations      | 270     |\n","|    time_elapsed    | 3464    |\n","|    total_timesteps | 3317760 |\n","--------------------------------\n","Eval num_timesteps=3324000, episode_reward=6.15 +/- 1.65\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 6.15         |\n","| time/                   |              |\n","|    total_timesteps      | 3324000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0025939185 |\n","|    clip_fraction        | 0.108        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.7        |\n","|    explained_variance   | 0.09         |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.101       |\n","|    n_updates            | 1350         |\n","|    policy_gradient_loss | -0.00318     |\n","|    std                  | 3.52         |\n","|    value_loss           | 0.241        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 957     |\n","|    iterations      | 271     |\n","|    time_elapsed    | 3476    |\n","|    total_timesteps | 3330048 |\n","--------------------------------\n","Eval num_timesteps=3336000, episode_reward=5.98 +/- 0.12\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 5.98         |\n","| time/                   |              |\n","|    total_timesteps      | 3336000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018199153 |\n","|    clip_fraction        | 0.0956       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.7        |\n","|    explained_variance   | 0.157        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.104       |\n","|    n_updates            | 1355         |\n","|    policy_gradient_loss | -0.00356     |\n","|    std                  | 3.52         |\n","|    value_loss           | 0.255        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 957     |\n","|    iterations      | 272     |\n","|    time_elapsed    | 3489    |\n","|    total_timesteps | 3342336 |\n","--------------------------------\n","Eval num_timesteps=3348000, episode_reward=10.20 +/- 3.89\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 10.2         |\n","| time/                   |              |\n","|    total_timesteps      | 3348000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0034504037 |\n","|    clip_fraction        | 0.131        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.7        |\n","|    explained_variance   | 0.373        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.186        |\n","|    n_updates            | 1360         |\n","|    policy_gradient_loss | -0.00292     |\n","|    std                  | 3.55         |\n","|    value_loss           | 15.3         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 957     |\n","|    iterations      | 273     |\n","|    time_elapsed    | 3504    |\n","|    total_timesteps | 3354624 |\n","--------------------------------\n","Eval num_timesteps=3360000, episode_reward=6.42 +/- 1.91\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 6.42         |\n","| time/                   |              |\n","|    total_timesteps      | 3360000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019539443 |\n","|    clip_fraction        | 0.103        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.7        |\n","|    explained_variance   | -0.391       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.102       |\n","|    n_updates            | 1365         |\n","|    policy_gradient_loss | -0.00406     |\n","|    std                  | 3.54         |\n","|    value_loss           | 0.356        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 957     |\n","|    iterations      | 274     |\n","|    time_elapsed    | 3515    |\n","|    total_timesteps | 3366912 |\n","--------------------------------\n","Eval num_timesteps=3372000, episode_reward=8.94 +/- 5.14\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 8.94         |\n","| time/                   |              |\n","|    total_timesteps      | 3372000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021036544 |\n","|    clip_fraction        | 0.104        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.7        |\n","|    explained_variance   | 0.086        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.104       |\n","|    n_updates            | 1370         |\n","|    policy_gradient_loss | -0.00366     |\n","|    std                  | 3.55         |\n","|    value_loss           | 0.237        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 957     |\n","|    iterations      | 275     |\n","|    time_elapsed    | 3528    |\n","|    total_timesteps | 3379200 |\n","--------------------------------\n","Eval num_timesteps=3384000, episode_reward=9.12 +/- 2.57\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 9.12         |\n","| time/                   |              |\n","|    total_timesteps      | 3384000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0023866901 |\n","|    clip_fraction        | 0.108        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.7        |\n","|    explained_variance   | 0.195        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.106       |\n","|    n_updates            | 1375         |\n","|    policy_gradient_loss | -0.00373     |\n","|    std                  | 3.55         |\n","|    value_loss           | 0.247        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 957     |\n","|    iterations      | 276     |\n","|    time_elapsed    | 3542    |\n","|    total_timesteps | 3391488 |\n","--------------------------------\n","Eval num_timesteps=3396000, episode_reward=9.04 +/- 2.75\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 9.04         |\n","| time/                   |              |\n","|    total_timesteps      | 3396000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020150414 |\n","|    clip_fraction        | 0.107        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.7        |\n","|    explained_variance   | 0.149        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.103       |\n","|    n_updates            | 1380         |\n","|    policy_gradient_loss | -0.00367     |\n","|    std                  | 3.56         |\n","|    value_loss           | 0.233        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 957     |\n","|    iterations      | 277     |\n","|    time_elapsed    | 3553    |\n","|    total_timesteps | 3403776 |\n","--------------------------------\n","Eval num_timesteps=3408000, episode_reward=5.88 +/- 2.15\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 5.88        |\n","| time/                   |             |\n","|    total_timesteps      | 3408000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002564219 |\n","|    clip_fraction        | 0.117       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10.8       |\n","|    explained_variance   | 0.0346      |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.103      |\n","|    n_updates            | 1385        |\n","|    policy_gradient_loss | -0.00433    |\n","|    std                  | 3.57        |\n","|    value_loss           | 0.228       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 957     |\n","|    iterations      | 278     |\n","|    time_elapsed    | 3568    |\n","|    total_timesteps | 3416064 |\n","--------------------------------\n","Eval num_timesteps=3420000, episode_reward=9.10 +/- 2.64\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 9.1          |\n","| time/                   |              |\n","|    total_timesteps      | 3420000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020707243 |\n","|    clip_fraction        | 0.112        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.8        |\n","|    explained_variance   | 0.1          |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.104       |\n","|    n_updates            | 1390         |\n","|    policy_gradient_loss | -0.00299     |\n","|    std                  | 3.59         |\n","|    value_loss           | 0.226        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 957     |\n","|    iterations      | 279     |\n","|    time_elapsed    | 3580    |\n","|    total_timesteps | 3428352 |\n","--------------------------------\n","Eval num_timesteps=3432000, episode_reward=14.81 +/- 0.91\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 14.8         |\n","| time/                   |              |\n","|    total_timesteps      | 3432000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020895218 |\n","|    clip_fraction        | 0.11         |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.8        |\n","|    explained_variance   | 0.112        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.103       |\n","|    n_updates            | 1395         |\n","|    policy_gradient_loss | -0.00358     |\n","|    std                  | 3.58         |\n","|    value_loss           | 0.217        |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 957     |\n","|    iterations      | 280     |\n","|    time_elapsed    | 3593    |\n","|    total_timesteps | 3440640 |\n","--------------------------------\n","Eval num_timesteps=3444000, episode_reward=13.72 +/- 0.10\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 13.7         |\n","| time/                   |              |\n","|    total_timesteps      | 3444000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0033372734 |\n","|    clip_fraction        | 0.135        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.8        |\n","|    explained_variance   | 0.486        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.528        |\n","|    n_updates            | 1400         |\n","|    policy_gradient_loss | -0.00262     |\n","|    std                  | 3.62         |\n","|    value_loss           | 15           |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 956     |\n","|    iterations      | 281     |\n","|    time_elapsed    | 3608    |\n","|    total_timesteps | 3452928 |\n","--------------------------------\n","Eval num_timesteps=3456000, episode_reward=8.20 +/- 1.18\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 8.2         |\n","| time/                   |             |\n","|    total_timesteps      | 3456000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002166792 |\n","|    clip_fraction        | 0.0951      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10.8       |\n","|    explained_variance   | -0.0904     |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.106      |\n","|    n_updates            | 1405        |\n","|    policy_gradient_loss | -0.0047     |\n","|    std                  | 3.62        |\n","|    value_loss           | 0.27        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 957     |\n","|    iterations      | 282     |\n","|    time_elapsed    | 3619    |\n","|    total_timesteps | 3465216 |\n","--------------------------------\n","Eval num_timesteps=3468000, episode_reward=14.47 +/- 2.04\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 14.5         |\n","| time/                   |              |\n","|    total_timesteps      | 3468000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0025251938 |\n","|    clip_fraction        | 0.116        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.8        |\n","|    explained_variance   | 0.084        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.101       |\n","|    n_updates            | 1410         |\n","|    policy_gradient_loss | -0.00398     |\n","|    std                  | 3.62         |\n","|    value_loss           | 0.21         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 956     |\n","|    iterations      | 283     |\n","|    time_elapsed    | 3633    |\n","|    total_timesteps | 3477504 |\n","--------------------------------\n","Eval num_timesteps=3480000, episode_reward=9.59 +/- 1.05\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 9.59         |\n","| time/                   |              |\n","|    total_timesteps      | 3480000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020312287 |\n","|    clip_fraction        | 0.0847       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.8        |\n","|    explained_variance   | 0.156        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.104       |\n","|    n_updates            | 1415         |\n","|    policy_gradient_loss | -0.00297     |\n","|    std                  | 3.64         |\n","|    value_loss           | 0.226        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 956     |\n","|    iterations      | 284     |\n","|    time_elapsed    | 3647    |\n","|    total_timesteps | 3489792 |\n","--------------------------------\n","Eval num_timesteps=3492000, episode_reward=11.83 +/- 0.39\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 11.8         |\n","| time/                   |              |\n","|    total_timesteps      | 3492000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0034143508 |\n","|    clip_fraction        | 0.147        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.9        |\n","|    explained_variance   | 0.52         |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.058       |\n","|    n_updates            | 1420         |\n","|    policy_gradient_loss | -0.00339     |\n","|    std                  | 3.69         |\n","|    value_loss           | 15           |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 956     |\n","|    iterations      | 285     |\n","|    time_elapsed    | 3659    |\n","|    total_timesteps | 3502080 |\n","--------------------------------\n","Eval num_timesteps=3504000, episode_reward=6.86 +/- 1.21\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 6.86         |\n","| time/                   |              |\n","|    total_timesteps      | 3504000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020899375 |\n","|    clip_fraction        | 0.0867       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.9        |\n","|    explained_variance   | -0.0242      |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0992      |\n","|    n_updates            | 1425         |\n","|    policy_gradient_loss | -0.00418     |\n","|    std                  | 3.69         |\n","|    value_loss           | 0.308        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 956     |\n","|    iterations      | 286     |\n","|    time_elapsed    | 3675    |\n","|    total_timesteps | 3514368 |\n","--------------------------------\n","Eval num_timesteps=3516000, episode_reward=6.94 +/- 0.22\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 6.94       |\n","| time/                   |            |\n","|    total_timesteps      | 3516000    |\n","| train/                  |            |\n","|    approx_kl            | 0.00233444 |\n","|    clip_fraction        | 0.107      |\n","|    clip_range           | 0.1        |\n","|    entropy_loss         | -10.9      |\n","|    explained_variance   | 0.212      |\n","|    learning_rate        | 0.000622   |\n","|    loss                 | -0.104     |\n","|    n_updates            | 1430       |\n","|    policy_gradient_loss | -0.00411   |\n","|    std                  | 3.7        |\n","|    value_loss           | 0.225      |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 955     |\n","|    iterations      | 287     |\n","|    time_elapsed    | 3689    |\n","|    total_timesteps | 3526656 |\n","--------------------------------\n","Eval num_timesteps=3528000, episode_reward=6.32 +/- 0.24\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 6.32         |\n","| time/                   |              |\n","|    total_timesteps      | 3528000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0023673044 |\n","|    clip_fraction        | 0.105        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.9        |\n","|    explained_variance   | 0.162        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.111       |\n","|    n_updates            | 1435         |\n","|    policy_gradient_loss | -0.00408     |\n","|    std                  | 3.71         |\n","|    value_loss           | 0.213        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 955     |\n","|    iterations      | 288     |\n","|    time_elapsed    | 3702    |\n","|    total_timesteps | 3538944 |\n","--------------------------------\n","Eval num_timesteps=3540000, episode_reward=10.67 +/- 0.74\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 10.7         |\n","| time/                   |              |\n","|    total_timesteps      | 3540000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022292903 |\n","|    clip_fraction        | 0.105        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.9        |\n","|    explained_variance   | 0.0997       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.11        |\n","|    n_updates            | 1440         |\n","|    policy_gradient_loss | -0.00408     |\n","|    std                  | 3.73         |\n","|    value_loss           | 0.229        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 955     |\n","|    iterations      | 289     |\n","|    time_elapsed    | 3718    |\n","|    total_timesteps | 3551232 |\n","--------------------------------\n","Eval num_timesteps=3552000, episode_reward=12.19 +/- 4.41\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 12.2         |\n","| time/                   |              |\n","|    total_timesteps      | 3552000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0025503219 |\n","|    clip_fraction        | 0.0984       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.9        |\n","|    explained_variance   | 0.191        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.106       |\n","|    n_updates            | 1445         |\n","|    policy_gradient_loss | -0.00313     |\n","|    std                  | 3.74         |\n","|    value_loss           | 0.268        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 954     |\n","|    iterations      | 290     |\n","|    time_elapsed    | 3732    |\n","|    total_timesteps | 3563520 |\n","--------------------------------\n","Eval num_timesteps=3564000, episode_reward=10.39 +/- 1.83\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 10.4        |\n","| time/                   |             |\n","|    total_timesteps      | 3564000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002059627 |\n","|    clip_fraction        | 0.1         |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11         |\n","|    explained_variance   | 0.0921      |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.103      |\n","|    n_updates            | 1450        |\n","|    policy_gradient_loss | -0.00328    |\n","|    std                  | 3.77        |\n","|    value_loss           | 0.262       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 954     |\n","|    iterations      | 291     |\n","|    time_elapsed    | 3745    |\n","|    total_timesteps | 3575808 |\n","--------------------------------\n","Eval num_timesteps=3576000, episode_reward=15.28 +/- 3.36\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 15.3         |\n","| time/                   |              |\n","|    total_timesteps      | 3576000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021287806 |\n","|    clip_fraction        | 0.117        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11          |\n","|    explained_variance   | 0.163        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.108       |\n","|    n_updates            | 1455         |\n","|    policy_gradient_loss | -0.00288     |\n","|    std                  | 3.8          |\n","|    value_loss           | 0.241        |\n","------------------------------------------\n","New best mean reward!\n","Eval num_timesteps=3588000, episode_reward=10.45 +/- 1.08\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 10.4     |\n","| time/              |          |\n","|    total_timesteps | 3588000  |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 952     |\n","|    iterations      | 292     |\n","|    time_elapsed    | 3767    |\n","|    total_timesteps | 3588096 |\n","--------------------------------\n","Eval num_timesteps=3600000, episode_reward=12.78 +/- 0.11\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 12.8         |\n","| time/                   |              |\n","|    total_timesteps      | 3600000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0023145725 |\n","|    clip_fraction        | 0.0964       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11          |\n","|    explained_variance   | 0.157        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0966      |\n","|    n_updates            | 1460         |\n","|    policy_gradient_loss | -0.00326     |\n","|    std                  | 3.81         |\n","|    value_loss           | 0.392        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 951     |\n","|    iterations      | 293     |\n","|    time_elapsed    | 3783    |\n","|    total_timesteps | 3600384 |\n","--------------------------------\n","Eval num_timesteps=3612000, episode_reward=13.42 +/- 2.48\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 13.4         |\n","| time/                   |              |\n","|    total_timesteps      | 3612000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0026056923 |\n","|    clip_fraction        | 0.105        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11          |\n","|    explained_variance   | 0.152        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.105       |\n","|    n_updates            | 1465         |\n","|    policy_gradient_loss | -0.00364     |\n","|    std                  | 3.82         |\n","|    value_loss           | 0.298        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 951     |\n","|    iterations      | 294     |\n","|    time_elapsed    | 3798    |\n","|    total_timesteps | 3612672 |\n","--------------------------------\n","Eval num_timesteps=3624000, episode_reward=15.44 +/- 1.04\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 15.4         |\n","| time/                   |              |\n","|    total_timesteps      | 3624000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019865278 |\n","|    clip_fraction        | 0.108        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11          |\n","|    explained_variance   | 0.249        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.104       |\n","|    n_updates            | 1470         |\n","|    policy_gradient_loss | -0.00397     |\n","|    std                  | 3.83         |\n","|    value_loss           | 0.289        |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 951     |\n","|    iterations      | 295     |\n","|    time_elapsed    | 3810    |\n","|    total_timesteps | 3624960 |\n","--------------------------------\n","Eval num_timesteps=3636000, episode_reward=17.07 +/- 1.06\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 17.1         |\n","| time/                   |              |\n","|    total_timesteps      | 3636000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021127595 |\n","|    clip_fraction        | 0.0812       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11          |\n","|    explained_variance   | 0.165        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.104       |\n","|    n_updates            | 1475         |\n","|    policy_gradient_loss | -0.00391     |\n","|    std                  | 3.83         |\n","|    value_loss           | 0.264        |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 950     |\n","|    iterations      | 296     |\n","|    time_elapsed    | 3826    |\n","|    total_timesteps | 3637248 |\n","--------------------------------\n","Eval num_timesteps=3648000, episode_reward=17.83 +/- 0.44\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 17.8        |\n","| time/                   |             |\n","|    total_timesteps      | 3648000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002231676 |\n","|    clip_fraction        | 0.091       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11         |\n","|    explained_variance   | 0.149       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.103      |\n","|    n_updates            | 1480        |\n","|    policy_gradient_loss | -0.00378    |\n","|    std                  | 3.85        |\n","|    value_loss           | 0.262       |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 950     |\n","|    iterations      | 297     |\n","|    time_elapsed    | 3839    |\n","|    total_timesteps | 3649536 |\n","--------------------------------\n","Eval num_timesteps=3660000, episode_reward=18.35 +/- 0.66\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 18.3         |\n","| time/                   |              |\n","|    total_timesteps      | 3660000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0023558883 |\n","|    clip_fraction        | 0.0952       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.1        |\n","|    explained_variance   | 0.259        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.107       |\n","|    n_updates            | 1485         |\n","|    policy_gradient_loss | -0.0038      |\n","|    std                  | 3.87         |\n","|    value_loss           | 0.267        |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 950     |\n","|    iterations      | 298     |\n","|    time_elapsed    | 3852    |\n","|    total_timesteps | 3661824 |\n","--------------------------------\n","Eval num_timesteps=3672000, episode_reward=17.31 +/- 0.44\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 17.3         |\n","| time/                   |              |\n","|    total_timesteps      | 3672000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019994082 |\n","|    clip_fraction        | 0.0937       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.1        |\n","|    explained_variance   | -0.424       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.1         |\n","|    n_updates            | 1490         |\n","|    policy_gradient_loss | -0.00431     |\n","|    std                  | 3.86         |\n","|    value_loss           | 0.439        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 949     |\n","|    iterations      | 299     |\n","|    time_elapsed    | 3868    |\n","|    total_timesteps | 3674112 |\n","--------------------------------\n","Eval num_timesteps=3684000, episode_reward=17.96 +/- 0.25\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 18           |\n","| time/                   |              |\n","|    total_timesteps      | 3684000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0023299055 |\n","|    clip_fraction        | 0.101        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.1        |\n","|    explained_variance   | 0.214        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.103       |\n","|    n_updates            | 1495         |\n","|    policy_gradient_loss | -0.0033      |\n","|    std                  | 3.88         |\n","|    value_loss           | 0.287        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 949     |\n","|    iterations      | 300     |\n","|    time_elapsed    | 3881    |\n","|    total_timesteps | 3686400 |\n","--------------------------------\n","Eval num_timesteps=3696000, episode_reward=16.32 +/- 0.36\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 16.3         |\n","| time/                   |              |\n","|    total_timesteps      | 3696000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022321427 |\n","|    clip_fraction        | 0.105        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.1        |\n","|    explained_variance   | 0.183        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0994      |\n","|    n_updates            | 1500         |\n","|    policy_gradient_loss | -0.00297     |\n","|    std                  | 3.9          |\n","|    value_loss           | 0.279        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 949     |\n","|    iterations      | 301     |\n","|    time_elapsed    | 3894    |\n","|    total_timesteps | 3698688 |\n","--------------------------------\n","Eval num_timesteps=3708000, episode_reward=19.62 +/- 0.72\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 19.6         |\n","| time/                   |              |\n","|    total_timesteps      | 3708000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0024201667 |\n","|    clip_fraction        | 0.109        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.1        |\n","|    explained_variance   | 0.263        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.104       |\n","|    n_updates            | 1505         |\n","|    policy_gradient_loss | -0.00365     |\n","|    std                  | 3.92         |\n","|    value_loss           | 0.269        |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 948     |\n","|    iterations      | 302     |\n","|    time_elapsed    | 3910    |\n","|    total_timesteps | 3710976 |\n","--------------------------------\n","Eval num_timesteps=3720000, episode_reward=17.76 +/- 0.64\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 17.8         |\n","| time/                   |              |\n","|    total_timesteps      | 3720000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022839599 |\n","|    clip_fraction        | 0.101        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.1        |\n","|    explained_variance   | 0.239        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.107       |\n","|    n_updates            | 1510         |\n","|    policy_gradient_loss | -0.00425     |\n","|    std                  | 3.94         |\n","|    value_loss           | 0.306        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 949     |\n","|    iterations      | 303     |\n","|    time_elapsed    | 3922    |\n","|    total_timesteps | 3723264 |\n","--------------------------------\n","Eval num_timesteps=3732000, episode_reward=17.85 +/- 1.87\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 17.9         |\n","| time/                   |              |\n","|    total_timesteps      | 3732000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0031589626 |\n","|    clip_fraction        | 0.135        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.2        |\n","|    explained_variance   | 0.385        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0513      |\n","|    n_updates            | 1515         |\n","|    policy_gradient_loss | -0.00367     |\n","|    std                  | 4            |\n","|    value_loss           | 6.17         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 948     |\n","|    iterations      | 304     |\n","|    time_elapsed    | 3936    |\n","|    total_timesteps | 3735552 |\n","--------------------------------\n","Eval num_timesteps=3744000, episode_reward=15.85 +/- 1.94\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 15.8         |\n","| time/                   |              |\n","|    total_timesteps      | 3744000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019533269 |\n","|    clip_fraction        | 0.0904       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.2        |\n","|    explained_variance   | -0.693       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.104       |\n","|    n_updates            | 1520         |\n","|    policy_gradient_loss | -0.00423     |\n","|    std                  | 4.01         |\n","|    value_loss           | 0.417        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 948     |\n","|    iterations      | 305     |\n","|    time_elapsed    | 3951    |\n","|    total_timesteps | 3747840 |\n","--------------------------------\n","Eval num_timesteps=3756000, episode_reward=17.35 +/- 0.86\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 17.4         |\n","| time/                   |              |\n","|    total_timesteps      | 3756000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0029907587 |\n","|    clip_fraction        | 0.118        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.2        |\n","|    explained_variance   | 0.655        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.94         |\n","|    n_updates            | 1525         |\n","|    policy_gradient_loss | -0.003       |\n","|    std                  | 4.05         |\n","|    value_loss           | 14.6         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 948     |\n","|    iterations      | 306     |\n","|    time_elapsed    | 3962    |\n","|    total_timesteps | 3760128 |\n","--------------------------------\n","Eval num_timesteps=3768000, episode_reward=15.77 +/- 1.49\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 15.8         |\n","| time/                   |              |\n","|    total_timesteps      | 3768000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018012259 |\n","|    clip_fraction        | 0.0857       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.3        |\n","|    explained_variance   | -0.851       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.107       |\n","|    n_updates            | 1530         |\n","|    policy_gradient_loss | -0.00339     |\n","|    std                  | 4.06         |\n","|    value_loss           | 0.358        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 948     |\n","|    iterations      | 307     |\n","|    time_elapsed    | 3977    |\n","|    total_timesteps | 3772416 |\n","--------------------------------\n","Eval num_timesteps=3780000, episode_reward=20.53 +/- 1.62\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 20.5         |\n","| time/                   |              |\n","|    total_timesteps      | 3780000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0024406624 |\n","|    clip_fraction        | 0.13         |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.3        |\n","|    explained_variance   | 0.589        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 1.22         |\n","|    n_updates            | 1535         |\n","|    policy_gradient_loss | -0.00331     |\n","|    std                  | 4.09         |\n","|    value_loss           | 29.1         |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 948     |\n","|    iterations      | 308     |\n","|    time_elapsed    | 3990    |\n","|    total_timesteps | 3784704 |\n","--------------------------------\n","Eval num_timesteps=3792000, episode_reward=16.78 +/- 1.08\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 16.8         |\n","| time/                   |              |\n","|    total_timesteps      | 3792000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022418548 |\n","|    clip_fraction        | 0.0947       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.3        |\n","|    explained_variance   | -0.00712     |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0641      |\n","|    n_updates            | 1540         |\n","|    policy_gradient_loss | -0.00394     |\n","|    std                  | 4.12         |\n","|    value_loss           | 2.08         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 948     |\n","|    iterations      | 309     |\n","|    time_elapsed    | 4002    |\n","|    total_timesteps | 3796992 |\n","--------------------------------\n","Eval num_timesteps=3804000, episode_reward=10.12 +/- 4.80\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 10.1         |\n","| time/                   |              |\n","|    total_timesteps      | 3804000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019900124 |\n","|    clip_fraction        | 0.0896       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.3        |\n","|    explained_variance   | -0.049       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.107       |\n","|    n_updates            | 1545         |\n","|    policy_gradient_loss | -0.00404     |\n","|    std                  | 4.13         |\n","|    value_loss           | 0.271        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 948     |\n","|    iterations      | 310     |\n","|    time_elapsed    | 4017    |\n","|    total_timesteps | 3809280 |\n","--------------------------------\n","Eval num_timesteps=3816000, episode_reward=18.78 +/- 0.79\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 18.8         |\n","| time/                   |              |\n","|    total_timesteps      | 3816000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020107196 |\n","|    clip_fraction        | 0.0953       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.3        |\n","|    explained_variance   | 0.137        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.108       |\n","|    n_updates            | 1550         |\n","|    policy_gradient_loss | -0.00394     |\n","|    std                  | 4.14         |\n","|    value_loss           | 0.266        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 948     |\n","|    iterations      | 311     |\n","|    time_elapsed    | 4028    |\n","|    total_timesteps | 3821568 |\n","--------------------------------\n","Eval num_timesteps=3828000, episode_reward=17.94 +/- 2.84\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 17.9         |\n","| time/                   |              |\n","|    total_timesteps      | 3828000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0023975081 |\n","|    clip_fraction        | 0.105        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.4        |\n","|    explained_variance   | 0.232        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.117       |\n","|    n_updates            | 1555         |\n","|    policy_gradient_loss | -0.00409     |\n","|    std                  | 4.15         |\n","|    value_loss           | 0.235        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 948     |\n","|    iterations      | 312     |\n","|    time_elapsed    | 4042    |\n","|    total_timesteps | 3833856 |\n","--------------------------------\n","Eval num_timesteps=3840000, episode_reward=16.26 +/- 0.62\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 16.3         |\n","| time/                   |              |\n","|    total_timesteps      | 3840000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0017449589 |\n","|    clip_fraction        | 0.0817       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.4        |\n","|    explained_variance   | 0.27         |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.111       |\n","|    n_updates            | 1560         |\n","|    policy_gradient_loss | -0.00338     |\n","|    std                  | 4.16         |\n","|    value_loss           | 0.252        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 948     |\n","|    iterations      | 313     |\n","|    time_elapsed    | 4056    |\n","|    total_timesteps | 3846144 |\n","--------------------------------\n","Eval num_timesteps=3852000, episode_reward=17.88 +/- 2.67\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 17.9         |\n","| time/                   |              |\n","|    total_timesteps      | 3852000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020972537 |\n","|    clip_fraction        | 0.0929       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.4        |\n","|    explained_variance   | 0.232        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.109       |\n","|    n_updates            | 1565         |\n","|    policy_gradient_loss | -0.00447     |\n","|    std                  | 4.16         |\n","|    value_loss           | 0.281        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 948     |\n","|    iterations      | 314     |\n","|    time_elapsed    | 4068    |\n","|    total_timesteps | 3858432 |\n","--------------------------------\n","Eval num_timesteps=3864000, episode_reward=18.35 +/- 1.92\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 18.4         |\n","| time/                   |              |\n","|    total_timesteps      | 3864000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0031799732 |\n","|    clip_fraction        | 0.151        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.4        |\n","|    explained_variance   | 0.511        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.56         |\n","|    n_updates            | 1570         |\n","|    policy_gradient_loss | -0.00296     |\n","|    std                  | 4.18         |\n","|    value_loss           | 15.4         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 947     |\n","|    iterations      | 315     |\n","|    time_elapsed    | 4084    |\n","|    total_timesteps | 3870720 |\n","--------------------------------\n","Eval num_timesteps=3876000, episode_reward=17.48 +/- 0.14\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 17.5         |\n","| time/                   |              |\n","|    total_timesteps      | 3876000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0029211289 |\n","|    clip_fraction        | 0.129        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.4        |\n","|    explained_variance   | 0.516        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.239        |\n","|    n_updates            | 1575         |\n","|    policy_gradient_loss | -0.00284     |\n","|    std                  | 4.21         |\n","|    value_loss           | 14.1         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 946     |\n","|    iterations      | 316     |\n","|    time_elapsed    | 4100    |\n","|    total_timesteps | 3883008 |\n","--------------------------------\n","Eval num_timesteps=3888000, episode_reward=19.30 +/- 0.52\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 19.3         |\n","| time/                   |              |\n","|    total_timesteps      | 3888000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0024809223 |\n","|    clip_fraction        | 0.0993       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.4        |\n","|    explained_variance   | -0.447       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.103       |\n","|    n_updates            | 1580         |\n","|    policy_gradient_loss | -0.00328     |\n","|    std                  | 4.22         |\n","|    value_loss           | 0.487        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 946     |\n","|    iterations      | 317     |\n","|    time_elapsed    | 4113    |\n","|    total_timesteps | 3895296 |\n","--------------------------------\n","Eval num_timesteps=3900000, episode_reward=17.93 +/- 0.82\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 17.9        |\n","| time/                   |             |\n","|    total_timesteps      | 3900000     |\n","| train/                  |             |\n","|    approx_kl            | 0.001985859 |\n","|    clip_fraction        | 0.0952      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11.4       |\n","|    explained_variance   | -0.514      |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0938     |\n","|    n_updates            | 1585        |\n","|    policy_gradient_loss | -0.00297    |\n","|    std                  | 4.23        |\n","|    value_loss           | 1.07        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 946     |\n","|    iterations      | 318     |\n","|    time_elapsed    | 4128    |\n","|    total_timesteps | 3907584 |\n","--------------------------------\n","Eval num_timesteps=3912000, episode_reward=19.11 +/- 0.09\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 19.1         |\n","| time/                   |              |\n","|    total_timesteps      | 3912000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0028739925 |\n","|    clip_fraction        | 0.108        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.5        |\n","|    explained_variance   | 0.561        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.256        |\n","|    n_updates            | 1590         |\n","|    policy_gradient_loss | -0.00377     |\n","|    std                  | 4.28         |\n","|    value_loss           | 25.4         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 945     |\n","|    iterations      | 319     |\n","|    time_elapsed    | 4144    |\n","|    total_timesteps | 3919872 |\n","--------------------------------\n","Eval num_timesteps=3924000, episode_reward=16.87 +/- 2.24\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 16.9         |\n","| time/                   |              |\n","|    total_timesteps      | 3924000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019815688 |\n","|    clip_fraction        | 0.0945       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.5        |\n","|    explained_variance   | -0.36        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.102       |\n","|    n_updates            | 1595         |\n","|    policy_gradient_loss | -0.00437     |\n","|    std                  | 4.28         |\n","|    value_loss           | 0.518        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 945     |\n","|    iterations      | 320     |\n","|    time_elapsed    | 4157    |\n","|    total_timesteps | 3932160 |\n","--------------------------------\n","Eval num_timesteps=3936000, episode_reward=12.08 +/- 1.87\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 12.1         |\n","| time/                   |              |\n","|    total_timesteps      | 3936000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022126453 |\n","|    clip_fraction        | 0.101        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.5        |\n","|    explained_variance   | 0.129        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.106       |\n","|    n_updates            | 1600         |\n","|    policy_gradient_loss | -0.0042      |\n","|    std                  | 4.29         |\n","|    value_loss           | 0.342        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 945     |\n","|    iterations      | 321     |\n","|    time_elapsed    | 4171    |\n","|    total_timesteps | 3944448 |\n","--------------------------------\n","Eval num_timesteps=3948000, episode_reward=14.95 +/- 0.76\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 14.9         |\n","| time/                   |              |\n","|    total_timesteps      | 3948000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022022766 |\n","|    clip_fraction        | 0.106        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.5        |\n","|    explained_variance   | 0.222        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.113       |\n","|    n_updates            | 1605         |\n","|    policy_gradient_loss | -0.00382     |\n","|    std                  | 4.3          |\n","|    value_loss           | 0.359        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 944     |\n","|    iterations      | 322     |\n","|    time_elapsed    | 4187    |\n","|    total_timesteps | 3956736 |\n","--------------------------------\n","Eval num_timesteps=3960000, episode_reward=15.75 +/- 0.90\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 15.8         |\n","| time/                   |              |\n","|    total_timesteps      | 3960000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019905465 |\n","|    clip_fraction        | 0.0993       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.5        |\n","|    explained_variance   | 0.225        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.106       |\n","|    n_updates            | 1610         |\n","|    policy_gradient_loss | -0.0038      |\n","|    std                  | 4.3          |\n","|    value_loss           | 0.273        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 944     |\n","|    iterations      | 323     |\n","|    time_elapsed    | 4200    |\n","|    total_timesteps | 3969024 |\n","--------------------------------\n","Eval num_timesteps=3972000, episode_reward=17.34 +/- 0.76\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 17.3         |\n","| time/                   |              |\n","|    total_timesteps      | 3972000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019165856 |\n","|    clip_fraction        | 0.087        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.5        |\n","|    explained_variance   | 0.298        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.106       |\n","|    n_updates            | 1615         |\n","|    policy_gradient_loss | -0.00378     |\n","|    std                  | 4.32         |\n","|    value_loss           | 0.29         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 944     |\n","|    iterations      | 324     |\n","|    time_elapsed    | 4214    |\n","|    total_timesteps | 3981312 |\n","--------------------------------\n","Eval num_timesteps=3984000, episode_reward=14.53 +/- 3.50\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 14.5         |\n","| time/                   |              |\n","|    total_timesteps      | 3984000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021550003 |\n","|    clip_fraction        | 0.104        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.5        |\n","|    explained_variance   | 0.342        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.109       |\n","|    n_updates            | 1620         |\n","|    policy_gradient_loss | -0.00338     |\n","|    std                  | 4.34         |\n","|    value_loss           | 0.27         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 943     |\n","|    iterations      | 325     |\n","|    time_elapsed    | 4230    |\n","|    total_timesteps | 3993600 |\n","--------------------------------\n","Eval num_timesteps=3996000, episode_reward=17.22 +/- 2.83\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 17.2         |\n","| time/                   |              |\n","|    total_timesteps      | 3996000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0028015077 |\n","|    clip_fraction        | 0.134        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.6        |\n","|    explained_variance   | 0.512        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.554        |\n","|    n_updates            | 1625         |\n","|    policy_gradient_loss | -0.0033      |\n","|    std                  | 4.38         |\n","|    value_loss           | 16.1         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 943     |\n","|    iterations      | 326     |\n","|    time_elapsed    | 4244    |\n","|    total_timesteps | 4005888 |\n","--------------------------------\n","Eval num_timesteps=4008000, episode_reward=12.22 +/- 2.72\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 12.2        |\n","| time/                   |             |\n","|    total_timesteps      | 4008000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002093069 |\n","|    clip_fraction        | 0.0913      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11.6       |\n","|    explained_variance   | -0.18       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.11       |\n","|    n_updates            | 1630        |\n","|    policy_gradient_loss | -0.00414    |\n","|    std                  | 4.38        |\n","|    value_loss           | 0.358       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 943     |\n","|    iterations      | 327     |\n","|    time_elapsed    | 4257    |\n","|    total_timesteps | 4018176 |\n","--------------------------------\n","Eval num_timesteps=4020000, episode_reward=14.61 +/- 3.55\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 14.6         |\n","| time/                   |              |\n","|    total_timesteps      | 4020000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019041332 |\n","|    clip_fraction        | 0.0866       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.6        |\n","|    explained_variance   | 0.212        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.113       |\n","|    n_updates            | 1635         |\n","|    policy_gradient_loss | -0.00312     |\n","|    std                  | 4.38         |\n","|    value_loss           | 0.285        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 943     |\n","|    iterations      | 328     |\n","|    time_elapsed    | 4273    |\n","|    total_timesteps | 4030464 |\n","--------------------------------\n","Eval num_timesteps=4032000, episode_reward=16.33 +/- 0.94\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 16.3         |\n","| time/                   |              |\n","|    total_timesteps      | 4032000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0024220499 |\n","|    clip_fraction        | 0.0958       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.6        |\n","|    explained_variance   | 0.229        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.107       |\n","|    n_updates            | 1640         |\n","|    policy_gradient_loss | -0.00414     |\n","|    std                  | 4.4          |\n","|    value_loss           | 0.29         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 942     |\n","|    iterations      | 329     |\n","|    time_elapsed    | 4287    |\n","|    total_timesteps | 4042752 |\n","--------------------------------\n","Eval num_timesteps=4044000, episode_reward=19.05 +/- 2.08\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 19.1         |\n","| time/                   |              |\n","|    total_timesteps      | 4044000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0023187862 |\n","|    clip_fraction        | 0.12         |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.6        |\n","|    explained_variance   | 0.336        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.113       |\n","|    n_updates            | 1645         |\n","|    policy_gradient_loss | -0.00348     |\n","|    std                  | 4.42         |\n","|    value_loss           | 0.259        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 943     |\n","|    iterations      | 330     |\n","|    time_elapsed    | 4299    |\n","|    total_timesteps | 4055040 |\n","--------------------------------\n","Eval num_timesteps=4056000, episode_reward=19.47 +/- 0.34\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 19.5         |\n","| time/                   |              |\n","|    total_timesteps      | 4056000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021444208 |\n","|    clip_fraction        | 0.0856       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.6        |\n","|    explained_variance   | 0.381        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.107       |\n","|    n_updates            | 1650         |\n","|    policy_gradient_loss | -0.00409     |\n","|    std                  | 4.43         |\n","|    value_loss           | 0.271        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 942     |\n","|    iterations      | 331     |\n","|    time_elapsed    | 4315    |\n","|    total_timesteps | 4067328 |\n","--------------------------------\n","Eval num_timesteps=4068000, episode_reward=21.27 +/- 1.30\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 21.3         |\n","| time/                   |              |\n","|    total_timesteps      | 4068000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020172072 |\n","|    clip_fraction        | 0.106        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.6        |\n","|    explained_variance   | 0.337        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.105       |\n","|    n_updates            | 1655         |\n","|    policy_gradient_loss | -0.00342     |\n","|    std                  | 4.45         |\n","|    value_loss           | 0.305        |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 942     |\n","|    iterations      | 332     |\n","|    time_elapsed    | 4330    |\n","|    total_timesteps | 4079616 |\n","--------------------------------\n","Eval num_timesteps=4080000, episode_reward=20.03 +/- 5.10\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 20          |\n","| time/                   |             |\n","|    total_timesteps      | 4080000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002449503 |\n","|    clip_fraction        | 0.0958      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11.6       |\n","|    explained_variance   | 0.333       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.108      |\n","|    n_updates            | 1660        |\n","|    policy_gradient_loss | -0.00422    |\n","|    std                  | 4.47        |\n","|    value_loss           | 0.301       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 942     |\n","|    iterations      | 333     |\n","|    time_elapsed    | 4342    |\n","|    total_timesteps | 4091904 |\n","--------------------------------\n","Eval num_timesteps=4092000, episode_reward=22.72 +/- 1.55\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 22.7         |\n","| time/                   |              |\n","|    total_timesteps      | 4092000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0024956947 |\n","|    clip_fraction        | 0.104        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.7        |\n","|    explained_variance   | 0.355        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.109       |\n","|    n_updates            | 1665         |\n","|    policy_gradient_loss | -0.00388     |\n","|    std                  | 4.49         |\n","|    value_loss           | 0.284        |\n","------------------------------------------\n","New best mean reward!\n","Eval num_timesteps=4104000, episode_reward=21.79 +/- 0.50\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 21.8     |\n","| time/              |          |\n","|    total_timesteps | 4104000  |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 940     |\n","|    iterations      | 334     |\n","|    time_elapsed    | 4364    |\n","|    total_timesteps | 4104192 |\n","--------------------------------\n","Eval num_timesteps=4116000, episode_reward=22.47 +/- 2.75\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 22.5         |\n","| time/                   |              |\n","|    total_timesteps      | 4116000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0036929424 |\n","|    clip_fraction        | 0.168        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.7        |\n","|    explained_variance   | 0.637        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0565      |\n","|    n_updates            | 1670         |\n","|    policy_gradient_loss | -0.00353     |\n","|    std                  | 4.56         |\n","|    value_loss           | 13.3         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 939     |\n","|    iterations      | 335     |\n","|    time_elapsed    | 4379    |\n","|    total_timesteps | 4116480 |\n","--------------------------------\n","Eval num_timesteps=4128000, episode_reward=18.38 +/- 1.70\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 18.4        |\n","| time/                   |             |\n","|    total_timesteps      | 4128000     |\n","| train/                  |             |\n","|    approx_kl            | 0.003225324 |\n","|    clip_fraction        | 0.139       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11.8       |\n","|    explained_variance   | 0.296       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 0.645       |\n","|    n_updates            | 1675        |\n","|    policy_gradient_loss | -0.00382    |\n","|    std                  | 4.62        |\n","|    value_loss           | 35.9        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 939     |\n","|    iterations      | 336     |\n","|    time_elapsed    | 4395    |\n","|    total_timesteps | 4128768 |\n","--------------------------------\n","Eval num_timesteps=4140000, episode_reward=22.42 +/- 1.78\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 22.4         |\n","| time/                   |              |\n","|    total_timesteps      | 4140000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021387301 |\n","|    clip_fraction        | 0.0819       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.8        |\n","|    explained_variance   | -0.38        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0962      |\n","|    n_updates            | 1680         |\n","|    policy_gradient_loss | -0.00454     |\n","|    std                  | 4.64         |\n","|    value_loss           | 1.28         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 939     |\n","|    iterations      | 337     |\n","|    time_elapsed    | 4407    |\n","|    total_timesteps | 4141056 |\n","--------------------------------\n","Eval num_timesteps=4152000, episode_reward=21.75 +/- 0.21\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 21.8         |\n","| time/                   |              |\n","|    total_timesteps      | 4152000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019555024 |\n","|    clip_fraction        | 0.0949       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.8        |\n","|    explained_variance   | -0.664       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.107       |\n","|    n_updates            | 1685         |\n","|    policy_gradient_loss | -0.00322     |\n","|    std                  | 4.66         |\n","|    value_loss           | 0.348        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 939     |\n","|    iterations      | 338     |\n","|    time_elapsed    | 4422    |\n","|    total_timesteps | 4153344 |\n","--------------------------------\n","Eval num_timesteps=4164000, episode_reward=15.32 +/- 0.19\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 15.3         |\n","| time/                   |              |\n","|    total_timesteps      | 4164000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0017712396 |\n","|    clip_fraction        | 0.0751       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.8        |\n","|    explained_variance   | 0.835        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.084       |\n","|    n_updates            | 1690         |\n","|    policy_gradient_loss | -0.00469     |\n","|    std                  | 4.68         |\n","|    value_loss           | 1            |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 938     |\n","|    iterations      | 339     |\n","|    time_elapsed    | 4438    |\n","|    total_timesteps | 4165632 |\n","--------------------------------\n","Eval num_timesteps=4176000, episode_reward=19.25 +/- 0.69\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 19.3         |\n","| time/                   |              |\n","|    total_timesteps      | 4176000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0027958006 |\n","|    clip_fraction        | 0.129        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.9        |\n","|    explained_variance   | 0.561        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.894        |\n","|    n_updates            | 1695         |\n","|    policy_gradient_loss | -0.00337     |\n","|    std                  | 4.74         |\n","|    value_loss           | 23.9         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 938     |\n","|    iterations      | 340     |\n","|    time_elapsed    | 4450    |\n","|    total_timesteps | 4177920 |\n","--------------------------------\n","Eval num_timesteps=4188000, episode_reward=21.38 +/- 0.16\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 21.4        |\n","| time/                   |             |\n","|    total_timesteps      | 4188000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002004686 |\n","|    clip_fraction        | 0.0964      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11.9       |\n","|    explained_variance   | -0.0317     |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.107      |\n","|    n_updates            | 1700        |\n","|    policy_gradient_loss | -0.00407    |\n","|    std                  | 4.76        |\n","|    value_loss           | 0.591       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 938     |\n","|    iterations      | 341     |\n","|    time_elapsed    | 4464    |\n","|    total_timesteps | 4190208 |\n","--------------------------------\n","Eval num_timesteps=4200000, episode_reward=21.64 +/- 1.08\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 21.6         |\n","| time/                   |              |\n","|    total_timesteps      | 4200000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018730297 |\n","|    clip_fraction        | 0.0892       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.9        |\n","|    explained_variance   | 0.415        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.111       |\n","|    n_updates            | 1705         |\n","|    policy_gradient_loss | -0.00393     |\n","|    std                  | 4.76         |\n","|    value_loss           | 0.483        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 937     |\n","|    iterations      | 342     |\n","|    time_elapsed    | 4480    |\n","|    total_timesteps | 4202496 |\n","--------------------------------\n","Eval num_timesteps=4212000, episode_reward=21.51 +/- 1.16\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 21.5         |\n","| time/                   |              |\n","|    total_timesteps      | 4212000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020860452 |\n","|    clip_fraction        | 0.0973       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.9        |\n","|    explained_variance   | 0.107        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.422        |\n","|    n_updates            | 1710         |\n","|    policy_gradient_loss | -0.00343     |\n","|    std                  | 4.8          |\n","|    value_loss           | 11.8         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 938     |\n","|    iterations      | 343     |\n","|    time_elapsed    | 4492    |\n","|    total_timesteps | 4214784 |\n","--------------------------------\n","Eval num_timesteps=4224000, episode_reward=21.24 +/- 1.05\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 21.2         |\n","| time/                   |              |\n","|    total_timesteps      | 4224000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020619808 |\n","|    clip_fraction        | 0.0985       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.9        |\n","|    explained_variance   | 0.411        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.1         |\n","|    n_updates            | 1715         |\n","|    policy_gradient_loss | -0.00481     |\n","|    std                  | 4.82         |\n","|    value_loss           | 1.57         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 937     |\n","|    iterations      | 344     |\n","|    time_elapsed    | 4507    |\n","|    total_timesteps | 4227072 |\n","--------------------------------\n","Eval num_timesteps=4236000, episode_reward=23.12 +/- 0.40\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 23.1         |\n","| time/                   |              |\n","|    total_timesteps      | 4236000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020441774 |\n","|    clip_fraction        | 0.0851       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12          |\n","|    explained_variance   | -0.332       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.116       |\n","|    n_updates            | 1720         |\n","|    policy_gradient_loss | -0.00475     |\n","|    std                  | 4.83         |\n","|    value_loss           | 0.368        |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 937     |\n","|    iterations      | 345     |\n","|    time_elapsed    | 4522    |\n","|    total_timesteps | 4239360 |\n","--------------------------------\n","Eval num_timesteps=4248000, episode_reward=17.32 +/- 4.25\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 17.3         |\n","| time/                   |              |\n","|    total_timesteps      | 4248000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022013823 |\n","|    clip_fraction        | 0.0926       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12          |\n","|    explained_variance   | 0.168        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0975      |\n","|    n_updates            | 1725         |\n","|    policy_gradient_loss | -0.00446     |\n","|    std                  | 4.84         |\n","|    value_loss           | 0.756        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 937     |\n","|    iterations      | 346     |\n","|    time_elapsed    | 4534    |\n","|    total_timesteps | 4251648 |\n","--------------------------------\n","Eval num_timesteps=4260000, episode_reward=19.31 +/- 2.67\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 19.3         |\n","| time/                   |              |\n","|    total_timesteps      | 4260000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018768637 |\n","|    clip_fraction        | 0.0886       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12          |\n","|    explained_variance   | 0.341        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.107       |\n","|    n_updates            | 1730         |\n","|    policy_gradient_loss | -0.00506     |\n","|    std                  | 4.83         |\n","|    value_loss           | 0.442        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 937     |\n","|    iterations      | 347     |\n","|    time_elapsed    | 4549    |\n","|    total_timesteps | 4263936 |\n","--------------------------------\n","Eval num_timesteps=4272000, episode_reward=22.14 +/- 2.02\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 22.1         |\n","| time/                   |              |\n","|    total_timesteps      | 4272000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020483814 |\n","|    clip_fraction        | 0.0957       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12          |\n","|    explained_variance   | 0.342        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.109       |\n","|    n_updates            | 1735         |\n","|    policy_gradient_loss | -0.00363     |\n","|    std                  | 4.84         |\n","|    value_loss           | 0.326        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 936     |\n","|    iterations      | 348     |\n","|    time_elapsed    | 4565    |\n","|    total_timesteps | 4276224 |\n","--------------------------------\n","Eval num_timesteps=4284000, episode_reward=22.79 +/- 0.45\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 22.8         |\n","| time/                   |              |\n","|    total_timesteps      | 4284000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0024330877 |\n","|    clip_fraction        | 0.1          |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12          |\n","|    explained_variance   | 0.362        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.118       |\n","|    n_updates            | 1740         |\n","|    policy_gradient_loss | -0.00419     |\n","|    std                  | 4.85         |\n","|    value_loss           | 0.284        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 936     |\n","|    iterations      | 349     |\n","|    time_elapsed    | 4577    |\n","|    total_timesteps | 4288512 |\n","--------------------------------\n","Eval num_timesteps=4296000, episode_reward=23.29 +/- 0.97\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 23.3        |\n","| time/                   |             |\n","|    total_timesteps      | 4296000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002281566 |\n","|    clip_fraction        | 0.106       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12         |\n","|    explained_variance   | 0.36        |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.115      |\n","|    n_updates            | 1745        |\n","|    policy_gradient_loss | -0.00402    |\n","|    std                  | 4.88        |\n","|    value_loss           | 0.318       |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 936     |\n","|    iterations      | 350     |\n","|    time_elapsed    | 4591    |\n","|    total_timesteps | 4300800 |\n","--------------------------------\n","Eval num_timesteps=4308000, episode_reward=24.30 +/- 1.01\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 24.3         |\n","| time/                   |              |\n","|    total_timesteps      | 4308000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022370664 |\n","|    clip_fraction        | 0.11         |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12          |\n","|    explained_variance   | 0.439        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.114       |\n","|    n_updates            | 1750         |\n","|    policy_gradient_loss | -0.00372     |\n","|    std                  | 4.9          |\n","|    value_loss           | 0.275        |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 936     |\n","|    iterations      | 351     |\n","|    time_elapsed    | 4607    |\n","|    total_timesteps | 4313088 |\n","--------------------------------\n","Eval num_timesteps=4320000, episode_reward=24.73 +/- 0.15\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 24.7        |\n","| time/                   |             |\n","|    total_timesteps      | 4320000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002422524 |\n","|    clip_fraction        | 0.108       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12         |\n","|    explained_variance   | 0.434       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.116      |\n","|    n_updates            | 1755        |\n","|    policy_gradient_loss | -0.00417    |\n","|    std                  | 4.9         |\n","|    value_loss           | 0.287       |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 936     |\n","|    iterations      | 352     |\n","|    time_elapsed    | 4620    |\n","|    total_timesteps | 4325376 |\n","--------------------------------\n","Eval num_timesteps=4332000, episode_reward=27.95 +/- 1.12\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 28           |\n","| time/                   |              |\n","|    total_timesteps      | 4332000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0025017168 |\n","|    clip_fraction        | 0.0992       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12          |\n","|    explained_variance   | 0.471        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.115       |\n","|    n_updates            | 1760         |\n","|    policy_gradient_loss | -0.00397     |\n","|    std                  | 4.92         |\n","|    value_loss           | 0.275        |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 936     |\n","|    iterations      | 353     |\n","|    time_elapsed    | 4633    |\n","|    total_timesteps | 4337664 |\n","--------------------------------\n","Eval num_timesteps=4344000, episode_reward=25.23 +/- 0.50\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 25.2         |\n","| time/                   |              |\n","|    total_timesteps      | 4344000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019778043 |\n","|    clip_fraction        | 0.0801       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.1        |\n","|    explained_variance   | -0.385       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.103       |\n","|    n_updates            | 1765         |\n","|    policy_gradient_loss | -0.00322     |\n","|    std                  | 4.95         |\n","|    value_loss           | 0.678        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 935     |\n","|    iterations      | 354     |\n","|    time_elapsed    | 4649    |\n","|    total_timesteps | 4349952 |\n","--------------------------------\n","Eval num_timesteps=4356000, episode_reward=24.70 +/- 0.63\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 24.7         |\n","| time/                   |              |\n","|    total_timesteps      | 4356000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022795906 |\n","|    clip_fraction        | 0.0955       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.1        |\n","|    explained_variance   | 0.468        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.115       |\n","|    n_updates            | 1770         |\n","|    policy_gradient_loss | -0.0037      |\n","|    std                  | 4.98         |\n","|    value_loss           | 0.312        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 935     |\n","|    iterations      | 355     |\n","|    time_elapsed    | 4662    |\n","|    total_timesteps | 4362240 |\n","--------------------------------\n","Eval num_timesteps=4368000, episode_reward=25.41 +/- 2.23\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 25.4         |\n","| time/                   |              |\n","|    total_timesteps      | 4368000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022285257 |\n","|    clip_fraction        | 0.0924       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.1        |\n","|    explained_variance   | 0.526        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.115       |\n","|    n_updates            | 1775         |\n","|    policy_gradient_loss | -0.00363     |\n","|    std                  | 4.99         |\n","|    value_loss           | 0.304        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 935     |\n","|    iterations      | 356     |\n","|    time_elapsed    | 4675    |\n","|    total_timesteps | 4374528 |\n","--------------------------------\n","Eval num_timesteps=4380000, episode_reward=25.40 +/- 0.19\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 25.4         |\n","| time/                   |              |\n","|    total_timesteps      | 4380000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0028801651 |\n","|    clip_fraction        | 0.116        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.1        |\n","|    explained_variance   | 0.601        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.114       |\n","|    n_updates            | 1780         |\n","|    policy_gradient_loss | -0.0036      |\n","|    std                  | 5.01         |\n","|    value_loss           | 0.264        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 935     |\n","|    iterations      | 357     |\n","|    time_elapsed    | 4691    |\n","|    total_timesteps | 4386816 |\n","--------------------------------\n","Eval num_timesteps=4392000, episode_reward=28.52 +/- 0.81\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 28.5         |\n","| time/                   |              |\n","|    total_timesteps      | 4392000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022061393 |\n","|    clip_fraction        | 0.0929       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.1        |\n","|    explained_variance   | 0.593        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.119       |\n","|    n_updates            | 1785         |\n","|    policy_gradient_loss | -0.00426     |\n","|    std                  | 5.02         |\n","|    value_loss           | 0.294        |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 935     |\n","|    iterations      | 358     |\n","|    time_elapsed    | 4704    |\n","|    total_timesteps | 4399104 |\n","--------------------------------\n","Eval num_timesteps=4404000, episode_reward=29.67 +/- 2.46\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 29.7        |\n","| time/                   |             |\n","|    total_timesteps      | 4404000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002367835 |\n","|    clip_fraction        | 0.0931      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12.1       |\n","|    explained_variance   | 0.648       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.12       |\n","|    n_updates            | 1790        |\n","|    policy_gradient_loss | -0.00398    |\n","|    std                  | 5.03        |\n","|    value_loss           | 0.273       |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 934     |\n","|    iterations      | 359     |\n","|    time_elapsed    | 4718    |\n","|    total_timesteps | 4411392 |\n","--------------------------------\n","Eval num_timesteps=4416000, episode_reward=28.53 +/- 0.79\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 28.5       |\n","| time/                   |            |\n","|    total_timesteps      | 4416000    |\n","| train/                  |            |\n","|    approx_kl            | 0.00211866 |\n","|    clip_fraction        | 0.0999     |\n","|    clip_range           | 0.1        |\n","|    entropy_loss         | -12.1      |\n","|    explained_variance   | 0.418      |\n","|    learning_rate        | 0.000622   |\n","|    loss                 | -0.11      |\n","|    n_updates            | 1795       |\n","|    policy_gradient_loss | -0.00352   |\n","|    std                  | 5.06       |\n","|    value_loss           | 0.321      |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 934     |\n","|    iterations      | 360     |\n","|    time_elapsed    | 4733    |\n","|    total_timesteps | 4423680 |\n","--------------------------------\n","Eval num_timesteps=4428000, episode_reward=28.76 +/- 0.59\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 28.8         |\n","| time/                   |              |\n","|    total_timesteps      | 4428000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0023673356 |\n","|    clip_fraction        | 0.0884       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.2        |\n","|    explained_variance   | 0.63         |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.114       |\n","|    n_updates            | 1800         |\n","|    policy_gradient_loss | -0.00335     |\n","|    std                  | 5.09         |\n","|    value_loss           | 0.279        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 934     |\n","|    iterations      | 361     |\n","|    time_elapsed    | 4746    |\n","|    total_timesteps | 4435968 |\n","--------------------------------\n","Eval num_timesteps=4440000, episode_reward=28.43 +/- 1.99\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 28.4       |\n","| time/                   |            |\n","|    total_timesteps      | 4440000    |\n","| train/                  |            |\n","|    approx_kl            | 0.00228297 |\n","|    clip_fraction        | 0.0977     |\n","|    clip_range           | 0.1        |\n","|    entropy_loss         | -12.2      |\n","|    explained_variance   | 0.514      |\n","|    learning_rate        | 0.000622   |\n","|    loss                 | -0.111     |\n","|    n_updates            | 1805       |\n","|    policy_gradient_loss | -0.00388   |\n","|    std                  | 5.13       |\n","|    value_loss           | 0.315      |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 934     |\n","|    iterations      | 362     |\n","|    time_elapsed    | 4760    |\n","|    total_timesteps | 4448256 |\n","--------------------------------\n","Eval num_timesteps=4452000, episode_reward=27.77 +/- 1.11\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 27.8         |\n","| time/                   |              |\n","|    total_timesteps      | 4452000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019312967 |\n","|    clip_fraction        | 0.085        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.2        |\n","|    explained_variance   | 0.644        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.117       |\n","|    n_updates            | 1810         |\n","|    policy_gradient_loss | -0.00365     |\n","|    std                  | 5.14         |\n","|    value_loss           | 0.28         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 934     |\n","|    iterations      | 363     |\n","|    time_elapsed    | 4775    |\n","|    total_timesteps | 4460544 |\n","--------------------------------\n","Eval num_timesteps=4464000, episode_reward=30.55 +/- 0.94\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 30.6         |\n","| time/                   |              |\n","|    total_timesteps      | 4464000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0024408184 |\n","|    clip_fraction        | 0.115        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.2        |\n","|    explained_variance   | 0.572        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.11        |\n","|    n_updates            | 1815         |\n","|    policy_gradient_loss | -0.00426     |\n","|    std                  | 5.16         |\n","|    value_loss           | 0.303        |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 933     |\n","|    iterations      | 364     |\n","|    time_elapsed    | 4789    |\n","|    total_timesteps | 4472832 |\n","--------------------------------\n","Eval num_timesteps=4476000, episode_reward=26.71 +/- 8.32\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 26.7        |\n","| time/                   |             |\n","|    total_timesteps      | 4476000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002097184 |\n","|    clip_fraction        | 0.0884      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12.2       |\n","|    explained_variance   | 0.589       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.119      |\n","|    n_updates            | 1820        |\n","|    policy_gradient_loss | -0.00411    |\n","|    std                  | 5.2         |\n","|    value_loss           | 0.319       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 934     |\n","|    iterations      | 365     |\n","|    time_elapsed    | 4801    |\n","|    total_timesteps | 4485120 |\n","--------------------------------\n","Eval num_timesteps=4488000, episode_reward=30.34 +/- 2.91\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 30.3         |\n","| time/                   |              |\n","|    total_timesteps      | 4488000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0025671662 |\n","|    clip_fraction        | 0.152        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.3        |\n","|    explained_variance   | 0.647        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.738        |\n","|    n_updates            | 1825         |\n","|    policy_gradient_loss | -0.00312     |\n","|    std                  | 5.25         |\n","|    value_loss           | 16.8         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 933     |\n","|    iterations      | 366     |\n","|    time_elapsed    | 4817    |\n","|    total_timesteps | 4497408 |\n","--------------------------------\n","Eval num_timesteps=4500000, episode_reward=29.76 +/- 2.76\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 29.8         |\n","| time/                   |              |\n","|    total_timesteps      | 4500000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018309649 |\n","|    clip_fraction        | 0.0706       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.3        |\n","|    explained_variance   | 0.319        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.111       |\n","|    n_updates            | 1830         |\n","|    policy_gradient_loss | -0.0037      |\n","|    std                  | 5.24         |\n","|    value_loss           | 0.397        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 933     |\n","|    iterations      | 367     |\n","|    time_elapsed    | 4831    |\n","|    total_timesteps | 4509696 |\n","--------------------------------\n","Eval num_timesteps=4512000, episode_reward=31.00 +/- 0.21\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 31          |\n","| time/                   |             |\n","|    total_timesteps      | 4512000     |\n","| train/                  |             |\n","|    approx_kl            | 0.001969789 |\n","|    clip_fraction        | 0.0902      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12.3       |\n","|    explained_variance   | 0.484       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.117      |\n","|    n_updates            | 1835        |\n","|    policy_gradient_loss | -0.00378    |\n","|    std                  | 5.24        |\n","|    value_loss           | 0.336       |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 933     |\n","|    iterations      | 368     |\n","|    time_elapsed    | 4843    |\n","|    total_timesteps | 4521984 |\n","--------------------------------\n","Eval num_timesteps=4524000, episode_reward=28.58 +/- 0.44\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 28.6         |\n","| time/                   |              |\n","|    total_timesteps      | 4524000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019980762 |\n","|    clip_fraction        | 0.0961       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.3        |\n","|    explained_variance   | 0.693        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.106       |\n","|    n_updates            | 1840         |\n","|    policy_gradient_loss | -0.00331     |\n","|    std                  | 5.28         |\n","|    value_loss           | 0.277        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 933     |\n","|    iterations      | 369     |\n","|    time_elapsed    | 4859    |\n","|    total_timesteps | 4534272 |\n","--------------------------------\n","Eval num_timesteps=4536000, episode_reward=30.67 +/- 1.31\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 30.7         |\n","| time/                   |              |\n","|    total_timesteps      | 4536000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019245245 |\n","|    clip_fraction        | 0.0852       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.3        |\n","|    explained_variance   | 0.505        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.11        |\n","|    n_updates            | 1845         |\n","|    policy_gradient_loss | -0.00442     |\n","|    std                  | 5.28         |\n","|    value_loss           | 0.347        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 933     |\n","|    iterations      | 370     |\n","|    time_elapsed    | 4872    |\n","|    total_timesteps | 4546560 |\n","--------------------------------\n","Eval num_timesteps=4548000, episode_reward=31.06 +/- 0.29\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 31.1         |\n","| time/                   |              |\n","|    total_timesteps      | 4548000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0027420197 |\n","|    clip_fraction        | 0.133        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.3        |\n","|    explained_variance   | 0.285        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.755        |\n","|    n_updates            | 1850         |\n","|    policy_gradient_loss | -0.00448     |\n","|    std                  | 5.34         |\n","|    value_loss           | 14.2         |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 933     |\n","|    iterations      | 371     |\n","|    time_elapsed    | 4885    |\n","|    total_timesteps | 4558848 |\n","--------------------------------\n","Eval num_timesteps=4560000, episode_reward=23.16 +/- 4.30\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 23.2         |\n","| time/                   |              |\n","|    total_timesteps      | 4560000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0023833315 |\n","|    clip_fraction        | 0.0918       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.4        |\n","|    explained_variance   | -0.378       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.103       |\n","|    n_updates            | 1855         |\n","|    policy_gradient_loss | -0.00498     |\n","|    std                  | 5.35         |\n","|    value_loss           | 0.698        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 932     |\n","|    iterations      | 372     |\n","|    time_elapsed    | 4901    |\n","|    total_timesteps | 4571136 |\n","--------------------------------\n","Eval num_timesteps=4572000, episode_reward=33.35 +/- 1.76\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 33.3         |\n","| time/                   |              |\n","|    total_timesteps      | 4572000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0023692725 |\n","|    clip_fraction        | 0.0992       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.4        |\n","|    explained_variance   | 0.465        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.114       |\n","|    n_updates            | 1860         |\n","|    policy_gradient_loss | -0.00398     |\n","|    std                  | 5.37         |\n","|    value_loss           | 0.302        |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 932     |\n","|    iterations      | 373     |\n","|    time_elapsed    | 4915    |\n","|    total_timesteps | 4583424 |\n","--------------------------------\n","Eval num_timesteps=4584000, episode_reward=29.02 +/- 2.88\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 29           |\n","| time/                   |              |\n","|    total_timesteps      | 4584000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020076477 |\n","|    clip_fraction        | 0.0795       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.4        |\n","|    explained_variance   | 0.517        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.12        |\n","|    n_updates            | 1865         |\n","|    policy_gradient_loss | -0.0037      |\n","|    std                  | 5.41         |\n","|    value_loss           | 0.303        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 932     |\n","|    iterations      | 374     |\n","|    time_elapsed    | 4928    |\n","|    total_timesteps | 4595712 |\n","--------------------------------\n","Eval num_timesteps=4596000, episode_reward=28.45 +/- 2.73\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 28.5         |\n","| time/                   |              |\n","|    total_timesteps      | 4596000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022107286 |\n","|    clip_fraction        | 0.0963       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.4        |\n","|    explained_variance   | 0.559        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.122       |\n","|    n_updates            | 1870         |\n","|    policy_gradient_loss | -0.0036      |\n","|    std                  | 5.45         |\n","|    value_loss           | 0.316        |\n","------------------------------------------\n","Eval num_timesteps=4608000, episode_reward=17.10 +/- 8.35\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 17.1     |\n","| time/              |          |\n","|    total_timesteps | 4608000  |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 930     |\n","|    iterations      | 375     |\n","|    time_elapsed    | 4949    |\n","|    total_timesteps | 4608000 |\n","--------------------------------\n","Eval num_timesteps=4620000, episode_reward=35.73 +/- 0.53\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 35.7         |\n","| time/                   |              |\n","|    total_timesteps      | 4620000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0024777057 |\n","|    clip_fraction        | 0.119        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.5        |\n","|    explained_variance   | 0.402        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.175        |\n","|    n_updates            | 1875         |\n","|    policy_gradient_loss | -0.00369     |\n","|    std                  | 5.5          |\n","|    value_loss           | 16.2         |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 930     |\n","|    iterations      | 376     |\n","|    time_elapsed    | 4965    |\n","|    total_timesteps | 4620288 |\n","--------------------------------\n","Eval num_timesteps=4632000, episode_reward=31.27 +/- 4.28\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 31.3         |\n","| time/                   |              |\n","|    total_timesteps      | 4632000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019859308 |\n","|    clip_fraction        | 0.0959       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.5        |\n","|    explained_variance   | 0.246        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.112       |\n","|    n_updates            | 1880         |\n","|    policy_gradient_loss | -0.00498     |\n","|    std                  | 5.51         |\n","|    value_loss           | 0.7          |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 930     |\n","|    iterations      | 377     |\n","|    time_elapsed    | 4979    |\n","|    total_timesteps | 4632576 |\n","--------------------------------\n","Eval num_timesteps=4644000, episode_reward=32.28 +/- 0.46\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 32.3         |\n","| time/                   |              |\n","|    total_timesteps      | 4644000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018142803 |\n","|    clip_fraction        | 0.0798       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.5        |\n","|    explained_variance   | 0.374        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.115       |\n","|    n_updates            | 1885         |\n","|    policy_gradient_loss | -0.00436     |\n","|    std                  | 5.5          |\n","|    value_loss           | 0.369        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 930     |\n","|    iterations      | 378     |\n","|    time_elapsed    | 4992    |\n","|    total_timesteps | 4644864 |\n","--------------------------------\n","Eval num_timesteps=4656000, episode_reward=26.55 +/- 1.46\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 26.6         |\n","| time/                   |              |\n","|    total_timesteps      | 4656000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020187476 |\n","|    clip_fraction        | 0.0882       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.5        |\n","|    explained_variance   | 0.645        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.116       |\n","|    n_updates            | 1890         |\n","|    policy_gradient_loss | -0.00394     |\n","|    std                  | 5.49         |\n","|    value_loss           | 0.343        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 930     |\n","|    iterations      | 379     |\n","|    time_elapsed    | 5007    |\n","|    total_timesteps | 4657152 |\n","--------------------------------\n","Eval num_timesteps=4668000, episode_reward=37.20 +/- 0.51\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 37.2         |\n","| time/                   |              |\n","|    total_timesteps      | 4668000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018914426 |\n","|    clip_fraction        | 0.0814       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.5        |\n","|    explained_variance   | 0.582        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.119       |\n","|    n_updates            | 1895         |\n","|    policy_gradient_loss | -0.00359     |\n","|    std                  | 5.5          |\n","|    value_loss           | 0.312        |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 929     |\n","|    iterations      | 380     |\n","|    time_elapsed    | 5021    |\n","|    total_timesteps | 4669440 |\n","--------------------------------\n","Eval num_timesteps=4680000, episode_reward=33.76 +/- 0.49\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 33.8        |\n","| time/                   |             |\n","|    total_timesteps      | 4680000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002076581 |\n","|    clip_fraction        | 0.097       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12.5       |\n","|    explained_variance   | 0.773       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.116      |\n","|    n_updates            | 1900        |\n","|    policy_gradient_loss | -0.00396    |\n","|    std                  | 5.53        |\n","|    value_loss           | 0.5         |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 930     |\n","|    iterations      | 381     |\n","|    time_elapsed    | 5033    |\n","|    total_timesteps | 4681728 |\n","--------------------------------\n","Eval num_timesteps=4692000, episode_reward=36.30 +/- 1.49\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 36.3         |\n","| time/                   |              |\n","|    total_timesteps      | 4692000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0028273466 |\n","|    clip_fraction        | 0.121        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.5        |\n","|    explained_variance   | 0.454        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 1.64         |\n","|    n_updates            | 1905         |\n","|    policy_gradient_loss | -0.00236     |\n","|    std                  | 5.59         |\n","|    value_loss           | 12.9         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 929     |\n","|    iterations      | 382     |\n","|    time_elapsed    | 5049    |\n","|    total_timesteps | 4694016 |\n","--------------------------------\n","Eval num_timesteps=4704000, episode_reward=29.96 +/- 14.09\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 30           |\n","| time/                   |              |\n","|    total_timesteps      | 4704000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022293117 |\n","|    clip_fraction        | 0.101        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.5        |\n","|    explained_variance   | 0.673        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.124       |\n","|    n_updates            | 1910         |\n","|    policy_gradient_loss | -0.00453     |\n","|    std                  | 5.58         |\n","|    value_loss           | 0.32         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 929     |\n","|    iterations      | 383     |\n","|    time_elapsed    | 5063    |\n","|    total_timesteps | 4706304 |\n","--------------------------------\n","Eval num_timesteps=4716000, episode_reward=32.69 +/- 1.45\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 32.7         |\n","| time/                   |              |\n","|    total_timesteps      | 4716000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019492187 |\n","|    clip_fraction        | 0.0774       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.5        |\n","|    explained_variance   | 0.691        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.126       |\n","|    n_updates            | 1915         |\n","|    policy_gradient_loss | -0.00376     |\n","|    std                  | 5.61         |\n","|    value_loss           | 0.292        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 929     |\n","|    iterations      | 384     |\n","|    time_elapsed    | 5075    |\n","|    total_timesteps | 4718592 |\n","--------------------------------\n","Eval num_timesteps=4728000, episode_reward=40.11 +/- 2.20\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 40.1         |\n","| time/                   |              |\n","|    total_timesteps      | 4728000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0027213662 |\n","|    clip_fraction        | 0.139        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.6        |\n","|    explained_variance   | 0.644        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.0517       |\n","|    n_updates            | 1920         |\n","|    policy_gradient_loss | -0.00351     |\n","|    std                  | 5.64         |\n","|    value_loss           | 14.8         |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 929     |\n","|    iterations      | 385     |\n","|    time_elapsed    | 5091    |\n","|    total_timesteps | 4730880 |\n","--------------------------------\n","Eval num_timesteps=4740000, episode_reward=37.60 +/- 1.79\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 37.6         |\n","| time/                   |              |\n","|    total_timesteps      | 4740000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018392174 |\n","|    clip_fraction        | 0.0811       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.6        |\n","|    explained_variance   | 0.301        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.121       |\n","|    n_updates            | 1925         |\n","|    policy_gradient_loss | -0.00433     |\n","|    std                  | 5.66         |\n","|    value_loss           | 0.347        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 929     |\n","|    iterations      | 386     |\n","|    time_elapsed    | 5105    |\n","|    total_timesteps | 4743168 |\n","--------------------------------\n","Eval num_timesteps=4752000, episode_reward=32.19 +/- 6.00\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 32.2        |\n","| time/                   |             |\n","|    total_timesteps      | 4752000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002559323 |\n","|    clip_fraction        | 0.0908      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12.6       |\n","|    explained_variance   | 0.31        |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0364     |\n","|    n_updates            | 1930        |\n","|    policy_gradient_loss | -0.00403    |\n","|    std                  | 5.72        |\n","|    value_loss           | 7.67        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 929     |\n","|    iterations      | 387     |\n","|    time_elapsed    | 5117    |\n","|    total_timesteps | 4755456 |\n","--------------------------------\n","Eval num_timesteps=4764000, episode_reward=44.73 +/- 0.34\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 44.7         |\n","| time/                   |              |\n","|    total_timesteps      | 4764000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020218696 |\n","|    clip_fraction        | 0.0911       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.6        |\n","|    explained_variance   | 0.0647       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.117       |\n","|    n_updates            | 1935         |\n","|    policy_gradient_loss | -0.0042      |\n","|    std                  | 5.72         |\n","|    value_loss           | 0.443        |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 928     |\n","|    iterations      | 388     |\n","|    time_elapsed    | 5133    |\n","|    total_timesteps | 4767744 |\n","--------------------------------\n","Eval num_timesteps=4776000, episode_reward=40.06 +/- 1.41\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 40.1         |\n","| time/                   |              |\n","|    total_timesteps      | 4776000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0023899465 |\n","|    clip_fraction        | 0.11         |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.6        |\n","|    explained_variance   | 0.298        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.118       |\n","|    n_updates            | 1940         |\n","|    policy_gradient_loss | -0.00372     |\n","|    std                  | 5.74         |\n","|    value_loss           | 0.572        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 928     |\n","|    iterations      | 389     |\n","|    time_elapsed    | 5148    |\n","|    total_timesteps | 4780032 |\n","--------------------------------\n","Eval num_timesteps=4788000, episode_reward=33.82 +/- 6.38\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 33.8        |\n","| time/                   |             |\n","|    total_timesteps      | 4788000     |\n","| train/                  |             |\n","|    approx_kl            | 0.003608654 |\n","|    clip_fraction        | 0.141       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12.7       |\n","|    explained_variance   | 0.648       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 0.825       |\n","|    n_updates            | 1945        |\n","|    policy_gradient_loss | -0.00425    |\n","|    std                  | 5.84        |\n","|    value_loss           | 18          |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 928     |\n","|    iterations      | 390     |\n","|    time_elapsed    | 5160    |\n","|    total_timesteps | 4792320 |\n","--------------------------------\n","Eval num_timesteps=4800000, episode_reward=42.12 +/- 0.20\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 42.1        |\n","| time/                   |             |\n","|    total_timesteps      | 4800000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002013113 |\n","|    clip_fraction        | 0.0995      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12.7       |\n","|    explained_variance   | 0.411       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.116      |\n","|    n_updates            | 1950        |\n","|    policy_gradient_loss | -0.00447    |\n","|    std                  | 5.85        |\n","|    value_loss           | 0.355       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 928     |\n","|    iterations      | 391     |\n","|    time_elapsed    | 5176    |\n","|    total_timesteps | 4804608 |\n","--------------------------------\n","Eval num_timesteps=4812000, episode_reward=26.53 +/- 9.82\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 26.5         |\n","| time/                   |              |\n","|    total_timesteps      | 4812000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021805353 |\n","|    clip_fraction        | 0.103        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.7        |\n","|    explained_variance   | 0.379        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.116       |\n","|    n_updates            | 1955         |\n","|    policy_gradient_loss | -0.00432     |\n","|    std                  | 5.85         |\n","|    value_loss           | 0.453        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 927     |\n","|    iterations      | 392     |\n","|    time_elapsed    | 5191    |\n","|    total_timesteps | 4816896 |\n","--------------------------------\n","Eval num_timesteps=4824000, episode_reward=38.34 +/- 0.36\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 38.3        |\n","| time/                   |             |\n","|    total_timesteps      | 4824000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002072884 |\n","|    clip_fraction        | 0.0919      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12.7       |\n","|    explained_variance   | 0.353       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.112      |\n","|    n_updates            | 1960        |\n","|    policy_gradient_loss | -0.00495    |\n","|    std                  | 5.88        |\n","|    value_loss           | 0.534       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 928     |\n","|    iterations      | 393     |\n","|    time_elapsed    | 5202    |\n","|    total_timesteps | 4829184 |\n","--------------------------------\n","Eval num_timesteps=4836000, episode_reward=37.95 +/- 0.37\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 38           |\n","| time/                   |              |\n","|    total_timesteps      | 4836000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0027456998 |\n","|    clip_fraction        | 0.118        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.7        |\n","|    explained_variance   | 0.533        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 4.03         |\n","|    n_updates            | 1965         |\n","|    policy_gradient_loss | -0.00399     |\n","|    std                  | 5.9          |\n","|    value_loss           | 53.2         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 927     |\n","|    iterations      | 394     |\n","|    time_elapsed    | 5217    |\n","|    total_timesteps | 4841472 |\n","--------------------------------\n","Eval num_timesteps=4848000, episode_reward=33.36 +/- 0.68\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 33.4         |\n","| time/                   |              |\n","|    total_timesteps      | 4848000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020322131 |\n","|    clip_fraction        | 0.0872       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.8        |\n","|    explained_variance   | 0.361        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.116       |\n","|    n_updates            | 1970         |\n","|    policy_gradient_loss | -0.00465     |\n","|    std                  | 5.89         |\n","|    value_loss           | 0.499        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 927     |\n","|    iterations      | 395     |\n","|    time_elapsed    | 5232    |\n","|    total_timesteps | 4853760 |\n","--------------------------------\n","Eval num_timesteps=4860000, episode_reward=37.32 +/- 3.13\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 37.3        |\n","| time/                   |             |\n","|    total_timesteps      | 4860000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002421017 |\n","|    clip_fraction        | 0.109       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12.8       |\n","|    explained_variance   | 0.402       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.119      |\n","|    n_updates            | 1975        |\n","|    policy_gradient_loss | -0.00372    |\n","|    std                  | 5.92        |\n","|    value_loss           | 0.419       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 927     |\n","|    iterations      | 396     |\n","|    time_elapsed    | 5243    |\n","|    total_timesteps | 4866048 |\n","--------------------------------\n","Eval num_timesteps=4872000, episode_reward=30.59 +/- 9.46\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 30.6         |\n","| time/                   |              |\n","|    total_timesteps      | 4872000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021558069 |\n","|    clip_fraction        | 0.0925       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.8        |\n","|    explained_variance   | 0.49         |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.126       |\n","|    n_updates            | 1980         |\n","|    policy_gradient_loss | -0.00485     |\n","|    std                  | 5.96         |\n","|    value_loss           | 0.314        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 927     |\n","|    iterations      | 397     |\n","|    time_elapsed    | 5259    |\n","|    total_timesteps | 4878336 |\n","--------------------------------\n","Eval num_timesteps=4884000, episode_reward=40.69 +/- 3.94\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 40.7        |\n","| time/                   |             |\n","|    total_timesteps      | 4884000     |\n","| train/                  |             |\n","|    approx_kl            | 0.001970423 |\n","|    clip_fraction        | 0.0914      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12.8       |\n","|    explained_variance   | 0.534       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.12       |\n","|    n_updates            | 1985        |\n","|    policy_gradient_loss | -0.00392    |\n","|    std                  | 6           |\n","|    value_loss           | 0.336       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 927     |\n","|    iterations      | 398     |\n","|    time_elapsed    | 5274    |\n","|    total_timesteps | 4890624 |\n","--------------------------------\n","Eval num_timesteps=4896000, episode_reward=-17.90 +/- 46.75\n","Episode length: 480.80 +/- 15.68\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 481         |\n","|    mean_reward          | -17.9       |\n","| time/                   |             |\n","|    total_timesteps      | 4896000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002343259 |\n","|    clip_fraction        | 0.0953      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12.8       |\n","|    explained_variance   | 0.596       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.119      |\n","|    n_updates            | 1990        |\n","|    policy_gradient_loss | -0.00448    |\n","|    std                  | 6.03        |\n","|    value_loss           | 0.327       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 927     |\n","|    iterations      | 399     |\n","|    time_elapsed    | 5285    |\n","|    total_timesteps | 4902912 |\n","--------------------------------\n","Eval num_timesteps=4908000, episode_reward=37.55 +/- 3.60\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 37.5         |\n","| time/                   |              |\n","|    total_timesteps      | 4908000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020632492 |\n","|    clip_fraction        | 0.0984       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.9        |\n","|    explained_variance   | 0.536        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.124       |\n","|    n_updates            | 1995         |\n","|    policy_gradient_loss | -0.00452     |\n","|    std                  | 6.05         |\n","|    value_loss           | 0.37         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 927     |\n","|    iterations      | 400     |\n","|    time_elapsed    | 5300    |\n","|    total_timesteps | 4915200 |\n","--------------------------------\n","Eval num_timesteps=4920000, episode_reward=36.75 +/- 3.95\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 36.7         |\n","| time/                   |              |\n","|    total_timesteps      | 4920000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020344914 |\n","|    clip_fraction        | 0.0867       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.9        |\n","|    explained_variance   | 0.626        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.123       |\n","|    n_updates            | 2000         |\n","|    policy_gradient_loss | -0.00389     |\n","|    std                  | 6.07         |\n","|    value_loss           | 0.318        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 926     |\n","|    iterations      | 401     |\n","|    time_elapsed    | 5316    |\n","|    total_timesteps | 4927488 |\n","--------------------------------\n","Eval num_timesteps=4932000, episode_reward=33.33 +/- 2.19\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 33.3         |\n","| time/                   |              |\n","|    total_timesteps      | 4932000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022170586 |\n","|    clip_fraction        | 0.0843       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.9        |\n","|    explained_variance   | 0.673        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.119       |\n","|    n_updates            | 2005         |\n","|    policy_gradient_loss | -0.00346     |\n","|    std                  | 6.09         |\n","|    value_loss           | 0.274        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 927     |\n","|    iterations      | 402     |\n","|    time_elapsed    | 5327    |\n","|    total_timesteps | 4939776 |\n","--------------------------------\n","Eval num_timesteps=4944000, episode_reward=39.28 +/- 2.59\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 39.3         |\n","| time/                   |              |\n","|    total_timesteps      | 4944000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0029817016 |\n","|    clip_fraction        | 0.125        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.9        |\n","|    explained_variance   | 0.344        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.0292       |\n","|    n_updates            | 2010         |\n","|    policy_gradient_loss | -0.00352     |\n","|    std                  | 6.19         |\n","|    value_loss           | 15           |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 926     |\n","|    iterations      | 403     |\n","|    time_elapsed    | 5342    |\n","|    total_timesteps | 4952064 |\n","--------------------------------\n","Eval num_timesteps=4956000, episode_reward=40.07 +/- 0.27\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 40.1         |\n","| time/                   |              |\n","|    total_timesteps      | 4956000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019039963 |\n","|    clip_fraction        | 0.0963       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13          |\n","|    explained_variance   | 0.125        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.118       |\n","|    n_updates            | 2015         |\n","|    policy_gradient_loss | -0.00488     |\n","|    std                  | 6.19         |\n","|    value_loss           | 0.445        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 926     |\n","|    iterations      | 404     |\n","|    time_elapsed    | 5358    |\n","|    total_timesteps | 4964352 |\n","--------------------------------\n","Eval num_timesteps=4968000, episode_reward=36.53 +/- 1.02\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 36.5         |\n","| time/                   |              |\n","|    total_timesteps      | 4968000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021109032 |\n","|    clip_fraction        | 0.0945       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13          |\n","|    explained_variance   | 0.629        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.126       |\n","|    n_updates            | 2020         |\n","|    policy_gradient_loss | -0.004       |\n","|    std                  | 6.2          |\n","|    value_loss           | 0.331        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 925     |\n","|    iterations      | 405     |\n","|    time_elapsed    | 5375    |\n","|    total_timesteps | 4976640 |\n","--------------------------------\n","Eval num_timesteps=4980000, episode_reward=36.94 +/- 0.49\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 36.9         |\n","| time/                   |              |\n","|    total_timesteps      | 4980000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0023177706 |\n","|    clip_fraction        | 0.104        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13          |\n","|    explained_variance   | 0.62         |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.127       |\n","|    n_updates            | 2025         |\n","|    policy_gradient_loss | -0.00334     |\n","|    std                  | 6.22         |\n","|    value_loss           | 0.343        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 925     |\n","|    iterations      | 406     |\n","|    time_elapsed    | 5389    |\n","|    total_timesteps | 4988928 |\n","--------------------------------\n","Eval num_timesteps=4992000, episode_reward=35.04 +/- 0.63\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 35           |\n","| time/                   |              |\n","|    total_timesteps      | 4992000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019344952 |\n","|    clip_fraction        | 0.0955       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13          |\n","|    explained_variance   | 0.654        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.126       |\n","|    n_updates            | 2030         |\n","|    policy_gradient_loss | -0.00447     |\n","|    std                  | 6.23         |\n","|    value_loss           | 0.307        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 925     |\n","|    iterations      | 407     |\n","|    time_elapsed    | 5404    |\n","|    total_timesteps | 5001216 |\n","--------------------------------\n"]}],"source":["from stable_baselines3.common.logger import configure\n","\n","tmp_path = \"./multiwalker_ppo_rew_0_log_eval/\"\n","# set up logger\n","new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n","\n","model = PPO(\"MlpPolicy\", env, verbose=3, gamma=0.99, n_steps=512, ent_coef=0.01, learning_rate=0.00062211, vf_coef=0.042202, max_grad_norm=0.9, gae_lambda=0.95, n_epochs=5, clip_range=0.1, batch_size=512)\n","# model.set_logger()\n","model.set_logger(new_logger)\n","model.learn(total_timesteps=5000000,callback=eval_callback)\n","model.save(\"multiwalker_ppo_rew_0\")"]},{"cell_type":"code","source":["from stable_baselines3 import PPO\n","# from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n","from pettingzoo.sisl import multiwalker_v9\n","import supersuit as ss\n","from stable_baselines3.common.monitor import Monitor"],"metadata":{"id":"-9R8WXIQWocM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env = multiwalker_v9.parallel_env(render_mode=\"rgb_array\",shared_reward=0)\n","\n","env = ss.frame_stack_v1(env, 3)\n","env = ss.pettingzoo_env_to_vec_env_v1(env)\n","env = ss.concat_vec_envs_v1(env, 8, num_cpus=4, base_class='stable_baselines3')"],"metadata":{"id":"Edc7cwLdWocO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3.common.callbacks import EvalCallback\n","eval_callback = EvalCallback(env, best_model_save_path=\"./multiwalker_ppo_0_2_log_eval/\",\n","                             log_path=\"./multiwalker_ppo_0_2_log_eval/\", eval_freq=500,\n","                             deterministic=True, render=False)"],"metadata":{"id":"aRb41qpcWocP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3.common.logger import configure\n","\n","tmp_path = \"./multiwalker_ppo_0_2_log_eval/\"\n","# set up logger\n","new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n","\n","model = PPO(\"MlpPolicy\", env, verbose=3, n_steps=512,batch_size=512)\n","# model.set_logger()\n","model.set_logger(new_logger)\n","model.learn(total_timesteps=5000000,callback=eval_callback)\n","model.save(\"multiwalker_ppo_0_2\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698945662730,"user_tz":-60,"elapsed":5783669,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"8fc6882f-df04-4dff-b852-2ae0b5672795","id":"MZBKVR8NWocQ"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n","| time/              |         |\n","|    fps             | 856     |\n","|    iterations      | 230     |\n","|    time_elapsed    | 3300    |\n","|    total_timesteps | 2826240 |\n","--------------------------------\n","Eval num_timesteps=2832000, episode_reward=-55.49 +/- 48.40\n","Episode length: 468.80 +/- 25.47\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 469         |\n","|    mean_reward          | -55.5       |\n","| time/                   |             |\n","|    total_timesteps      | 2832000     |\n","| train/                  |             |\n","|    approx_kl            | 0.005993673 |\n","|    clip_fraction        | 0.0349      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.24       |\n","|    explained_variance   | 0.792       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 18          |\n","|    n_updates            | 2300        |\n","|    policy_gradient_loss | -0.0049     |\n","|    std                  | 0.424       |\n","|    value_loss           | 21.2        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 857     |\n","|    iterations      | 231     |\n","|    time_elapsed    | 3311    |\n","|    total_timesteps | 2838528 |\n","--------------------------------\n","Eval num_timesteps=2844000, episode_reward=10.52 +/- 3.58\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 10.5        |\n","| time/                   |             |\n","|    total_timesteps      | 2844000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010517724 |\n","|    clip_fraction        | 0.104       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.24       |\n","|    explained_variance   | 0.666       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.796       |\n","|    n_updates            | 2310        |\n","|    policy_gradient_loss | -0.00606    |\n","|    std                  | 0.424       |\n","|    value_loss           | 10.9        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 856     |\n","|    iterations      | 232     |\n","|    time_elapsed    | 3326    |\n","|    total_timesteps | 2850816 |\n","--------------------------------\n","Eval num_timesteps=2856000, episode_reward=5.70 +/- 1.66\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 5.7          |\n","| time/                   |              |\n","|    total_timesteps      | 2856000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0127397785 |\n","|    clip_fraction        | 0.16         |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.23        |\n","|    explained_variance   | 0.586        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.109        |\n","|    n_updates            | 2320         |\n","|    policy_gradient_loss | -0.00778     |\n","|    std                  | 0.422        |\n","|    value_loss           | 0.389        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 856     |\n","|    iterations      | 233     |\n","|    time_elapsed    | 3342    |\n","|    total_timesteps | 2863104 |\n","--------------------------------\n","Eval num_timesteps=2868000, episode_reward=4.28 +/- 1.93\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 4.28        |\n","| time/                   |             |\n","|    total_timesteps      | 2868000     |\n","| train/                  |             |\n","|    approx_kl            | 0.005882057 |\n","|    clip_fraction        | 0.0705      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.22       |\n","|    explained_variance   | 0.788       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.92        |\n","|    n_updates            | 2330        |\n","|    policy_gradient_loss | -0.0031     |\n","|    std                  | 0.422       |\n","|    value_loss           | 15.8        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 857     |\n","|    iterations      | 234     |\n","|    time_elapsed    | 3353    |\n","|    total_timesteps | 2875392 |\n","--------------------------------\n","Eval num_timesteps=2880000, episode_reward=-53.91 +/- 50.30\n","Episode length: 465.80 +/- 27.92\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 466        |\n","|    mean_reward          | -53.9      |\n","| time/                   |            |\n","|    total_timesteps      | 2880000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01191339 |\n","|    clip_fraction        | 0.16       |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.21      |\n","|    explained_variance   | 0.424      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.333      |\n","|    n_updates            | 2340       |\n","|    policy_gradient_loss | -0.00418   |\n","|    std                  | 0.419      |\n","|    value_loss           | 1.05       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 857     |\n","|    iterations      | 235     |\n","|    time_elapsed    | 3368    |\n","|    total_timesteps | 2887680 |\n","--------------------------------\n","Eval num_timesteps=2892000, episode_reward=8.17 +/- 3.88\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 8.17        |\n","| time/                   |             |\n","|    total_timesteps      | 2892000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011811745 |\n","|    clip_fraction        | 0.17        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.17       |\n","|    explained_variance   | 0.646       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0962      |\n","|    n_updates            | 2350        |\n","|    policy_gradient_loss | -0.00659    |\n","|    std                  | 0.416       |\n","|    value_loss           | 0.223       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 856     |\n","|    iterations      | 236     |\n","|    time_elapsed    | 3384    |\n","|    total_timesteps | 2899968 |\n","--------------------------------\n","Eval num_timesteps=2904000, episode_reward=3.18 +/- 1.71\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 3.18        |\n","| time/                   |             |\n","|    total_timesteps      | 2904000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012982044 |\n","|    clip_fraction        | 0.159       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.16       |\n","|    explained_variance   | 0.798       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0773      |\n","|    n_updates            | 2360        |\n","|    policy_gradient_loss | -0.00854    |\n","|    std                  | 0.416       |\n","|    value_loss           | 0.201       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 857     |\n","|    iterations      | 237     |\n","|    time_elapsed    | 3397    |\n","|    total_timesteps | 2912256 |\n","--------------------------------\n","Eval num_timesteps=2916000, episode_reward=6.05 +/- 3.53\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 6.05        |\n","| time/                   |             |\n","|    total_timesteps      | 2916000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011876933 |\n","|    clip_fraction        | 0.149       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.15       |\n","|    explained_variance   | 0.81        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0841      |\n","|    n_updates            | 2370        |\n","|    policy_gradient_loss | -0.00706    |\n","|    std                  | 0.413       |\n","|    value_loss           | 0.177       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 857     |\n","|    iterations      | 238     |\n","|    time_elapsed    | 3411    |\n","|    total_timesteps | 2924544 |\n","--------------------------------\n","Eval num_timesteps=2928000, episode_reward=-49.59 +/- 56.54\n","Episode length: 494.60 +/- 4.41\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 495         |\n","|    mean_reward          | -49.6       |\n","| time/                   |             |\n","|    total_timesteps      | 2928000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010902795 |\n","|    clip_fraction        | 0.141       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.14       |\n","|    explained_variance   | 0.715       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0825      |\n","|    n_updates            | 2380        |\n","|    policy_gradient_loss | -0.00671    |\n","|    std                  | 0.413       |\n","|    value_loss           | 0.212       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 856     |\n","|    iterations      | 239     |\n","|    time_elapsed    | 3427    |\n","|    total_timesteps | 2936832 |\n","--------------------------------\n","Eval num_timesteps=2940000, episode_reward=19.66 +/- 8.11\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 19.7        |\n","| time/                   |             |\n","|    total_timesteps      | 2940000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011537492 |\n","|    clip_fraction        | 0.137       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.15       |\n","|    explained_variance   | 0.733       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.101       |\n","|    n_updates            | 2390        |\n","|    policy_gradient_loss | -0.0078     |\n","|    std                  | 0.415       |\n","|    value_loss           | 0.223       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 856     |\n","|    iterations      | 240     |\n","|    time_elapsed    | 3441    |\n","|    total_timesteps | 2949120 |\n","--------------------------------\n","Eval num_timesteps=2952000, episode_reward=2.79 +/- 0.49\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 2.79        |\n","| time/                   |             |\n","|    total_timesteps      | 2952000     |\n","| train/                  |             |\n","|    approx_kl            | 0.006932857 |\n","|    clip_fraction        | 0.099       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.16       |\n","|    explained_variance   | 0.652       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.521       |\n","|    n_updates            | 2400        |\n","|    policy_gradient_loss | -0.00308    |\n","|    std                  | 0.416       |\n","|    value_loss           | 2.11        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 857     |\n","|    iterations      | 241     |\n","|    time_elapsed    | 3454    |\n","|    total_timesteps | 2961408 |\n","--------------------------------\n","Eval num_timesteps=2964000, episode_reward=-31.31 +/- 47.67\n","Episode length: 481.60 +/- 22.54\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 482         |\n","|    mean_reward          | -31.3       |\n","| time/                   |             |\n","|    total_timesteps      | 2964000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013852009 |\n","|    clip_fraction        | 0.139       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.16       |\n","|    explained_variance   | 0.787       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.146       |\n","|    n_updates            | 2410        |\n","|    policy_gradient_loss | -0.00269    |\n","|    std                  | 0.416       |\n","|    value_loss           | 0.833       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 856     |\n","|    iterations      | 242     |\n","|    time_elapsed    | 3471    |\n","|    total_timesteps | 2973696 |\n","--------------------------------\n","Eval num_timesteps=2976000, episode_reward=5.70 +/- 1.65\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 5.7         |\n","| time/                   |             |\n","|    total_timesteps      | 2976000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012952219 |\n","|    clip_fraction        | 0.144       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.15       |\n","|    explained_variance   | 0.347       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.133       |\n","|    n_updates            | 2420        |\n","|    policy_gradient_loss | -0.00555    |\n","|    std                  | 0.413       |\n","|    value_loss           | 0.412       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 856     |\n","|    iterations      | 243     |\n","|    time_elapsed    | 3485    |\n","|    total_timesteps | 2985984 |\n","--------------------------------\n","Eval num_timesteps=2988000, episode_reward=8.46 +/- 0.34\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 8.46       |\n","| time/                   |            |\n","|    total_timesteps      | 2988000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01201452 |\n","|    clip_fraction        | 0.162      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.16      |\n","|    explained_variance   | 0.585      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.143      |\n","|    n_updates            | 2430       |\n","|    policy_gradient_loss | -0.00848   |\n","|    std                  | 0.417      |\n","|    value_loss           | 0.313      |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 857     |\n","|    iterations      | 244     |\n","|    time_elapsed    | 3497    |\n","|    total_timesteps | 2998272 |\n","--------------------------------\n","Eval num_timesteps=3000000, episode_reward=12.93 +/- 8.16\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 12.9        |\n","| time/                   |             |\n","|    total_timesteps      | 3000000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011480798 |\n","|    clip_fraction        | 0.147       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.19       |\n","|    explained_variance   | 0.703       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.076       |\n","|    n_updates            | 2440        |\n","|    policy_gradient_loss | -0.00823    |\n","|    std                  | 0.419       |\n","|    value_loss           | 0.321       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 857     |\n","|    iterations      | 245     |\n","|    time_elapsed    | 3512    |\n","|    total_timesteps | 3010560 |\n","--------------------------------\n","Eval num_timesteps=3012000, episode_reward=29.81 +/- 1.03\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 29.8        |\n","| time/                   |             |\n","|    total_timesteps      | 3012000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008495224 |\n","|    clip_fraction        | 0.112       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.2        |\n","|    explained_variance   | 0.783       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.295       |\n","|    n_updates            | 2450        |\n","|    policy_gradient_loss | -0.00445    |\n","|    std                  | 0.42        |\n","|    value_loss           | 0.97        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 857     |\n","|    iterations      | 246     |\n","|    time_elapsed    | 3525    |\n","|    total_timesteps | 3022848 |\n","--------------------------------\n","Eval num_timesteps=3024000, episode_reward=24.27 +/- 1.70\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 24.3        |\n","| time/                   |             |\n","|    total_timesteps      | 3024000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011337326 |\n","|    clip_fraction        | 0.141       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.2        |\n","|    explained_variance   | 0.886       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.114       |\n","|    n_updates            | 2460        |\n","|    policy_gradient_loss | -0.00687    |\n","|    std                  | 0.42        |\n","|    value_loss           | 0.368       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 857     |\n","|    iterations      | 247     |\n","|    time_elapsed    | 3537    |\n","|    total_timesteps | 3035136 |\n","--------------------------------\n","Eval num_timesteps=3036000, episode_reward=31.22 +/- 0.55\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 31.2        |\n","| time/                   |             |\n","|    total_timesteps      | 3036000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012246355 |\n","|    clip_fraction        | 0.143       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.21       |\n","|    explained_variance   | 0.464       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.214       |\n","|    n_updates            | 2470        |\n","|    policy_gradient_loss | -0.00317    |\n","|    std                  | 0.421       |\n","|    value_loss           | 0.754       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 857     |\n","|    iterations      | 248     |\n","|    time_elapsed    | 3552    |\n","|    total_timesteps | 3047424 |\n","--------------------------------\n","Eval num_timesteps=3048000, episode_reward=28.87 +/- 3.34\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 28.9         |\n","| time/                   |              |\n","|    total_timesteps      | 3048000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0072451816 |\n","|    clip_fraction        | 0.108        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.2         |\n","|    explained_variance   | 0.557        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.355        |\n","|    n_updates            | 2480         |\n","|    policy_gradient_loss | -0.00412     |\n","|    std                  | 0.42         |\n","|    value_loss           | 2.4          |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 858     |\n","|    iterations      | 249     |\n","|    time_elapsed    | 3565    |\n","|    total_timesteps | 3059712 |\n","--------------------------------\n","Eval num_timesteps=3060000, episode_reward=19.22 +/- 10.42\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 19.2        |\n","| time/                   |             |\n","|    total_timesteps      | 3060000     |\n","| train/                  |             |\n","|    approx_kl            | 0.006198276 |\n","|    clip_fraction        | 0.0376      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.2        |\n","|    explained_variance   | 0.893       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 6.98        |\n","|    n_updates            | 2490        |\n","|    policy_gradient_loss | -0.00322    |\n","|    std                  | 0.42        |\n","|    value_loss           | 10.1        |\n","-----------------------------------------\n","Eval num_timesteps=3072000, episode_reward=27.86 +/- 0.24\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 27.9     |\n","| time/              |          |\n","|    total_timesteps | 3072000  |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 857     |\n","|    iterations      | 250     |\n","|    time_elapsed    | 3584    |\n","|    total_timesteps | 3072000 |\n","--------------------------------\n","Eval num_timesteps=3084000, episode_reward=21.05 +/- 9.70\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 21.1         |\n","| time/                   |              |\n","|    total_timesteps      | 3084000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0064878687 |\n","|    clip_fraction        | 0.0429       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.2         |\n","|    explained_variance   | 0.697        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 13.8         |\n","|    n_updates            | 2500         |\n","|    policy_gradient_loss | -0.00307     |\n","|    std                  | 0.42         |\n","|    value_loss           | 15.8         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 857     |\n","|    iterations      | 251     |\n","|    time_elapsed    | 3598    |\n","|    total_timesteps | 3084288 |\n","--------------------------------\n","Eval num_timesteps=3096000, episode_reward=28.83 +/- 4.18\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 28.8        |\n","| time/                   |             |\n","|    total_timesteps      | 3096000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010559742 |\n","|    clip_fraction        | 0.11        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.19       |\n","|    explained_variance   | 0.461       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.565       |\n","|    n_updates            | 2510        |\n","|    policy_gradient_loss | -0.00404    |\n","|    std                  | 0.418       |\n","|    value_loss           | 2.53        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 855     |\n","|    iterations      | 252     |\n","|    time_elapsed    | 3617    |\n","|    total_timesteps | 3096576 |\n","--------------------------------\n","Eval num_timesteps=3108000, episode_reward=17.82 +/- 1.59\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 17.8        |\n","| time/                   |             |\n","|    total_timesteps      | 3108000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012939417 |\n","|    clip_fraction        | 0.179       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.17       |\n","|    explained_variance   | 0.813       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.156       |\n","|    n_updates            | 2520        |\n","|    policy_gradient_loss | -0.00256    |\n","|    std                  | 0.416       |\n","|    value_loss           | 0.594       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 856     |\n","|    iterations      | 253     |\n","|    time_elapsed    | 3629    |\n","|    total_timesteps | 3108864 |\n","--------------------------------\n","Eval num_timesteps=3120000, episode_reward=25.69 +/- 0.85\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 25.7        |\n","| time/                   |             |\n","|    total_timesteps      | 3120000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010602682 |\n","|    clip_fraction        | 0.112       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.16       |\n","|    explained_variance   | 0.472       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.84        |\n","|    n_updates            | 2530        |\n","|    policy_gradient_loss | -0.00228    |\n","|    std                  | 0.416       |\n","|    value_loss           | 10.6        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 856     |\n","|    iterations      | 254     |\n","|    time_elapsed    | 3643    |\n","|    total_timesteps | 3121152 |\n","--------------------------------\n","Eval num_timesteps=3132000, episode_reward=-23.93 +/- 45.80\n","Episode length: 486.80 +/- 10.78\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 487         |\n","|    mean_reward          | -23.9       |\n","| time/                   |             |\n","|    total_timesteps      | 3132000     |\n","| train/                  |             |\n","|    approx_kl            | 0.004519657 |\n","|    clip_fraction        | 0.0344      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.16       |\n","|    explained_variance   | 0.75        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.47        |\n","|    n_updates            | 2540        |\n","|    policy_gradient_loss | -0.00358    |\n","|    std                  | 0.416       |\n","|    value_loss           | 16.6        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 856     |\n","|    iterations      | 255     |\n","|    time_elapsed    | 3658    |\n","|    total_timesteps | 3133440 |\n","--------------------------------\n","Eval num_timesteps=3144000, episode_reward=34.70 +/- 0.44\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 34.7        |\n","| time/                   |             |\n","|    total_timesteps      | 3144000     |\n","| train/                  |             |\n","|    approx_kl            | 0.006704072 |\n","|    clip_fraction        | 0.0685      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.17       |\n","|    explained_variance   | 0.884       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.95        |\n","|    n_updates            | 2550        |\n","|    policy_gradient_loss | -0.00473    |\n","|    std                  | 0.417       |\n","|    value_loss           | 6.14        |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 857     |\n","|    iterations      | 256     |\n","|    time_elapsed    | 3670    |\n","|    total_timesteps | 3145728 |\n","--------------------------------\n","Eval num_timesteps=3156000, episode_reward=29.05 +/- 2.62\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 29.1        |\n","| time/                   |             |\n","|    total_timesteps      | 3156000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008550179 |\n","|    clip_fraction        | 0.0598      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.17       |\n","|    explained_variance   | 0.685       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 15.5        |\n","|    n_updates            | 2560        |\n","|    policy_gradient_loss | -0.00349    |\n","|    std                  | 0.417       |\n","|    value_loss           | 23          |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 857     |\n","|    iterations      | 257     |\n","|    time_elapsed    | 3684    |\n","|    total_timesteps | 3158016 |\n","--------------------------------\n","Eval num_timesteps=3168000, episode_reward=30.83 +/- 0.89\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 30.8         |\n","| time/                   |              |\n","|    total_timesteps      | 3168000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0082588075 |\n","|    clip_fraction        | 0.0906       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.17        |\n","|    explained_variance   | 0.874        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 4.73         |\n","|    n_updates            | 2570         |\n","|    policy_gradient_loss | -0.00457     |\n","|    std                  | 0.417        |\n","|    value_loss           | 11.7         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 856     |\n","|    iterations      | 258     |\n","|    time_elapsed    | 3700    |\n","|    total_timesteps | 3170304 |\n","--------------------------------\n","Eval num_timesteps=3180000, episode_reward=30.39 +/- 3.11\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 30.4        |\n","| time/                   |             |\n","|    total_timesteps      | 3180000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008152805 |\n","|    clip_fraction        | 0.0831      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.18       |\n","|    explained_variance   | 0.919       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 5.32        |\n","|    n_updates            | 2580        |\n","|    policy_gradient_loss | -0.00553    |\n","|    std                  | 0.419       |\n","|    value_loss           | 12.1        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 857     |\n","|    iterations      | 259     |\n","|    time_elapsed    | 3711    |\n","|    total_timesteps | 3182592 |\n","--------------------------------\n","Eval num_timesteps=3192000, episode_reward=26.92 +/- 2.54\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 26.9         |\n","| time/                   |              |\n","|    total_timesteps      | 3192000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0059776083 |\n","|    clip_fraction        | 0.0543       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.19        |\n","|    explained_variance   | 0.885        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 13.3         |\n","|    n_updates            | 2590         |\n","|    policy_gradient_loss | -0.00403     |\n","|    std                  | 0.419        |\n","|    value_loss           | 28.1         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 857     |\n","|    iterations      | 260     |\n","|    time_elapsed    | 3725    |\n","|    total_timesteps | 3194880 |\n","--------------------------------\n","Eval num_timesteps=3204000, episode_reward=13.33 +/- 5.19\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 13.3        |\n","| time/                   |             |\n","|    total_timesteps      | 3204000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013611108 |\n","|    clip_fraction        | 0.131       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.19       |\n","|    explained_variance   | 0.962       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.1         |\n","|    n_updates            | 2600        |\n","|    policy_gradient_loss | -0.00413    |\n","|    std                  | 0.419       |\n","|    value_loss           | 2.43        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 857     |\n","|    iterations      | 261     |\n","|    time_elapsed    | 3739    |\n","|    total_timesteps | 3207168 |\n","--------------------------------\n","Eval num_timesteps=3216000, episode_reward=30.10 +/- 0.82\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 30.1        |\n","| time/                   |             |\n","|    total_timesteps      | 3216000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013757687 |\n","|    clip_fraction        | 0.143       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.19       |\n","|    explained_variance   | 0.612       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.822       |\n","|    n_updates            | 2610        |\n","|    policy_gradient_loss | -0.00322    |\n","|    std                  | 0.417       |\n","|    value_loss           | 3.5         |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 858     |\n","|    iterations      | 262     |\n","|    time_elapsed    | 3750    |\n","|    total_timesteps | 3219456 |\n","--------------------------------\n","Eval num_timesteps=3228000, episode_reward=-74.78 +/- 16.41\n","Episode length: 397.00 +/- 51.44\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 397         |\n","|    mean_reward          | -74.8       |\n","| time/                   |             |\n","|    total_timesteps      | 3228000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009534677 |\n","|    clip_fraction        | 0.121       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.17       |\n","|    explained_variance   | 0.876       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.65        |\n","|    n_updates            | 2620        |\n","|    policy_gradient_loss | -0.00528    |\n","|    std                  | 0.415       |\n","|    value_loss           | 8.47        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 858     |\n","|    iterations      | 263     |\n","|    time_elapsed    | 3765    |\n","|    total_timesteps | 3231744 |\n","--------------------------------\n","Eval num_timesteps=3240000, episode_reward=23.42 +/- 13.37\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 23.4         |\n","| time/                   |              |\n","|    total_timesteps      | 3240000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0076847984 |\n","|    clip_fraction        | 0.0599       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.15        |\n","|    explained_variance   | 0.671        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 26.6         |\n","|    n_updates            | 2630         |\n","|    policy_gradient_loss | -0.00465     |\n","|    std                  | 0.414        |\n","|    value_loss           | 63.5         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 858     |\n","|    iterations      | 264     |\n","|    time_elapsed    | 3778    |\n","|    total_timesteps | 3244032 |\n","--------------------------------\n","Eval num_timesteps=3252000, episode_reward=31.10 +/- 3.44\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 31.1        |\n","| time/                   |             |\n","|    total_timesteps      | 3252000     |\n","| train/                  |             |\n","|    approx_kl            | 0.007423772 |\n","|    clip_fraction        | 0.073       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.15       |\n","|    explained_variance   | 0.902       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 5.96        |\n","|    n_updates            | 2640        |\n","|    policy_gradient_loss | -0.00423    |\n","|    std                  | 0.414       |\n","|    value_loss           | 15.1        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 859     |\n","|    iterations      | 265     |\n","|    time_elapsed    | 3790    |\n","|    total_timesteps | 3256320 |\n","--------------------------------\n","Eval num_timesteps=3264000, episode_reward=29.76 +/- 3.40\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 29.8         |\n","| time/                   |              |\n","|    total_timesteps      | 3264000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0059306137 |\n","|    clip_fraction        | 0.0734       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.15        |\n","|    explained_variance   | 0.928        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 5.73         |\n","|    n_updates            | 2650         |\n","|    policy_gradient_loss | -0.00409     |\n","|    std                  | 0.414        |\n","|    value_loss           | 14.2         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 858     |\n","|    iterations      | 266     |\n","|    time_elapsed    | 3805    |\n","|    total_timesteps | 3268608 |\n","--------------------------------\n","Eval num_timesteps=3276000, episode_reward=31.83 +/- 1.68\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 31.8        |\n","| time/                   |             |\n","|    total_timesteps      | 3276000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009947856 |\n","|    clip_fraction        | 0.117       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.14       |\n","|    explained_variance   | 0.918       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 5.5         |\n","|    n_updates            | 2660        |\n","|    policy_gradient_loss | -0.00515    |\n","|    std                  | 0.413       |\n","|    value_loss           | 20.7        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 859     |\n","|    iterations      | 267     |\n","|    time_elapsed    | 3818    |\n","|    total_timesteps | 3280896 |\n","--------------------------------\n","Eval num_timesteps=3288000, episode_reward=23.35 +/- 13.96\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 23.3        |\n","| time/                   |             |\n","|    total_timesteps      | 3288000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010040036 |\n","|    clip_fraction        | 0.104       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.14       |\n","|    explained_variance   | 0.942       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.87        |\n","|    n_updates            | 2670        |\n","|    policy_gradient_loss | -0.00387    |\n","|    std                  | 0.413       |\n","|    value_loss           | 12.1        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 859     |\n","|    iterations      | 268     |\n","|    time_elapsed    | 3831    |\n","|    total_timesteps | 3293184 |\n","--------------------------------\n","Eval num_timesteps=3300000, episode_reward=32.20 +/- 1.21\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 32.2         |\n","| time/                   |              |\n","|    total_timesteps      | 3300000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0063460935 |\n","|    clip_fraction        | 0.0909       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.14        |\n","|    explained_variance   | 0.941        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 3.66         |\n","|    n_updates            | 2680         |\n","|    policy_gradient_loss | -0.00339     |\n","|    std                  | 0.413        |\n","|    value_loss           | 18.3         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 859     |\n","|    iterations      | 269     |\n","|    time_elapsed    | 3846    |\n","|    total_timesteps | 3305472 |\n","--------------------------------\n","Eval num_timesteps=3312000, episode_reward=30.42 +/- 3.99\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 30.4       |\n","| time/                   |            |\n","|    total_timesteps      | 3312000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01280345 |\n","|    clip_fraction        | 0.151      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.13      |\n","|    explained_variance   | 0.898      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 4.77       |\n","|    n_updates            | 2690       |\n","|    policy_gradient_loss | -0.00425   |\n","|    std                  | 0.412      |\n","|    value_loss           | 17         |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 859     |\n","|    iterations      | 270     |\n","|    time_elapsed    | 3859    |\n","|    total_timesteps | 3317760 |\n","--------------------------------\n","Eval num_timesteps=3324000, episode_reward=20.35 +/- 2.25\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 20.4       |\n","| time/                   |            |\n","|    total_timesteps      | 3324000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01581687 |\n","|    clip_fraction        | 0.174      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.12      |\n","|    explained_variance   | 0.903      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 2.84       |\n","|    n_updates            | 2700       |\n","|    policy_gradient_loss | -0.00356   |\n","|    std                  | 0.41       |\n","|    value_loss           | 5.81       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 859     |\n","|    iterations      | 271     |\n","|    time_elapsed    | 3873    |\n","|    total_timesteps | 3330048 |\n","--------------------------------\n","Eval num_timesteps=3336000, episode_reward=19.94 +/- 2.14\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 19.9        |\n","| time/                   |             |\n","|    total_timesteps      | 3336000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009759164 |\n","|    clip_fraction        | 0.125       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.11       |\n","|    explained_variance   | 0.93        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 9.52        |\n","|    n_updates            | 2710        |\n","|    policy_gradient_loss | -0.00311    |\n","|    std                  | 0.409       |\n","|    value_loss           | 21.4        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 859     |\n","|    iterations      | 272     |\n","|    time_elapsed    | 3889    |\n","|    total_timesteps | 3342336 |\n","--------------------------------\n","Eval num_timesteps=3348000, episode_reward=29.35 +/- 1.38\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 29.4        |\n","| time/                   |             |\n","|    total_timesteps      | 3348000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015635313 |\n","|    clip_fraction        | 0.18        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.09       |\n","|    explained_variance   | 0.922       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.23        |\n","|    n_updates            | 2720        |\n","|    policy_gradient_loss | -0.0042     |\n","|    std                  | 0.407       |\n","|    value_loss           | 5.38        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 859     |\n","|    iterations      | 273     |\n","|    time_elapsed    | 3903    |\n","|    total_timesteps | 3354624 |\n","--------------------------------\n","Eval num_timesteps=3360000, episode_reward=22.59 +/- 2.32\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 22.6        |\n","| time/                   |             |\n","|    total_timesteps      | 3360000     |\n","| train/                  |             |\n","|    approx_kl            | 0.017979832 |\n","|    clip_fraction        | 0.172       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.08       |\n","|    explained_variance   | 0.942       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.8         |\n","|    n_updates            | 2730        |\n","|    policy_gradient_loss | -0.00523    |\n","|    std                  | 0.407       |\n","|    value_loss           | 13.5        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 859     |\n","|    iterations      | 274     |\n","|    time_elapsed    | 3916    |\n","|    total_timesteps | 3366912 |\n","--------------------------------\n","Eval num_timesteps=3372000, episode_reward=16.86 +/- 3.28\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 16.9       |\n","| time/                   |            |\n","|    total_timesteps      | 3372000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01781903 |\n","|    clip_fraction        | 0.209      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.08      |\n","|    explained_variance   | 0.904      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 3.5        |\n","|    n_updates            | 2740       |\n","|    policy_gradient_loss | -0.00398   |\n","|    std                  | 0.407      |\n","|    value_loss           | 6.77       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 859     |\n","|    iterations      | 275     |\n","|    time_elapsed    | 3932    |\n","|    total_timesteps | 3379200 |\n","--------------------------------\n","Eval num_timesteps=3384000, episode_reward=26.80 +/- 1.74\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 26.8        |\n","| time/                   |             |\n","|    total_timesteps      | 3384000     |\n","| train/                  |             |\n","|    approx_kl            | 0.016395805 |\n","|    clip_fraction        | 0.176       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.08       |\n","|    explained_variance   | 0.914       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.62        |\n","|    n_updates            | 2750        |\n","|    policy_gradient_loss | -0.00233    |\n","|    std                  | 0.407       |\n","|    value_loss           | 6.83        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 859     |\n","|    iterations      | 276     |\n","|    time_elapsed    | 3946    |\n","|    total_timesteps | 3391488 |\n","--------------------------------\n","Eval num_timesteps=3396000, episode_reward=26.52 +/- 4.30\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 26.5        |\n","| time/                   |             |\n","|    total_timesteps      | 3396000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012643546 |\n","|    clip_fraction        | 0.166       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.06       |\n","|    explained_variance   | 0.852       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 9.52        |\n","|    n_updates            | 2760        |\n","|    policy_gradient_loss | -0.00239    |\n","|    std                  | 0.403       |\n","|    value_loss           | 20.3        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 859     |\n","|    iterations      | 277     |\n","|    time_elapsed    | 3958    |\n","|    total_timesteps | 3403776 |\n","--------------------------------\n","Eval num_timesteps=3408000, episode_reward=18.42 +/- 10.20\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 18.4         |\n","| time/                   |              |\n","|    total_timesteps      | 3408000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0124952225 |\n","|    clip_fraction        | 0.173        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.04        |\n","|    explained_variance   | 0.905        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 9.09         |\n","|    n_updates            | 2770         |\n","|    policy_gradient_loss | -0.00426     |\n","|    std                  | 0.403        |\n","|    value_loss           | 19           |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 859     |\n","|    iterations      | 278     |\n","|    time_elapsed    | 3973    |\n","|    total_timesteps | 3416064 |\n","--------------------------------\n","Eval num_timesteps=3420000, episode_reward=-8.84 +/- 45.54\n","Episode length: 496.40 +/- 4.41\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 496         |\n","|    mean_reward          | -8.84       |\n","| time/                   |             |\n","|    total_timesteps      | 3420000     |\n","| train/                  |             |\n","|    approx_kl            | 0.023532243 |\n","|    clip_fraction        | 0.306       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.02       |\n","|    explained_variance   | 0.548       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.49        |\n","|    n_updates            | 2780        |\n","|    policy_gradient_loss | -0.0012     |\n","|    std                  | 0.399       |\n","|    value_loss           | 4.11        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 859     |\n","|    iterations      | 279     |\n","|    time_elapsed    | 3987    |\n","|    total_timesteps | 3428352 |\n","--------------------------------\n","Eval num_timesteps=3432000, episode_reward=27.29 +/- 1.24\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 27.3        |\n","| time/                   |             |\n","|    total_timesteps      | 3432000     |\n","| train/                  |             |\n","|    approx_kl            | 0.023829805 |\n","|    clip_fraction        | 0.292       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.99       |\n","|    explained_variance   | 0.704       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.448       |\n","|    n_updates            | 2790        |\n","|    policy_gradient_loss | -0.00195    |\n","|    std                  | 0.398       |\n","|    value_loss           | 1.55        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 859     |\n","|    iterations      | 280     |\n","|    time_elapsed    | 4004    |\n","|    total_timesteps | 3440640 |\n","--------------------------------\n","Eval num_timesteps=3444000, episode_reward=28.55 +/- 2.45\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 28.6        |\n","| time/                   |             |\n","|    total_timesteps      | 3444000     |\n","| train/                  |             |\n","|    approx_kl            | 0.031337366 |\n","|    clip_fraction        | 0.23        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.96       |\n","|    explained_variance   | 0.542       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.282       |\n","|    n_updates            | 2800        |\n","|    policy_gradient_loss | -0.00235    |\n","|    std                  | 0.393       |\n","|    value_loss           | 6.68        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 859     |\n","|    iterations      | 281     |\n","|    time_elapsed    | 4019    |\n","|    total_timesteps | 3452928 |\n","--------------------------------\n","Eval num_timesteps=3456000, episode_reward=27.12 +/- 6.13\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 27.1        |\n","| time/                   |             |\n","|    total_timesteps      | 3456000     |\n","| train/                  |             |\n","|    approx_kl            | 0.026502306 |\n","|    clip_fraction        | 0.28        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.93       |\n","|    explained_variance   | 0.515       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.801       |\n","|    n_updates            | 2810        |\n","|    policy_gradient_loss | 0.0011      |\n","|    std                  | 0.393       |\n","|    value_loss           | 1.46        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 859     |\n","|    iterations      | 282     |\n","|    time_elapsed    | 4032    |\n","|    total_timesteps | 3465216 |\n","--------------------------------\n","Eval num_timesteps=3468000, episode_reward=20.75 +/- 3.24\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 20.7        |\n","| time/                   |             |\n","|    total_timesteps      | 3468000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014043823 |\n","|    clip_fraction        | 0.168       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.95       |\n","|    explained_variance   | 0.699       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.29        |\n","|    n_updates            | 2820        |\n","|    policy_gradient_loss | -0.00335    |\n","|    std                  | 0.396       |\n","|    value_loss           | 3.62        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 859     |\n","|    iterations      | 283     |\n","|    time_elapsed    | 4045    |\n","|    total_timesteps | 3477504 |\n","--------------------------------\n","Eval num_timesteps=3480000, episode_reward=23.33 +/- 0.40\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 23.3        |\n","| time/                   |             |\n","|    total_timesteps      | 3480000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015342034 |\n","|    clip_fraction        | 0.214       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.97       |\n","|    explained_variance   | 0.407       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.963       |\n","|    n_updates            | 2830        |\n","|    policy_gradient_loss | -0.00921    |\n","|    std                  | 0.396       |\n","|    value_loss           | 16.6        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 859     |\n","|    iterations      | 284     |\n","|    time_elapsed    | 4060    |\n","|    total_timesteps | 3489792 |\n","--------------------------------\n","Eval num_timesteps=3492000, episode_reward=17.29 +/- 8.05\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 17.3        |\n","| time/                   |             |\n","|    total_timesteps      | 3492000     |\n","| train/                  |             |\n","|    approx_kl            | 0.017196769 |\n","|    clip_fraction        | 0.193       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.95       |\n","|    explained_variance   | 0.209       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.456       |\n","|    n_updates            | 2840        |\n","|    policy_gradient_loss | -0.0034     |\n","|    std                  | 0.392       |\n","|    value_loss           | 2.95        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 859     |\n","|    iterations      | 285     |\n","|    time_elapsed    | 4073    |\n","|    total_timesteps | 3502080 |\n","--------------------------------\n","Eval num_timesteps=3504000, episode_reward=31.29 +/- 2.04\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 31.3        |\n","| time/                   |             |\n","|    total_timesteps      | 3504000     |\n","| train/                  |             |\n","|    approx_kl            | 0.018827157 |\n","|    clip_fraction        | 0.219       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.91       |\n","|    explained_variance   | 0.901       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.378       |\n","|    n_updates            | 2850        |\n","|    policy_gradient_loss | -0.00846    |\n","|    std                  | 0.39        |\n","|    value_loss           | 0.587       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 859     |\n","|    iterations      | 286     |\n","|    time_elapsed    | 4086    |\n","|    total_timesteps | 3514368 |\n","--------------------------------\n","Eval num_timesteps=3516000, episode_reward=22.27 +/- 3.27\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 22.3        |\n","| time/                   |             |\n","|    total_timesteps      | 3516000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009531547 |\n","|    clip_fraction        | 0.156       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.91       |\n","|    explained_variance   | 0.664       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.83        |\n","|    n_updates            | 2860        |\n","|    policy_gradient_loss | -0.00499    |\n","|    std                  | 0.39        |\n","|    value_loss           | 14.2        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 859     |\n","|    iterations      | 287     |\n","|    time_elapsed    | 4102    |\n","|    total_timesteps | 3526656 |\n","--------------------------------\n","Eval num_timesteps=3528000, episode_reward=24.95 +/- 1.44\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 25          |\n","| time/                   |             |\n","|    total_timesteps      | 3528000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014957468 |\n","|    clip_fraction        | 0.175       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.9        |\n","|    explained_variance   | 0.922       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.202       |\n","|    n_updates            | 2870        |\n","|    policy_gradient_loss | -0.00622    |\n","|    std                  | 0.389       |\n","|    value_loss           | 0.576       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 860     |\n","|    iterations      | 288     |\n","|    time_elapsed    | 4114    |\n","|    total_timesteps | 3538944 |\n","--------------------------------\n","Eval num_timesteps=3540000, episode_reward=-28.37 +/- 44.38\n","Episode length: 483.80 +/- 13.23\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 484         |\n","|    mean_reward          | -28.4       |\n","| time/                   |             |\n","|    total_timesteps      | 3540000     |\n","| train/                  |             |\n","|    approx_kl            | 0.007876828 |\n","|    clip_fraction        | 0.0877      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.9        |\n","|    explained_variance   | 0.676       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 11.5        |\n","|    n_updates            | 2880        |\n","|    policy_gradient_loss | -0.00703    |\n","|    std                  | 0.389       |\n","|    value_loss           | 34.6        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 860     |\n","|    iterations      | 289     |\n","|    time_elapsed    | 4128    |\n","|    total_timesteps | 3551232 |\n","--------------------------------\n","Eval num_timesteps=3552000, episode_reward=14.35 +/- 0.35\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 14.3        |\n","| time/                   |             |\n","|    total_timesteps      | 3552000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010271575 |\n","|    clip_fraction        | 0.129       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.9        |\n","|    explained_variance   | 0.915       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.626       |\n","|    n_updates            | 2890        |\n","|    policy_gradient_loss | -0.00732    |\n","|    std                  | 0.39        |\n","|    value_loss           | 1.93        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 859     |\n","|    iterations      | 290     |\n","|    time_elapsed    | 4143    |\n","|    total_timesteps | 3563520 |\n","--------------------------------\n","Eval num_timesteps=3564000, episode_reward=23.43 +/- 15.68\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 23.4        |\n","| time/                   |             |\n","|    total_timesteps      | 3564000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009598258 |\n","|    clip_fraction        | 0.1         |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.91       |\n","|    explained_variance   | 0.929       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.37        |\n","|    n_updates            | 2900        |\n","|    policy_gradient_loss | -0.00504    |\n","|    std                  | 0.389       |\n","|    value_loss           | 5.6         |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 860     |\n","|    iterations      | 291     |\n","|    time_elapsed    | 4155    |\n","|    total_timesteps | 3575808 |\n","--------------------------------\n","Eval num_timesteps=3576000, episode_reward=18.69 +/- 8.16\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 18.7        |\n","| time/                   |             |\n","|    total_timesteps      | 3576000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009083674 |\n","|    clip_fraction        | 0.114       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.9        |\n","|    explained_variance   | 0.872       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.97        |\n","|    n_updates            | 2910        |\n","|    policy_gradient_loss | -0.008      |\n","|    std                  | 0.39        |\n","|    value_loss           | 19.5        |\n","-----------------------------------------\n","Eval num_timesteps=3588000, episode_reward=24.79 +/- 9.51\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 24.8     |\n","| time/              |          |\n","|    total_timesteps | 3588000  |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 859     |\n","|    iterations      | 292     |\n","|    time_elapsed    | 4175    |\n","|    total_timesteps | 3588096 |\n","--------------------------------\n","Eval num_timesteps=3600000, episode_reward=30.01 +/- 0.36\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 30          |\n","| time/                   |             |\n","|    total_timesteps      | 3600000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009257714 |\n","|    clip_fraction        | 0.124       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.9        |\n","|    explained_variance   | 0.938       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.21        |\n","|    n_updates            | 2920        |\n","|    policy_gradient_loss | -0.00386    |\n","|    std                  | 0.389       |\n","|    value_loss           | 2.86        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 859     |\n","|    iterations      | 293     |\n","|    time_elapsed    | 4189    |\n","|    total_timesteps | 3600384 |\n","--------------------------------\n","Eval num_timesteps=3612000, episode_reward=-50.29 +/- 48.36\n","Episode length: 493.40 +/- 5.39\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 493         |\n","|    mean_reward          | -50.3       |\n","| time/                   |             |\n","|    total_timesteps      | 3612000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010218435 |\n","|    clip_fraction        | 0.0925      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.89       |\n","|    explained_variance   | 0.788       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.6         |\n","|    n_updates            | 2930        |\n","|    policy_gradient_loss | -0.00451    |\n","|    std                  | 0.388       |\n","|    value_loss           | 12.6        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 859     |\n","|    iterations      | 294     |\n","|    time_elapsed    | 4204    |\n","|    total_timesteps | 3612672 |\n","--------------------------------\n","Eval num_timesteps=3624000, episode_reward=28.64 +/- 3.65\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 28.6        |\n","| time/                   |             |\n","|    total_timesteps      | 3624000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011558403 |\n","|    clip_fraction        | 0.13        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.88       |\n","|    explained_variance   | 0.476       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.72        |\n","|    n_updates            | 2940        |\n","|    policy_gradient_loss | -0.0052     |\n","|    std                  | 0.385       |\n","|    value_loss           | 6.99        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 860     |\n","|    iterations      | 295     |\n","|    time_elapsed    | 4214    |\n","|    total_timesteps | 3624960 |\n","--------------------------------\n","Eval num_timesteps=3636000, episode_reward=21.49 +/- 9.43\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 21.5        |\n","| time/                   |             |\n","|    total_timesteps      | 3636000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008954226 |\n","|    clip_fraction        | 0.101       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.86       |\n","|    explained_variance   | 0.826       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.14        |\n","|    n_updates            | 2950        |\n","|    policy_gradient_loss | -0.00499    |\n","|    std                  | 0.386       |\n","|    value_loss           | 10.6        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 859     |\n","|    iterations      | 296     |\n","|    time_elapsed    | 4229    |\n","|    total_timesteps | 3637248 |\n","--------------------------------\n","Eval num_timesteps=3648000, episode_reward=17.27 +/- 1.30\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 17.3         |\n","| time/                   |              |\n","|    total_timesteps      | 3648000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0139302155 |\n","|    clip_fraction        | 0.138        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -1.86        |\n","|    explained_variance   | 0.824        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 5.14         |\n","|    n_updates            | 2960         |\n","|    policy_gradient_loss | -0.00795     |\n","|    std                  | 0.385        |\n","|    value_loss           | 16.9         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 860     |\n","|    iterations      | 297     |\n","|    time_elapsed    | 4243    |\n","|    total_timesteps | 3649536 |\n","--------------------------------\n","Eval num_timesteps=3660000, episode_reward=19.84 +/- 10.75\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 19.8         |\n","| time/                   |              |\n","|    total_timesteps      | 3660000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0090545835 |\n","|    clip_fraction        | 0.0898       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -1.85        |\n","|    explained_variance   | 0.83         |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 9.51         |\n","|    n_updates            | 2970         |\n","|    policy_gradient_loss | -0.00453     |\n","|    std                  | 0.385        |\n","|    value_loss           | 23           |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 860     |\n","|    iterations      | 298     |\n","|    time_elapsed    | 4254    |\n","|    total_timesteps | 3661824 |\n","--------------------------------\n","Eval num_timesteps=3672000, episode_reward=9.60 +/- 2.54\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 9.6         |\n","| time/                   |             |\n","|    total_timesteps      | 3672000     |\n","| train/                  |             |\n","|    approx_kl            | 0.025469745 |\n","|    clip_fraction        | 0.25        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.82       |\n","|    explained_variance   | 0.699       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.302       |\n","|    n_updates            | 2980        |\n","|    policy_gradient_loss | -0.00496    |\n","|    std                  | 0.38        |\n","|    value_loss           | 0.893       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 860     |\n","|    iterations      | 299     |\n","|    time_elapsed    | 4270    |\n","|    total_timesteps | 3674112 |\n","--------------------------------\n","Eval num_timesteps=3684000, episode_reward=17.44 +/- 11.93\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 17.4        |\n","| time/                   |             |\n","|    total_timesteps      | 3684000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009071611 |\n","|    clip_fraction        | 0.11        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.8        |\n","|    explained_variance   | 0.847       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 10.1        |\n","|    n_updates            | 2990        |\n","|    policy_gradient_loss | -0.00315    |\n","|    std                  | 0.38        |\n","|    value_loss           | 17.9        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 860     |\n","|    iterations      | 300     |\n","|    time_elapsed    | 4283    |\n","|    total_timesteps | 3686400 |\n","--------------------------------\n","Eval num_timesteps=3696000, episode_reward=24.36 +/- 14.31\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 24.4        |\n","| time/                   |             |\n","|    total_timesteps      | 3696000     |\n","| train/                  |             |\n","|    approx_kl            | 0.018983983 |\n","|    clip_fraction        | 0.261       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.82       |\n","|    explained_variance   | 0.922       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.182       |\n","|    n_updates            | 3000        |\n","|    policy_gradient_loss | -0.00136    |\n","|    std                  | 0.384       |\n","|    value_loss           | 0.724       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 861     |\n","|    iterations      | 301     |\n","|    time_elapsed    | 4295    |\n","|    total_timesteps | 3698688 |\n","--------------------------------\n","Eval num_timesteps=3708000, episode_reward=25.18 +/- 5.43\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 25.2        |\n","| time/                   |             |\n","|    total_timesteps      | 3708000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010505383 |\n","|    clip_fraction        | 0.127       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.83       |\n","|    explained_variance   | 0.942       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.765       |\n","|    n_updates            | 3010        |\n","|    policy_gradient_loss | -0.00568    |\n","|    std                  | 0.382       |\n","|    value_loss           | 2.93        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 860     |\n","|    iterations      | 302     |\n","|    time_elapsed    | 4310    |\n","|    total_timesteps | 3710976 |\n","--------------------------------\n","Eval num_timesteps=3720000, episode_reward=20.68 +/- 2.87\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 20.7        |\n","| time/                   |             |\n","|    total_timesteps      | 3720000     |\n","| train/                  |             |\n","|    approx_kl            | 0.020501902 |\n","|    clip_fraction        | 0.232       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.82       |\n","|    explained_variance   | 0.228       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.802       |\n","|    n_updates            | 3020        |\n","|    policy_gradient_loss | -0.000998   |\n","|    std                  | 0.381       |\n","|    value_loss           | 2.23        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 861     |\n","|    iterations      | 303     |\n","|    time_elapsed    | 4321    |\n","|    total_timesteps | 3723264 |\n","--------------------------------\n","Eval num_timesteps=3732000, episode_reward=8.05 +/- 5.97\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 8.05        |\n","| time/                   |             |\n","|    total_timesteps      | 3732000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010249357 |\n","|    clip_fraction        | 0.135       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.81       |\n","|    explained_variance   | 0.912       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.07        |\n","|    n_updates            | 3030        |\n","|    policy_gradient_loss | -0.00441    |\n","|    std                  | 0.381       |\n","|    value_loss           | 3.37        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 861     |\n","|    iterations      | 304     |\n","|    time_elapsed    | 4335    |\n","|    total_timesteps | 3735552 |\n","--------------------------------\n","Eval num_timesteps=3744000, episode_reward=14.57 +/- 8.90\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 14.6        |\n","| time/                   |             |\n","|    total_timesteps      | 3744000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008697613 |\n","|    clip_fraction        | 0.112       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.81       |\n","|    explained_variance   | 0.797       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.14        |\n","|    n_updates            | 3040        |\n","|    policy_gradient_loss | -0.00618    |\n","|    std                  | 0.381       |\n","|    value_loss           | 11          |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 861     |\n","|    iterations      | 305     |\n","|    time_elapsed    | 4350    |\n","|    total_timesteps | 3747840 |\n","--------------------------------\n","Eval num_timesteps=3756000, episode_reward=11.32 +/- 0.29\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 11.3        |\n","| time/                   |             |\n","|    total_timesteps      | 3756000     |\n","| train/                  |             |\n","|    approx_kl            | 0.019815246 |\n","|    clip_fraction        | 0.241       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.8        |\n","|    explained_variance   | 0.886       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0772      |\n","|    n_updates            | 3050        |\n","|    policy_gradient_loss | -0.007      |\n","|    std                  | 0.379       |\n","|    value_loss           | 0.253       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 862     |\n","|    iterations      | 306     |\n","|    time_elapsed    | 4360    |\n","|    total_timesteps | 3760128 |\n","--------------------------------\n","Eval num_timesteps=3768000, episode_reward=11.65 +/- 2.14\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 11.7        |\n","| time/                   |             |\n","|    total_timesteps      | 3768000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009804983 |\n","|    clip_fraction        | 0.135       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.8        |\n","|    explained_variance   | 0.759       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.89        |\n","|    n_updates            | 3060        |\n","|    policy_gradient_loss | -0.00465    |\n","|    std                  | 0.38        |\n","|    value_loss           | 5.08        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 861     |\n","|    iterations      | 307     |\n","|    time_elapsed    | 4379    |\n","|    total_timesteps | 3772416 |\n","--------------------------------\n","Eval num_timesteps=3780000, episode_reward=19.36 +/- 0.33\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 19.4       |\n","| time/                   |            |\n","|    total_timesteps      | 3780000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01856391 |\n","|    clip_fraction        | 0.213      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.8       |\n","|    explained_variance   | 0.567      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.218      |\n","|    n_updates            | 3070       |\n","|    policy_gradient_loss | -0.00441   |\n","|    std                  | 0.379      |\n","|    value_loss           | 0.482      |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 861     |\n","|    iterations      | 308     |\n","|    time_elapsed    | 4393    |\n","|    total_timesteps | 3784704 |\n","--------------------------------\n","Eval num_timesteps=3792000, episode_reward=25.35 +/- 3.06\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 25.3        |\n","| time/                   |             |\n","|    total_timesteps      | 3792000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009489943 |\n","|    clip_fraction        | 0.127       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.79       |\n","|    explained_variance   | 0.826       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.647       |\n","|    n_updates            | 3080        |\n","|    policy_gradient_loss | -0.0045     |\n","|    std                  | 0.381       |\n","|    value_loss           | 3.56        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 861     |\n","|    iterations      | 309     |\n","|    time_elapsed    | 4405    |\n","|    total_timesteps | 3796992 |\n","--------------------------------\n","Eval num_timesteps=3804000, episode_reward=31.28 +/- 2.04\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 31.3        |\n","| time/                   |             |\n","|    total_timesteps      | 3804000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011933493 |\n","|    clip_fraction        | 0.123       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.81       |\n","|    explained_variance   | 0.97        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.52        |\n","|    n_updates            | 3090        |\n","|    policy_gradient_loss | -0.0063     |\n","|    std                  | 0.382       |\n","|    value_loss           | 1.92        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 861     |\n","|    iterations      | 310     |\n","|    time_elapsed    | 4420    |\n","|    total_timesteps | 3809280 |\n","--------------------------------\n","Eval num_timesteps=3816000, episode_reward=20.27 +/- 11.94\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 20.3         |\n","| time/                   |              |\n","|    total_timesteps      | 3816000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0062068314 |\n","|    clip_fraction        | 0.0498       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -1.82        |\n","|    explained_variance   | 0.898        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 2.9          |\n","|    n_updates            | 3100         |\n","|    policy_gradient_loss | -0.00339     |\n","|    std                  | 0.383        |\n","|    value_loss           | 10.8         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 861     |\n","|    iterations      | 311     |\n","|    time_elapsed    | 4434    |\n","|    total_timesteps | 3821568 |\n","--------------------------------\n","Eval num_timesteps=3828000, episode_reward=12.62 +/- 5.27\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 12.6        |\n","| time/                   |             |\n","|    total_timesteps      | 3828000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013652568 |\n","|    clip_fraction        | 0.116       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.82       |\n","|    explained_variance   | 0.472       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.34        |\n","|    n_updates            | 3110        |\n","|    policy_gradient_loss | -0.00436    |\n","|    std                  | 0.381       |\n","|    value_loss           | 6.18        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 862     |\n","|    iterations      | 312     |\n","|    time_elapsed    | 4445    |\n","|    total_timesteps | 3833856 |\n","--------------------------------\n","Eval num_timesteps=3840000, episode_reward=13.58 +/- 9.78\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 13.6         |\n","| time/                   |              |\n","|    total_timesteps      | 3840000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0071112197 |\n","|    clip_fraction        | 0.0809       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -1.81        |\n","|    explained_variance   | 0.785        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 3.66         |\n","|    n_updates            | 3120         |\n","|    policy_gradient_loss | -0.00374     |\n","|    std                  | 0.381        |\n","|    value_loss           | 11.5         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 862     |\n","|    iterations      | 313     |\n","|    time_elapsed    | 4460    |\n","|    total_timesteps | 3846144 |\n","--------------------------------\n","Eval num_timesteps=3852000, episode_reward=21.32 +/- 2.75\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 21.3        |\n","| time/                   |             |\n","|    total_timesteps      | 3852000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011004757 |\n","|    clip_fraction        | 0.128       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.82       |\n","|    explained_variance   | 0.901       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.36        |\n","|    n_updates            | 3130        |\n","|    policy_gradient_loss | -0.00485    |\n","|    std                  | 0.383       |\n","|    value_loss           | 2.87        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 862     |\n","|    iterations      | 314     |\n","|    time_elapsed    | 4473    |\n","|    total_timesteps | 3858432 |\n","--------------------------------\n","Eval num_timesteps=3864000, episode_reward=7.82 +/- 0.42\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 7.82        |\n","| time/                   |             |\n","|    total_timesteps      | 3864000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012226251 |\n","|    clip_fraction        | 0.131       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.84       |\n","|    explained_variance   | 0.919       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.05        |\n","|    n_updates            | 3140        |\n","|    policy_gradient_loss | -0.0045     |\n","|    std                  | 0.385       |\n","|    value_loss           | 2.8         |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 862     |\n","|    iterations      | 315     |\n","|    time_elapsed    | 4485    |\n","|    total_timesteps | 3870720 |\n","--------------------------------\n","Eval num_timesteps=3876000, episode_reward=32.40 +/- 3.27\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 32.4        |\n","| time/                   |             |\n","|    total_timesteps      | 3876000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010165013 |\n","|    clip_fraction        | 0.0893      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.85       |\n","|    explained_variance   | 0.76        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 13.6        |\n","|    n_updates            | 3150        |\n","|    policy_gradient_loss | -0.00483    |\n","|    std                  | 0.384       |\n","|    value_loss           | 17.7        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 862     |\n","|    iterations      | 316     |\n","|    time_elapsed    | 4501    |\n","|    total_timesteps | 3883008 |\n","--------------------------------\n","Eval num_timesteps=3888000, episode_reward=27.39 +/- 1.77\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 27.4        |\n","| time/                   |             |\n","|    total_timesteps      | 3888000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009786173 |\n","|    clip_fraction        | 0.106       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.84       |\n","|    explained_variance   | 0.822       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 6.6         |\n","|    n_updates            | 3160        |\n","|    policy_gradient_loss | -0.00578    |\n","|    std                  | 0.384       |\n","|    value_loss           | 20.4        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 863     |\n","|    iterations      | 317     |\n","|    time_elapsed    | 4512    |\n","|    total_timesteps | 3895296 |\n","--------------------------------\n","Eval num_timesteps=3900000, episode_reward=17.96 +/- 8.33\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 18           |\n","| time/                   |              |\n","|    total_timesteps      | 3900000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0154109495 |\n","|    clip_fraction        | 0.199        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -1.82        |\n","|    explained_variance   | 0.88         |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 1.22         |\n","|    n_updates            | 3170         |\n","|    policy_gradient_loss | -0.00575     |\n","|    std                  | 0.381        |\n","|    value_loss           | 3.11         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 863     |\n","|    iterations      | 318     |\n","|    time_elapsed    | 4526    |\n","|    total_timesteps | 3907584 |\n","--------------------------------\n","Eval num_timesteps=3912000, episode_reward=15.73 +/- 3.36\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 15.7         |\n","| time/                   |              |\n","|    total_timesteps      | 3912000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0120262345 |\n","|    clip_fraction        | 0.127        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -1.81        |\n","|    explained_variance   | 0.823        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 5.47         |\n","|    n_updates            | 3180         |\n","|    policy_gradient_loss | -0.00442     |\n","|    std                  | 0.381        |\n","|    value_loss           | 13.6         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 863     |\n","|    iterations      | 319     |\n","|    time_elapsed    | 4541    |\n","|    total_timesteps | 3919872 |\n","--------------------------------\n","Eval num_timesteps=3924000, episode_reward=18.31 +/- 0.36\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 18.3       |\n","| time/                   |            |\n","|    total_timesteps      | 3924000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01633736 |\n","|    clip_fraction        | 0.182      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.8       |\n","|    explained_variance   | 0.92       |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.601      |\n","|    n_updates            | 3190       |\n","|    policy_gradient_loss | -0.00558   |\n","|    std                  | 0.38       |\n","|    value_loss           | 3.99       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 863     |\n","|    iterations      | 320     |\n","|    time_elapsed    | 4551    |\n","|    total_timesteps | 3932160 |\n","--------------------------------\n","Eval num_timesteps=3936000, episode_reward=18.17 +/- 6.07\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 18.2        |\n","| time/                   |             |\n","|    total_timesteps      | 3936000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013860643 |\n","|    clip_fraction        | 0.189       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.8        |\n","|    explained_variance   | 0.877       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.88        |\n","|    n_updates            | 3200        |\n","|    policy_gradient_loss | -0.00476    |\n","|    std                  | 0.38        |\n","|    value_loss           | 5.56        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 863     |\n","|    iterations      | 321     |\n","|    time_elapsed    | 4566    |\n","|    total_timesteps | 3944448 |\n","--------------------------------\n","Eval num_timesteps=3948000, episode_reward=21.72 +/- 7.88\n","Episode length: 500.00 +/- 0.00\n","---------------------------------------\n","| eval/                   |           |\n","|    mean_ep_length       | 500       |\n","|    mean_reward          | 21.7      |\n","| time/                   |           |\n","|    total_timesteps      | 3948000   |\n","| train/                  |           |\n","|    approx_kl            | 0.0272172 |\n","|    clip_fraction        | 0.293     |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -1.79     |\n","|    explained_variance   | 0.699     |\n","|    learning_rate        | 0.0003    |\n","|    loss                 | 0.273     |\n","|    n_updates            | 3210      |\n","|    policy_gradient_loss | -0.00601  |\n","|    std                  | 0.378     |\n","|    value_loss           | 0.815     |\n","---------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 863     |\n","|    iterations      | 322     |\n","|    time_elapsed    | 4580    |\n","|    total_timesteps | 3956736 |\n","--------------------------------\n","Eval num_timesteps=3960000, episode_reward=17.59 +/- 9.82\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 17.6        |\n","| time/                   |             |\n","|    total_timesteps      | 3960000     |\n","| train/                  |             |\n","|    approx_kl            | 0.021984411 |\n","|    clip_fraction        | 0.242       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.75       |\n","|    explained_variance   | 0.799       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.25        |\n","|    n_updates            | 3220        |\n","|    policy_gradient_loss | -0.00654    |\n","|    std                  | 0.373       |\n","|    value_loss           | 0.708       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 323     |\n","|    time_elapsed    | 4591    |\n","|    total_timesteps | 3969024 |\n","--------------------------------\n","Eval num_timesteps=3972000, episode_reward=25.89 +/- 0.20\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 25.9        |\n","| time/                   |             |\n","|    total_timesteps      | 3972000     |\n","| train/                  |             |\n","|    approx_kl            | 0.019848168 |\n","|    clip_fraction        | 0.233       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.73       |\n","|    explained_variance   | 0.613       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.165       |\n","|    n_updates            | 3230        |\n","|    policy_gradient_loss | -0.0056     |\n","|    std                  | 0.374       |\n","|    value_loss           | 0.677       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 324     |\n","|    time_elapsed    | 4606    |\n","|    total_timesteps | 3981312 |\n","--------------------------------\n","Eval num_timesteps=3984000, episode_reward=8.32 +/- 8.27\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 8.32       |\n","| time/                   |            |\n","|    total_timesteps      | 3984000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01012237 |\n","|    clip_fraction        | 0.129      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.73      |\n","|    explained_variance   | 0.481      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 6.78       |\n","|    n_updates            | 3240       |\n","|    policy_gradient_loss | -0.00706   |\n","|    std                  | 0.375      |\n","|    value_loss           | 16.9       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 325     |\n","|    time_elapsed    | 4619    |\n","|    total_timesteps | 3993600 |\n","--------------------------------\n","Eval num_timesteps=3996000, episode_reward=11.81 +/- 0.40\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 11.8        |\n","| time/                   |             |\n","|    total_timesteps      | 3996000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008468237 |\n","|    clip_fraction        | 0.0717      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.74       |\n","|    explained_variance   | 0.63        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.29        |\n","|    n_updates            | 3250        |\n","|    policy_gradient_loss | -0.0039     |\n","|    std                  | 0.374       |\n","|    value_loss           | 7.51        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 326     |\n","|    time_elapsed    | 4631    |\n","|    total_timesteps | 4005888 |\n","--------------------------------\n","Eval num_timesteps=4008000, episode_reward=8.27 +/- 8.36\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 8.27       |\n","| time/                   |            |\n","|    total_timesteps      | 4008000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01590863 |\n","|    clip_fraction        | 0.166      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.72      |\n","|    explained_variance   | 0.127      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.364      |\n","|    n_updates            | 3260       |\n","|    policy_gradient_loss | -0.00315   |\n","|    std                  | 0.37       |\n","|    value_loss           | 1.9        |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 327     |\n","|    time_elapsed    | 4647    |\n","|    total_timesteps | 4018176 |\n","--------------------------------\n","Eval num_timesteps=4020000, episode_reward=15.79 +/- 4.17\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 15.8        |\n","| time/                   |             |\n","|    total_timesteps      | 4020000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011446486 |\n","|    clip_fraction        | 0.16        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.7        |\n","|    explained_variance   | 0.482       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.66        |\n","|    n_updates            | 3270        |\n","|    policy_gradient_loss | -0.00271    |\n","|    std                  | 0.371       |\n","|    value_loss           | 9.09        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 865     |\n","|    iterations      | 328     |\n","|    time_elapsed    | 4659    |\n","|    total_timesteps | 4030464 |\n","--------------------------------\n","Eval num_timesteps=4032000, episode_reward=18.92 +/- 6.11\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 18.9        |\n","| time/                   |             |\n","|    total_timesteps      | 4032000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012871572 |\n","|    clip_fraction        | 0.156       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.71       |\n","|    explained_variance   | 0.769       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.252       |\n","|    n_updates            | 3280        |\n","|    policy_gradient_loss | -0.00411    |\n","|    std                  | 0.372       |\n","|    value_loss           | 3.4         |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 865     |\n","|    iterations      | 329     |\n","|    time_elapsed    | 4672    |\n","|    total_timesteps | 4042752 |\n","--------------------------------\n","Eval num_timesteps=4044000, episode_reward=12.92 +/- 6.69\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 12.9         |\n","| time/                   |              |\n","|    total_timesteps      | 4044000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0097974045 |\n","|    clip_fraction        | 0.111        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -1.72        |\n","|    explained_variance   | 0.888        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 2.7          |\n","|    n_updates            | 3290         |\n","|    policy_gradient_loss | -0.00591     |\n","|    std                  | 0.373        |\n","|    value_loss           | 7.04         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 865     |\n","|    iterations      | 330     |\n","|    time_elapsed    | 4687    |\n","|    total_timesteps | 4055040 |\n","--------------------------------\n","Eval num_timesteps=4056000, episode_reward=7.58 +/- 0.22\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 7.58        |\n","| time/                   |             |\n","|    total_timesteps      | 4056000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013090621 |\n","|    clip_fraction        | 0.181       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.72       |\n","|    explained_variance   | 0.627       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 24.1        |\n","|    n_updates            | 3300        |\n","|    policy_gradient_loss | -0.00795    |\n","|    std                  | 0.373       |\n","|    value_loss           | 24.6        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 865     |\n","|    iterations      | 331     |\n","|    time_elapsed    | 4698    |\n","|    total_timesteps | 4067328 |\n","--------------------------------\n","Eval num_timesteps=4068000, episode_reward=11.22 +/- 2.05\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 11.2        |\n","| time/                   |             |\n","|    total_timesteps      | 4068000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013101273 |\n","|    clip_fraction        | 0.0999      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.72       |\n","|    explained_variance   | 0.735       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 7.94        |\n","|    n_updates            | 3310        |\n","|    policy_gradient_loss | -0.00454    |\n","|    std                  | 0.372       |\n","|    value_loss           | 16.6        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 865     |\n","|    iterations      | 332     |\n","|    time_elapsed    | 4713    |\n","|    total_timesteps | 4079616 |\n","--------------------------------\n","Eval num_timesteps=4080000, episode_reward=-36.90 +/- 61.23\n","Episode length: 335.20 +/- 201.84\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 335         |\n","|    mean_reward          | -36.9       |\n","| time/                   |             |\n","|    total_timesteps      | 4080000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012469664 |\n","|    clip_fraction        | 0.123       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.71       |\n","|    explained_variance   | 0.959       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.66        |\n","|    n_updates            | 3320        |\n","|    policy_gradient_loss | -0.00567    |\n","|    std                  | 0.372       |\n","|    value_loss           | 3.38        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 865     |\n","|    iterations      | 333     |\n","|    time_elapsed    | 4727    |\n","|    total_timesteps | 4091904 |\n","--------------------------------\n","Eval num_timesteps=4092000, episode_reward=31.06 +/- 0.98\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 31.1       |\n","| time/                   |            |\n","|    total_timesteps      | 4092000    |\n","| train/                  |            |\n","|    approx_kl            | 0.00895887 |\n","|    clip_fraction        | 0.0755     |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.72      |\n","|    explained_variance   | 0.919      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 5.12       |\n","|    n_updates            | 3330       |\n","|    policy_gradient_loss | -0.00367   |\n","|    std                  | 0.373      |\n","|    value_loss           | 14.6       |\n","----------------------------------------\n","Eval num_timesteps=4104000, episode_reward=26.62 +/- 0.25\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 26.6     |\n","| time/              |          |\n","|    total_timesteps | 4104000  |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 334     |\n","|    time_elapsed    | 4747    |\n","|    total_timesteps | 4104192 |\n","--------------------------------\n","Eval num_timesteps=4116000, episode_reward=-48.92 +/- 57.81\n","Episode length: 455.60 +/- 36.25\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 456         |\n","|    mean_reward          | -48.9       |\n","| time/                   |             |\n","|    total_timesteps      | 4116000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010116902 |\n","|    clip_fraction        | 0.105       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.72       |\n","|    explained_variance   | 0.774       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 6.61        |\n","|    n_updates            | 3340        |\n","|    policy_gradient_loss | -0.00583    |\n","|    std                  | 0.372       |\n","|    value_loss           | 17.1        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 335     |\n","|    time_elapsed    | 4762    |\n","|    total_timesteps | 4116480 |\n","--------------------------------\n","Eval num_timesteps=4128000, episode_reward=28.24 +/- 10.16\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 28.2        |\n","| time/                   |             |\n","|    total_timesteps      | 4128000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010752191 |\n","|    clip_fraction        | 0.128       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.7        |\n","|    explained_variance   | 0.815       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.71        |\n","|    n_updates            | 3350        |\n","|    policy_gradient_loss | -0.00716    |\n","|    std                  | 0.369       |\n","|    value_loss           | 10.4        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 336     |\n","|    time_elapsed    | 4777    |\n","|    total_timesteps | 4128768 |\n","--------------------------------\n","Eval num_timesteps=4140000, episode_reward=23.95 +/- 8.65\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 24          |\n","| time/                   |             |\n","|    total_timesteps      | 4140000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013789579 |\n","|    clip_fraction        | 0.197       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.66       |\n","|    explained_variance   | 0.916       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.3         |\n","|    n_updates            | 3360        |\n","|    policy_gradient_loss | -0.00525    |\n","|    std                  | 0.365       |\n","|    value_loss           | 3.57        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 337     |\n","|    time_elapsed    | 4788    |\n","|    total_timesteps | 4141056 |\n","--------------------------------\n","Eval num_timesteps=4152000, episode_reward=32.04 +/- 2.46\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 32          |\n","| time/                   |             |\n","|    total_timesteps      | 4152000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009885914 |\n","|    clip_fraction        | 0.119       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.64       |\n","|    explained_variance   | 0.81        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.62        |\n","|    n_updates            | 3370        |\n","|    policy_gradient_loss | -0.00601    |\n","|    std                  | 0.366       |\n","|    value_loss           | 21.8        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 338     |\n","|    time_elapsed    | 4803    |\n","|    total_timesteps | 4153344 |\n","--------------------------------\n","Eval num_timesteps=4164000, episode_reward=31.60 +/- 0.70\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 31.6        |\n","| time/                   |             |\n","|    total_timesteps      | 4164000     |\n","| train/                  |             |\n","|    approx_kl            | 0.020131556 |\n","|    clip_fraction        | 0.22        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.63       |\n","|    explained_variance   | 0.679       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.75        |\n","|    n_updates            | 3380        |\n","|    policy_gradient_loss | -0.00391    |\n","|    std                  | 0.362       |\n","|    value_loss           | 3.77        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 339     |\n","|    time_elapsed    | 4818    |\n","|    total_timesteps | 4165632 |\n","--------------------------------\n","Eval num_timesteps=4176000, episode_reward=21.57 +/- 7.51\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 21.6        |\n","| time/                   |             |\n","|    total_timesteps      | 4176000     |\n","| train/                  |             |\n","|    approx_kl            | 0.032348316 |\n","|    clip_fraction        | 0.289       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.62       |\n","|    explained_variance   | 0.749       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.281       |\n","|    n_updates            | 3390        |\n","|    policy_gradient_loss | -0.0043     |\n","|    std                  | 0.365       |\n","|    value_loss           | 2.02        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 865     |\n","|    iterations      | 340     |\n","|    time_elapsed    | 4829    |\n","|    total_timesteps | 4177920 |\n","--------------------------------\n","Eval num_timesteps=4188000, episode_reward=18.56 +/- 0.51\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 18.6        |\n","| time/                   |             |\n","|    total_timesteps      | 4188000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015369055 |\n","|    clip_fraction        | 0.221       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.64       |\n","|    explained_variance   | 0.791       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 14.7        |\n","|    n_updates            | 3400        |\n","|    policy_gradient_loss | -0.00523    |\n","|    std                  | 0.366       |\n","|    value_loss           | 17.4        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 341     |\n","|    time_elapsed    | 4844    |\n","|    total_timesteps | 4190208 |\n","--------------------------------\n","Eval num_timesteps=4200000, episode_reward=-43.77 +/- 57.40\n","Episode length: 456.80 +/- 35.27\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 457          |\n","|    mean_reward          | -43.8        |\n","| time/                   |              |\n","|    total_timesteps      | 4200000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0111577585 |\n","|    clip_fraction        | 0.137        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -1.65        |\n","|    explained_variance   | 0.87         |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 1.14         |\n","|    n_updates            | 3410         |\n","|    policy_gradient_loss | -0.00536     |\n","|    std                  | 0.366        |\n","|    value_loss           | 6.46         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 342     |\n","|    time_elapsed    | 4859    |\n","|    total_timesteps | 4202496 |\n","--------------------------------\n","Eval num_timesteps=4212000, episode_reward=17.32 +/- 6.28\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 17.3        |\n","| time/                   |             |\n","|    total_timesteps      | 4212000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015604336 |\n","|    clip_fraction        | 0.161       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.64       |\n","|    explained_variance   | 0.931       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.15        |\n","|    n_updates            | 3420        |\n","|    policy_gradient_loss | -0.00439    |\n","|    std                  | 0.365       |\n","|    value_loss           | 3.82        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 865     |\n","|    iterations      | 343     |\n","|    time_elapsed    | 4871    |\n","|    total_timesteps | 4214784 |\n","--------------------------------\n","Eval num_timesteps=4224000, episode_reward=27.84 +/- 1.87\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 27.8        |\n","| time/                   |             |\n","|    total_timesteps      | 4224000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015289181 |\n","|    clip_fraction        | 0.136       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.63       |\n","|    explained_variance   | 0.765       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 6.73        |\n","|    n_updates            | 3430        |\n","|    policy_gradient_loss | -0.0051     |\n","|    std                  | 0.364       |\n","|    value_loss           | 16.3        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 865     |\n","|    iterations      | 344     |\n","|    time_elapsed    | 4886    |\n","|    total_timesteps | 4227072 |\n","--------------------------------\n","Eval num_timesteps=4236000, episode_reward=21.09 +/- 4.92\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 21.1        |\n","| time/                   |             |\n","|    total_timesteps      | 4236000     |\n","| train/                  |             |\n","|    approx_kl            | 0.017045902 |\n","|    clip_fraction        | 0.18        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.62       |\n","|    explained_variance   | 0.831       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.71        |\n","|    n_updates            | 3440        |\n","|    policy_gradient_loss | -0.00578    |\n","|    std                  | 0.363       |\n","|    value_loss           | 8.34        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 865     |\n","|    iterations      | 345     |\n","|    time_elapsed    | 4900    |\n","|    total_timesteps | 4239360 |\n","--------------------------------\n","Eval num_timesteps=4248000, episode_reward=37.50 +/- 5.44\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 37.5       |\n","| time/                   |            |\n","|    total_timesteps      | 4248000    |\n","| train/                  |            |\n","|    approx_kl            | 0.02614904 |\n","|    clip_fraction        | 0.198      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.61      |\n","|    explained_variance   | 0.938      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 1.25       |\n","|    n_updates            | 3450       |\n","|    policy_gradient_loss | -0.00386   |\n","|    std                  | 0.363      |\n","|    value_loss           | 3.39       |\n","----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 865     |\n","|    iterations      | 346     |\n","|    time_elapsed    | 4913    |\n","|    total_timesteps | 4251648 |\n","--------------------------------\n","Eval num_timesteps=4260000, episode_reward=15.64 +/- 1.51\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 15.6        |\n","| time/                   |             |\n","|    total_timesteps      | 4260000     |\n","| train/                  |             |\n","|    approx_kl            | 0.025552178 |\n","|    clip_fraction        | 0.31        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.61       |\n","|    explained_variance   | 0.659       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.337       |\n","|    n_updates            | 3460        |\n","|    policy_gradient_loss | 0.000236    |\n","|    std                  | 0.363       |\n","|    value_loss           | 0.732       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 347     |\n","|    time_elapsed    | 4929    |\n","|    total_timesteps | 4263936 |\n","--------------------------------\n","Eval num_timesteps=4272000, episode_reward=17.52 +/- 8.87\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 17.5        |\n","| time/                   |             |\n","|    total_timesteps      | 4272000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008304699 |\n","|    clip_fraction        | 0.134       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.61       |\n","|    explained_variance   | 0.948       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.13        |\n","|    n_updates            | 3470        |\n","|    policy_gradient_loss | -0.00297    |\n","|    std                  | 0.363       |\n","|    value_loss           | 6.66        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 348     |\n","|    time_elapsed    | 4945    |\n","|    total_timesteps | 4276224 |\n","--------------------------------\n","Eval num_timesteps=4284000, episode_reward=19.55 +/- 11.33\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 19.5        |\n","| time/                   |             |\n","|    total_timesteps      | 4284000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009289379 |\n","|    clip_fraction        | 0.0747      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.61       |\n","|    explained_variance   | 0.81        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 7.93        |\n","|    n_updates            | 3480        |\n","|    policy_gradient_loss | -0.00448    |\n","|    std                  | 0.362       |\n","|    value_loss           | 22          |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 865     |\n","|    iterations      | 349     |\n","|    time_elapsed    | 4957    |\n","|    total_timesteps | 4288512 |\n","--------------------------------\n","Eval num_timesteps=4296000, episode_reward=-8.60 +/- 48.44\n","Episode length: 489.60 +/- 12.74\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 490         |\n","|    mean_reward          | -8.6        |\n","| time/                   |             |\n","|    total_timesteps      | 4296000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011033141 |\n","|    clip_fraction        | 0.1         |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.6        |\n","|    explained_variance   | 0.746       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 15.6        |\n","|    n_updates            | 3490        |\n","|    policy_gradient_loss | -0.00556    |\n","|    std                  | 0.362       |\n","|    value_loss           | 31.6        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 350     |\n","|    time_elapsed    | 4972    |\n","|    total_timesteps | 4300800 |\n","--------------------------------\n","Eval num_timesteps=4308000, episode_reward=28.95 +/- 1.65\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 29          |\n","| time/                   |             |\n","|    total_timesteps      | 4308000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015149628 |\n","|    clip_fraction        | 0.156       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.6        |\n","|    explained_variance   | 0.876       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.52        |\n","|    n_updates            | 3500        |\n","|    policy_gradient_loss | -0.00468    |\n","|    std                  | 0.362       |\n","|    value_loss           | 3.22        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 351     |\n","|    time_elapsed    | 4988    |\n","|    total_timesteps | 4313088 |\n","--------------------------------\n","Eval num_timesteps=4320000, episode_reward=27.23 +/- 6.10\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 27.2        |\n","| time/                   |             |\n","|    total_timesteps      | 4320000     |\n","| train/                  |             |\n","|    approx_kl            | 0.021451058 |\n","|    clip_fraction        | 0.162       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.59       |\n","|    explained_variance   | 0.414       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.83        |\n","|    n_updates            | 3510        |\n","|    policy_gradient_loss | -0.00333    |\n","|    std                  | 0.361       |\n","|    value_loss           | 7.5         |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 865     |\n","|    iterations      | 352     |\n","|    time_elapsed    | 5000    |\n","|    total_timesteps | 4325376 |\n","--------------------------------\n","Eval num_timesteps=4332000, episode_reward=23.08 +/- 1.46\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 23.1        |\n","| time/                   |             |\n","|    total_timesteps      | 4332000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010855657 |\n","|    clip_fraction        | 0.143       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.58       |\n","|    explained_variance   | 0.749       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.54        |\n","|    n_updates            | 3520        |\n","|    policy_gradient_loss | -0.0068     |\n","|    std                  | 0.359       |\n","|    value_loss           | 7.81        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 353     |\n","|    time_elapsed    | 5015    |\n","|    total_timesteps | 4337664 |\n","--------------------------------\n","Eval num_timesteps=4344000, episode_reward=15.95 +/- 12.87\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 15.9       |\n","| time/                   |            |\n","|    total_timesteps      | 4344000    |\n","| train/                  |            |\n","|    approx_kl            | 0.02007311 |\n","|    clip_fraction        | 0.241      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.56      |\n","|    explained_variance   | 0.63       |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.914      |\n","|    n_updates            | 3530       |\n","|    policy_gradient_loss | -0.00885   |\n","|    std                  | 0.359      |\n","|    value_loss           | 14.9       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 354     |\n","|    time_elapsed    | 5030    |\n","|    total_timesteps | 4349952 |\n","--------------------------------\n","Eval num_timesteps=4356000, episode_reward=20.80 +/- 8.76\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 20.8       |\n","| time/                   |            |\n","|    total_timesteps      | 4356000    |\n","| train/                  |            |\n","|    approx_kl            | 0.02055759 |\n","|    clip_fraction        | 0.256      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.55      |\n","|    explained_variance   | 0.776      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.222      |\n","|    n_updates            | 3540       |\n","|    policy_gradient_loss | -0.00625   |\n","|    std                  | 0.356      |\n","|    value_loss           | 0.668      |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 865     |\n","|    iterations      | 355     |\n","|    time_elapsed    | 5042    |\n","|    total_timesteps | 4362240 |\n","--------------------------------\n","Eval num_timesteps=4368000, episode_reward=24.45 +/- 4.21\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 24.5        |\n","| time/                   |             |\n","|    total_timesteps      | 4368000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008935603 |\n","|    clip_fraction        | 0.195       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.53       |\n","|    explained_variance   | 0.456       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.82        |\n","|    n_updates            | 3550        |\n","|    policy_gradient_loss | -0.00146    |\n","|    std                  | 0.356       |\n","|    value_loss           | 12.1        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 356     |\n","|    time_elapsed    | 5058    |\n","|    total_timesteps | 4374528 |\n","--------------------------------\n","Eval num_timesteps=4380000, episode_reward=24.27 +/- 6.57\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 24.3        |\n","| time/                   |             |\n","|    total_timesteps      | 4380000     |\n","| train/                  |             |\n","|    approx_kl            | 0.016242465 |\n","|    clip_fraction        | 0.211       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.54       |\n","|    explained_variance   | 0.845       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.353       |\n","|    n_updates            | 3560        |\n","|    policy_gradient_loss | -0.00607    |\n","|    std                  | 0.357       |\n","|    value_loss           | 0.895       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 357     |\n","|    time_elapsed    | 5073    |\n","|    total_timesteps | 4386816 |\n","--------------------------------\n","Eval num_timesteps=4392000, episode_reward=24.09 +/- 3.82\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 24.1        |\n","| time/                   |             |\n","|    total_timesteps      | 4392000     |\n","| train/                  |             |\n","|    approx_kl            | 0.019278936 |\n","|    clip_fraction        | 0.228       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.53       |\n","|    explained_variance   | 0.615       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.174       |\n","|    n_updates            | 3570        |\n","|    policy_gradient_loss | -0.00513    |\n","|    std                  | 0.354       |\n","|    value_loss           | 0.71        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 865     |\n","|    iterations      | 358     |\n","|    time_elapsed    | 5085    |\n","|    total_timesteps | 4399104 |\n","--------------------------------\n","Eval num_timesteps=4404000, episode_reward=18.87 +/- 5.71\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 18.9        |\n","| time/                   |             |\n","|    total_timesteps      | 4404000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014731745 |\n","|    clip_fraction        | 0.178       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.52       |\n","|    explained_variance   | 0.103       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 8.75        |\n","|    n_updates            | 3580        |\n","|    policy_gradient_loss | -0.00529    |\n","|    std                  | 0.356       |\n","|    value_loss           | 20.6        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 359     |\n","|    time_elapsed    | 5101    |\n","|    total_timesteps | 4411392 |\n","--------------------------------\n","Eval num_timesteps=4416000, episode_reward=25.06 +/- 0.40\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 25.1        |\n","| time/                   |             |\n","|    total_timesteps      | 4416000     |\n","| train/                  |             |\n","|    approx_kl            | 0.018121688 |\n","|    clip_fraction        | 0.188       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.54       |\n","|    explained_variance   | 0.887       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.235       |\n","|    n_updates            | 3590        |\n","|    policy_gradient_loss | -0.00539    |\n","|    std                  | 0.358       |\n","|    value_loss           | 1.01        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 360     |\n","|    time_elapsed    | 5117    |\n","|    total_timesteps | 4423680 |\n","--------------------------------\n","Eval num_timesteps=4428000, episode_reward=14.05 +/- 0.51\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 14          |\n","| time/                   |             |\n","|    total_timesteps      | 4428000     |\n","| train/                  |             |\n","|    approx_kl            | 0.018456453 |\n","|    clip_fraction        | 0.229       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.55       |\n","|    explained_variance   | 0.727       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.127       |\n","|    n_updates            | 3600        |\n","|    policy_gradient_loss | -0.0103     |\n","|    std                  | 0.357       |\n","|    value_loss           | 0.343       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 361     |\n","|    time_elapsed    | 5133    |\n","|    total_timesteps | 4435968 |\n","--------------------------------\n","Eval num_timesteps=4440000, episode_reward=17.16 +/- 1.28\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 17.2        |\n","| time/                   |             |\n","|    total_timesteps      | 4440000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008863512 |\n","|    clip_fraction        | 0.146       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.55       |\n","|    explained_variance   | 0.738       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.342       |\n","|    n_updates            | 3610        |\n","|    policy_gradient_loss | -0.00431    |\n","|    std                  | 0.358       |\n","|    value_loss           | 2.41        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 863     |\n","|    iterations      | 362     |\n","|    time_elapsed    | 5149    |\n","|    total_timesteps | 4448256 |\n","--------------------------------\n","Eval num_timesteps=4452000, episode_reward=-30.72 +/- 47.16\n","Episode length: 469.60 +/- 37.23\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 470        |\n","|    mean_reward          | -30.7      |\n","| time/                   |            |\n","|    total_timesteps      | 4452000    |\n","| train/                  |            |\n","|    approx_kl            | 0.00802747 |\n","|    clip_fraction        | 0.0748     |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.55      |\n","|    explained_variance   | 0.82       |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 1.42       |\n","|    n_updates            | 3620       |\n","|    policy_gradient_loss | -0.00378   |\n","|    std                  | 0.357      |\n","|    value_loss           | 7.99       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 863     |\n","|    iterations      | 363     |\n","|    time_elapsed    | 5164    |\n","|    total_timesteps | 4460544 |\n","--------------------------------\n","Eval num_timesteps=4464000, episode_reward=-20.43 +/- 59.07\n","Episode length: 469.60 +/- 37.23\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 470         |\n","|    mean_reward          | -20.4       |\n","| time/                   |             |\n","|    total_timesteps      | 4464000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015668029 |\n","|    clip_fraction        | 0.192       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.52       |\n","|    explained_variance   | 0.332       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.267       |\n","|    n_updates            | 3630        |\n","|    policy_gradient_loss | -0.00465    |\n","|    std                  | 0.353       |\n","|    value_loss           | 1.14        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 863     |\n","|    iterations      | 364     |\n","|    time_elapsed    | 5177    |\n","|    total_timesteps | 4472832 |\n","--------------------------------\n","Eval num_timesteps=4476000, episode_reward=28.72 +/- 5.56\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 28.7        |\n","| time/                   |             |\n","|    total_timesteps      | 4476000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014421125 |\n","|    clip_fraction        | 0.19        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.48       |\n","|    explained_variance   | 0.754       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.174       |\n","|    n_updates            | 3640        |\n","|    policy_gradient_loss | -0.00865    |\n","|    std                  | 0.35        |\n","|    value_loss           | 0.308       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 863     |\n","|    iterations      | 365     |\n","|    time_elapsed    | 5192    |\n","|    total_timesteps | 4485120 |\n","--------------------------------\n","Eval num_timesteps=4488000, episode_reward=15.35 +/- 1.11\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 15.3        |\n","| time/                   |             |\n","|    total_timesteps      | 4488000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011941496 |\n","|    clip_fraction        | 0.17        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.46       |\n","|    explained_variance   | 0.649       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.402       |\n","|    n_updates            | 3650        |\n","|    policy_gradient_loss | -0.00199    |\n","|    std                  | 0.35        |\n","|    value_loss           | 1.88        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 863     |\n","|    iterations      | 366     |\n","|    time_elapsed    | 5207    |\n","|    total_timesteps | 4497408 |\n","--------------------------------\n","Eval num_timesteps=4500000, episode_reward=14.00 +/- 3.62\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 14          |\n","| time/                   |             |\n","|    total_timesteps      | 4500000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013919793 |\n","|    clip_fraction        | 0.164       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.44       |\n","|    explained_variance   | 0.691       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.313       |\n","|    n_updates            | 3660        |\n","|    policy_gradient_loss | -0.00681    |\n","|    std                  | 0.347       |\n","|    value_loss           | 0.567       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 367     |\n","|    time_elapsed    | 5219    |\n","|    total_timesteps | 4509696 |\n","--------------------------------\n","Eval num_timesteps=4512000, episode_reward=15.72 +/- 3.53\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 15.7        |\n","| time/                   |             |\n","|    total_timesteps      | 4512000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011851896 |\n","|    clip_fraction        | 0.145       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.41       |\n","|    explained_variance   | 0.651       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.448       |\n","|    n_updates            | 3670        |\n","|    policy_gradient_loss | -0.00543    |\n","|    std                  | 0.344       |\n","|    value_loss           | 1.29        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 863     |\n","|    iterations      | 368     |\n","|    time_elapsed    | 5234    |\n","|    total_timesteps | 4521984 |\n","--------------------------------\n","Eval num_timesteps=4524000, episode_reward=-39.93 +/- 59.61\n","Episode length: 484.40 +/- 12.74\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 484         |\n","|    mean_reward          | -39.9       |\n","| time/                   |             |\n","|    total_timesteps      | 4524000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012400344 |\n","|    clip_fraction        | 0.16        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.39       |\n","|    explained_variance   | 0.775       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.25        |\n","|    n_updates            | 3680        |\n","|    policy_gradient_loss | -0.00591    |\n","|    std                  | 0.344       |\n","|    value_loss           | 1.31        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 863     |\n","|    iterations      | 369     |\n","|    time_elapsed    | 5248    |\n","|    total_timesteps | 4534272 |\n","--------------------------------\n","Eval num_timesteps=4536000, episode_reward=22.70 +/- 2.26\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 22.7        |\n","| time/                   |             |\n","|    total_timesteps      | 4536000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013973896 |\n","|    clip_fraction        | 0.2         |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.38       |\n","|    explained_variance   | 0.611       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.303       |\n","|    n_updates            | 3690        |\n","|    policy_gradient_loss | -0.00517    |\n","|    std                  | 0.342       |\n","|    value_loss           | 0.612       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 370     |\n","|    time_elapsed    | 5259    |\n","|    total_timesteps | 4546560 |\n","--------------------------------\n","Eval num_timesteps=4548000, episode_reward=20.41 +/- 3.84\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 20.4        |\n","| time/                   |             |\n","|    total_timesteps      | 4548000     |\n","| train/                  |             |\n","|    approx_kl            | 0.016342442 |\n","|    clip_fraction        | 0.181       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.35       |\n","|    explained_variance   | 0.798       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.11        |\n","|    n_updates            | 3700        |\n","|    policy_gradient_loss | -0.00763    |\n","|    std                  | 0.339       |\n","|    value_loss           | 0.332       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 371     |\n","|    time_elapsed    | 5275    |\n","|    total_timesteps | 4558848 |\n","--------------------------------\n","Eval num_timesteps=4560000, episode_reward=-26.94 +/- 52.86\n","Episode length: 497.60 +/- 2.94\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 498         |\n","|    mean_reward          | -26.9       |\n","| time/                   |             |\n","|    total_timesteps      | 4560000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013214408 |\n","|    clip_fraction        | 0.181       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.33       |\n","|    explained_variance   | 0.834       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.123       |\n","|    n_updates            | 3710        |\n","|    policy_gradient_loss | -0.00929    |\n","|    std                  | 0.338       |\n","|    value_loss           | 0.334       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 372     |\n","|    time_elapsed    | 5288    |\n","|    total_timesteps | 4571136 |\n","--------------------------------\n","Eval num_timesteps=4572000, episode_reward=23.46 +/- 6.34\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 23.5        |\n","| time/                   |             |\n","|    total_timesteps      | 4572000     |\n","| train/                  |             |\n","|    approx_kl            | 0.016597362 |\n","|    clip_fraction        | 0.154       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.32       |\n","|    explained_variance   | 0.281       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.195       |\n","|    n_updates            | 3720        |\n","|    policy_gradient_loss | -0.00267    |\n","|    std                  | 0.338       |\n","|    value_loss           | 3.06        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 373     |\n","|    time_elapsed    | 5301    |\n","|    total_timesteps | 4583424 |\n","--------------------------------\n","Eval num_timesteps=4584000, episode_reward=14.35 +/- 0.49\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 14.3        |\n","| time/                   |             |\n","|    total_timesteps      | 4584000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011931072 |\n","|    clip_fraction        | 0.166       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.3        |\n","|    explained_variance   | 0.885       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.217       |\n","|    n_updates            | 3730        |\n","|    policy_gradient_loss | -0.00621    |\n","|    std                  | 0.335       |\n","|    value_loss           | 0.633       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 374     |\n","|    time_elapsed    | 5316    |\n","|    total_timesteps | 4595712 |\n","--------------------------------\n","Eval num_timesteps=4596000, episode_reward=-44.12 +/- 52.76\n","Episode length: 460.40 +/- 32.33\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 460         |\n","|    mean_reward          | -44.1       |\n","| time/                   |             |\n","|    total_timesteps      | 4596000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010749421 |\n","|    clip_fraction        | 0.154       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.29       |\n","|    explained_variance   | 0.462       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.362       |\n","|    n_updates            | 3740        |\n","|    policy_gradient_loss | -0.0055     |\n","|    std                  | 0.335       |\n","|    value_loss           | 3.17        |\n","-----------------------------------------\n","Eval num_timesteps=4608000, episode_reward=20.39 +/- 6.83\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 20.4     |\n","| time/              |          |\n","|    total_timesteps | 4608000  |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 863     |\n","|    iterations      | 375     |\n","|    time_elapsed    | 5336    |\n","|    total_timesteps | 4608000 |\n","--------------------------------\n","Eval num_timesteps=4620000, episode_reward=26.12 +/- 2.71\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 26.1        |\n","| time/                   |             |\n","|    total_timesteps      | 4620000     |\n","| train/                  |             |\n","|    approx_kl            | 0.017985336 |\n","|    clip_fraction        | 0.187       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.26       |\n","|    explained_variance   | 0.0736      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.235       |\n","|    n_updates            | 3750        |\n","|    policy_gradient_loss | -0.00549    |\n","|    std                  | 0.331       |\n","|    value_loss           | 0.811       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 376     |\n","|    time_elapsed    | 5347    |\n","|    total_timesteps | 4620288 |\n","--------------------------------\n","Eval num_timesteps=4632000, episode_reward=27.36 +/- 0.80\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 27.4         |\n","| time/                   |              |\n","|    total_timesteps      | 4632000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0071107685 |\n","|    clip_fraction        | 0.0848       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -1.24        |\n","|    explained_variance   | 0.611        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 8.93         |\n","|    n_updates            | 3760         |\n","|    policy_gradient_loss | -0.00593     |\n","|    std                  | 0.331        |\n","|    value_loss           | 32.8         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 863     |\n","|    iterations      | 377     |\n","|    time_elapsed    | 5362    |\n","|    total_timesteps | 4632576 |\n","--------------------------------\n","Eval num_timesteps=4644000, episode_reward=12.71 +/- 0.34\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 12.7        |\n","| time/                   |             |\n","|    total_timesteps      | 4644000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015943803 |\n","|    clip_fraction        | 0.183       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.21       |\n","|    explained_variance   | 0.64        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.277       |\n","|    n_updates            | 3770        |\n","|    policy_gradient_loss | -0.0143     |\n","|    std                  | 0.328       |\n","|    value_loss           | 0.649       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 863     |\n","|    iterations      | 378     |\n","|    time_elapsed    | 5376    |\n","|    total_timesteps | 4644864 |\n","--------------------------------\n","Eval num_timesteps=4656000, episode_reward=23.61 +/- 5.09\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 23.6        |\n","| time/                   |             |\n","|    total_timesteps      | 4656000     |\n","| train/                  |             |\n","|    approx_kl            | 0.007268278 |\n","|    clip_fraction        | 0.116       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.2        |\n","|    explained_variance   | 0.728       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.36        |\n","|    n_updates            | 3780        |\n","|    policy_gradient_loss | -0.00364    |\n","|    std                  | 0.328       |\n","|    value_loss           | 9.09        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 379     |\n","|    time_elapsed    | 5388    |\n","|    total_timesteps | 4657152 |\n","--------------------------------\n","Eval num_timesteps=4668000, episode_reward=17.26 +/- 0.89\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 17.3        |\n","| time/                   |             |\n","|    total_timesteps      | 4668000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015524149 |\n","|    clip_fraction        | 0.204       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.19       |\n","|    explained_variance   | 0.608       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.137       |\n","|    n_updates            | 3790        |\n","|    policy_gradient_loss | -0.0063     |\n","|    std                  | 0.326       |\n","|    value_loss           | 0.447       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 863     |\n","|    iterations      | 380     |\n","|    time_elapsed    | 5405    |\n","|    total_timesteps | 4669440 |\n","--------------------------------\n","Eval num_timesteps=4680000, episode_reward=-53.07 +/- 48.30\n","Episode length: 494.00 +/- 4.90\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 494         |\n","|    mean_reward          | -53.1       |\n","| time/                   |             |\n","|    total_timesteps      | 4680000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013624261 |\n","|    clip_fraction        | 0.166       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.17       |\n","|    explained_variance   | 0.768       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.17        |\n","|    n_updates            | 3800        |\n","|    policy_gradient_loss | -0.0101     |\n","|    std                  | 0.326       |\n","|    value_loss           | 0.35        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 863     |\n","|    iterations      | 381     |\n","|    time_elapsed    | 5419    |\n","|    total_timesteps | 4681728 |\n","--------------------------------\n","Eval num_timesteps=4692000, episode_reward=16.23 +/- 4.76\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 16.2        |\n","| time/                   |             |\n","|    total_timesteps      | 4692000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013979573 |\n","|    clip_fraction        | 0.175       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.16       |\n","|    explained_variance   | 0.706       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.15        |\n","|    n_updates            | 3810        |\n","|    policy_gradient_loss | -0.00907    |\n","|    std                  | 0.324       |\n","|    value_loss           | 0.41        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 382     |\n","|    time_elapsed    | 5432    |\n","|    total_timesteps | 4694016 |\n","--------------------------------\n","Eval num_timesteps=4704000, episode_reward=25.98 +/- 0.68\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 26          |\n","| time/                   |             |\n","|    total_timesteps      | 4704000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011353381 |\n","|    clip_fraction        | 0.158       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.14       |\n","|    explained_variance   | 0.459       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.255       |\n","|    n_updates            | 3820        |\n","|    policy_gradient_loss | -0.0011     |\n","|    std                  | 0.323       |\n","|    value_loss           | 2.15        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 863     |\n","|    iterations      | 383     |\n","|    time_elapsed    | 5447    |\n","|    total_timesteps | 4706304 |\n","--------------------------------\n","Eval num_timesteps=4716000, episode_reward=23.52 +/- 0.22\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 23.5        |\n","| time/                   |             |\n","|    total_timesteps      | 4716000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008226089 |\n","|    clip_fraction        | 0.09        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.14       |\n","|    explained_variance   | 0.79        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.81        |\n","|    n_updates            | 3830        |\n","|    policy_gradient_loss | -0.00373    |\n","|    std                  | 0.323       |\n","|    value_loss           | 6.25        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 863     |\n","|    iterations      | 384     |\n","|    time_elapsed    | 5461    |\n","|    total_timesteps | 4718592 |\n","--------------------------------\n","Eval num_timesteps=4728000, episode_reward=-21.52 +/- 44.40\n","Episode length: 473.00 +/- 22.05\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 473         |\n","|    mean_reward          | -21.5       |\n","| time/                   |             |\n","|    total_timesteps      | 4728000     |\n","| train/                  |             |\n","|    approx_kl            | 0.016607074 |\n","|    clip_fraction        | 0.201       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.12       |\n","|    explained_variance   | 0.816       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.113       |\n","|    n_updates            | 3840        |\n","|    policy_gradient_loss | -0.00732    |\n","|    std                  | 0.321       |\n","|    value_loss           | 0.342       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 385     |\n","|    time_elapsed    | 5473    |\n","|    total_timesteps | 4730880 |\n","--------------------------------\n","Eval num_timesteps=4740000, episode_reward=30.20 +/- 0.51\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 30.2        |\n","| time/                   |             |\n","|    total_timesteps      | 4740000     |\n","| train/                  |             |\n","|    approx_kl            | 0.018234236 |\n","|    clip_fraction        | 0.193       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.1        |\n","|    explained_variance   | 0.898       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.11        |\n","|    n_updates            | 3850        |\n","|    policy_gradient_loss | -0.00903    |\n","|    std                  | 0.319       |\n","|    value_loss           | 0.249       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 863     |\n","|    iterations      | 386     |\n","|    time_elapsed    | 5493    |\n","|    total_timesteps | 4743168 |\n","--------------------------------\n","Eval num_timesteps=4752000, episode_reward=23.55 +/- 0.89\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 23.5        |\n","| time/                   |             |\n","|    total_timesteps      | 4752000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014471854 |\n","|    clip_fraction        | 0.188       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.07       |\n","|    explained_variance   | 0.896       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0838      |\n","|    n_updates            | 3860        |\n","|    policy_gradient_loss | -0.0101     |\n","|    std                  | 0.317       |\n","|    value_loss           | 0.247       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 863     |\n","|    iterations      | 387     |\n","|    time_elapsed    | 5505    |\n","|    total_timesteps | 4755456 |\n","--------------------------------\n","Eval num_timesteps=4764000, episode_reward=36.08 +/- 8.65\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 36.1        |\n","| time/                   |             |\n","|    total_timesteps      | 4764000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008686258 |\n","|    clip_fraction        | 0.117       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.06       |\n","|    explained_variance   | 0.703       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.27        |\n","|    n_updates            | 3870        |\n","|    policy_gradient_loss | -0.00179    |\n","|    std                  | 0.317       |\n","|    value_loss           | 4.03        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 863     |\n","|    iterations      | 388     |\n","|    time_elapsed    | 5519    |\n","|    total_timesteps | 4767744 |\n","--------------------------------\n","Eval num_timesteps=4776000, episode_reward=23.99 +/- 5.49\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 24           |\n","| time/                   |              |\n","|    total_timesteps      | 4776000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0075639947 |\n","|    clip_fraction        | 0.0622       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -1.06        |\n","|    explained_variance   | 0.871        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 1.09         |\n","|    n_updates            | 3880         |\n","|    policy_gradient_loss | -0.00391     |\n","|    std                  | 0.317        |\n","|    value_loss           | 2.53         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 863     |\n","|    iterations      | 389     |\n","|    time_elapsed    | 5535    |\n","|    total_timesteps | 4780032 |\n","--------------------------------\n","Eval num_timesteps=4788000, episode_reward=29.68 +/- 0.35\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 29.7        |\n","| time/                   |             |\n","|    total_timesteps      | 4788000     |\n","| train/                  |             |\n","|    approx_kl            | 0.017622417 |\n","|    clip_fraction        | 0.202       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.04       |\n","|    explained_variance   | 0.76        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.0905      |\n","|    n_updates            | 3890        |\n","|    policy_gradient_loss | -0.00854    |\n","|    std                  | 0.314       |\n","|    value_loss           | 0.377       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 390     |\n","|    time_elapsed    | 5546    |\n","|    total_timesteps | 4792320 |\n","--------------------------------\n","Eval num_timesteps=4800000, episode_reward=24.24 +/- 2.48\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 24.2        |\n","| time/                   |             |\n","|    total_timesteps      | 4800000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015906343 |\n","|    clip_fraction        | 0.191       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.01       |\n","|    explained_variance   | 0.88        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.114       |\n","|    n_updates            | 3900        |\n","|    policy_gradient_loss | -0.00856    |\n","|    std                  | 0.312       |\n","|    value_loss           | 0.261       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 863     |\n","|    iterations      | 391     |\n","|    time_elapsed    | 5561    |\n","|    total_timesteps | 4804608 |\n","--------------------------------\n","Eval num_timesteps=4812000, episode_reward=19.62 +/- 10.52\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 19.6        |\n","| time/                   |             |\n","|    total_timesteps      | 4812000     |\n","| train/                  |             |\n","|    approx_kl            | 0.006848505 |\n","|    clip_fraction        | 0.112       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.997      |\n","|    explained_variance   | 0.814       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.84        |\n","|    n_updates            | 3910        |\n","|    policy_gradient_loss | -0.00144    |\n","|    std                  | 0.312       |\n","|    value_loss           | 5.11        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 863     |\n","|    iterations      | 392     |\n","|    time_elapsed    | 5576    |\n","|    total_timesteps | 4816896 |\n","--------------------------------\n","Eval num_timesteps=4824000, episode_reward=30.85 +/- 3.01\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 30.9        |\n","| time/                   |             |\n","|    total_timesteps      | 4824000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012730099 |\n","|    clip_fraction        | 0.118       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.996      |\n","|    explained_variance   | 0.901       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.526       |\n","|    n_updates            | 3920        |\n","|    policy_gradient_loss | -0.00252    |\n","|    std                  | 0.312       |\n","|    value_loss           | 1.39        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 393     |\n","|    time_elapsed    | 5587    |\n","|    total_timesteps | 4829184 |\n","--------------------------------\n","Eval num_timesteps=4836000, episode_reward=34.73 +/- 2.60\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 34.7         |\n","| time/                   |              |\n","|    total_timesteps      | 4836000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0113824075 |\n","|    clip_fraction        | 0.0955       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -0.997       |\n","|    explained_variance   | 0.614        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 4.76         |\n","|    n_updates            | 3930         |\n","|    policy_gradient_loss | -0.00445     |\n","|    std                  | 0.312        |\n","|    value_loss           | 10.7         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 394     |\n","|    time_elapsed    | 5602    |\n","|    total_timesteps | 4841472 |\n","--------------------------------\n","Eval num_timesteps=4848000, episode_reward=25.14 +/- 6.75\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 25.1         |\n","| time/                   |              |\n","|    total_timesteps      | 4848000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0067511313 |\n","|    clip_fraction        | 0.0693       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -0.993       |\n","|    explained_variance   | 0.818        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 5.28         |\n","|    n_updates            | 3940         |\n","|    policy_gradient_loss | -0.00365     |\n","|    std                  | 0.312        |\n","|    value_loss           | 8.82         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 395     |\n","|    time_elapsed    | 5617    |\n","|    total_timesteps | 4853760 |\n","--------------------------------\n","Eval num_timesteps=4860000, episode_reward=36.65 +/- 0.98\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 36.6        |\n","| time/                   |             |\n","|    total_timesteps      | 4860000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010963943 |\n","|    clip_fraction        | 0.0994      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.987      |\n","|    explained_variance   | 0.867       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.27        |\n","|    n_updates            | 3950        |\n","|    policy_gradient_loss | -0.00482    |\n","|    std                  | 0.311       |\n","|    value_loss           | 5.69        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 396     |\n","|    time_elapsed    | 5628    |\n","|    total_timesteps | 4866048 |\n","--------------------------------\n","Eval num_timesteps=4872000, episode_reward=-7.51 +/- 42.21\n","Episode length: 474.00 +/- 31.84\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 474         |\n","|    mean_reward          | -7.51       |\n","| time/                   |             |\n","|    total_timesteps      | 4872000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012730821 |\n","|    clip_fraction        | 0.127       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.978      |\n","|    explained_variance   | 0.909       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.76        |\n","|    n_updates            | 3960        |\n","|    policy_gradient_loss | -0.00597    |\n","|    std                  | 0.31        |\n","|    value_loss           | 5.32        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 397     |\n","|    time_elapsed    | 5644    |\n","|    total_timesteps | 4878336 |\n","--------------------------------\n","Eval num_timesteps=4884000, episode_reward=30.86 +/- 0.82\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 30.9        |\n","| time/                   |             |\n","|    total_timesteps      | 4884000     |\n","| train/                  |             |\n","|    approx_kl            | 0.017366977 |\n","|    clip_fraction        | 0.192       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.986      |\n","|    explained_variance   | 0.937       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.421       |\n","|    n_updates            | 3970        |\n","|    policy_gradient_loss | -0.00582    |\n","|    std                  | 0.312       |\n","|    value_loss           | 1.41        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 398     |\n","|    time_elapsed    | 5659    |\n","|    total_timesteps | 4890624 |\n","--------------------------------\n","Eval num_timesteps=4896000, episode_reward=29.16 +/- 4.98\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 29.2        |\n","| time/                   |             |\n","|    total_timesteps      | 4896000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008683524 |\n","|    clip_fraction        | 0.0996      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.992      |\n","|    explained_variance   | 0.874       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 9.02        |\n","|    n_updates            | 3980        |\n","|    policy_gradient_loss | -0.00667    |\n","|    std                  | 0.311       |\n","|    value_loss           | 16.1        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 399     |\n","|    time_elapsed    | 5670    |\n","|    total_timesteps | 4902912 |\n","--------------------------------\n","Eval num_timesteps=4908000, episode_reward=30.32 +/- 4.55\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 30.3        |\n","| time/                   |             |\n","|    total_timesteps      | 4908000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009987681 |\n","|    clip_fraction        | 0.107       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.977      |\n","|    explained_variance   | 0.869       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.12        |\n","|    n_updates            | 3990        |\n","|    policy_gradient_loss | -0.00498    |\n","|    std                  | 0.31        |\n","|    value_loss           | 10.5        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 400     |\n","|    time_elapsed    | 5685    |\n","|    total_timesteps | 4915200 |\n","--------------------------------\n","Eval num_timesteps=4920000, episode_reward=28.72 +/- 9.76\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 28.7        |\n","| time/                   |             |\n","|    total_timesteps      | 4920000     |\n","| train/                  |             |\n","|    approx_kl            | 0.016192796 |\n","|    clip_fraction        | 0.154       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.969      |\n","|    explained_variance   | 0.672       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.33        |\n","|    n_updates            | 4000        |\n","|    policy_gradient_loss | -0.0036     |\n","|    std                  | 0.309       |\n","|    value_loss           | 7.82        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 401     |\n","|    time_elapsed    | 5699    |\n","|    total_timesteps | 4927488 |\n","--------------------------------\n","Eval num_timesteps=4932000, episode_reward=29.12 +/- 0.46\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 29.1        |\n","| time/                   |             |\n","|    total_timesteps      | 4932000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015258163 |\n","|    clip_fraction        | 0.194       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.969      |\n","|    explained_variance   | 0.784       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.94        |\n","|    n_updates            | 4010        |\n","|    policy_gradient_loss | -0.00323    |\n","|    std                  | 0.31        |\n","|    value_loss           | 4.82        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 402     |\n","|    time_elapsed    | 5711    |\n","|    total_timesteps | 4939776 |\n","--------------------------------\n","Eval num_timesteps=4944000, episode_reward=35.81 +/- 1.42\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 35.8        |\n","| time/                   |             |\n","|    total_timesteps      | 4944000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011395487 |\n","|    clip_fraction        | 0.133       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.96       |\n","|    explained_variance   | 0.857       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.75        |\n","|    n_updates            | 4020        |\n","|    policy_gradient_loss | -0.00256    |\n","|    std                  | 0.308       |\n","|    value_loss           | 8.72        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 403     |\n","|    time_elapsed    | 5726    |\n","|    total_timesteps | 4952064 |\n","--------------------------------\n","Eval num_timesteps=4956000, episode_reward=21.56 +/- 13.36\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 21.6       |\n","| time/                   |            |\n","|    total_timesteps      | 4956000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01482597 |\n","|    clip_fraction        | 0.149      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -0.942     |\n","|    explained_variance   | 0.925      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 1.41       |\n","|    n_updates            | 4030       |\n","|    policy_gradient_loss | -0.00346   |\n","|    std                  | 0.307      |\n","|    value_loss           | 4.2        |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 404     |\n","|    time_elapsed    | 5740    |\n","|    total_timesteps | 4964352 |\n","--------------------------------\n","Eval num_timesteps=4968000, episode_reward=23.40 +/- 7.94\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 23.4        |\n","| time/                   |             |\n","|    total_timesteps      | 4968000     |\n","| train/                  |             |\n","|    approx_kl            | 0.017500754 |\n","|    clip_fraction        | 0.207       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.941      |\n","|    explained_variance   | 0.974       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.41        |\n","|    n_updates            | 4040        |\n","|    policy_gradient_loss | -0.00361    |\n","|    std                  | 0.308       |\n","|    value_loss           | 1.41        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 865     |\n","|    iterations      | 405     |\n","|    time_elapsed    | 5752    |\n","|    total_timesteps | 4976640 |\n","--------------------------------\n","Eval num_timesteps=4980000, episode_reward=21.64 +/- 6.63\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 21.6       |\n","| time/                   |            |\n","|    total_timesteps      | 4980000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01654669 |\n","|    clip_fraction        | 0.214      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -0.942     |\n","|    explained_variance   | 0.202      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 13.2       |\n","|    n_updates            | 4050       |\n","|    policy_gradient_loss | -0.00964   |\n","|    std                  | 0.308      |\n","|    value_loss           | 17         |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 406     |\n","|    time_elapsed    | 5767    |\n","|    total_timesteps | 4988928 |\n","--------------------------------\n","Eval num_timesteps=4992000, episode_reward=19.27 +/- 10.52\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 19.3        |\n","| time/                   |             |\n","|    total_timesteps      | 4992000     |\n","| train/                  |             |\n","|    approx_kl            | 0.021692239 |\n","|    clip_fraction        | 0.26        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.927      |\n","|    explained_variance   | 0.918       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.352       |\n","|    n_updates            | 4060        |\n","|    policy_gradient_loss | -0.00486    |\n","|    std                  | 0.307       |\n","|    value_loss           | 1.27        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 864     |\n","|    iterations      | 407     |\n","|    time_elapsed    | 5781    |\n","|    total_timesteps | 5001216 |\n","--------------------------------\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyPY5o3ZypgKAdKlyUVTIgGz"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}