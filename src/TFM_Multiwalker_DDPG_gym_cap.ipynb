{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"d8JAmEUyj9De"},"outputs":[],"source":["# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n","mount='/content/gdrive'\n","drive_root = mount + \"/My Drive/TFM\"\n","\n","try:\n","  from google.colab import drive\n","  IN_COLAB=True\n","except:\n","  IN_COLAB=False"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32686,"status":"ok","timestamp":1698848490349,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-60},"id":"DrEo9QnxkAne","outputId":"cf69f81a-e5c5-46d5-cbc7-eb4c30d32027"},"outputs":[{"output_type":"stream","name":"stdout","text":["We're running Colab\n","Colab: mounting Google drive on  /content/gdrive\n","Mounted at /content/gdrive\n","\n","Colab: making sure  /content/gdrive/My Drive/TFM  exists.\n","\n","Colab: Changing directory to  /content/gdrive/My Drive/TFM\n","/content/gdrive/My Drive/TFM\n","Archivos en el directorio: \n","['Entrenamientos antiguos sin logs', '.ipynb_checkpoints', 'Entrenamientos_log_no_eval', 'PPO_policies', 'DQN_new_pettingzoo_gym_cap.ipynb', 'multi_car_racing', 'policy_log_eval', 'DQN_policies', 'results_rllib', 'MCR_TFM.ipynb', 'multiwalker_ddpg_log_eval', 'multiwalker_sac_log_eval', 'multiwalker_ddpg.zip', 'multiwalker_ppo_log_eval', 'multiwalker_ppo.zip', 'multiwalker_td3_log_eval', 'multiwalker_sac2_log_eval', 'multiwalker_td3_2_log_eval', 'multiwalker_sac3_log_eval', 'multiwalker_sac3.zip', 'multiwalker_ppo_2_log_eval', 'multiwalker_ddpg2_log_eval', 'multiwalker_ppo_2.zip', 'TFM_Multiwalker_DDPG_PPO_gym_cap.ipynb', 'multiwalker_td3_3_log_eval', 'TFM_Multiwalker_TD3_gym_cap.ipynb', 'TFM_Multiwalker_SAC_gym_cap.ipynb', 'TFM_Multiwalker_SAC_recompensas_gym_cap.ipynb', 'TFM_PPO_KAZ_gym_cap.ipynb', 'TFM_PPO_new_KAZ_gym_cap.ipynb', 'TFM_Multiwalker_DDPG_recompensas_gym_cap.ipynb', 'multiwalker_ddpg2_5_log_eval', 'multiwalker_ppo_rew_08_log_eval', 'multiwalker_ppo_rew_08.zip', '=2.13', 'multiwalker_ppo_08_2_log_eval', 'TFM_Multiwalker_PPO_recompensas_gym_cap.ipynb', 'TFM_Multiwalker_DDPG_gym_cap.ipynb']\n"]}],"source":["# Switch to the directory on the Google Drive that you want to use\n","import os\n","if IN_COLAB:\n","  print(\"We're running Colab\")\n","\n","  if IN_COLAB:\n","    # Mount the Google Drive at mount\n","    print(\"Colab: mounting Google drive on \", mount)\n","\n","    drive.mount(mount)\n","\n","    # Create drive_root if it doesn't exist\n","    create_drive_root = True\n","    if create_drive_root:\n","      print(\"\\nColab: making sure \", drive_root, \" exists.\")\n","      os.makedirs(drive_root, exist_ok=True)\n","\n","    # Change to the directory\n","    print(\"\\nColab: Changing directory to \", drive_root)\n","    %cd $drive_root\n","# Verify we're in the correct working directory\n","%pwd\n","print(\"Archivos en el directorio: \")\n","print(os.listdir())"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":283885,"status":"ok","timestamp":1698848774220,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-60},"id":"xAZDg478kEbs","colab":{"base_uri":"https://localhost:8080/"},"outputId":"38a38754-b2f4-4075-dbc2-2912457ef53b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gym==0.17.3\n","  Downloading gym-0.17.3.tar.gz (1.6 MB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.6 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym==0.17.3) (1.11.3)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.17.3) (1.23.5)\n","Collecting pyglet<=1.5.0,>=1.4.0 (from gym==0.17.3)\n","  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cloudpickle<1.7.0,>=1.2.0 (from gym==0.17.3)\n","  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3) (0.18.3)\n","Building wheels for collected packages: gym\n","  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym: filename=gym-0.17.3-py3-none-any.whl size=1654617 sha256=c0691af195a71e3245b3e4b713e28c237e0f4656ee2d1db1300ec073b1e60609\n","  Stored in directory: /root/.cache/pip/wheels/af/4b/74/fcfc8238472c34d7f96508a63c962ff3ac9485a9a4137afd4e\n","Successfully built gym\n","Installing collected packages: pyglet, cloudpickle, gym\n","  Attempting uninstall: cloudpickle\n","    Found existing installation: cloudpickle 2.2.1\n","    Uninstalling cloudpickle-2.2.1:\n","      Successfully uninstalled cloudpickle-2.2.1\n","  Attempting uninstall: gym\n","    Found existing installation: gym 0.25.2\n","    Uninstalling gym-0.25.2:\n","      Successfully uninstalled gym-0.25.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","bigframes 0.10.0 requires cloudpickle>=2.0.0, but you have cloudpickle 1.6.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed cloudpickle-1.6.0 gym-0.17.3 pyglet-1.5.0\n","Collecting git+https://github.com/Kojoley/atari-py.git\n","  Cloning https://github.com/Kojoley/atari-py.git to /tmp/pip-req-build-3xgx_xox\n","  Running command git clone --filter=blob:none --quiet https://github.com/Kojoley/atari-py.git /tmp/pip-req-build-3xgx_xox\n","  Resolved https://github.com/Kojoley/atari-py.git to commit 86a1e05c0a95e9e6233c3a413521fdb34ca8a089\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from atari-py==1.2.2) (1.23.5)\n","Building wheels for collected packages: atari-py\n","  Building wheel for atari-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for atari-py: filename=atari_py-1.2.2-cp310-cp310-linux_x86_64.whl size=4738553 sha256=f82c65f6a2d49bde8a0c5acadcde4cc510e6c93b99787fddd5b19521bf2ae505\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-4vh6c3ia/wheels/6c/9b/f1/8a80a63a233d6f4eb2e2ee8c7357f4875f21c3ffc7f2613f9f\n","Successfully built atari-py\n","Installing collected packages: atari-py\n","Successfully installed atari-py-1.2.2\n","Collecting keras-rl2==1.0.5\n","  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from keras-rl2==1.0.5) (2.14.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (23.5.26)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (16.0.6)\n","Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.2.0)\n","Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.23.5)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (23.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (4.5.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.34.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.59.0)\n","Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.14.1)\n","Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.14.0)\n","Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.14.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2==1.0.5) (0.41.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.2.2)\n","Installing collected packages: keras-rl2\n","Successfully installed keras-rl2-1.0.5\n","Collecting tensorflow==2.8\n","  Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl (497.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.6.3)\n","Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (23.5.26)\n","Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.9.0)\n","Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.8)\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (16.0.6)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.23.5)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.3.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (4.5.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.14.1)\n","Collecting tensorboard<2.9,>=2.8 (from tensorflow==2.8)\n","  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tf-estimator-nightly==2.8.0.dev2021122109 (from tensorflow==2.8)\n","  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting keras<2.9,>=2.8.0rc0 (from tensorflow==2.8)\n","  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.34.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.59.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.8) (0.41.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.17.3)\n","Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.31.0)\n","Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n","  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n","  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.2.2)\n","Installing collected packages: tf-estimator-nightly, tensorboard-plugin-wit, keras, tensorboard-data-server, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.14.0\n","    Uninstalling keras-2.14.0:\n","      Successfully uninstalled keras-2.14.0\n","  Attempting uninstall: tensorboard-data-server\n","    Found existing installation: tensorboard-data-server 0.7.2\n","    Uninstalling tensorboard-data-server-0.7.2:\n","      Successfully uninstalled tensorboard-data-server-0.7.2\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 1.0.0\n","    Uninstalling google-auth-oauthlib-1.0.0:\n","      Successfully uninstalled google-auth-oauthlib-1.0.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.14.1\n","    Uninstalling tensorboard-2.14.1:\n","      Successfully uninstalled tensorboard-2.14.1\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.14.0\n","    Uninstalling tensorflow-2.14.0:\n","      Successfully uninstalled tensorflow-2.14.0\n","Successfully installed google-auth-oauthlib-0.4.6 keras-2.8.0 keras-preprocessing-1.1.2 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tf-estimator-nightly-2.8.0.dev2021122109\n","Collecting gym-cap\n","  Downloading gym_cap-0.1.4.12-py3-none-any.whl (32 kB)\n","Requirement already satisfied: gym>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from gym-cap) (0.17.3)\n","Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from gym-cap) (1.23.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym>=0.16.0->gym-cap) (1.11.3)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.16.0->gym-cap) (1.5.0)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.16.0->gym-cap) (1.6.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.16.0->gym-cap) (0.18.3)\n","Installing collected packages: gym-cap\n","Successfully installed gym-cap-0.1.4.12\n","Collecting pettingzoo[butterfly]\n","  Downloading pettingzoo-1.24.1-py3-none-any.whl (840 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.8/840.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[butterfly]) (1.23.5)\n","Collecting gymnasium>=0.28.0 (from pettingzoo[butterfly])\n","  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pygame==2.3.0 (from pettingzoo[butterfly])\n","  Downloading pygame-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pymunk==6.2.0 (from pettingzoo[butterfly])\n","  Downloading pymunk-6.2.0.zip (7.1 MB)\n","\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/7.1 MB\u001b[0m \u001b[31m226.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n","\u001b[?25h\u001b[31mERROR: Exception:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/urllib3/response.py\", line 438, in _error_catcher\n","    yield\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/urllib3/response.py\", line 561, in read\n","    data = self._fp_read(amt) if not fp_closed else b\"\"\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/urllib3/response.py\", line 527, in _fp_read\n","    return self._fp.read(amt) if amt is not None else self._fp.read()\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 90, in read\n","    data = self.__fp.read(amt)\n","  File \"/usr/lib/python3.10/http/client.py\", line 466, in read\n","    s = self.fp.read(amt)\n","  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n","    return self._sock.recv_into(b)\n","  File \"/usr/lib/python3.10/ssl.py\", line 1274, in recv_into\n","    return self.read(nbytes, buffer)\n","  File \"/usr/lib/python3.10/ssl.py\", line 1130, in read\n","    return self._sslobj.read(len, buffer)\n","TimeoutError: The read operation timed out\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 169, in exc_logging_wrapper\n","    status = run_func(*args)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 242, in wrapper\n","    return func(self, options, args)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 377, in run\n","    requirement_set = resolver.resolve(\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 92, in resolve\n","    result = self._result = resolver.resolve(\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 546, in resolve\n","    state = resolution.resolve(requirements, max_rounds=max_rounds)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 427, in resolve\n","    failure_causes = self._attempt_to_pin_criterion(name)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 239, in _attempt_to_pin_criterion\n","    criteria = self._get_updated_criteria(candidate)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 230, in _get_updated_criteria\n","    self._add_to_criteria(criteria, requirement, parent=candidate)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 173, in _add_to_criteria\n","    if not criterion.candidates:\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/structs.py\", line 156, in __bool__\n","    return bool(self._sequence)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 155, in __bool__\n","    return any(self)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 143, in <genexpr>\n","    return (c for c in iterator if id(c) not in self._incompatible_ids)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 47, in _iter_built\n","    candidate = func()\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 206, in _make_candidate_from_link\n","    self._link_candidate_cache[link] = LinkCandidate(\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 293, in __init__\n","    super().__init__(\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 156, in __init__\n","    self.dist = self._prepare()\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 225, in _prepare\n","    dist = self._prepare_distribution()\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 304, in _prepare_distribution\n","    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/prepare.py\", line 516, in prepare_linked_requirement\n","    return self._prepare_linked_requirement(req, parallel_builds)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/prepare.py\", line 587, in _prepare_linked_requirement\n","    local_file = unpack_url(\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/prepare.py\", line 166, in unpack_url\n","    file = get_http_url(\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/prepare.py\", line 107, in get_http_url\n","    from_path, content_type = download(link, temp_dir.path)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/network/download.py\", line 147, in __call__\n","    for chunk in chunks:\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/progress_bars.py\", line 53, in _rich_progress_bar\n","    for chunk in iterable:\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/network/utils.py\", line 63, in response_chunks\n","    for chunk in response.raw.stream(\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/urllib3/response.py\", line 622, in stream\n","    data = self.read(amt=amt, decode_content=decode_content)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/urllib3/response.py\", line 560, in read\n","    with self._error_catcher():\n","  File \"/usr/lib/python3.10/contextlib.py\", line 153, in __exit__\n","    self.gen.throw(typ, value, traceback)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/urllib3/response.py\", line 443, in _error_catcher\n","    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n","pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["if IN_COLAB:\n","  %pip install gym==0.17.3\n","  %pip install git+https://github.com/Kojoley/atari-py.git\n","  %pip install keras-rl2==1.0.5\n","  %pip install tensorflow==2.8\n","  %pip install gym-cap\n","  %pip install pettingzoo[butterfly]\n","else:\n","  %pip install gym==0.17.3\n","  %pip install git+https://github.com/Kojoley/atari-py.git\n","  %pip install pyglet==1.5.0\n","  %pip install h5py==3.1.0\n","  %pip install Pillow==9.5.0\n","  %pip install keras-rl2==1.0.5\n","  %pip install Keras==2.2.4\n","  %pip install tensorflow==2.5.3\n","  %pip install torch==2.0.1\n","  %pip install agents==1.4.0"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1182,"status":"ok","timestamp":1698848775395,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-60},"id":"G8cw-IX3laE9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4bb26308-a266-4668-aa6b-d468c79d6222"},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'multi_car_racing' already exists and is not an empty directory.\n"]}],"source":["!git clone https://github.com/igilitschenski/multi_car_racing.git\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IqbMo3gK7vBG"},"outputs":[],"source":["!cd multi_car_racing\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6067,"status":"ok","timestamp":1698848781456,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-60},"id":"Jekec6f98b3A","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5ab534f0-a5bb-46a0-9dc0-364159bff9fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting swig\n","  Downloading swig-4.1.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: swig\n","Successfully installed swig-4.1.1\n"]}],"source":["!pip install swig"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":68682,"status":"ok","timestamp":1698848850129,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-60},"id":"KKxRPBFx85k6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d4452b65-0978-4d48-f6a1-f8a243774581"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting box2d\n","  Downloading Box2D-2.3.2.tar.gz (427 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.9/427.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: box2d\n","  Building wheel for box2d (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d: filename=Box2D-2.3.2-cp310-cp310-linux_x86_64.whl size=2391313 sha256=c851c95a816565459c3bf4ab2fb92f01072c0c1b31d2fc12a81438c20b5e6ab0\n","  Stored in directory: /root/.cache/pip/wheels/eb/cb/be/e663f3ce9aba6580611c0febaf7cd3cf7603f87047de2a52f9\n","Successfully built box2d\n","Installing collected packages: box2d\n","Successfully installed box2d-2.3.2\n"]}],"source":["!pip install box2d"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":69430,"status":"ok","timestamp":1698848919509,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-60},"id":"ijp5V0i09MRF","colab":{"base_uri":"https://localhost:8080/"},"outputId":"094a2057-0c8b-466e-f941-cf6dbcb66a18"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting box2d-py\n","  Downloading box2d-py-2.3.8.tar.gz (374 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/374.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/374.5 kB\u001b[0m \u001b[31m871.5 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/374.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.5/374.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: box2d-py\n","  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-py: filename=box2d_py-2.3.8-cp310-cp310-linux_x86_64.whl size=2373076 sha256=bba064c4c3e5f0ae6f3a00ec7915fdec7a40f81751f4912899c4446f6ea9a70d\n","  Stored in directory: /root/.cache/pip/wheels/47/01/d2/6a780da77ccb98b1d2facdd520a8d10838a03b590f6f8d50c0\n","Successfully built box2d-py\n","Installing collected packages: box2d-py\n","Successfully installed box2d-py-2.3.8\n"]}],"source":["!pip install box2d-py"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":36482,"status":"ok","timestamp":1698848955945,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-60},"id":"BwjugqI99g0I","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ca93f241-42b3-45d6-f4ef-51c3200dda25"},"outputs":[{"output_type":"stream","name":"stdout","text":["Obtaining file:///content/gdrive/MyDrive/TFM/multi_car_racing\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: box2d-py~=2.3.5 in /usr/local/lib/python3.10/dist-packages (from gym-multi-car-racing==0.0.1) (2.3.8)\n","Collecting shapely~=1.7.0 (from gym-multi-car-racing==0.0.1)\n","  Downloading Shapely-1.7.1.tar.gz (383 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.2/383.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from gym-multi-car-racing==0.0.1) (1.23.5)\n","Requirement already satisfied: gym~=0.17.2 in /usr/local/lib/python3.10/dist-packages (from gym-multi-car-racing==0.0.1) (0.17.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym~=0.17.2->gym-multi-car-racing==0.0.1) (1.11.3)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from gym~=0.17.2->gym-multi-car-racing==0.0.1) (1.5.0)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym~=0.17.2->gym-multi-car-racing==0.0.1) (1.6.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym~=0.17.2->gym-multi-car-racing==0.0.1) (0.18.3)\n","Building wheels for collected packages: shapely\n","  Building wheel for shapely (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for shapely: filename=Shapely-1.7.1-cp310-cp310-linux_x86_64.whl size=997164 sha256=c8c8d8d44968bc4955643ddd954c47e4b2dd8b3cc7f2988b6305865a165f121a\n","  Stored in directory: /root/.cache/pip/wheels/2e/fa/97/c85f587c35afcaf4a81c481741d36592518d1e50445572f0d4\n","Successfully built shapely\n","Installing collected packages: shapely, gym-multi-car-racing\n","  Attempting uninstall: shapely\n","    Found existing installation: shapely 2.0.2\n","    Uninstalling shapely-2.0.2:\n","      Successfully uninstalled shapely-2.0.2\n","  Running setup.py develop for gym-multi-car-racing\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires fastapi, which is not installed.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed gym-multi-car-racing-0.0.1 shapely-1.7.1\n"]}],"source":["!pip install -e multi_car_racing"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13514,"status":"ok","timestamp":1698848969410,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-60},"id":"IrAvXzCW-Z3e","colab":{"base_uri":"https://localhost:8080/"},"outputId":"283ad414-7544-4a52-c5fe-8c402d56a112"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  freeglut3 libegl-dev libgl-dev libgl1-mesa-dev libgles-dev libgles1\n","  libglu1-mesa libglu1-mesa-dev libglvnd-core-dev libglvnd-dev libglx-dev\n","  libice-dev libopengl-dev libsm-dev libxt-dev\n","Suggested packages:\n","  libice-doc libsm-doc libxt-doc\n","The following NEW packages will be installed:\n","  freeglut3 freeglut3-dev libegl-dev libgl-dev libgl1-mesa-dev libgles-dev\n","  libgles1 libglu1-mesa libglu1-mesa-dev libglvnd-core-dev libglvnd-dev\n","  libglx-dev libice-dev libopengl-dev libsm-dev libxt-dev\n","0 upgraded, 16 newly installed, 0 to remove and 19 not upgraded.\n","Need to get 1,261 kB of archives.\n","After this operation, 6,753 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3 amd64 2.8.1-6 [74.0 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglx-dev amd64 1.4.0-1 [14.1 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgl-dev amd64 1.4.0-1 [101 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-core-dev amd64 1.4.0-1 [12.7 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libegl-dev amd64 1.4.0-1 [18.0 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles1 amd64 1.4.0-1 [11.5 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles-dev amd64 1.4.0-1 [49.4 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libopengl-dev amd64 1.4.0-1 [3,400 B]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-dev amd64 1.4.0-1 [3,162 B]\n","Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgl1-mesa-dev amd64 23.0.4-0ubuntu1~22.04.1 [6,510 B]\n","Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n","Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa-dev amd64 9.0.2-1 [231 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 libice-dev amd64 2:1.0.10-1build2 [51.4 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsm-dev amd64 2:1.2.3-1build2 [18.1 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3-dev amd64 2.8.1-6 [126 kB]\n","Fetched 1,261 kB in 3s (398 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 16.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package freeglut3:amd64.\n","(Reading database ... 120874 files and directories currently installed.)\n","Preparing to unpack .../00-freeglut3_2.8.1-6_amd64.deb ...\n","Unpacking freeglut3:amd64 (2.8.1-6) ...\n","Selecting previously unselected package libglx-dev:amd64.\n","Preparing to unpack .../01-libglx-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglx-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgl-dev:amd64.\n","Preparing to unpack .../02-libgl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libgl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libglvnd-core-dev:amd64.\n","Preparing to unpack .../03-libglvnd-core-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglvnd-core-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libegl-dev:amd64.\n","Preparing to unpack .../04-libegl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libegl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgles1:amd64.\n","Preparing to unpack .../05-libgles1_1.4.0-1_amd64.deb ...\n","Unpacking libgles1:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgles-dev:amd64.\n","Preparing to unpack .../06-libgles-dev_1.4.0-1_amd64.deb ...\n","Unpacking libgles-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libopengl-dev:amd64.\n","Preparing to unpack .../07-libopengl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libopengl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libglvnd-dev:amd64.\n","Preparing to unpack .../08-libglvnd-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglvnd-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgl1-mesa-dev:amd64.\n","Preparing to unpack .../09-libgl1-mesa-dev_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n","Unpacking libgl1-mesa-dev:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Selecting previously unselected package libglu1-mesa:amd64.\n","Preparing to unpack .../10-libglu1-mesa_9.0.2-1_amd64.deb ...\n","Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n","Selecting previously unselected package libglu1-mesa-dev:amd64.\n","Preparing to unpack .../11-libglu1-mesa-dev_9.0.2-1_amd64.deb ...\n","Unpacking libglu1-mesa-dev:amd64 (9.0.2-1) ...\n","Selecting previously unselected package libice-dev:amd64.\n","Preparing to unpack .../12-libice-dev_2%3a1.0.10-1build2_amd64.deb ...\n","Unpacking libice-dev:amd64 (2:1.0.10-1build2) ...\n","Selecting previously unselected package libsm-dev:amd64.\n","Preparing to unpack .../13-libsm-dev_2%3a1.2.3-1build2_amd64.deb ...\n","Unpacking libsm-dev:amd64 (2:1.2.3-1build2) ...\n","Selecting previously unselected package libxt-dev:amd64.\n","Preparing to unpack .../14-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n","Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n","Selecting previously unselected package freeglut3-dev:amd64.\n","Preparing to unpack .../15-freeglut3-dev_2.8.1-6_amd64.deb ...\n","Unpacking freeglut3-dev:amd64 (2.8.1-6) ...\n","Setting up freeglut3:amd64 (2.8.1-6) ...\n","Setting up libglvnd-core-dev:amd64 (1.4.0-1) ...\n","Setting up libice-dev:amd64 (2:1.0.10-1build2) ...\n","Setting up libsm-dev:amd64 (2:1.2.3-1build2) ...\n","Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n","Setting up libgles1:amd64 (1.4.0-1) ...\n","Setting up libglx-dev:amd64 (1.4.0-1) ...\n","Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n","Setting up libopengl-dev:amd64 (1.4.0-1) ...\n","Setting up libgl-dev:amd64 (1.4.0-1) ...\n","Setting up libegl-dev:amd64 (1.4.0-1) ...\n","Setting up libglu1-mesa-dev:amd64 (9.0.2-1) ...\n","Setting up libgles-dev:amd64 (1.4.0-1) ...\n","Setting up libglvnd-dev:amd64 (1.4.0-1) ...\n","Setting up libgl1-mesa-dev:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Setting up freeglut3-dev:amd64 (2.8.1-6) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n"]}],"source":["!sudo apt-get install freeglut3-dev"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":24185,"status":"ok","timestamp":1698848993550,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-60},"id":"cgGdQ6n9EERW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9707c13e-aed6-4db4-bd3b-55391eae1ce7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n","  xserver-common\n","The following NEW packages will be installed:\n","  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n","  xserver-common xvfb\n","0 upgraded, 9 newly installed, 0 to remove and 19 not upgraded.\n","Need to get 7,814 kB of archives.\n","After this operation, 11.9 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.2 [28.1 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.2 [864 kB]\n","Fetched 7,814 kB in 2s (3,766 kB/s)\n","Selecting previously unselected package libfontenc1:amd64.\n","(Reading database ... 121332 files and directories currently installed.)\n","Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n","Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n","Selecting previously unselected package libxfont2:amd64.\n","Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n","Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n","Selecting previously unselected package libxkbfile1:amd64.\n","Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n","Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n","Selecting previously unselected package x11-xkb-utils.\n","Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n","Unpacking x11-xkb-utils (7.7+5build4) ...\n","Selecting previously unselected package xfonts-encodings.\n","Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n","Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n","Selecting previously unselected package xfonts-utils.\n","Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n","Unpacking xfonts-utils (1:7.7+6build2) ...\n","Selecting previously unselected package xfonts-base.\n","Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n","Unpacking xfonts-base (1:1.0.5) ...\n","Selecting previously unselected package xserver-common.\n","Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.2_all.deb ...\n","Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n","Selecting previously unselected package xvfb.\n","Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.2_amd64.deb ...\n","Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.2) ...\n","Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n","Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n","Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n","Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n","Setting up x11-xkb-utils (7.7+5build4) ...\n","Setting up xfonts-utils (1:7.7+6build2) ...\n","Setting up xfonts-base (1:1.0.5) ...\n","Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n","Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.2) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","Collecting pyvirtualdisplay\n","  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n","Installing collected packages: pyvirtualdisplay\n","Successfully installed pyvirtualdisplay-3.0\n","Collecting piglet\n","  Downloading piglet-1.0.0-py2.py3-none-any.whl (2.2 kB)\n","Collecting piglet-templates (from piglet)\n","  Downloading piglet_templates-1.3.0-py3-none-any.whl (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.5/67.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from piglet-templates->piglet) (3.1.1)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from piglet-templates->piglet) (23.1.0)\n","Requirement already satisfied: astunparse in /usr/local/lib/python3.10/dist-packages (from piglet-templates->piglet) (1.6.3)\n","Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from piglet-templates->piglet) (2.1.3)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse->piglet-templates->piglet) (0.41.2)\n","Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from astunparse->piglet-templates->piglet) (1.16.0)\n","Installing collected packages: piglet-templates, piglet\n","Successfully installed piglet-1.0.0 piglet-templates-1.3.0\n"]}],"source":["!apt install xvfb -y\n","!pip install pyvirtualdisplay\n","!pip install piglet"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18400,"status":"ok","timestamp":1698849011902,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-60},"id":"5OaWkBSmhm6R","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d990fc7a-3bc3-483f-ed6c-b980eeecd532"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting supersuit\n","  Downloading SuperSuit-3.9.0-py3-none-any.whl (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 kB\u001b[0m \u001b[31m761.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from supersuit) (1.23.5)\n","Collecting gymnasium>=0.28.1 (from supersuit)\n","  Using cached gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","Collecting tinyscaler>=1.2.6 (from supersuit)\n","  Downloading tinyscaler-1.2.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (517 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.1/517.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (1.6.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (4.5.0)\n","Collecting farama-notifications>=0.0.1 (from gymnasium>=0.28.1->supersuit)\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Installing collected packages: farama-notifications, tinyscaler, gymnasium, supersuit\n","Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1 supersuit-3.9.0 tinyscaler-1.2.7\n","Collecting stable_baselines3\n","  Downloading stable_baselines3-2.1.0-py3-none-any.whl (178 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (0.29.1)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.23.5)\n","Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.1.0+cu118)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.6.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.5.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (3.7.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (0.0.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.12.4)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2.1.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.1.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (4.43.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (23.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable_baselines3) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable_baselines3) (1.3.0)\n","Installing collected packages: stable_baselines3\n","Successfully installed stable_baselines3-2.1.0\n"]}],"source":["!pip install supersuit\n","!pip install stable_baselines3"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7378,"status":"ok","timestamp":1698849019233,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-60},"id":"thmOvcHdjKHw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bdc627d4-1fa6-41bf-d747-9cfb41dfd0be"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting shimmy>=0.2.1\n","  Downloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from shimmy>=0.2.1) (1.23.5)\n","Requirement already satisfied: gymnasium>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from shimmy>=0.2.1) (0.29.1)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy>=0.2.1) (1.6.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy>=0.2.1) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy>=0.2.1) (0.0.4)\n","Installing collected packages: shimmy\n","Successfully installed shimmy-1.3.0\n"]}],"source":["!pip install 'shimmy>=0.2.1'"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":33215,"status":"ok","timestamp":1698849052442,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-60},"id":"k0iVvep_spQz","colab":{"base_uri":"https://localhost:8080/"},"outputId":"894e4058-682a-4b6d-e02d-1d42e7c0e083"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tf-agents-nightly\n","  Downloading tf_agents_nightly-0.19.0.dev20231010-py3-none-any.whl (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.4.0)\n","Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.6.0)\n","Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (0.5.0)\n","Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (0.17.3)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.23.5)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (9.4.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.16.0)\n","Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (3.20.3)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.14.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (4.5.0)\n","Collecting pygame==2.1.3 (from tf-agents-nightly)\n","  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m956.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tfp-nightly (from tf-agents-nightly)\n","  Downloading tfp_nightly-0.23.0.dev20231101-py2.py3-none-any.whl (6.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents-nightly) (1.11.3)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents-nightly) (1.5.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tfp-nightly->tf-agents-nightly) (4.4.2)\n","Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tfp-nightly->tf-agents-nightly) (0.5.4)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tfp-nightly->tf-agents-nightly) (0.1.8)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<=0.23.0,>=0.17.0->tf-agents-nightly) (0.18.3)\n","Installing collected packages: tfp-nightly, pygame, tf-agents-nightly\n","  Attempting uninstall: pygame\n","    Found existing installation: pygame 2.5.2\n","    Uninstalling pygame-2.5.2:\n","      Successfully uninstalled pygame-2.5.2\n","Successfully installed pygame-2.1.3 tf-agents-nightly-0.19.0.dev20231010 tfp-nightly-0.23.0.dev20231101\n"]}],"source":["!pip install tf-agents-nightly"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10126,"status":"ok","timestamp":1698849062522,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-60},"id":"UlXxViz9tdvH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0167a218-4b86-4780-e964-f8d9c3fa196b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: tensorflow 2.8.0\n","Uninstalling tensorflow-2.8.0:\n","  Would remove:\n","    /usr/local/bin/estimator_ckpt_converter\n","    /usr/local/bin/import_pb_to_tensorboard\n","    /usr/local/bin/saved_model_cli\n","    /usr/local/bin/tensorboard\n","    /usr/local/bin/tf_upgrade_v2\n","    /usr/local/bin/tflite_convert\n","    /usr/local/bin/toco\n","    /usr/local/bin/toco_from_protos\n","    /usr/local/lib/python3.10/dist-packages/tensorflow-2.8.0.dist-info/*\n","    /usr/local/lib/python3.10/dist-packages/tensorflow/*\n","Proceed (Y/n)? Y\n","  Successfully uninstalled tensorflow-2.8.0\n"]}],"source":["!pip uninstall tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dsWlVQ6MtKLj"},"outputs":[],"source":["!pip install tensorflow>=2.13"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4559,"status":"ok","timestamp":1698849119833,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-60},"id":"wE5AiVtFtZDc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"616c5b59-a5a3-4721-cade-2b46c7a39d25"},"outputs":[{"output_type":"stream","name":"stdout","text":["Name: tensorflow\n","Version: 2.14.0\n","Summary: TensorFlow is an open source machine learning framework for everyone.\n","Home-page: https://www.tensorflow.org/\n","Author: Google Inc.\n","Author-email: packages@tensorflow.org\n","License: Apache 2.0\n","Location: /usr/local/lib/python3.10/dist-packages\n","Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, libclang, ml-dtypes, numpy, opt-einsum, packaging, protobuf, setuptools, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n","Required-by: dopamine-rl, keras-rl2\n"]}],"source":["!pip show tensorflow"]},{"cell_type":"code","source":["!pip install pettingzoo[sisl]"],"metadata":{"id":"PZa1qybXZKSX","executionInfo":{"status":"ok","timestamp":1698849221105,"user_tz":-60,"elapsed":101279,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0d73c2d4-ec60-4d91-a896-179d12098790"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pettingzoo[sisl]\n","  Using cached pettingzoo-1.24.1-py3-none-any.whl (840 kB)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[sisl]) (1.23.5)\n","Requirement already satisfied: gymnasium>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[sisl]) (0.29.1)\n","Collecting pygame==2.3.0 (from pettingzoo[sisl])\n","  Using cached pygame-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\n","Collecting pymunk==6.2.0 (from pettingzoo[sisl])\n","  Downloading pymunk-6.2.0.zip (7.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting box2d-py==2.3.5 (from pettingzoo[sisl])\n","  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[sisl]) (1.11.3)\n","Requirement already satisfied: cffi>1.14.0 in /usr/local/lib/python3.10/dist-packages (from pymunk==6.2.0->pettingzoo[sisl]) (1.16.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[sisl]) (1.6.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[sisl]) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[sisl]) (0.0.4)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>1.14.0->pymunk==6.2.0->pettingzoo[sisl]) (2.21)\n","Building wheels for collected packages: box2d-py, pymunk\n","  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2373124 sha256=612236779132bf64ae60b3eb75843043e43382ef0178456d246b0feddea45fd6\n","  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n","  Building wheel for pymunk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pymunk: filename=pymunk-6.2.0-cp310-cp310-linux_x86_64.whl size=801636 sha256=ba128402ccabf799bbedbc70d58bc65aded740601107b0ba9f4f53948aa4d50e\n","  Stored in directory: /root/.cache/pip/wheels/2e/81/a2/f941a9ff417bb4020c37ae218fb7117d12d3fc019ea493d66f\n","Successfully built box2d-py pymunk\n","Installing collected packages: box2d-py, pygame, pymunk, pettingzoo\n","  Attempting uninstall: box2d-py\n","    Found existing installation: box2d-py 2.3.8\n","    Uninstalling box2d-py-2.3.8:\n","      Successfully uninstalled box2d-py-2.3.8\n","  Attempting uninstall: pygame\n","    Found existing installation: pygame 2.1.3\n","    Uninstalling pygame-2.1.3:\n","      Successfully uninstalled pygame-2.1.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tf-agents-nightly 0.19.0.dev20231010 requires pygame==2.1.3, but you have pygame 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed box2d-py-2.3.5 pettingzoo-1.24.1 pygame-2.3.0 pymunk-6.2.0\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JpA_YhKzCeC2"},"outputs":[],"source":["############################# Código para entrenar Multiwalker ######################"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tnz3fJDA33w8"},"outputs":[],"source":["from stable_baselines3 import DDPG\n","# from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n","from pettingzoo.sisl import multiwalker_v9\n","import supersuit as ss\n","from stable_baselines3.common.monitor import Monitor"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":556,"status":"ok","timestamp":1697649056527,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"6-kdhb3CI5VC","outputId":"c6003468-c14d-4ee8-e786-b1447befe97a"},"outputs":[{"output_type":"stream","name":"stdout","text":["ls: cannot open directory '.': Transport endpoint is not connected\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":319,"status":"ok","timestamp":1696848224082,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"04CbnRTvI9L2","outputId":"7e3b8b24-a1bb-49f2-e4da-e8287d01683e"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/TFM\n"]}],"source":["cd /content/drive/MyDrive/TFM"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1696848225257,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"bUEH8254I_kb","outputId":"00564d52-42b1-49fd-b614-1e041a847e34"},"outputs":[{"name":"stdout","output_type":"stream","text":["'=2.13'\t\t\t\t     policy3_log_eval\n"," Atari_TFM.ipynb\t\t     policy_eval.zip\n","'Entrenamientos antiguos sin logs'   policy_log_eval\n"," Entrenamientos_log_no_eval\t     policy_new2_log_eval\n"," MCR_TFM.ipynb\t\t\t     policy_new_log_eval\n"," multi_car_racing\t\t     TFM_new_pettingzoo_gym_cap.ipynb\n"," og_multi_car_racing\t\t     TFM_pettingzoo_gym_cap.ipynb\n"," policy2_log_eval\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2HubK-2G3_vH"},"outputs":[],"source":["env = multiwalker_v9.parallel_env(render_mode=\"rgb_array\")\n","\n","# #---------------------------------\n","# log_dir = \"./policy_log\"\n","# os.makedirs(log_dir, exist_ok=True)\n","# env = Monitor(env, log_dir)\n","# #---------------------------------\n","# env = ss.color_reduction_v0(env, mode='B')\n","# env = ss.black_death_v3(env)\n","# env = ss.resize_v1(env, x_size=84, y_size=84)\n","env = ss.frame_stack_v1(env, 3)\n","env = ss.pettingzoo_env_to_vec_env_v1(env)\n","env = ss.concat_vec_envs_v1(env, 8, num_cpus=4, base_class='stable_baselines3')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"foI4bTFGbQo1"},"outputs":[],"source":["from stable_baselines3.common.callbacks import EvalCallback\n","eval_callback = EvalCallback(env, best_model_save_path=\"./multiwalker_ddpg_log_eval/\",\n","                             log_path=\"./multiwalker_ddpg_log_eval/\", eval_freq=100,\n","                             deterministic=False, render=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ib9VPIVgec47","outputId":"060c8d2b-e2d6-449c-d826-baf0dfc246be","executionInfo":{"status":"ok","timestamp":1697647956634,"user_tz":-120,"elapsed":3888951,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n","---------------------------------\n","Eval num_timesteps=271992, episode_reward=-107.28 +/- 2.86\n","Episode length: 77.40 +/- 0.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 77.4     |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 271992   |\n","| train/             |          |\n","|    actor_loss      | 69.5     |\n","|    critic_loss     | 6.01     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 271872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3610     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2113     |\n","|    total_timesteps | 272328   |\n","| train/             |          |\n","|    actor_loss      | 69.2     |\n","|    critic_loss     | 5.97     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 272208   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3620     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2118     |\n","|    total_timesteps | 273096   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 9.12     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 272976   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3630     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2125     |\n","|    total_timesteps | 274176   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 5.42     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 274056   |\n","---------------------------------\n","Eval num_timesteps=274392, episode_reward=-100.31 +/- 1.52\n","Episode length: 76.60 +/- 4.41\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 76.6     |\n","|    mean_reward     | -100     |\n","| time/              |          |\n","|    total_timesteps | 274392   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 8.58     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 274272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3640     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2136     |\n","|    total_timesteps | 275328   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 7.14     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 275208   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3650     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2142     |\n","|    total_timesteps | 276240   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 8.53     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 276120   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3660     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2143     |\n","|    total_timesteps | 276408   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 8.7      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 276288   |\n","---------------------------------\n","Eval num_timesteps=276792, episode_reward=-105.42 +/- 1.43\n","Episode length: 72.80 +/- 35.27\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 72.8     |\n","|    mean_reward     | -105     |\n","| time/              |          |\n","|    total_timesteps | 276792   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 8.35     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 276672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3670     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2156     |\n","|    total_timesteps | 277848   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 8.67     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 277728   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3680     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2160     |\n","|    total_timesteps | 278496   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 8.67     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 278376   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3690     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2162     |\n","|    total_timesteps | 278808   |\n","| train/             |          |\n","|    actor_loss      | 67.8     |\n","|    critic_loss     | 10.8     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 278688   |\n","---------------------------------\n","Eval num_timesteps=279192, episode_reward=-107.94 +/- 0.22\n","Episode length: 45.80 +/- 1.47\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 45.8     |\n","|    mean_reward     | -108     |\n","| time/              |          |\n","|    total_timesteps | 279192   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 12.8     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 279072   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3700     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2172     |\n","|    total_timesteps | 279984   |\n","| train/             |          |\n","|    actor_loss      | 67.8     |\n","|    critic_loss     | 9.45     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 279864   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3710     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2179     |\n","|    total_timesteps | 280896   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 6.38     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 280776   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3720     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2180     |\n","|    total_timesteps | 281184   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 8.08     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 281064   |\n","---------------------------------\n","Eval num_timesteps=281592, episode_reward=-105.28 +/- 0.58\n","Episode length: 81.00 +/- 4.90\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 81       |\n","|    mean_reward     | -105     |\n","| time/              |          |\n","|    total_timesteps | 281592   |\n","| train/             |          |\n","|    actor_loss      | 68       |\n","|    critic_loss     | 10.8     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 281472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3730     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2192     |\n","|    total_timesteps | 282672   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 8.24     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 282552   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3740     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2199     |\n","|    total_timesteps | 283632   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 8.85     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 283512   |\n","---------------------------------\n","Eval num_timesteps=283992, episode_reward=-103.25 +/- 3.37\n","Episode length: 86.40 +/- 2.94\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 86.4     |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 283992   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 8.64     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 283872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3750     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2209     |\n","|    total_timesteps | 285024   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 8.67     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 284904   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3760     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2218     |\n","|    total_timesteps | 286080   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 7.12     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 285960   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3770     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2219     |\n","|    total_timesteps | 286224   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 7.44     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 286104   |\n","---------------------------------\n","Eval num_timesteps=286392, episode_reward=-106.22 +/- 0.54\n","Episode length: 69.00 +/- 19.60\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 69       |\n","|    mean_reward     | -106     |\n","| time/              |          |\n","|    total_timesteps | 286392   |\n","| train/             |          |\n","|    actor_loss      | 67.8     |\n","|    critic_loss     | 7.53     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 286272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3780     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2222     |\n","|    total_timesteps | 286536   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 9.18     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 286416   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3790     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2229     |\n","|    total_timesteps | 287640   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 7.67     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 287520   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3800     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2233     |\n","|    total_timesteps | 288144   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 7.94     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 288024   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3810     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2238     |\n","|    total_timesteps | 288648   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 13.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 288528   |\n","---------------------------------\n","Eval num_timesteps=288792, episode_reward=-106.55 +/- 3.64\n","Episode length: 62.20 +/- 18.62\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 62.2     |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 288792   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 9.33     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 288672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3820     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2240     |\n","|    total_timesteps | 288888   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 8.4      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 288768   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3830     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2246     |\n","|    total_timesteps | 289896   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 9.42     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 289776   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3840     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2255     |\n","|    total_timesteps | 290928   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 7.45     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 290808   |\n","---------------------------------\n","Eval num_timesteps=291192, episode_reward=-103.48 +/- 3.65\n","Episode length: 61.60 +/- 21.56\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 61.6     |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 291192   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 9.97     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 291072   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3850     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2259     |\n","|    total_timesteps | 291264   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 11       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 291144   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3860     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2262     |\n","|    total_timesteps | 291840   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 8        |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 291720   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3870     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2265     |\n","|    total_timesteps | 292344   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 6.37     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 292224   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3880     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2274     |\n","|    total_timesteps | 293544   |\n","| train/             |          |\n","|    actor_loss      | 69.3     |\n","|    critic_loss     | 11.3     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 293424   |\n","---------------------------------\n","Eval num_timesteps=293592, episode_reward=-103.56 +/- 3.50\n","Episode length: 58.80 +/- 15.68\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 58.8     |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 293592   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 8.89     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 293472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3890     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2278     |\n","|    total_timesteps | 293808   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 7.48     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 293688   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3900     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2283     |\n","|    total_timesteps | 294696   |\n","| train/             |          |\n","|    actor_loss      | 67.9     |\n","|    critic_loss     | 11.1     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 294576   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3910     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2290     |\n","|    total_timesteps | 295728   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 5.64     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 295608   |\n","---------------------------------\n","Eval num_timesteps=295992, episode_reward=-107.13 +/- 0.10\n","Episode length: 48.80 +/- 3.92\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 48.8     |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 295992   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 4.86     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 295872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3920     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2298     |\n","|    total_timesteps | 296448   |\n","| train/             |          |\n","|    actor_loss      | 67.9     |\n","|    critic_loss     | 8.3      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 296328   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3930     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2300     |\n","|    total_timesteps | 296856   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 8.09     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 296736   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3940     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2306     |\n","|    total_timesteps | 297744   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 8.73     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 297624   |\n","---------------------------------\n","Eval num_timesteps=298392, episode_reward=-100.70 +/- 1.64\n","Episode length: 77.00 +/- 9.80\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 77       |\n","|    mean_reward     | -101     |\n","| time/              |          |\n","|    total_timesteps | 298392   |\n","| train/             |          |\n","|    actor_loss      | 67.9     |\n","|    critic_loss     | 7.31     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 298272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3950     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2313     |\n","|    total_timesteps | 298512   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 6.15     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 298392   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3960     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2321     |\n","|    total_timesteps | 299496   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 9.78     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 299376   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3970     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2324     |\n","|    total_timesteps | 299952   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 7.43     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 299832   |\n","---------------------------------\n","Eval num_timesteps=300792, episode_reward=-104.95 +/- 1.82\n","Episode length: 64.80 +/- 16.17\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 64.8     |\n","|    mean_reward     | -105     |\n","| time/              |          |\n","|    total_timesteps | 300792   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 9.26     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 300672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3980     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2331     |\n","|    total_timesteps | 300984   |\n","| train/             |          |\n","|    actor_loss      | 68       |\n","|    critic_loss     | 8.38     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 300864   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3990     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2333     |\n","|    total_timesteps | 301248   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 9.87     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 301128   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4000     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2342     |\n","|    total_timesteps | 302256   |\n","| train/             |          |\n","|    actor_loss      | 68.9     |\n","|    critic_loss     | 8.73     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 302136   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4010     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2344     |\n","|    total_timesteps | 302640   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 11       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 302520   |\n","---------------------------------\n","Eval num_timesteps=303192, episode_reward=-100.50 +/- 3.34\n","Episode length: 81.20 +/- 3.92\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 81.2     |\n","|    mean_reward     | -100     |\n","| time/              |          |\n","|    total_timesteps | 303192   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 6.12     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 303072   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4020     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2350     |\n","|    total_timesteps | 303336   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 5.96     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 303216   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4030     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2359     |\n","|    total_timesteps | 304440   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 8.79     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 304320   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4040     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2366     |\n","|    total_timesteps | 305424   |\n","| train/             |          |\n","|    actor_loss      | 67.9     |\n","|    critic_loss     | 8.72     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 305304   |\n","---------------------------------\n","Eval num_timesteps=305592, episode_reward=-106.04 +/- 3.46\n","Episode length: 58.60 +/- 14.21\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 58.6     |\n","|    mean_reward     | -106     |\n","| time/              |          |\n","|    total_timesteps | 305592   |\n","| train/             |          |\n","|    actor_loss      | 69       |\n","|    critic_loss     | 7.3      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 305472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4050     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2369     |\n","|    total_timesteps | 305832   |\n","| train/             |          |\n","|    actor_loss      | 69       |\n","|    critic_loss     | 9.09     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 305712   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4060     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2375     |\n","|    total_timesteps | 306600   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 8.43     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 306480   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4070     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2378     |\n","|    total_timesteps | 306960   |\n","| train/             |          |\n","|    actor_loss      | 68       |\n","|    critic_loss     | 5.96     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 306840   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4080     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2383     |\n","|    total_timesteps | 307680   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 10.3     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 307560   |\n","---------------------------------\n","Eval num_timesteps=307992, episode_reward=-106.25 +/- 4.29\n","Episode length: 66.00 +/- 19.60\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 66       |\n","|    mean_reward     | -106     |\n","| time/              |          |\n","|    total_timesteps | 307992   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 5.83     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 307872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4090     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2387     |\n","|    total_timesteps | 308160   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 8.55     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 308040   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4100     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2389     |\n","|    total_timesteps | 308424   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 7.71     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 308304   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4110     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2396     |\n","|    total_timesteps | 309360   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 8.86     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 309240   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4120     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2404     |\n","|    total_timesteps | 310344   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 6.51     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 310224   |\n","---------------------------------\n","Eval num_timesteps=310392, episode_reward=-104.14 +/- 0.39\n","Episode length: 100.20 +/- 5.88\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 100      |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 310392   |\n","| train/             |          |\n","|    actor_loss      | 69       |\n","|    critic_loss     | 6.99     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 310272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4130     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2408     |\n","|    total_timesteps | 310824   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 6.87     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 310704   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4140     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2414     |\n","|    total_timesteps | 311808   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 9.47     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 311688   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4150     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2422     |\n","|    total_timesteps | 312624   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 9.56     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 312504   |\n","---------------------------------\n","Eval num_timesteps=312792, episode_reward=-110.68 +/- 3.88\n","Episode length: 43.80 +/- 0.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 43.8     |\n","|    mean_reward     | -111     |\n","| time/              |          |\n","|    total_timesteps | 312792   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 10.6     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 312672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4160     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2424     |\n","|    total_timesteps | 312816   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 9.4      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 312696   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4170     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2429     |\n","|    total_timesteps | 313776   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 8.76     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 313656   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4180     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2436     |\n","|    total_timesteps | 314760   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 8.34     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 314640   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4190     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2439     |\n","|    total_timesteps | 315048   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 9.69     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 314928   |\n","---------------------------------\n","Eval num_timesteps=315192, episode_reward=-104.68 +/- 1.73\n","Episode length: 67.40 +/- 19.11\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 67.4     |\n","|    mean_reward     | -105     |\n","| time/              |          |\n","|    total_timesteps | 315192   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 7.99     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 315072   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4200     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2444     |\n","|    total_timesteps | 315456   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 13.8     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 315336   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4210     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2450     |\n","|    total_timesteps | 316536   |\n","| train/             |          |\n","|    actor_loss      | 69       |\n","|    critic_loss     | 9.41     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 316416   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4220     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2453     |\n","|    total_timesteps | 316992   |\n","| train/             |          |\n","|    actor_loss      | 68.9     |\n","|    critic_loss     | 5.92     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 316872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4230     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2457     |\n","|    total_timesteps | 317472   |\n","| train/             |          |\n","|    actor_loss      | 69.1     |\n","|    critic_loss     | 8.39     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 317352   |\n","---------------------------------\n","Eval num_timesteps=317592, episode_reward=-106.62 +/- 0.42\n","Episode length: 61.80 +/- 20.58\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 61.8     |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 317592   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 10.3     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 317472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4240     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2468     |\n","|    total_timesteps | 318768   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 10.9     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 318648   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4250     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2473     |\n","|    total_timesteps | 319512   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 7.18     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 319392   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4260     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2476     |\n","|    total_timesteps | 319896   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 8.29     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 319776   |\n","---------------------------------\n","Eval num_timesteps=319992, episode_reward=-105.36 +/- 0.44\n","Episode length: 90.40 +/- 4.41\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 90.4     |\n","|    mean_reward     | -105     |\n","| time/              |          |\n","|    total_timesteps | 319992   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 9.12     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 319872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4270     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2487     |\n","|    total_timesteps | 321120   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 6.72     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 321000   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4280     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2491     |\n","|    total_timesteps | 321768   |\n","| train/             |          |\n","|    actor_loss      | 69       |\n","|    critic_loss     | 7.31     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 321648   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4290     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2494     |\n","|    total_timesteps | 322320   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 9.15     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 322200   |\n","---------------------------------\n","Eval num_timesteps=322392, episode_reward=-106.19 +/- 0.18\n","Episode length: 58.40 +/- 15.19\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 58.4     |\n","|    mean_reward     | -106     |\n","| time/              |          |\n","|    total_timesteps | 322392   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 6.3      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 322272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4300     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2499     |\n","|    total_timesteps | 322728   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 7.65     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 322608   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4310     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2508     |\n","|    total_timesteps | 323832   |\n","| train/             |          |\n","|    actor_loss      | 68.9     |\n","|    critic_loss     | 6.99     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 323712   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4320     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2513     |\n","|    total_timesteps | 324624   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 8.64     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 324504   |\n","---------------------------------\n","Eval num_timesteps=324792, episode_reward=-103.92 +/- 0.11\n","Episode length: 83.40 +/- 0.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 83.4     |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 324792   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 6.88     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 324672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4330     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2522     |\n","|    total_timesteps | 325680   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 8.74     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 325560   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4340     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2525     |\n","|    total_timesteps | 326016   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 8.54     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 325896   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4350     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2531     |\n","|    total_timesteps | 326976   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 5.27     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 326856   |\n","---------------------------------\n","Eval num_timesteps=327192, episode_reward=-104.13 +/- 2.20\n","Episode length: 87.00 +/- 14.70\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 87       |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 327192   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 6.5      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 327072   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4360     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2540     |\n","|    total_timesteps | 328152   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 6.44     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 328032   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4370     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2548     |\n","|    total_timesteps | 329112   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 7.1      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 328992   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4380     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2549     |\n","|    total_timesteps | 329208   |\n","| train/             |          |\n","|    actor_loss      | 67.9     |\n","|    critic_loss     | 8.74     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 329088   |\n","---------------------------------\n","Eval num_timesteps=329592, episode_reward=-101.24 +/- 1.39\n","Episode length: 93.60 +/- 1.96\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 93.6     |\n","|    mean_reward     | -101     |\n","| time/              |          |\n","|    total_timesteps | 329592   |\n","| train/             |          |\n","|    actor_loss      | 68       |\n","|    critic_loss     | 11       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 329472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4390     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2556     |\n","|    total_timesteps | 330168   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 10.3     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 330048   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4400     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2558     |\n","|    total_timesteps | 330480   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 8.08     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 330360   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4410     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2563     |\n","|    total_timesteps | 331104   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 6.29     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 330984   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4420     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2568     |\n","|    total_timesteps | 331752   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 10.3     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 331632   |\n","---------------------------------\n","Eval num_timesteps=331992, episode_reward=-106.50 +/- 0.56\n","Episode length: 99.20 +/- 6.37\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 99.2     |\n","|    mean_reward     | -106     |\n","| time/              |          |\n","|    total_timesteps | 331992   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 6.73     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 331872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4430     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2576     |\n","|    total_timesteps | 332808   |\n","| train/             |          |\n","|    actor_loss      | 67.9     |\n","|    critic_loss     | 5.74     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 332688   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4440     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2578     |\n","|    total_timesteps | 333144   |\n","| train/             |          |\n","|    actor_loss      | 68.9     |\n","|    critic_loss     | 7.1      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 333024   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4450     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2586     |\n","|    total_timesteps | 333960   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 7.88     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 333840   |\n","---------------------------------\n","Eval num_timesteps=334392, episode_reward=-105.28 +/- 1.94\n","Episode length: 62.00 +/- 14.70\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 62       |\n","|    mean_reward     | -105     |\n","| time/              |          |\n","|    total_timesteps | 334392   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 6.07     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 334272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4460     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2592     |\n","|    total_timesteps | 334752   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 8.4      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 334632   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4470     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2596     |\n","|    total_timesteps | 335568   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 7.39     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 335448   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4480     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2605     |\n","|    total_timesteps | 336552   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 6.98     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 336432   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4490     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2606     |\n","|    total_timesteps | 336696   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 10       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 336576   |\n","---------------------------------\n","Eval num_timesteps=336792, episode_reward=-106.42 +/- 0.44\n","Episode length: 94.00 +/- 4.90\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 94       |\n","|    mean_reward     | -106     |\n","| time/              |          |\n","|    total_timesteps | 336792   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 6.42     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 336672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4500     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2614     |\n","|    total_timesteps | 337752   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 6.24     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 337632   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4510     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2619     |\n","|    total_timesteps | 338496   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 6.54     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 338376   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4520     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2622     |\n","|    total_timesteps | 338880   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 8.13     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 338760   |\n","---------------------------------\n","Eval num_timesteps=339192, episode_reward=-107.57 +/- 0.74\n","Episode length: 48.20 +/- 1.47\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 48.2     |\n","|    mean_reward     | -108     |\n","| time/              |          |\n","|    total_timesteps | 339192   |\n","| train/             |          |\n","|    actor_loss      | 68.9     |\n","|    critic_loss     | 8.87     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 339072   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4530     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2631     |\n","|    total_timesteps | 339912   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 8.7      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 339792   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4540     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2632     |\n","|    total_timesteps | 340176   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 11       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 340056   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4550     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2638     |\n","|    total_timesteps | 341160   |\n","| train/             |          |\n","|    actor_loss      | 69.3     |\n","|    critic_loss     | 11.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 341040   |\n","---------------------------------\n","Eval num_timesteps=341592, episode_reward=-105.90 +/- 3.14\n","Episode length: 94.80 +/- 1.47\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 94.8     |\n","|    mean_reward     | -106     |\n","| time/              |          |\n","|    total_timesteps | 341592   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 10.3     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 341472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4560     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2649     |\n","|    total_timesteps | 342240   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 8.32     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 342120   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4570     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2653     |\n","|    total_timesteps | 342840   |\n","| train/             |          |\n","|    actor_loss      | 69       |\n","|    critic_loss     | 6.97     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 342720   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4580     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2657     |\n","|    total_timesteps | 343440   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 7.37     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 343320   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4590     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2660     |\n","|    total_timesteps | 343896   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 5.63     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 343776   |\n","---------------------------------\n","Eval num_timesteps=343992, episode_reward=-106.91 +/- 0.14\n","Episode length: 66.00 +/- 26.94\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 66       |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 343992   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 4.41     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 343872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4600     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2670     |\n","|    total_timesteps | 344952   |\n","| train/             |          |\n","|    actor_loss      | 69.2     |\n","|    critic_loss     | 8.47     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 344832   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4610     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2676     |\n","|    total_timesteps | 345792   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 8.18     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 345672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4620     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2678     |\n","|    total_timesteps | 346224   |\n","| train/             |          |\n","|    actor_loss      | 69.2     |\n","|    critic_loss     | 10.9     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 346104   |\n","---------------------------------\n","Eval num_timesteps=346392, episode_reward=-100.87 +/- 2.18\n","Episode length: 88.00 +/- 2.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 88       |\n","|    mean_reward     | -101     |\n","| time/              |          |\n","|    total_timesteps | 346392   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 9.15     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 346272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4630     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2688     |\n","|    total_timesteps | 347208   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 7.87     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 347088   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4640     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2690     |\n","|    total_timesteps | 347424   |\n","| train/             |          |\n","|    actor_loss      | 69.1     |\n","|    critic_loss     | 7.23     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 347304   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4650     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2697     |\n","|    total_timesteps | 348552   |\n","| train/             |          |\n","|    actor_loss      | 69       |\n","|    critic_loss     | 7.21     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 348432   |\n","---------------------------------\n","Eval num_timesteps=348792, episode_reward=-103.44 +/- 1.63\n","Episode length: 87.00 +/- 7.35\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 87       |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 348792   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 6.77     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 348672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4660     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2705     |\n","|    total_timesteps | 349488   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 6.44     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 349368   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4670     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2709     |\n","|    total_timesteps | 349920   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 10.1     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 349800   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4680     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2713     |\n","|    total_timesteps | 350616   |\n","| train/             |          |\n","|    actor_loss      | 69.1     |\n","|    critic_loss     | 8.37     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 350496   |\n","---------------------------------\n","Eval num_timesteps=351192, episode_reward=-106.80 +/- 5.00\n","Episode length: 93.40 +/- 6.86\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 93.4     |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 351192   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 5.47     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 351072   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4690     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2726     |\n","|    total_timesteps | 352296   |\n","| train/             |          |\n","|    actor_loss      | 69.3     |\n","|    critic_loss     | 7.21     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 352176   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4700     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2729     |\n","|    total_timesteps | 352632   |\n","| train/             |          |\n","|    actor_loss      | 69.2     |\n","|    critic_loss     | 8.29     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 352512   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4710     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2732     |\n","|    total_timesteps | 353088   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 7.24     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 352968   |\n","---------------------------------\n","Eval num_timesteps=353592, episode_reward=-103.34 +/- 2.96\n","Episode length: 71.40 +/- 16.66\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 71.4     |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 353592   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 9.09     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 353472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4720     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2738     |\n","|    total_timesteps | 353832   |\n","| train/             |          |\n","|    actor_loss      | 69.2     |\n","|    critic_loss     | 7.05     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 353712   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4730     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2743     |\n","|    total_timesteps | 354624   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 8.37     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 354504   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4740     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2751     |\n","|    total_timesteps | 355608   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 7.25     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 355488   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4750     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2753     |\n","|    total_timesteps | 355920   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 9.47     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 355800   |\n","---------------------------------\n","Eval num_timesteps=355992, episode_reward=-103.01 +/- 0.18\n","Episode length: 92.40 +/- 4.41\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 92.4     |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 355992   |\n","| train/             |          |\n","|    actor_loss      | 68.9     |\n","|    critic_loss     | 8.07     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 355872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4760     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2760     |\n","|    total_timesteps | 356856   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 12.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 356736   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4770     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2765     |\n","|    total_timesteps | 357504   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 8.95     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 357384   |\n","---------------------------------\n","Eval num_timesteps=358392, episode_reward=-105.15 +/- 1.68\n","Episode length: 86.20 +/- 10.78\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 86.2     |\n","|    mean_reward     | -105     |\n","| time/              |          |\n","|    total_timesteps | 358392   |\n","| train/             |          |\n","|    actor_loss      | 69.2     |\n","|    critic_loss     | 9.18     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 358272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4780     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2780     |\n","|    total_timesteps | 359496   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 7.67     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 359376   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4790     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2785     |\n","|    total_timesteps | 360048   |\n","| train/             |          |\n","|    actor_loss      | 69.2     |\n","|    critic_loss     | 5.96     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 359928   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4800     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2790     |\n","|    total_timesteps | 360552   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 6.97     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 360432   |\n","---------------------------------\n","Eval num_timesteps=360792, episode_reward=-105.16 +/- 1.03\n","Episode length: 87.60 +/- 1.96\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 87.6     |\n","|    mean_reward     | -105     |\n","| time/              |          |\n","|    total_timesteps | 360792   |\n","| train/             |          |\n","|    actor_loss      | 69       |\n","|    critic_loss     | 9.03     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 360672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4810     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2800     |\n","|    total_timesteps | 361920   |\n","| train/             |          |\n","|    actor_loss      | 68.9     |\n","|    critic_loss     | 11.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 361800   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4820     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2806     |\n","|    total_timesteps | 362832   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 6.82     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 362712   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4830     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2809     |\n","|    total_timesteps | 363168   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 8.02     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 363048   |\n","---------------------------------\n","Eval num_timesteps=363192, episode_reward=-106.82 +/- 1.87\n","Episode length: 63.60 +/- 21.56\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 63.6     |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 363192   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 8.95     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 363072   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4840     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2817     |\n","|    total_timesteps | 364080   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 8.21     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 363960   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4850     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2818     |\n","|    total_timesteps | 364272   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 11       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 364152   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4860     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2825     |\n","|    total_timesteps | 365304   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 8.9      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 365184   |\n","---------------------------------\n","Eval num_timesteps=365592, episode_reward=-105.11 +/- 4.78\n","Episode length: 62.80 +/- 16.17\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 62.8     |\n","|    mean_reward     | -105     |\n","| time/              |          |\n","|    total_timesteps | 365592   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 7.4      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 365472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4870     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2837     |\n","|    total_timesteps | 366648   |\n","| train/             |          |\n","|    actor_loss      | 68       |\n","|    critic_loss     | 9.29     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 366528   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4880     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2839     |\n","|    total_timesteps | 367080   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 7.87     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 366960   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4890     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2844     |\n","|    total_timesteps | 367824   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 9.3      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 367704   |\n","---------------------------------\n","Eval num_timesteps=367992, episode_reward=-98.41 +/- 0.25\n","Episode length: 76.80 +/- 0.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 76.8     |\n","|    mean_reward     | -98.4    |\n","| time/              |          |\n","|    total_timesteps | 367992   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 9.37     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 367872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4900     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2853     |\n","|    total_timesteps | 368616   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 9.69     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 368496   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4910     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2858     |\n","|    total_timesteps | 369456   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 10.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 369336   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4920     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2861     |\n","|    total_timesteps | 370008   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 9.63     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 369888   |\n","---------------------------------\n","Eval num_timesteps=370392, episode_reward=-106.75 +/- 3.05\n","Episode length: 87.80 +/- 3.92\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 87.8     |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 370392   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 13.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 370272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4930     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2873     |\n","|    total_timesteps | 371208   |\n","| train/             |          |\n","|    actor_loss      | 67.6     |\n","|    critic_loss     | 4.63     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 371088   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4940     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2878     |\n","|    total_timesteps | 372024   |\n","| train/             |          |\n","|    actor_loss      | 67.8     |\n","|    critic_loss     | 6.03     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 371904   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4950     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2881     |\n","|    total_timesteps | 372408   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 9.43     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 372288   |\n","---------------------------------\n","Eval num_timesteps=372792, episode_reward=-103.41 +/- 0.10\n","Episode length: 76.80 +/- 3.92\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 76.8     |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 372792   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 5.57     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 372672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4960     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2885     |\n","|    total_timesteps | 372960   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 8.58     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 372840   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4970     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2886     |\n","|    total_timesteps | 373128   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 6.09     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 373008   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4980     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2894     |\n","|    total_timesteps | 374016   |\n","| train/             |          |\n","|    actor_loss      | 67.9     |\n","|    critic_loss     | 8.92     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 373896   |\n","---------------------------------\n","Eval num_timesteps=375192, episode_reward=-99.97 +/- 2.84\n","Episode length: 85.40 +/- 10.29\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 85.4     |\n","|    mean_reward     | -100     |\n","| time/              |          |\n","|    total_timesteps | 375192   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 6.38     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 375072   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4990     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2906     |\n","|    total_timesteps | 375768   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 8.54     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 375648   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5000     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2915     |\n","|    total_timesteps | 376752   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 8.21     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 376632   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5010     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2917     |\n","|    total_timesteps | 377016   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 6.22     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 376896   |\n","---------------------------------\n","Eval num_timesteps=377592, episode_reward=-105.11 +/- 2.02\n","Episode length: 58.80 +/- 10.78\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 58.8     |\n","|    mean_reward     | -105     |\n","| time/              |          |\n","|    total_timesteps | 377592   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 7.17     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 377472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5020     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2923     |\n","|    total_timesteps | 377856   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 6.56     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 377736   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5030     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2925     |\n","|    total_timesteps | 378216   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 7.51     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 378096   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5040     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2931     |\n","|    total_timesteps | 378960   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 8.28     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 378840   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5050     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2938     |\n","|    total_timesteps | 379848   |\n","| train/             |          |\n","|    actor_loss      | 67.9     |\n","|    critic_loss     | 7.68     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 379728   |\n","---------------------------------\n","Eval num_timesteps=379992, episode_reward=-108.03 +/- 3.83\n","Episode length: 62.40 +/- 24.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 62.4     |\n","|    mean_reward     | -108     |\n","| time/              |          |\n","|    total_timesteps | 379992   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 9.93     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 379872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5060     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2943     |\n","|    total_timesteps | 380568   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 8.98     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 380448   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5070     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2946     |\n","|    total_timesteps | 380976   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 7.88     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 380856   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5080     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2954     |\n","|    total_timesteps | 381936   |\n","| train/             |          |\n","|    actor_loss      | 67.4     |\n","|    critic_loss     | 7.27     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 381816   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5090     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2955     |\n","|    total_timesteps | 382032   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 8.74     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 381912   |\n","---------------------------------\n","Eval num_timesteps=382392, episode_reward=-103.60 +/- 3.62\n","Episode length: 84.60 +/- 7.84\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 84.6     |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 382392   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 6.83     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 382272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5100     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2959     |\n","|    total_timesteps | 382416   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 6.06     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 382296   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5110     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2966     |\n","|    total_timesteps | 383616   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 6.54     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 383496   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5120     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2971     |\n","|    total_timesteps | 384240   |\n","| train/             |          |\n","|    actor_loss      | 67.9     |\n","|    critic_loss     | 11.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 384120   |\n","---------------------------------\n","Eval num_timesteps=384792, episode_reward=-108.06 +/- 1.80\n","Episode length: 47.80 +/- 0.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 47.8     |\n","|    mean_reward     | -108     |\n","| time/              |          |\n","|    total_timesteps | 384792   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 7.43     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 384672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5130     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2980     |\n","|    total_timesteps | 385344   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 5.57     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 385224   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5140     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2984     |\n","|    total_timesteps | 385944   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 7.62     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 385824   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5150     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2988     |\n","|    total_timesteps | 386616   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 7.4      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 386496   |\n","---------------------------------\n","Eval num_timesteps=387192, episode_reward=-104.01 +/- 0.49\n","Episode length: 83.80 +/- 10.78\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 83.8     |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 387192   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 5.84     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 387072   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5160     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2999     |\n","|    total_timesteps | 387720   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 5.44     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 387600   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5170     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3005     |\n","|    total_timesteps | 388704   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 11.6     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 388584   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5180     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3009     |\n","|    total_timesteps | 389328   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 7.77     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 389208   |\n","---------------------------------\n","Eval num_timesteps=389592, episode_reward=-106.62 +/- 0.04\n","Episode length: 44.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 44       |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 389592   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 7.98     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 389472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5190     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3022     |\n","|    total_timesteps | 390840   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 8.57     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 390720   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5200     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3028     |\n","|    total_timesteps | 391632   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 6.46     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 391512   |\n","---------------------------------\n","Eval num_timesteps=391992, episode_reward=-109.20 +/- 1.90\n","Episode length: 45.40 +/- 1.96\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 45.4     |\n","|    mean_reward     | -109     |\n","| time/              |          |\n","|    total_timesteps | 391992   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 11.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 391872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5210     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3040     |\n","|    total_timesteps | 392712   |\n","| train/             |          |\n","|    actor_loss      | 69.3     |\n","|    critic_loss     | 8.41     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 392592   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5220     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3044     |\n","|    total_timesteps | 393048   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 5.38     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 392928   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5230     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3049     |\n","|    total_timesteps | 393888   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 10.3     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 393768   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5240     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3052     |\n","|    total_timesteps | 394344   |\n","| train/             |          |\n","|    actor_loss      | 68.9     |\n","|    critic_loss     | 8.97     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 394224   |\n","---------------------------------\n","Eval num_timesteps=394392, episode_reward=-103.97 +/- 0.61\n","Episode length: 77.80 +/- 3.43\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 77.8     |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 394392   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 8.83     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 394272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5250     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3056     |\n","|    total_timesteps | 394752   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 8.32     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 394632   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5260     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3065     |\n","|    total_timesteps | 395808   |\n","| train/             |          |\n","|    actor_loss      | 69.1     |\n","|    critic_loss     | 9.17     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 395688   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5270     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3069     |\n","|    total_timesteps | 396408   |\n","| train/             |          |\n","|    actor_loss      | 68       |\n","|    critic_loss     | 9.17     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 396288   |\n","---------------------------------\n","Eval num_timesteps=396792, episode_reward=-108.52 +/- 0.04\n","Episode length: 44.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 44       |\n","|    mean_reward     | -109     |\n","| time/              |          |\n","|    total_timesteps | 396792   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 8.44     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 396672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5280     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3078     |\n","|    total_timesteps | 397680   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 6.2      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 397560   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5290     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3082     |\n","|    total_timesteps | 398088   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 6.1      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 397968   |\n","---------------------------------\n","Eval num_timesteps=399192, episode_reward=-103.58 +/- 1.09\n","Episode length: 78.80 +/- 10.78\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 78.8     |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 399192   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 8.99     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 399072   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5300     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3098     |\n","|    total_timesteps | 400224   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 9.04     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 400104   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5310     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3105     |\n","|    total_timesteps | 400992   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 7.26     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 400872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5320     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3107     |\n","|    total_timesteps | 401328   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 7.69     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 401208   |\n","---------------------------------\n","Eval num_timesteps=401592, episode_reward=-104.73 +/- 3.06\n","Episode length: 66.20 +/- 18.13\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 66.2     |\n","|    mean_reward     | -105     |\n","| time/              |          |\n","|    total_timesteps | 401592   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 6.66     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 401472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5330     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3111     |\n","|    total_timesteps | 401784   |\n","| train/             |          |\n","|    actor_loss      | 69       |\n","|    critic_loss     | 8.78     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 401664   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5340     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3117     |\n","|    total_timesteps | 402696   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 8.06     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 402576   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5350     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3126     |\n","|    total_timesteps | 403728   |\n","| train/             |          |\n","|    actor_loss      | 67.7     |\n","|    critic_loss     | 6.48     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 403608   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5360     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3127     |\n","|    total_timesteps | 403872   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 10.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 403752   |\n","---------------------------------\n","Eval num_timesteps=403992, episode_reward=-103.48 +/- 0.91\n","Episode length: 86.60 +/- 10.29\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 86.6     |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 403992   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 5.99     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 403872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5370     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3133     |\n","|    total_timesteps | 404616   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 10.6     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 404496   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5380     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3142     |\n","|    total_timesteps | 405816   |\n","| train/             |          |\n","|    actor_loss      | 67.8     |\n","|    critic_loss     | 7.54     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 405696   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5390     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3144     |\n","|    total_timesteps | 406008   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 6.93     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 405888   |\n","---------------------------------\n","Eval num_timesteps=406392, episode_reward=-104.22 +/- 1.49\n","Episode length: 73.00 +/- 2.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 73       |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 406392   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 7.68     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 406272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5400     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3149     |\n","|    total_timesteps | 406512   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 6        |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 406392   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5410     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3155     |\n","|    total_timesteps | 407544   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 8.69     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 407424   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5420     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3161     |\n","|    total_timesteps | 408336   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 9.81     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 408216   |\n","---------------------------------\n","Eval num_timesteps=408792, episode_reward=-106.99 +/- 0.58\n","Episode length: 89.80 +/- 11.27\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 89.8     |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 408792   |\n","| train/             |          |\n","|    actor_loss      | 68.9     |\n","|    critic_loss     | 8.01     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 408672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5430     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3168     |\n","|    total_timesteps | 408888   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 9.28     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 408768   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5440     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3172     |\n","|    total_timesteps | 409560   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 7.29     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 409440   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5450     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3177     |\n","|    total_timesteps | 410400   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 13.7     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 410280   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5460     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3180     |\n","|    total_timesteps | 410712   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 8.2      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 410592   |\n","---------------------------------\n","Eval num_timesteps=411192, episode_reward=-103.40 +/- 4.04\n","Episode length: 58.40 +/- 15.19\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 58.4     |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 411192   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 9.33     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 411072   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5470     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3193     |\n","|    total_timesteps | 412320   |\n","| train/             |          |\n","|    actor_loss      | 69.1     |\n","|    critic_loss     | 6.26     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 412200   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5480     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3196     |\n","|    total_timesteps | 412728   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 8.26     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 412608   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5490     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3202     |\n","|    total_timesteps | 413472   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 6.47     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 413352   |\n","---------------------------------\n","Eval num_timesteps=413592, episode_reward=-107.20 +/- 2.12\n","Episode length: 63.80 +/- 20.58\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 63.8     |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 413592   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 9.97     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 413472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5500     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3211     |\n","|    total_timesteps | 414456   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 7.75     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 414336   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5510     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3214     |\n","|    total_timesteps | 414840   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 12       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 414720   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5520     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3219     |\n","|    total_timesteps | 415704   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 10       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 415584   |\n","---------------------------------\n","Eval num_timesteps=415992, episode_reward=-100.00 +/- 0.49\n","Episode length: 86.00 +/- 9.80\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 86       |\n","|    mean_reward     | -100     |\n","| time/              |          |\n","|    total_timesteps | 415992   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 6.75     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 415872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5530     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3230     |\n","|    total_timesteps | 416712   |\n","| train/             |          |\n","|    actor_loss      | 69.7     |\n","|    critic_loss     | 7.08     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 416592   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5540     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3232     |\n","|    total_timesteps | 417048   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 9.3      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 416928   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5550     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3239     |\n","|    total_timesteps | 418152   |\n","| train/             |          |\n","|    actor_loss      | 69.1     |\n","|    critic_loss     | 7.89     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 418032   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5560     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3240     |\n","|    total_timesteps | 418296   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 5.97     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 418176   |\n","---------------------------------\n","Eval num_timesteps=418392, episode_reward=-102.50 +/- 0.22\n","Episode length: 69.60 +/- 0.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 69.6     |\n","|    mean_reward     | -102     |\n","| time/              |          |\n","|    total_timesteps | 418392   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 9.83     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 418272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5570     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3248     |\n","|    total_timesteps | 419016   |\n","| train/             |          |\n","|    actor_loss      | 69       |\n","|    critic_loss     | 10.3     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 418896   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5580     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3252     |\n","|    total_timesteps | 419496   |\n","| train/             |          |\n","|    actor_loss      | 68.9     |\n","|    critic_loss     | 8.6      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 419376   |\n","---------------------------------\n","Eval num_timesteps=420792, episode_reward=-103.78 +/- 2.37\n","Episode length: 83.00 +/- 14.70\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 83       |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 420792   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 7.54     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 420672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5590     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3262     |\n","|    total_timesteps | 420936   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 11.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 420816   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5600     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3270     |\n","|    total_timesteps | 421776   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 7.57     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 421656   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5610     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3274     |\n","|    total_timesteps | 422232   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 10.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 422112   |\n","---------------------------------\n","Eval num_timesteps=423192, episode_reward=-107.21 +/- 1.60\n","Episode length: 64.40 +/- 22.54\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 64.4     |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 423192   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 8.78     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 423072   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5620     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3286     |\n","|    total_timesteps | 423936   |\n","| train/             |          |\n","|    actor_loss      | 69       |\n","|    critic_loss     | 10.9     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 423816   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5630     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3294     |\n","|    total_timesteps | 424752   |\n","| train/             |          |\n","|    actor_loss      | 69.1     |\n","|    critic_loss     | 8.06     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 424632   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5640     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3296     |\n","|    total_timesteps | 425136   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 8.07     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 425016   |\n","---------------------------------\n","Eval num_timesteps=425592, episode_reward=-104.42 +/- 2.07\n","Episode length: 75.40 +/- 26.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 75.4     |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 425592   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 7.66     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 425472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5650     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3303     |\n","|    total_timesteps | 426120   |\n","| train/             |          |\n","|    actor_loss      | 69       |\n","|    critic_loss     | 8.92     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 426000   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5660     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3306     |\n","|    total_timesteps | 426456   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 9.12     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 426336   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5670     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3315     |\n","|    total_timesteps | 427488   |\n","| train/             |          |\n","|    actor_loss      | 68       |\n","|    critic_loss     | 12.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 427368   |\n","---------------------------------\n","Eval num_timesteps=427992, episode_reward=-106.42 +/- 2.45\n","Episode length: 59.60 +/- 19.11\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 59.6     |\n","|    mean_reward     | -106     |\n","| time/              |          |\n","|    total_timesteps | 427992   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 7.99     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 427872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5680     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3320     |\n","|    total_timesteps | 428160   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 9.58     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 428040   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5690     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3327     |\n","|    total_timesteps | 429072   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 6.36     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 428952   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5700     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3335     |\n","|    total_timesteps | 430008   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 7.92     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 429888   |\n","---------------------------------\n","Eval num_timesteps=430392, episode_reward=-103.91 +/- 2.40\n","Episode length: 76.00 +/- 7.35\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 76       |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 430392   |\n","| train/             |          |\n","|    actor_loss      | 68       |\n","|    critic_loss     | 10       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 430272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5710     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3338     |\n","|    total_timesteps | 430416   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 6.7      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 430296   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5720     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3340     |\n","|    total_timesteps | 430680   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 9.92     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 430560   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5730     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3345     |\n","|    total_timesteps | 431472   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 8.17     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 431352   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5740     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3353     |\n","|    total_timesteps | 432360   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 8.13     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 432240   |\n","---------------------------------\n","Eval num_timesteps=432792, episode_reward=-103.38 +/- 1.57\n","Episode length: 76.60 +/- 4.41\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 76.6     |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 432792   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 10.9     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 432672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5750     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3359     |\n","|    total_timesteps | 433008   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 8.22     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 432888   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5760     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3364     |\n","|    total_timesteps | 433824   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 10.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 433704   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5770     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3371     |\n","|    total_timesteps | 434736   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 7.14     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 434616   |\n","---------------------------------\n","Eval num_timesteps=435192, episode_reward=-102.50 +/- 1.95\n","Episode length: 95.40 +/- 7.84\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 95.4     |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 435192   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 7.26     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 435072   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5780     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3378     |\n","|    total_timesteps | 435312   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 11.3     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 435192   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5790     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3381     |\n","|    total_timesteps | 435840   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 3.44     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 435720   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5800     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3386     |\n","|    total_timesteps | 436608   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 7.69     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 436488   |\n","---------------------------------\n","Eval num_timesteps=437592, episode_reward=-104.82 +/- 5.07\n","Episode length: 57.80 +/- 13.23\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 57.8     |\n","|    mean_reward     | -105     |\n","| time/              |          |\n","|    total_timesteps | 437592   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 7.84     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 437472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5810     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3398     |\n","|    total_timesteps | 437904   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 8.71     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 437784   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5820     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3399     |\n","|    total_timesteps | 438120   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 8.45     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 438000   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5830     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3406     |\n","|    total_timesteps | 439200   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 9.45     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 439080   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5840     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3411     |\n","|    total_timesteps | 439776   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 7.48     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 439656   |\n","---------------------------------\n","Eval num_timesteps=439992, episode_reward=-103.59 +/- 1.94\n","Episode length: 94.00 +/- 4.90\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 94       |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 439992   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 10.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 439872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5850     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3423     |\n","|    total_timesteps | 441096   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 10.1     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 440976   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5860     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3426     |\n","|    total_timesteps | 441720   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 6.54     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 441600   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5870     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3430     |\n","|    total_timesteps | 442200   |\n","| train/             |          |\n","|    actor_loss      | 68.9     |\n","|    critic_loss     | 7.06     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 442080   |\n","---------------------------------\n","Eval num_timesteps=442392, episode_reward=-106.78 +/- 3.78\n","Episode length: 73.60 +/- 24.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 73.6     |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 442392   |\n","| train/             |          |\n","|    actor_loss      | 69       |\n","|    critic_loss     | 6.29     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 442272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5880     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3440     |\n","|    total_timesteps | 443160   |\n","| train/             |          |\n","|    actor_loss      | 69       |\n","|    critic_loss     | 9.99     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 443040   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5890     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3446     |\n","|    total_timesteps | 444072   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 10.9     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 443952   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5900     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3448     |\n","|    total_timesteps | 444408   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 9.45     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 444288   |\n","---------------------------------\n","Eval num_timesteps=444792, episode_reward=-108.10 +/- 2.52\n","Episode length: 72.60 +/- 24.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 72.6     |\n","|    mean_reward     | -108     |\n","| time/              |          |\n","|    total_timesteps | 444792   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 7.1      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 444672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5910     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3454     |\n","|    total_timesteps | 444912   |\n","| train/             |          |\n","|    actor_loss      | 67.8     |\n","|    critic_loss     | 6.24     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 444792   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5920     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3465     |\n","|    total_timesteps | 446376   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 7.76     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 446256   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5930     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3467     |\n","|    total_timesteps | 446760   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 12.1     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 446640   |\n","---------------------------------\n","Eval num_timesteps=447192, episode_reward=-107.95 +/- 0.61\n","Episode length: 44.40 +/- 0.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 44.4     |\n","|    mean_reward     | -108     |\n","| time/              |          |\n","|    total_timesteps | 447192   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 11.9     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 447072   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5940     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3478     |\n","|    total_timesteps | 447960   |\n","| train/             |          |\n","|    actor_loss      | 67.7     |\n","|    critic_loss     | 11.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 447840   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5950     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3482     |\n","|    total_timesteps | 448512   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 9.83     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 448392   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5960     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3486     |\n","|    total_timesteps | 449040   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 8.16     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 448920   |\n","---------------------------------\n","Eval num_timesteps=449592, episode_reward=-103.78 +/- 0.63\n","Episode length: 80.80 +/- 8.33\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 80.8     |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 449592   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 7.53     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 449472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5970     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3492     |\n","|    total_timesteps | 449880   |\n","| train/             |          |\n","|    actor_loss      | 68.9     |\n","|    critic_loss     | 12.6     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 449760   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5980     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3497     |\n","|    total_timesteps | 450360   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 7.18     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 450240   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5990     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3504     |\n","|    total_timesteps | 451248   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 7.51     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 451128   |\n","---------------------------------\n","Eval num_timesteps=451992, episode_reward=-103.28 +/- 0.32\n","Episode length: 73.00 +/- 4.90\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 73       |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 451992   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 10       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 451872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6000     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3509     |\n","|    total_timesteps | 452016   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 8.74     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 451896   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6010     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3517     |\n","|    total_timesteps | 453024   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 5.65     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 452904   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6020     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3523     |\n","|    total_timesteps | 453648   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 7.4      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 453528   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6030     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3525     |\n","|    total_timesteps | 454032   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 5.8      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 453912   |\n","---------------------------------\n","Eval num_timesteps=454392, episode_reward=-108.94 +/- 1.41\n","Episode length: 107.60 +/- 0.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 108      |\n","|    mean_reward     | -109     |\n","| time/              |          |\n","|    total_timesteps | 454392   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 9.18     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 454272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6040     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3533     |\n","|    total_timesteps | 455088   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 8.45     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 454968   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6050     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3536     |\n","|    total_timesteps | 455496   |\n","| train/             |          |\n","|    actor_loss      | 68       |\n","|    critic_loss     | 7.89     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 455376   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6060     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3543     |\n","|    total_timesteps | 456264   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 5.05     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 456144   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6070     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3545     |\n","|    total_timesteps | 456648   |\n","| train/             |          |\n","|    actor_loss      | 68       |\n","|    critic_loss     | 5.65     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 456528   |\n","---------------------------------\n","Eval num_timesteps=456792, episode_reward=-103.57 +/- 1.31\n","Episode length: 85.00 +/- 7.35\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 85       |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 456792   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 12.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 456672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6080     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3552     |\n","|    total_timesteps | 457584   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 8.24     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 457464   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6090     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3558     |\n","|    total_timesteps | 458280   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 8.26     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 458160   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6100     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3563     |\n","|    total_timesteps | 458880   |\n","| train/             |          |\n","|    actor_loss      | 68.9     |\n","|    critic_loss     | 7.27     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 458760   |\n","---------------------------------\n","Eval num_timesteps=459192, episode_reward=-106.66 +/- 2.00\n","Episode length: 65.60 +/- 24.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 65.6     |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 459192   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 8.5      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 459072   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6110     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3568     |\n","|    total_timesteps | 459384   |\n","| train/             |          |\n","|    actor_loss      | 67.8     |\n","|    critic_loss     | 7.16     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 459264   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6120     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3578     |\n","|    total_timesteps | 460848   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 8.67     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 460728   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6130     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3583     |\n","|    total_timesteps | 461400   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 7.11     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 461280   |\n","---------------------------------\n","Eval num_timesteps=461592, episode_reward=-106.10 +/- 1.28\n","Episode length: 57.00 +/- 17.15\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 57       |\n","|    mean_reward     | -106     |\n","| time/              |          |\n","|    total_timesteps | 461592   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 11.1     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 461472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6140     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3587     |\n","|    total_timesteps | 461904   |\n","| train/             |          |\n","|    actor_loss      | 68.9     |\n","|    critic_loss     | 6.22     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 461784   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6150     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3587     |\n","|    total_timesteps | 461952   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 8.35     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 461832   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6160     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3595     |\n","|    total_timesteps | 463128   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 10       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 463008   |\n","---------------------------------\n","Eval num_timesteps=463992, episode_reward=-107.42 +/- 1.23\n","Episode length: 63.00 +/- 19.60\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 63       |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 463992   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 10.8     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 463872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6170     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3605     |\n","|    total_timesteps | 464040   |\n","| train/             |          |\n","|    actor_loss      | 67.7     |\n","|    critic_loss     | 10.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 463920   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6180     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3611     |\n","|    total_timesteps | 465024   |\n","| train/             |          |\n","|    actor_loss      | 69       |\n","|    critic_loss     | 8.74     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 464904   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6190     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3612     |\n","|    total_timesteps | 465216   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 9.48     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 465096   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6200     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3618     |\n","|    total_timesteps | 466056   |\n","| train/             |          |\n","|    actor_loss      | 68       |\n","|    critic_loss     | 8.5      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 465936   |\n","---------------------------------\n","Eval num_timesteps=466392, episode_reward=-103.52 +/- 4.48\n","Episode length: 71.60 +/- 20.09\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 71.6     |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 466392   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 8.83     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 466272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6210     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3625     |\n","|    total_timesteps | 466608   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 7.43     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 466488   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6220     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3631     |\n","|    total_timesteps | 467544   |\n","| train/             |          |\n","|    actor_loss      | 68       |\n","|    critic_loss     | 5.54     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 467424   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6230     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3636     |\n","|    total_timesteps | 468408   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 8.97     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 468288   |\n","---------------------------------\n","Eval num_timesteps=468792, episode_reward=-107.12 +/- 1.26\n","Episode length: 70.60 +/- 20.09\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 70.6     |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 468792   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 6.5      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 468672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6240     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3647     |\n","|    total_timesteps | 469416   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 8.17     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 469296   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6250     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3649     |\n","|    total_timesteps | 469848   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 7.37     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 469728   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6260     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3657     |\n","|    total_timesteps | 470760   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 11.9     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 470640   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6270     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3659     |\n","|    total_timesteps | 470952   |\n","| train/             |          |\n","|    actor_loss      | 68       |\n","|    critic_loss     | 11.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 470832   |\n","---------------------------------\n","Eval num_timesteps=471192, episode_reward=-109.27 +/- 3.38\n","Episode length: 63.20 +/- 23.52\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 63.2     |\n","|    mean_reward     | -109     |\n","| time/              |          |\n","|    total_timesteps | 471192   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 6.98     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 471072   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6280     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3664     |\n","|    total_timesteps | 471264   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 7.61     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 471144   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6290     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3673     |\n","|    total_timesteps | 472320   |\n","| train/             |          |\n","|    actor_loss      | 67.8     |\n","|    critic_loss     | 7.28     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 472200   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6300     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3677     |\n","|    total_timesteps | 473064   |\n","| train/             |          |\n","|    actor_loss      | 69       |\n","|    critic_loss     | 10.7     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 472944   |\n","---------------------------------\n","Eval num_timesteps=473592, episode_reward=-103.55 +/- 0.43\n","Episode length: 79.20 +/- 8.82\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 79.2     |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 473592   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 6.38     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 473472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6310     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3683     |\n","|    total_timesteps | 473688   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 9.93     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 473568   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6320     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3692     |\n","|    total_timesteps | 474720   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 5.28     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 474600   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6330     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3696     |\n","|    total_timesteps | 475344   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 9.99     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 475224   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6340     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3699     |\n","|    total_timesteps | 475776   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 6.62     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 475656   |\n","---------------------------------\n","Eval num_timesteps=475992, episode_reward=-108.23 +/- 0.84\n","Episode length: 79.40 +/- 26.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 79.4     |\n","|    mean_reward     | -108     |\n","| time/              |          |\n","|    total_timesteps | 475992   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 10.5     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 475872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6350     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3710     |\n","|    total_timesteps | 477000   |\n","| train/             |          |\n","|    actor_loss      | 68       |\n","|    critic_loss     | 10.1     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 476880   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6360     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3714     |\n","|    total_timesteps | 477552   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 9.9      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 477432   |\n","---------------------------------\n","Eval num_timesteps=478392, episode_reward=-102.64 +/- 5.62\n","Episode length: 88.40 +/- 5.39\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 88.4     |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 478392   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 9.87     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 478272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6370     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3724     |\n","|    total_timesteps | 478992   |\n","| train/             |          |\n","|    actor_loss      | 68       |\n","|    critic_loss     | 9.17     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 478872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6380     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3728     |\n","|    total_timesteps | 479424   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 9.01     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 479304   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6390     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3734     |\n","|    total_timesteps | 480096   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 10.8     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 479976   |\n","---------------------------------\n","Eval num_timesteps=480792, episode_reward=-103.47 +/- 6.30\n","Episode length: 58.80 +/- 18.13\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 58.8     |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 480792   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 8.77     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 480672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6400     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3740     |\n","|    total_timesteps | 480984   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 9.01     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 480864   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6410     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3741     |\n","|    total_timesteps | 481152   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 8.94     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 481032   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6420     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3749     |\n","|    total_timesteps | 482064   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 7.51     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 481944   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6430     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3754     |\n","|    total_timesteps | 482640   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 10.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 482520   |\n","---------------------------------\n","Eval num_timesteps=483192, episode_reward=-109.23 +/- 1.41\n","Episode length: 44.40 +/- 1.96\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 44.4     |\n","|    mean_reward     | -109     |\n","| time/              |          |\n","|    total_timesteps | 483192   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 8.24     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 483072   |\n","---------------------------------\n","-------------------------------\n","| time/              |        |\n","|    episodes        | 6440   |\n","|    fps             | 128    |\n","|    time_elapsed    | 3757   |\n","|    total_timesteps | 483192 |\n","-------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6450     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3762     |\n","|    total_timesteps | 483912   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 6.09     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 483792   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6460     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3764     |\n","|    total_timesteps | 484248   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 8.38     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 484128   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6470     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3772     |\n","|    total_timesteps | 485088   |\n","| train/             |          |\n","|    actor_loss      | 67.8     |\n","|    critic_loss     | 10.8     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 484968   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6480     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3775     |\n","|    total_timesteps | 485424   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 7.79     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 485304   |\n","---------------------------------\n","Eval num_timesteps=485592, episode_reward=-101.36 +/- 0.58\n","Episode length: 90.80 +/- 0.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 90.8     |\n","|    mean_reward     | -101     |\n","| time/              |          |\n","|    total_timesteps | 485592   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 7.55     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 485472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6490     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3783     |\n","|    total_timesteps | 486600   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 12       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 486480   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6500     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3790     |\n","|    total_timesteps | 487536   |\n","| train/             |          |\n","|    actor_loss      | 69       |\n","|    critic_loss     | 7.89     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 487416   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6510     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3793     |\n","|    total_timesteps | 487848   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 3.75     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 487728   |\n","---------------------------------\n","Eval num_timesteps=487992, episode_reward=-104.72 +/- 1.82\n","Episode length: 82.80 +/- 10.78\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 82.8     |\n","|    mean_reward     | -105     |\n","| time/              |          |\n","|    total_timesteps | 487992   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 9.23     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 487872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6520     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3807     |\n","|    total_timesteps | 489744   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 8.06     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 489624   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6530     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3810     |\n","|    total_timesteps | 490032   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 9.76     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 489912   |\n","---------------------------------\n","Eval num_timesteps=490392, episode_reward=-103.35 +/- 2.90\n","Episode length: 56.40 +/- 5.39\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 56.4     |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 490392   |\n","| train/             |          |\n","|    actor_loss      | 67.7     |\n","|    critic_loss     | 10.7     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 490272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6540     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3816     |\n","|    total_timesteps | 490632   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 10.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 490512   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6550     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3818     |\n","|    total_timesteps | 491064   |\n","| train/             |          |\n","|    actor_loss      | 69.1     |\n","|    critic_loss     | 8.48     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 490944   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6560     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3825     |\n","|    total_timesteps | 492096   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 8.64     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 491976   |\n","---------------------------------\n","Eval num_timesteps=492792, episode_reward=-104.36 +/- 0.98\n","Episode length: 70.00 +/- 19.60\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 70       |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 492792   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 9.85     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 492672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6570     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3833     |\n","|    total_timesteps | 492888   |\n","| train/             |          |\n","|    actor_loss      | 67.9     |\n","|    critic_loss     | 5.12     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 492768   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6580     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3839     |\n","|    total_timesteps | 493752   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 8.72     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 493632   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6590     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3846     |\n","|    total_timesteps | 494832   |\n","| train/             |          |\n","|    actor_loss      | 68.9     |\n","|    critic_loss     | 5.79     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 494712   |\n","---------------------------------\n","Eval num_timesteps=495192, episode_reward=-101.59 +/- 0.33\n","Episode length: 82.60 +/- 0.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 82.6     |\n","|    mean_reward     | -102     |\n","| time/              |          |\n","|    total_timesteps | 495192   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 7.02     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 495072   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6600     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3859     |\n","|    total_timesteps | 496272   |\n","| train/             |          |\n","|    actor_loss      | 69.2     |\n","|    critic_loss     | 9.73     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 496152   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6610     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3861     |\n","|    total_timesteps | 496584   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 11.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 496464   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6620     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3865     |\n","|    total_timesteps | 497280   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 6.97     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 497160   |\n","---------------------------------\n","Eval num_timesteps=497592, episode_reward=-103.89 +/- 0.74\n","Episode length: 92.60 +/- 4.41\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 92.6     |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 497592   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 8.19     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 497472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6630     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3877     |\n","|    total_timesteps | 498504   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 7.11     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 498384   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6640     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3883     |\n","|    total_timesteps | 499392   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 7.98     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 499272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6650     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3886     |\n","|    total_timesteps | 499896   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 6.18     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 499776   |\n","---------------------------------\n","Eval num_timesteps=499992, episode_reward=-102.97 +/- 1.80\n","Episode length: 99.00 +/- 12.25\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 99       |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 499992   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 7.42     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 499872   |\n","---------------------------------\n"]}],"source":["from stable_baselines3.common.logger import configure\n","\n","tmp_path = \"multiwalker_ddpg_log_eval/\"\n","# set up logger\n","new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n","\n","model = DDPG(\"MlpPolicy\", env, verbose=3,train_freq=1)\n","\n","model.set_logger(new_logger)\n","model.learn(total_timesteps=500000, log_interval=10,callback=eval_callback)\n","model.save(\"multiwalker_ddpg\")"]},{"cell_type":"code","source":["from stable_baselines3 import DDPG\n","# from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n","from pettingzoo.sisl import multiwalker_v9\n","import supersuit as ss\n","from stable_baselines3.common.monitor import Monitor"],"metadata":{"id":"Je7cG1t1FKFa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env = multiwalker_v9.parallel_env(render_mode=\"rgb_array\")\n","\n","# #---------------------------------\n","# log_dir = \"./policy_log\"\n","# os.makedirs(log_dir, exist_ok=True)\n","# env = Monitor(env, log_dir)\n","# #---------------------------------\n","# env = ss.color_reduction_v0(env, mode='B')\n","# env = ss.black_death_v3(env)\n","# env = ss.resize_v1(env, x_size=84, y_size=84)\n","env = ss.frame_stack_v1(env, 3)\n","env = ss.pettingzoo_env_to_vec_env_v1(env)\n","env = ss.concat_vec_envs_v1(env, 8, num_cpus=4, base_class='stable_baselines3')"],"metadata":{"id":"18AOPfzeFQ_E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3.common.callbacks import EvalCallback\n","eval_callback = EvalCallback(env, best_model_save_path=\"./multiwalker_ddpg2_log_eval/\",\n","                             log_path=\"./multiwalker_ddpg2_log_eval/\", eval_freq=100,\n","                             deterministic=False, render=False)\n"],"metadata":{"id":"b_gWNyo4FRq0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3.common.logger import configure\n","\n","tmp_path = \"multiwalker_ddpg2_log_eval/\"\n","# set up logger\n","new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n","\n","model = DDPG(\"MlpPolicy\", env, verbose=3,train_freq=1)\n","\n","model.set_logger(new_logger)\n","model.learn(total_timesteps=5000000, log_interval=10,callback=eval_callback)\n","model.save(\"multiwalker_ddpg2\")"],"metadata":{"id":"2IViGHTSFUcl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["################## Continuacion del entrenamiento de ddpg2"],"metadata":{"id":"e99sdjH3Zb-O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8l_VYV0fZlnq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3 import DDPG\n","# from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n","from pettingzoo.sisl import multiwalker_v9\n","import supersuit as ss\n","from stable_baselines3.common.monitor import Monitor"],"metadata":{"id":"kL5h_87SZnR3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env = multiwalker_v9.parallel_env(render_mode=\"rgb_array\")\n","\n","env = ss.frame_stack_v1(env, 3)\n","env = ss.pettingzoo_env_to_vec_env_v1(env)\n","env = ss.concat_vec_envs_v1(env, 8, num_cpus=4, base_class='stable_baselines3')"],"metadata":{"id":"XstMpNk6ZnR4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3.common.callbacks import EvalCallback\n","eval_callback = EvalCallback(env, best_model_save_path=\"./multiwalker_ddpg2_5_log_eval/\",\n","                             log_path=\"./multiwalker_ddpg2_5_log_eval/\", eval_freq=100,\n","                             deterministic=False, render=False)\n"],"metadata":{"id":"TPB3AQcFZnR4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3.common.logger import configure\n","\n","tmp_path = \"multiwalker_ddpg2_5_log_eval/\"\n","# set up logger\n","new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n","\n","model = DDPG.load(\"./multiwalker_ddpg2_log_eval//best_model\")\n","model.set_env(env)\n","model.set_parameters(\"./multiwalker_ddpg2_log_eval//best_model\")\n","\n","# model = DDPG(\"MlpPolicy\", env, verbose=3,train_freq=1)\n","\n","model.set_logger(new_logger)\n","model.learn(total_timesteps=5000000, log_interval=10,callback=eval_callback)\n","model.save(\"multiwalker_ddpg2_5\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rLjUVuYVZnR5","outputId":"cf22b057-d5f6-44e6-b479-c5f4b85ebcd3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Logging to multiwalker_ddpg2_5_log_eval/\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -5.98    |\n","| time/              |          |\n","|    total_timesteps | 628800   |\n","| train/             |          |\n","|    actor_loss      | 9.04     |\n","|    critic_loss     | 9.24     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1228560  |\n","---------------------------------\n","Eval num_timesteps=631200, episode_reward=-8.59 +/- 1.13\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -8.59    |\n","| time/              |          |\n","|    total_timesteps | 631200   |\n","| train/             |          |\n","|    actor_loss      | 9.82     |\n","|    critic_loss     | 13       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1230960  |\n","---------------------------------\n","Eval num_timesteps=633600, episode_reward=-7.71 +/- 0.11\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -7.71    |\n","| time/              |          |\n","|    total_timesteps | 633600   |\n","| train/             |          |\n","|    actor_loss      | 9.38     |\n","|    critic_loss     | 13.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1233360  |\n","---------------------------------\n","Eval num_timesteps=636000, episode_reward=-3.72 +/- 1.27\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -3.72    |\n","| time/              |          |\n","|    total_timesteps | 636000   |\n","| train/             |          |\n","|    actor_loss      | 9.5      |\n","|    critic_loss     | 14.1     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1235760  |\n","---------------------------------\n","Eval num_timesteps=638400, episode_reward=-1.39 +/- 0.11\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.39    |\n","| time/              |          |\n","|    total_timesteps | 638400   |\n","| train/             |          |\n","|    actor_loss      | 9.55     |\n","|    critic_loss     | 12.8     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1238160  |\n","---------------------------------\n","Eval num_timesteps=640800, episode_reward=-1.36 +/- 1.12\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.36    |\n","| time/              |          |\n","|    total_timesteps | 640800   |\n","| train/             |          |\n","|    actor_loss      | 9.01     |\n","|    critic_loss     | 10.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1240560  |\n","---------------------------------\n","Eval num_timesteps=643200, episode_reward=0.12 +/- 0.44\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.123    |\n","| time/              |          |\n","|    total_timesteps | 643200   |\n","| train/             |          |\n","|    actor_loss      | 8.9      |\n","|    critic_loss     | 14.5     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1242960  |\n","---------------------------------\n","Eval num_timesteps=645600, episode_reward=4.05 +/- 6.19\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.05     |\n","| time/              |          |\n","|    total_timesteps | 645600   |\n","| train/             |          |\n","|    actor_loss      | 9.25     |\n","|    critic_loss     | 16       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1245360  |\n","---------------------------------\n","Eval num_timesteps=648000, episode_reward=0.31 +/- 0.29\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.307    |\n","| time/              |          |\n","|    total_timesteps | 648000   |\n","| train/             |          |\n","|    actor_loss      | 9.5      |\n","|    critic_loss     | 10.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1247760  |\n","---------------------------------\n","Eval num_timesteps=650400, episode_reward=-1.43 +/- 0.08\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.43    |\n","| time/              |          |\n","|    total_timesteps | 650400   |\n","| train/             |          |\n","|    actor_loss      | 9.18     |\n","|    critic_loss     | 8.25     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1250160  |\n","---------------------------------\n","Eval num_timesteps=652800, episode_reward=-4.91 +/- 0.59\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -4.91    |\n","| time/              |          |\n","|    total_timesteps | 652800   |\n","| train/             |          |\n","|    actor_loss      | 9.22     |\n","|    critic_loss     | 15.8     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1252560  |\n","---------------------------------\n","Eval num_timesteps=655200, episode_reward=-4.98 +/- 0.75\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -4.98    |\n","| time/              |          |\n","|    total_timesteps | 655200   |\n","| train/             |          |\n","|    actor_loss      | 8.39     |\n","|    critic_loss     | 9.71     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1254960  |\n","---------------------------------\n","Eval num_timesteps=657600, episode_reward=-3.07 +/- 0.17\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -3.07    |\n","| time/              |          |\n","|    total_timesteps | 657600   |\n","| train/             |          |\n","|    actor_loss      | 8.11     |\n","|    critic_loss     | 12.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1257360  |\n","---------------------------------\n","Eval num_timesteps=660000, episode_reward=-3.75 +/- 1.11\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -3.75    |\n","| time/              |          |\n","|    total_timesteps | 660000   |\n","| train/             |          |\n","|    actor_loss      | 7.53     |\n","|    critic_loss     | 14.1     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1259760  |\n","---------------------------------\n","Eval num_timesteps=662400, episode_reward=-3.54 +/- 1.02\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -3.54    |\n","| time/              |          |\n","|    total_timesteps | 662400   |\n","| train/             |          |\n","|    actor_loss      | 7.27     |\n","|    critic_loss     | 8.42     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1262160  |\n","---------------------------------\n","Eval num_timesteps=664800, episode_reward=-5.23 +/- 1.54\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -5.23    |\n","| time/              |          |\n","|    total_timesteps | 664800   |\n","| train/             |          |\n","|    actor_loss      | 7.67     |\n","|    critic_loss     | 14.9     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1264560  |\n","---------------------------------\n","Eval num_timesteps=667200, episode_reward=-1.14 +/- 2.32\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.14    |\n","| time/              |          |\n","|    total_timesteps | 667200   |\n","| train/             |          |\n","|    actor_loss      | 8.36     |\n","|    critic_loss     | 13       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1266960  |\n","---------------------------------\n","Eval num_timesteps=669600, episode_reward=-2.92 +/- 0.11\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -2.92    |\n","| time/              |          |\n","|    total_timesteps | 669600   |\n","| train/             |          |\n","|    actor_loss      | 8.45     |\n","|    critic_loss     | 19.8     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1269360  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 1660     |\n","|    fps             | 100      |\n","|    time_elapsed    | 6664     |\n","|    total_timesteps | 671856   |\n","| train/             |          |\n","|    actor_loss      | 8.21     |\n","|    critic_loss     | 9.22     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1271616  |\n","---------------------------------\n","Eval num_timesteps=672000, episode_reward=-3.97 +/- 4.15\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -3.97    |\n","| time/              |          |\n","|    total_timesteps | 672000   |\n","| train/             |          |\n","|    actor_loss      | 7.97     |\n","|    critic_loss     | 8.59     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1271760  |\n","---------------------------------\n","Eval num_timesteps=674400, episode_reward=-108.95 +/- 0.07\n","Episode length: 80.40 +/- 1.96\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 80.4     |\n","|    mean_reward     | -109     |\n","| time/              |          |\n","|    total_timesteps | 674400   |\n","| train/             |          |\n","|    actor_loss      | 7.92     |\n","|    critic_loss     | 10.7     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1274160  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 1670     |\n","|    fps             | 100      |\n","|    time_elapsed    | 6690     |\n","|    total_timesteps | 674496   |\n","| train/             |          |\n","|    actor_loss      | 7.89     |\n","|    critic_loss     | 6.79     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1274256  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 1680     |\n","|    fps             | 100      |\n","|    time_elapsed    | 6699     |\n","|    total_timesteps | 675648   |\n","| train/             |          |\n","|    actor_loss      | 8.51     |\n","|    critic_loss     | 7.69     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1275408  |\n","---------------------------------\n","Eval num_timesteps=676800, episode_reward=-109.68 +/- 0.00\n","Episode length: 246.60 +/- 135.21\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 247      |\n","|    mean_reward     | -110     |\n","| time/              |          |\n","|    total_timesteps | 676800   |\n","| train/             |          |\n","|    actor_loss      | 8.12     |\n","|    critic_loss     | 14       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1276560  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 1690     |\n","|    fps             | 100      |\n","|    time_elapsed    | 6728     |\n","|    total_timesteps | 678912   |\n","| train/             |          |\n","|    actor_loss      | 8.1      |\n","|    critic_loss     | 17       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1278672  |\n","---------------------------------\n","Eval num_timesteps=679200, episode_reward=-72.39 +/- 49.97\n","Episode length: 365.60 +/- 109.74\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 366      |\n","|    mean_reward     | -72.4    |\n","| time/              |          |\n","|    total_timesteps | 679200   |\n","| train/             |          |\n","|    actor_loss      | 7.97     |\n","|    critic_loss     | 10.1     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1278960  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 1700     |\n","|    fps             | 100      |\n","|    time_elapsed    | 6752     |\n","|    total_timesteps | 681312   |\n","| train/             |          |\n","|    actor_loss      | 7.49     |\n","|    critic_loss     | 10.1     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1281072  |\n","---------------------------------\n","Eval num_timesteps=681600, episode_reward=1.20 +/- 0.59\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.2      |\n","| time/              |          |\n","|    total_timesteps | 681600   |\n","| train/             |          |\n","|    actor_loss      | 8.42     |\n","|    critic_loss     | 21.3     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1281360  |\n","---------------------------------\n","Eval num_timesteps=684000, episode_reward=-2.68 +/- 0.78\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -2.68    |\n","| time/              |          |\n","|    total_timesteps | 684000   |\n","| train/             |          |\n","|    actor_loss      | 7.97     |\n","|    critic_loss     | 19.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1283760  |\n","---------------------------------\n","Eval num_timesteps=686400, episode_reward=-0.61 +/- 0.62\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.609   |\n","| time/              |          |\n","|    total_timesteps | 686400   |\n","| train/             |          |\n","|    actor_loss      | 7.67     |\n","|    critic_loss     | 8.51     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1286160  |\n","---------------------------------\n","Eval num_timesteps=688800, episode_reward=-0.09 +/- 0.95\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.0937  |\n","| time/              |          |\n","|    total_timesteps | 688800   |\n","| train/             |          |\n","|    actor_loss      | 8.85     |\n","|    critic_loss     | 13.6     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1288560  |\n","---------------------------------\n","Eval num_timesteps=691200, episode_reward=1.64 +/- 0.24\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.64     |\n","| time/              |          |\n","|    total_timesteps | 691200   |\n","| train/             |          |\n","|    actor_loss      | 9.31     |\n","|    critic_loss     | 17.5     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1290960  |\n","---------------------------------\n","Eval num_timesteps=693600, episode_reward=6.98 +/- 0.47\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.98     |\n","| time/              |          |\n","|    total_timesteps | 693600   |\n","| train/             |          |\n","|    actor_loss      | 8.95     |\n","|    critic_loss     | 4.83     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1293360  |\n","---------------------------------\n","Eval num_timesteps=696000, episode_reward=3.94 +/- 2.74\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.94     |\n","| time/              |          |\n","|    total_timesteps | 696000   |\n","| train/             |          |\n","|    actor_loss      | 8.05     |\n","|    critic_loss     | 8.31     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1295760  |\n","---------------------------------\n","Eval num_timesteps=698400, episode_reward=-0.91 +/- 0.96\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.914   |\n","| time/              |          |\n","|    total_timesteps | 698400   |\n","| train/             |          |\n","|    actor_loss      | 8.95     |\n","|    critic_loss     | 8.26     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1298160  |\n","---------------------------------\n","Eval num_timesteps=700800, episode_reward=0.28 +/- 1.86\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.275    |\n","| time/              |          |\n","|    total_timesteps | 700800   |\n","| train/             |          |\n","|    actor_loss      | 8.96     |\n","|    critic_loss     | 15.8     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1300560  |\n","---------------------------------\n","Eval num_timesteps=703200, episode_reward=2.71 +/- 2.42\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.71     |\n","| time/              |          |\n","|    total_timesteps | 703200   |\n","| train/             |          |\n","|    actor_loss      | 8.95     |\n","|    critic_loss     | 10       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1302960  |\n","---------------------------------\n","Eval num_timesteps=705600, episode_reward=7.08 +/- 0.14\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.08     |\n","| time/              |          |\n","|    total_timesteps | 705600   |\n","| train/             |          |\n","|    actor_loss      | 9.01     |\n","|    critic_loss     | 24.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1305360  |\n","---------------------------------\n","Eval num_timesteps=708000, episode_reward=0.49 +/- 3.46\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.485    |\n","| time/              |          |\n","|    total_timesteps | 708000   |\n","| train/             |          |\n","|    actor_loss      | 8.27     |\n","|    critic_loss     | 5.31     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1307760  |\n","---------------------------------\n","Eval num_timesteps=710400, episode_reward=-2.82 +/- 2.87\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -2.82    |\n","| time/              |          |\n","|    total_timesteps | 710400   |\n","| train/             |          |\n","|    actor_loss      | 8.01     |\n","|    critic_loss     | 16.1     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1310160  |\n","---------------------------------\n","Eval num_timesteps=712800, episode_reward=-1.23 +/- 1.56\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.23    |\n","| time/              |          |\n","|    total_timesteps | 712800   |\n","| train/             |          |\n","|    actor_loss      | 7.85     |\n","|    critic_loss     | 9.62     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1312560  |\n","---------------------------------\n","Eval num_timesteps=715200, episode_reward=-3.78 +/- 1.13\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -3.78    |\n","| time/              |          |\n","|    total_timesteps | 715200   |\n","| train/             |          |\n","|    actor_loss      | 7.65     |\n","|    critic_loss     | 20.7     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1314960  |\n","---------------------------------\n","Eval num_timesteps=717600, episode_reward=-31.65 +/- 54.93\n","Episode length: 426.80 +/- 89.65\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 427      |\n","|    mean_reward     | -31.6    |\n","| time/              |          |\n","|    total_timesteps | 717600   |\n","| train/             |          |\n","|    actor_loss      | 6.93     |\n","|    critic_loss     | 7.37     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1317360  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 1710     |\n","|    fps             | 100      |\n","|    time_elapsed    | 7154     |\n","|    total_timesteps | 719640   |\n","| train/             |          |\n","|    actor_loss      | 7.09     |\n","|    critic_loss     | 8.18     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1319400  |\n","---------------------------------\n","Eval num_timesteps=720000, episode_reward=-3.05 +/- 0.30\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -3.05    |\n","| time/              |          |\n","|    total_timesteps | 720000   |\n","| train/             |          |\n","|    actor_loss      | 6.95     |\n","|    critic_loss     | 13.9     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1319760  |\n","---------------------------------\n","Eval num_timesteps=722400, episode_reward=-1.71 +/- 2.86\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.71    |\n","| time/              |          |\n","|    total_timesteps | 722400   |\n","| train/             |          |\n","|    actor_loss      | 6.54     |\n","|    critic_loss     | 8.35     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1322160  |\n","---------------------------------\n","Eval num_timesteps=724800, episode_reward=-44.41 +/- 47.33\n","Episode length: 417.60 +/- 100.92\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 418      |\n","|    mean_reward     | -44.4    |\n","| time/              |          |\n","|    total_timesteps | 724800   |\n","| train/             |          |\n","|    actor_loss      | 6.49     |\n","|    critic_loss     | 12.7     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1324560  |\n","---------------------------------\n","Eval num_timesteps=727200, episode_reward=-4.86 +/- 1.02\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -4.86    |\n","| time/              |          |\n","|    total_timesteps | 727200   |\n","| train/             |          |\n","|    actor_loss      | 5.81     |\n","|    critic_loss     | 5.14     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1326960  |\n","---------------------------------\n","Eval num_timesteps=729600, episode_reward=-0.79 +/- 1.22\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.793   |\n","| time/              |          |\n","|    total_timesteps | 729600   |\n","| train/             |          |\n","|    actor_loss      | 6.15     |\n","|    critic_loss     | 9.47     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1329360  |\n","---------------------------------\n","Eval num_timesteps=732000, episode_reward=-60.59 +/- 48.51\n","Episode length: 470.60 +/- 24.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 471      |\n","|    mean_reward     | -60.6    |\n","| time/              |          |\n","|    total_timesteps | 732000   |\n","| train/             |          |\n","|    actor_loss      | 6.38     |\n","|    critic_loss     | 15.5     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1331760  |\n","---------------------------------\n","Eval num_timesteps=734400, episode_reward=-61.03 +/- 49.58\n","Episode length: 386.60 +/- 92.59\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 387      |\n","|    mean_reward     | -61      |\n","| time/              |          |\n","|    total_timesteps | 734400   |\n","| train/             |          |\n","|    actor_loss      | 5.55     |\n","|    critic_loss     | 17.6     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1334160  |\n","---------------------------------\n","Eval num_timesteps=736800, episode_reward=-1.74 +/- 0.06\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.74    |\n","| time/              |          |\n","|    total_timesteps | 736800   |\n","| train/             |          |\n","|    actor_loss      | 6.58     |\n","|    critic_loss     | 12.6     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1336560  |\n","---------------------------------\n","Eval num_timesteps=739200, episode_reward=-62.53 +/- 51.76\n","Episode length: 367.40 +/- 108.27\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 367      |\n","|    mean_reward     | -62.5    |\n","| time/              |          |\n","|    total_timesteps | 739200   |\n","| train/             |          |\n","|    actor_loss      | 6.52     |\n","|    critic_loss     | 4.85     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1338960  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 1720     |\n","|    fps             | 100      |\n","|    time_elapsed    | 7378     |\n","|    total_timesteps | 741480   |\n","| train/             |          |\n","|    actor_loss      | 6.9      |\n","|    critic_loss     | 12.1     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1341240  |\n","---------------------------------\n","Eval num_timesteps=741600, episode_reward=-68.07 +/- 53.66\n","Episode length: 411.80 +/- 72.01\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 412      |\n","|    mean_reward     | -68.1    |\n","| time/              |          |\n","|    total_timesteps | 741600   |\n","| train/             |          |\n","|    actor_loss      | 6.66     |\n","|    critic_loss     | 6.72     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1341360  |\n","---------------------------------\n","Eval num_timesteps=744000, episode_reward=0.54 +/- 0.53\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.538    |\n","| time/              |          |\n","|    total_timesteps | 744000   |\n","| train/             |          |\n","|    actor_loss      | 6.53     |\n","|    critic_loss     | 6.22     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1343760  |\n","---------------------------------\n","Eval num_timesteps=746400, episode_reward=-3.12 +/- 1.31\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -3.12    |\n","| time/              |          |\n","|    total_timesteps | 746400   |\n","| train/             |          |\n","|    actor_loss      | 6.51     |\n","|    critic_loss     | 8.69     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1346160  |\n","---------------------------------\n","Eval num_timesteps=748800, episode_reward=-3.97 +/- 0.33\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -3.97    |\n","| time/              |          |\n","|    total_timesteps | 748800   |\n","| train/             |          |\n","|    actor_loss      | 7.26     |\n","|    critic_loss     | 9.01     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1348560  |\n","---------------------------------\n","Eval num_timesteps=751200, episode_reward=-2.41 +/- 1.00\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -2.41    |\n","| time/              |          |\n","|    total_timesteps | 751200   |\n","| train/             |          |\n","|    actor_loss      | 6.48     |\n","|    critic_loss     | 10.8     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1350960  |\n","---------------------------------\n","Eval num_timesteps=753600, episode_reward=-60.25 +/- 49.73\n","Episode length: 419.00 +/- 66.14\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 419      |\n","|    mean_reward     | -60.2    |\n","| time/              |          |\n","|    total_timesteps | 753600   |\n","| train/             |          |\n","|    actor_loss      | 6.92     |\n","|    critic_loss     | 13.8     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1353360  |\n","---------------------------------\n","Eval num_timesteps=756000, episode_reward=-106.10 +/- 4.32\n","Episode length: 93.80 +/- 25.47\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 93.8     |\n","|    mean_reward     | -106     |\n","| time/              |          |\n","|    total_timesteps | 756000   |\n","| train/             |          |\n","|    actor_loss      | 6.41     |\n","|    critic_loss     | 9.15     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1355760  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 1730     |\n","|    fps             | 100      |\n","|    time_elapsed    | 7532     |\n","|    total_timesteps | 756264   |\n","| train/             |          |\n","|    actor_loss      | 7.2      |\n","|    critic_loss     | 13.3     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1356024  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 1740     |\n","|    fps             | 100      |\n","|    time_elapsed    | 7537     |\n","|    total_timesteps | 756744   |\n","| train/             |          |\n","|    actor_loss      | 6.55     |\n","|    critic_loss     | 19.9     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1356504  |\n","---------------------------------\n","Eval num_timesteps=758400, episode_reward=-64.20 +/- 52.44\n","Episode length: 496.40 +/- 2.94\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 496      |\n","|    mean_reward     | -64.2    |\n","| time/              |          |\n","|    total_timesteps | 758400   |\n","| train/             |          |\n","|    actor_loss      | 6.04     |\n","|    critic_loss     | 7.17     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1358160  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 1750     |\n","|    fps             | 100      |\n","|    time_elapsed    | 7569     |\n","|    total_timesteps | 760128   |\n","| train/             |          |\n","|    actor_loss      | 7.01     |\n","|    critic_loss     | 15.6     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1359888  |\n","---------------------------------\n","Eval num_timesteps=760800, episode_reward=-105.41 +/- 2.28\n","Episode length: 134.60 +/- 33.80\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 135      |\n","|    mean_reward     | -105     |\n","| time/              |          |\n","|    total_timesteps | 760800   |\n","| train/             |          |\n","|    actor_loss      | 6.49     |\n","|    critic_loss     | 5.27     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1360560  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 1760     |\n","|    fps             | 100      |\n","|    time_elapsed    | 7578     |\n","|    total_timesteps | 760872   |\n","| train/             |          |\n","|    actor_loss      | 6.49     |\n","|    critic_loss     | 17       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1360632  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 1770     |\n","|    fps             | 100      |\n","|    time_elapsed    | 7588     |\n","|    total_timesteps | 762216   |\n","| train/             |          |\n","|    actor_loss      | 6.73     |\n","|    critic_loss     | 14.9     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1361976  |\n","---------------------------------\n","Eval num_timesteps=763200, episode_reward=-58.26 +/- 52.07\n","Episode length: 263.00 +/- 193.51\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 263      |\n","|    mean_reward     | -58.3    |\n","| time/              |          |\n","|    total_timesteps | 763200   |\n","| train/             |          |\n","|    actor_loss      | 7.02     |\n","|    critic_loss     | 18.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1362960  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 1780     |\n","|    fps             | 100      |\n","|    time_elapsed    | 7618     |\n","|    total_timesteps | 765216   |\n","| train/             |          |\n","|    actor_loss      | 6.5      |\n","|    critic_loss     | 10.7     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1364976  |\n","---------------------------------\n","Eval num_timesteps=765600, episode_reward=-61.72 +/- 52.08\n","Episode length: 345.20 +/- 126.39\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 345      |\n","|    mean_reward     | -61.7    |\n","| time/              |          |\n","|    total_timesteps | 765600   |\n","| train/             |          |\n","|    actor_loss      | 6.56     |\n","|    critic_loss     | 12.5     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1365360  |\n","---------------------------------\n","Eval num_timesteps=768000, episode_reward=-103.43 +/- 2.13\n","Episode length: 201.40 +/- 46.05\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 201      |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 768000   |\n","| train/             |          |\n","|    actor_loss      | 6.46     |\n","|    critic_loss     | 16.7     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1367760  |\n","---------------------------------\n","Eval num_timesteps=770400, episode_reward=-47.61 +/- 56.55\n","Episode length: 297.20 +/- 165.59\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 297      |\n","|    mean_reward     | -47.6    |\n","| time/              |          |\n","|    total_timesteps | 770400   |\n","| train/             |          |\n","|    actor_loss      | 6.54     |\n","|    critic_loss     | 17.6     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1370160  |\n","---------------------------------\n","Eval num_timesteps=772800, episode_reward=9.41 +/- 3.81\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 9.41     |\n","| time/              |          |\n","|    total_timesteps | 772800   |\n","| train/             |          |\n","|    actor_loss      | 6.12     |\n","|    critic_loss     | 8.22     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1372560  |\n","---------------------------------\n","New best mean reward!\n","Eval num_timesteps=775200, episode_reward=10.53 +/- 1.41\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 10.5     |\n","| time/              |          |\n","|    total_timesteps | 775200   |\n","| train/             |          |\n","|    actor_loss      | 6.19     |\n","|    critic_loss     | 8.11     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1374960  |\n","---------------------------------\n","New best mean reward!\n","Eval num_timesteps=777600, episode_reward=10.76 +/- 0.28\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 10.8     |\n","| time/              |          |\n","|    total_timesteps | 777600   |\n","| train/             |          |\n","|    actor_loss      | 6.63     |\n","|    critic_loss     | 11.7     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1377360  |\n","---------------------------------\n","New best mean reward!\n","Eval num_timesteps=780000, episode_reward=-64.80 +/- 53.47\n","Episode length: 477.20 +/- 18.62\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 477      |\n","|    mean_reward     | -64.8    |\n","| time/              |          |\n","|    total_timesteps | 780000   |\n","| train/             |          |\n","|    actor_loss      | 5.96     |\n","|    critic_loss     | 9.12     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1379760  |\n","---------------------------------\n","Eval num_timesteps=782400, episode_reward=-63.89 +/- 52.32\n","Episode length: 498.20 +/- 1.47\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 498      |\n","|    mean_reward     | -63.9    |\n","| time/              |          |\n","|    total_timesteps | 782400   |\n","| train/             |          |\n","|    actor_loss      | 5.57     |\n","|    critic_loss     | 13.1     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1382160  |\n","---------------------------------\n","Eval num_timesteps=784800, episode_reward=-54.19 +/- 55.79\n","Episode length: 394.40 +/- 86.22\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 394      |\n","|    mean_reward     | -54.2    |\n","| time/              |          |\n","|    total_timesteps | 784800   |\n","| train/             |          |\n","|    actor_loss      | 5.71     |\n","|    critic_loss     | 9.47     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1384560  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 1790     |\n","|    fps             | 100      |\n","|    time_elapsed    | 7835     |\n","|    total_timesteps | 786408   |\n","| train/             |          |\n","|    actor_loss      | 5.73     |\n","|    critic_loss     | 11.6     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1386168  |\n","---------------------------------\n","Eval num_timesteps=787200, episode_reward=-61.00 +/- 55.96\n","Episode length: 498.20 +/- 1.47\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 498      |\n","|    mean_reward     | -61      |\n","| time/              |          |\n","|    total_timesteps | 787200   |\n","| train/             |          |\n","|    actor_loss      | 5.99     |\n","|    critic_loss     | 7.2      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1386960  |\n","---------------------------------\n","Eval num_timesteps=789600, episode_reward=-133.44 +/- 8.92\n","Episode length: 466.60 +/- 2.94\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 467      |\n","|    mean_reward     | -133     |\n","| time/              |          |\n","|    total_timesteps | 789600   |\n","| train/             |          |\n","|    actor_loss      | 5.86     |\n","|    critic_loss     | 6.56     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1389360  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 1800     |\n","|    fps             | 100      |\n","|    time_elapsed    | 7878     |\n","|    total_timesteps | 790344   |\n","| train/             |          |\n","|    actor_loss      | 5.12     |\n","|    critic_loss     | 10.9     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1390104  |\n","---------------------------------\n","Eval num_timesteps=792000, episode_reward=-63.46 +/- 53.31\n","Episode length: 392.00 +/- 88.18\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 392      |\n","|    mean_reward     | -63.5    |\n","| time/              |          |\n","|    total_timesteps | 792000   |\n","| train/             |          |\n","|    actor_loss      | 4.66     |\n","|    critic_loss     | 10.7     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1391760  |\n","---------------------------------\n","Eval num_timesteps=794400, episode_reward=7.01 +/- 0.28\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.01     |\n","| time/              |          |\n","|    total_timesteps | 794400   |\n","| train/             |          |\n","|    actor_loss      | 4.64     |\n","|    critic_loss     | 12.1     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1394160  |\n","---------------------------------\n","Eval num_timesteps=796800, episode_reward=-98.57 +/- 0.21\n","Episode length: 456.40 +/- 28.90\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 456      |\n","|    mean_reward     | -98.6    |\n","| time/              |          |\n","|    total_timesteps | 796800   |\n","| train/             |          |\n","|    actor_loss      | 4.51     |\n","|    critic_loss     | 16.1     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1396560  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 1810     |\n","|    fps             | 100      |\n","|    time_elapsed    | 7951     |\n","|    total_timesteps | 797280   |\n","| train/             |          |\n","|    actor_loss      | 5.11     |\n","|    critic_loss     | 11.1     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1397040  |\n","---------------------------------\n","-------------------------------\n","| time/              |        |\n","|    episodes        | 1820   |\n","|    fps             | 100    |\n","|    time_elapsed    | 7951   |\n","|    total_timesteps | 797280 |\n","-------------------------------\n","Eval num_timesteps=799200, episode_reward=-3.81 +/- 2.27\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -3.81    |\n","| time/              |          |\n","|    total_timesteps | 799200   |\n","| train/             |          |\n","|    actor_loss      | 5.03     |\n","|    critic_loss     | 8.22     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1398960  |\n","---------------------------------\n","Eval num_timesteps=801600, episode_reward=-103.71 +/- 0.73\n","Episode length: 418.40 +/- 17.64\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 418      |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 801600   |\n","| train/             |          |\n","|    actor_loss      | 5        |\n","|    critic_loss     | 13.9     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1401360  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 1830     |\n","|    fps             | 100      |\n","|    time_elapsed    | 8008     |\n","|    total_timesteps | 803040   |\n","| train/             |          |\n","|    actor_loss      | 5.42     |\n","|    critic_loss     | 10.5     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1402800  |\n","---------------------------------\n","Eval num_timesteps=804000, episode_reward=-101.50 +/- 1.05\n","Episode length: 190.40 +/- 61.73\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 190      |\n","|    mean_reward     | -102     |\n","| time/              |          |\n","|    total_timesteps | 804000   |\n","| train/             |          |\n","|    actor_loss      | 6.07     |\n","|    critic_loss     | 6.48     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1403760  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 1840     |\n","|    fps             | 100      |\n","|    time_elapsed    | 8031     |\n","|    total_timesteps | 805512   |\n","| train/             |          |\n","|    actor_loss      | 6.32     |\n","|    critic_loss     | 21       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1405272  |\n","---------------------------------\n","Eval num_timesteps=806400, episode_reward=-3.38 +/- 4.68\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -3.38    |\n","| time/              |          |\n","|    total_timesteps | 806400   |\n","| train/             |          |\n","|    actor_loss      | 5.87     |\n","|    critic_loss     | 7.47     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1406160  |\n","---------------------------------\n","Eval num_timesteps=808800, episode_reward=-105.76 +/- 0.25\n","Episode length: 289.20 +/- 42.62\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 289      |\n","|    mean_reward     | -106     |\n","| time/              |          |\n","|    total_timesteps | 808800   |\n","| train/             |          |\n","|    actor_loss      | 6.04     |\n","|    critic_loss     | 24.8     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1408560  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 1850     |\n","|    fps             | 100      |\n","|    time_elapsed    | 8075     |\n","|    total_timesteps | 809856   |\n","| train/             |          |\n","|    actor_loss      | 6.35     |\n","|    critic_loss     | 6.04     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1409616  |\n","---------------------------------\n","Eval num_timesteps=811200, episode_reward=-4.75 +/- 6.49\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -4.75    |\n","| time/              |          |\n","|    total_timesteps | 811200   |\n","| train/             |          |\n","|    actor_loss      | 6.24     |\n","|    critic_loss     | 9.16     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1410960  |\n","---------------------------------\n","Eval num_timesteps=813600, episode_reward=-109.09 +/- 0.96\n","Episode length: 294.40 +/- 14.21\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 294      |\n","|    mean_reward     | -109     |\n","| time/              |          |\n","|    total_timesteps | 813600   |\n","| train/             |          |\n","|    actor_loss      | 6.85     |\n","|    critic_loss     | 13.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1413360  |\n","---------------------------------\n","Eval num_timesteps=816000, episode_reward=-108.23 +/- 2.24\n","Episode length: 312.60 +/- 65.65\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 313      |\n","|    mean_reward     | -108     |\n","| time/              |          |\n","|    total_timesteps | 816000   |\n","| train/             |          |\n","|    actor_loss      | 6.46     |\n","|    critic_loss     | 12.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1415760  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 1860     |\n","|    fps             | 100      |\n","|    time_elapsed    | 8153     |\n","|    total_timesteps | 817968   |\n","| train/             |          |\n","|    actor_loss      | 6.57     |\n","|    critic_loss     | 11.1     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1417728  |\n","---------------------------------\n","Eval num_timesteps=818400, episode_reward=-117.16 +/- 8.92\n","Episode length: 238.20 +/- 96.51\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 238      |\n","|    mean_reward     | -117     |\n","| time/              |          |\n","|    total_timesteps | 818400   |\n","| train/             |          |\n","|    actor_loss      | 6.84     |\n","|    critic_loss     | 19.7     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1418160  |\n","---------------------------------\n","Eval num_timesteps=820800, episode_reward=-112.77 +/- 4.37\n","Episode length: 340.40 +/- 44.58\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 340      |\n","|    mean_reward     | -113     |\n","| time/              |          |\n","|    total_timesteps | 820800   |\n","| train/             |          |\n","|    actor_loss      | 6.3      |\n","|    critic_loss     | 11.7     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1420560  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 1870     |\n","|    fps             | 100      |\n","|    time_elapsed    | 8186     |\n","|    total_timesteps | 821424   |\n","| train/             |          |\n","|    actor_loss      | 6.55     |\n","|    critic_loss     | 9.54     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1421184  |\n","---------------------------------\n","Eval num_timesteps=823200, episode_reward=-105.51 +/- 0.82\n","Episode length: 241.40 +/- 186.65\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 241      |\n","|    mean_reward     | -106     |\n","| time/              |          |\n","|    total_timesteps | 823200   |\n","| train/             |          |\n","|    actor_loss      | 6.73     |\n","|    critic_loss     | 11.1     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1422960  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 1880     |\n","|    fps             | 100      |\n","|    time_elapsed    | 8212     |\n","|    total_timesteps | 823920   |\n","| train/             |          |\n","|    actor_loss      | 6.55     |\n","|    critic_loss     | 21.8     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1423680  |\n","---------------------------------\n","-------------------------------\n","| time/              |        |\n","|    episodes        | 1890   |\n","|    fps             | 100    |\n","|    time_elapsed    | 8212   |\n","|    total_timesteps | 823920 |\n","-------------------------------\n","Eval num_timesteps=825600, episode_reward=-35.93 +/- 54.11\n","Episode length: 382.00 +/- 144.52\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 382      |\n","|    mean_reward     | -35.9    |\n","| time/              |          |\n","|    total_timesteps | 825600   |\n","| train/             |          |\n","|    actor_loss      | 6.47     |\n","|    critic_loss     | 10.9     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1425360  |\n","---------------------------------\n","Eval num_timesteps=828000, episode_reward=-39.68 +/- 61.61\n","Episode length: 431.60 +/- 83.77\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 432      |\n","|    mean_reward     | -39.7    |\n","| time/              |          |\n","|    total_timesteps | 828000   |\n","| train/             |          |\n","|    actor_loss      | 6.09     |\n","|    critic_loss     | 11.8     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1427760  |\n","---------------------------------\n","Eval num_timesteps=830400, episode_reward=-115.38 +/- 1.75\n","Episode length: 274.20 +/- 28.41\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 274      |\n","|    mean_reward     | -115     |\n","| time/              |          |\n","|    total_timesteps | 830400   |\n","| train/             |          |\n","|    actor_loss      | 6.61     |\n","|    critic_loss     | 15.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1430160  |\n","---------------------------------\n","Eval num_timesteps=832800, episode_reward=-110.44 +/- 3.43\n","Episode length: 259.40 +/- 6.86\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 259      |\n","|    mean_reward     | -110     |\n","| time/              |          |\n","|    total_timesteps | 832800   |\n","| train/             |          |\n","|    actor_loss      | 6.78     |\n","|    critic_loss     | 6.55     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1432560  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 1900     |\n","|    fps             | 100      |\n","|    time_elapsed    | 8306     |\n","|    total_timesteps | 833304   |\n","| train/             |          |\n","|    actor_loss      | 5.97     |\n","|    critic_loss     | 7.78     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1433064  |\n","---------------------------------\n","Eval num_timesteps=835200, episode_reward=-115.15 +/- 1.26\n","Episode length: 328.80 +/- 28.41\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 329      |\n","|    mean_reward     | -115     |\n","| time/              |          |\n","|    total_timesteps | 835200   |\n","| train/             |          |\n","|    actor_loss      | 6.92     |\n","|    critic_loss     | 7.36     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1434960  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 1910     |\n","|    fps             | 100      |\n","|    time_elapsed    | 8327     |\n","|    total_timesteps | 835416   |\n","| train/             |          |\n","|    actor_loss      | 6.79     |\n","|    critic_loss     | 10.6     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1435176  |\n","---------------------------------\n","Eval num_timesteps=837600, episode_reward=-119.26 +/- 11.38\n","Episode length: 424.80 +/- 11.27\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 425      |\n","|    mean_reward     | -119     |\n","| time/              |          |\n","|    total_timesteps | 837600   |\n","| train/             |          |\n","|    actor_loss      | 6.9      |\n","|    critic_loss     | 10.8     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1437360  |\n","---------------------------------\n","Eval num_timesteps=840000, episode_reward=-7.91 +/- 2.87\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -7.91    |\n","| time/              |          |\n","|    total_timesteps | 840000   |\n","| train/             |          |\n","|    actor_loss      | 6.77     |\n","|    critic_loss     | 16.9     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1439760  |\n","---------------------------------\n","Eval num_timesteps=842400, episode_reward=-8.73 +/- 0.93\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -8.73    |\n","| time/              |          |\n","|    total_timesteps | 842400   |\n","| train/             |          |\n","|    actor_loss      | 7.02     |\n","|    critic_loss     | 8.92     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1442160  |\n","---------------------------------\n","Eval num_timesteps=844800, episode_reward=-48.51 +/- 58.07\n","Episode length: 490.80 +/- 11.27\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 491      |\n","|    mean_reward     | -48.5    |\n","| time/              |          |\n","|    total_timesteps | 844800   |\n","| train/             |          |\n","|    actor_loss      | 7.09     |\n","|    critic_loss     | 11.1     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1444560  |\n","---------------------------------\n","Eval num_timesteps=847200, episode_reward=4.35 +/- 4.99\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.35     |\n","| time/              |          |\n","|    total_timesteps | 847200   |\n","| train/             |          |\n","|    actor_loss      | 6.64     |\n","|    critic_loss     | 7.86     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1446960  |\n","---------------------------------\n","Eval num_timesteps=849600, episode_reward=-6.41 +/- 2.65\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -6.41    |\n","| time/              |          |\n","|    total_timesteps | 849600   |\n","| train/             |          |\n","|    actor_loss      | 7.1      |\n","|    critic_loss     | 12.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1449360  |\n","---------------------------------\n","Eval num_timesteps=852000, episode_reward=-7.73 +/- 0.32\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -7.73    |\n","| time/              |          |\n","|    total_timesteps | 852000   |\n","| train/             |          |\n","|    actor_loss      | 6.49     |\n","|    critic_loss     | 7.53     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1451760  |\n","---------------------------------\n","Eval num_timesteps=854400, episode_reward=-10.05 +/- 2.24\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -10      |\n","| time/              |          |\n","|    total_timesteps | 854400   |\n","| train/             |          |\n","|    actor_loss      | 6.01     |\n","|    critic_loss     | 15.3     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1454160  |\n","---------------------------------\n","Eval num_timesteps=856800, episode_reward=-68.81 +/- 46.72\n","Episode length: 485.60 +/- 11.76\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 486      |\n","|    mean_reward     | -68.8    |\n","| time/              |          |\n","|    total_timesteps | 856800   |\n","| train/             |          |\n","|    actor_loss      | 5.12     |\n","|    critic_loss     | 10.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1456560  |\n","---------------------------------\n","Eval num_timesteps=859200, episode_reward=-9.63 +/- 0.89\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -9.63    |\n","| time/              |          |\n","|    total_timesteps | 859200   |\n","| train/             |          |\n","|    actor_loss      | 4.92     |\n","|    critic_loss     | 14.8     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1458960  |\n","---------------------------------\n","Eval num_timesteps=861600, episode_reward=-3.06 +/- 2.27\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -3.06    |\n","| time/              |          |\n","|    total_timesteps | 861600   |\n","| train/             |          |\n","|    actor_loss      | 3.95     |\n","|    critic_loss     | 15.5     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1461360  |\n","---------------------------------\n","Eval num_timesteps=864000, episode_reward=2.09 +/- 0.82\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.09     |\n","| time/              |          |\n","|    total_timesteps | 864000   |\n","| train/             |          |\n","|    actor_loss      | 2.74     |\n","|    critic_loss     | 8.57     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1463760  |\n","---------------------------------\n","Eval num_timesteps=866400, episode_reward=3.65 +/- 1.52\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.65     |\n","| time/              |          |\n","|    total_timesteps | 866400   |\n","| train/             |          |\n","|    actor_loss      | 2.47     |\n","|    critic_loss     | 12.5     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1466160  |\n","---------------------------------\n","Eval num_timesteps=868800, episode_reward=-44.41 +/- 49.89\n","Episode length: 396.00 +/- 127.37\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 396      |\n","|    mean_reward     | -44.4    |\n","| time/              |          |\n","|    total_timesteps | 868800   |\n","| train/             |          |\n","|    actor_loss      | 2.49     |\n","|    critic_loss     | 16.9     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1468560  |\n","---------------------------------\n","Eval num_timesteps=871200, episode_reward=-97.55 +/- 0.33\n","Episode length: 336.60 +/- 58.30\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 337      |\n","|    mean_reward     | -97.5    |\n","| time/              |          |\n","|    total_timesteps | 871200   |\n","| train/             |          |\n","|    actor_loss      | 2.06     |\n","|    critic_loss     | 3.17     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1470960  |\n","---------------------------------\n","Eval num_timesteps=873600, episode_reward=-70.31 +/- 48.13\n","Episode length: 353.60 +/- 119.54\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 354      |\n","|    mean_reward     | -70.3    |\n","| time/              |          |\n","|    total_timesteps | 873600   |\n","| train/             |          |\n","|    actor_loss      | 2.55     |\n","|    critic_loss     | 12.9     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1473360  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 1920     |\n","|    fps             | 100      |\n","|    time_elapsed    | 8733     |\n","|    total_timesteps | 875712   |\n","| train/             |          |\n","|    actor_loss      | 3.08     |\n","|    critic_loss     | 16.1     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1475472  |\n","---------------------------------\n","Eval num_timesteps=876000, episode_reward=-99.61 +/- 1.55\n","Episode length: 217.60 +/- 70.55\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 218      |\n","|    mean_reward     | -99.6    |\n","| time/              |          |\n","|    total_timesteps | 876000   |\n","| train/             |          |\n","|    actor_loss      | 3.22     |\n","|    critic_loss     | 10.7     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1475760  |\n","---------------------------------\n","Eval num_timesteps=878400, episode_reward=0.15 +/- 0.25\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.15     |\n","| time/              |          |\n","|    total_timesteps | 878400   |\n","| train/             |          |\n","|    actor_loss      | 3.2      |\n","|    critic_loss     | 8.73     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1478160  |\n","---------------------------------\n","Eval num_timesteps=880800, episode_reward=4.39 +/- 0.97\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.39     |\n","| time/              |          |\n","|    total_timesteps | 880800   |\n","| train/             |          |\n","|    actor_loss      | 3.6      |\n","|    critic_loss     | 7.88     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1480560  |\n","---------------------------------\n","Eval num_timesteps=883200, episode_reward=-2.39 +/- 2.72\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -2.39    |\n","| time/              |          |\n","|    total_timesteps | 883200   |\n","| train/             |          |\n","|    actor_loss      | 2.85     |\n","|    critic_loss     | 3.47     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1482960  |\n","---------------------------------\n","Eval num_timesteps=885600, episode_reward=5.72 +/- 1.51\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.72     |\n","| time/              |          |\n","|    total_timesteps | 885600   |\n","| train/             |          |\n","|    actor_loss      | 2.99     |\n","|    critic_loss     | 10.5     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1485360  |\n","---------------------------------\n","Eval num_timesteps=888000, episode_reward=-61.14 +/- 48.31\n","Episode length: 408.80 +/- 74.46\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 409      |\n","|    mean_reward     | -61.1    |\n","| time/              |          |\n","|    total_timesteps | 888000   |\n","| train/             |          |\n","|    actor_loss      | 2.76     |\n","|    critic_loss     | 17.8     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1487760  |\n","---------------------------------\n","Eval num_timesteps=890400, episode_reward=-32.35 +/- 52.68\n","Episode length: 462.80 +/- 45.56\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 463      |\n","|    mean_reward     | -32.3    |\n","| time/              |          |\n","|    total_timesteps | 890400   |\n","| train/             |          |\n","|    actor_loss      | 3.59     |\n","|    critic_loss     | 6.91     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1490160  |\n","---------------------------------\n","Eval num_timesteps=892800, episode_reward=7.89 +/- 1.73\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.89     |\n","| time/              |          |\n","|    total_timesteps | 892800   |\n","| train/             |          |\n","|    actor_loss      | 4.04     |\n","|    critic_loss     | 20.5     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1492560  |\n","---------------------------------\n","Eval num_timesteps=895200, episode_reward=-43.80 +/- 51.56\n","Episode length: 430.80 +/- 84.75\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 431      |\n","|    mean_reward     | -43.8    |\n","| time/              |          |\n","|    total_timesteps | 895200   |\n","| train/             |          |\n","|    actor_loss      | 4.02     |\n","|    critic_loss     | 5.47     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1494960  |\n","---------------------------------\n","Eval num_timesteps=897600, episode_reward=-106.30 +/- 0.45\n","Episode length: 155.60 +/- 17.64\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 156      |\n","|    mean_reward     | -106     |\n","| time/              |          |\n","|    total_timesteps | 897600   |\n","| train/             |          |\n","|    actor_loss      | 3.51     |\n","|    critic_loss     | 4.41     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1497360  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 1930     |\n","|    fps             | 100      |\n","|    time_elapsed    | 8972     |\n","|    total_timesteps | 899472   |\n","| train/             |          |\n","|    actor_loss      | 4.19     |\n","|    critic_loss     | 12.6     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1499232  |\n","---------------------------------\n","Eval num_timesteps=900000, episode_reward=-5.53 +/- 0.19\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -5.53    |\n","| time/              |          |\n","|    total_timesteps | 900000   |\n","| train/             |          |\n","|    actor_loss      | 4.1      |\n","|    critic_loss     | 18       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1499760  |\n","---------------------------------\n","Eval num_timesteps=902400, episode_reward=-7.51 +/- 0.91\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -7.51    |\n","| time/              |          |\n","|    total_timesteps | 902400   |\n","| train/             |          |\n","|    actor_loss      | 4.13     |\n","|    critic_loss     | 3.86     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1502160  |\n","---------------------------------\n","Eval num_timesteps=904800, episode_reward=-11.02 +/- 1.66\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -11      |\n","| time/              |          |\n","|    total_timesteps | 904800   |\n","| train/             |          |\n","|    actor_loss      | 4.06     |\n","|    critic_loss     | 7.28     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1504560  |\n","---------------------------------\n","Eval num_timesteps=907200, episode_reward=-2.93 +/- 0.46\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -2.93    |\n","| time/              |          |\n","|    total_timesteps | 907200   |\n","| train/             |          |\n","|    actor_loss      | 4.14     |\n","|    critic_loss     | 15       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1506960  |\n","---------------------------------\n","Eval num_timesteps=909600, episode_reward=-10.61 +/- 1.28\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -10.6    |\n","| time/              |          |\n","|    total_timesteps | 909600   |\n","| train/             |          |\n","|    actor_loss      | 3.68     |\n","|    critic_loss     | 14.1     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1509360  |\n","---------------------------------\n","Eval num_timesteps=912000, episode_reward=-2.48 +/- 1.07\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -2.48    |\n","| time/              |          |\n","|    total_timesteps | 912000   |\n","| train/             |          |\n","|    actor_loss      | 4.39     |\n","|    critic_loss     | 3.57     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1511760  |\n","---------------------------------\n","Eval num_timesteps=914400, episode_reward=-104.29 +/- 1.19\n","Episode length: 366.20 +/- 8.33\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 366      |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 914400   |\n","| train/             |          |\n","|    actor_loss      | 3.74     |\n","|    critic_loss     | 7.46     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1514160  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 1940     |\n","|    fps             | 100      |\n","|    time_elapsed    | 9133     |\n","|    total_timesteps | 914880   |\n","| train/             |          |\n","|    actor_loss      | 4.07     |\n","|    critic_loss     | 10.7     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1514640  |\n","---------------------------------\n","Eval num_timesteps=916800, episode_reward=-77.53 +/- 47.10\n","Episode length: 335.60 +/- 134.23\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 336      |\n","|    mean_reward     | -77.5    |\n","| time/              |          |\n","|    total_timesteps | 916800   |\n","| train/             |          |\n","|    actor_loss      | 3.47     |\n","|    critic_loss     | 4.92     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1516560  |\n","---------------------------------\n","Eval num_timesteps=919200, episode_reward=-10.67 +/- 3.56\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -10.7    |\n","| time/              |          |\n","|    total_timesteps | 919200   |\n","| train/             |          |\n","|    actor_loss      | 3.9      |\n","|    critic_loss     | 4.02     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1518960  |\n","---------------------------------\n","Eval num_timesteps=921600, episode_reward=-46.48 +/- 49.70\n","Episode length: 486.80 +/- 16.17\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 487      |\n","|    mean_reward     | -46.5    |\n","| time/              |          |\n","|    total_timesteps | 921600   |\n","| train/             |          |\n","|    actor_loss      | 4.47     |\n","|    critic_loss     | 11.9     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1521360  |\n","---------------------------------\n","Eval num_timesteps=924000, episode_reward=-69.62 +/- 57.06\n","Episode length: 485.60 +/- 11.76\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 486      |\n","|    mean_reward     | -69.6    |\n","| time/              |          |\n","|    total_timesteps | 924000   |\n","| train/             |          |\n","|    actor_loss      | 4.73     |\n","|    critic_loss     | 6.63     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1523760  |\n","---------------------------------\n","Eval num_timesteps=926400, episode_reward=-10.09 +/- 0.40\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -10.1    |\n","| time/              |          |\n","|    total_timesteps | 926400   |\n","| train/             |          |\n","|    actor_loss      | 5.07     |\n","|    critic_loss     | 6.97     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1526160  |\n","---------------------------------\n","Eval num_timesteps=928800, episode_reward=-16.95 +/- 1.51\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -17      |\n","| time/              |          |\n","|    total_timesteps | 928800   |\n","| train/             |          |\n","|    actor_loss      | 5.13     |\n","|    critic_loss     | 8.62     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1528560  |\n","---------------------------------\n","Eval num_timesteps=931200, episode_reward=-14.72 +/- 0.82\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -14.7    |\n","| time/              |          |\n","|    total_timesteps | 931200   |\n","| train/             |          |\n","|    actor_loss      | 4.86     |\n","|    critic_loss     | 8.94     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1530960  |\n","---------------------------------\n","Eval num_timesteps=933600, episode_reward=-63.37 +/- 49.87\n","Episode length: 486.40 +/- 16.66\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 486      |\n","|    mean_reward     | -63.4    |\n","| time/              |          |\n","|    total_timesteps | 933600   |\n","| train/             |          |\n","|    actor_loss      | 5.67     |\n","|    critic_loss     | 8.39     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1533360  |\n","---------------------------------\n","Eval num_timesteps=936000, episode_reward=-2.78 +/- 0.11\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -2.78    |\n","| time/              |          |\n","|    total_timesteps | 936000   |\n","| train/             |          |\n","|    actor_loss      | 5.3      |\n","|    critic_loss     | 6.72     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1535760  |\n","---------------------------------\n","Eval num_timesteps=938400, episode_reward=-7.95 +/- 0.81\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -7.95    |\n","| time/              |          |\n","|    total_timesteps | 938400   |\n","| train/             |          |\n","|    actor_loss      | 5.79     |\n","|    critic_loss     | 10.6     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1538160  |\n","---------------------------------\n","Eval num_timesteps=940800, episode_reward=-10.73 +/- 0.11\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -10.7    |\n","| time/              |          |\n","|    total_timesteps | 940800   |\n","| train/             |          |\n","|    actor_loss      | 5.84     |\n","|    critic_loss     | 18.9     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1540560  |\n","---------------------------------\n","Eval num_timesteps=943200, episode_reward=-13.35 +/- 3.01\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -13.4    |\n","| time/              |          |\n","|    total_timesteps | 943200   |\n","| train/             |          |\n","|    actor_loss      | 5.24     |\n","|    critic_loss     | 23.9     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1542960  |\n","---------------------------------\n","Eval num_timesteps=945600, episode_reward=-18.60 +/- 0.03\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -18.6    |\n","| time/              |          |\n","|    total_timesteps | 945600   |\n","| train/             |          |\n","|    actor_loss      | -1.34    |\n","|    critic_loss     | 127      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1545360  |\n","---------------------------------\n","Eval num_timesteps=948000, episode_reward=-7.57 +/- 2.11\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -7.57    |\n","| time/              |          |\n","|    total_timesteps | 948000   |\n","| train/             |          |\n","|    actor_loss      | -2.97    |\n","|    critic_loss     | 44       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1547760  |\n","---------------------------------\n","Eval num_timesteps=950400, episode_reward=-7.38 +/- 3.21\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -7.38    |\n","| time/              |          |\n","|    total_timesteps | 950400   |\n","| train/             |          |\n","|    actor_loss      | -4.8     |\n","|    critic_loss     | 49       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1550160  |\n","---------------------------------\n","Eval num_timesteps=952800, episode_reward=-7.71 +/- 0.57\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -7.71    |\n","| time/              |          |\n","|    total_timesteps | 952800   |\n","| train/             |          |\n","|    actor_loss      | -5.51    |\n","|    critic_loss     | 76       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1552560  |\n","---------------------------------\n","Eval num_timesteps=955200, episode_reward=-0.87 +/- 1.28\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.873   |\n","| time/              |          |\n","|    total_timesteps | 955200   |\n","| train/             |          |\n","|    actor_loss      | -1.01    |\n","|    critic_loss     | 108      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1554960  |\n","---------------------------------\n","Eval num_timesteps=957600, episode_reward=-44.25 +/- 48.62\n","Episode length: 453.20 +/- 57.32\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 453      |\n","|    mean_reward     | -44.3    |\n","| time/              |          |\n","|    total_timesteps | 957600   |\n","| train/             |          |\n","|    actor_loss      | 0.0491   |\n","|    critic_loss     | 12.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1557360  |\n","---------------------------------\n","Eval num_timesteps=960000, episode_reward=-11.58 +/- 1.08\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -11.6    |\n","| time/              |          |\n","|    total_timesteps | 960000   |\n","| train/             |          |\n","|    actor_loss      | -0.488   |\n","|    critic_loss     | 17.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1559760  |\n","---------------------------------\n","Eval num_timesteps=962400, episode_reward=3.84 +/- 2.39\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.84     |\n","| time/              |          |\n","|    total_timesteps | 962400   |\n","| train/             |          |\n","|    actor_loss      | -3.51    |\n","|    critic_loss     | 67.6     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1562160  |\n","---------------------------------\n","Eval num_timesteps=964800, episode_reward=-102.16 +/- 1.38\n","Episode length: 80.00 +/- 4.90\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 80       |\n","|    mean_reward     | -102     |\n","| time/              |          |\n","|    total_timesteps | 964800   |\n","| train/             |          |\n","|    actor_loss      | -2.75    |\n","|    critic_loss     | 41.5     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1564560  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 1950     |\n","|    fps             | 100      |\n","|    time_elapsed    | 9642     |\n","|    total_timesteps | 964848   |\n","| train/             |          |\n","|    actor_loss      | -3       |\n","|    critic_loss     | 98.1     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1564608  |\n","---------------------------------\n","Eval num_timesteps=967200, episode_reward=-10.78 +/- 3.41\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -10.8    |\n","| time/              |          |\n","|    total_timesteps | 967200   |\n","| train/             |          |\n","|    actor_loss      | -1.48    |\n","|    critic_loss     | 29.3     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1566960  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 1960     |\n","|    fps             | 100      |\n","|    time_elapsed    | 9680     |\n","|    total_timesteps | 968904   |\n","| train/             |          |\n","|    actor_loss      | -2.82    |\n","|    critic_loss     | 56.7     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1568664  |\n","---------------------------------\n","Eval num_timesteps=969600, episode_reward=-106.09 +/- 1.00\n","Episode length: 63.80 +/- 3.43\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 63.8     |\n","|    mean_reward     | -106     |\n","| time/              |          |\n","|    total_timesteps | 969600   |\n","| train/             |          |\n","|    actor_loss      | -2.95    |\n","|    critic_loss     | 33.8     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1569360  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 1970     |\n","|    fps             | 100      |\n","|    time_elapsed    | 9696     |\n","|    total_timesteps | 970848   |\n","| train/             |          |\n","|    actor_loss      | -2.47    |\n","|    critic_loss     | 25.5     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1570608  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 1980     |\n","|    fps             | 100      |\n","|    time_elapsed    | 9699     |\n","|    total_timesteps | 971256   |\n","| train/             |          |\n","|    actor_loss      | -3.09    |\n","|    critic_loss     | 36       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1571016  |\n","---------------------------------\n","Eval num_timesteps=972000, episode_reward=-40.27 +/- 52.65\n","Episode length: 478.40 +/- 26.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 478      |\n","|    mean_reward     | -40.3    |\n","| time/              |          |\n","|    total_timesteps | 972000   |\n","| train/             |          |\n","|    actor_loss      | -4.23    |\n","|    critic_loss     | 20.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1571760  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 1990     |\n","|    fps             | 100      |\n","|    time_elapsed    | 9727     |\n","|    total_timesteps | 974016   |\n","| train/             |          |\n","|    actor_loss      | -7.3     |\n","|    critic_loss     | 27.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1573776  |\n","---------------------------------\n","Eval num_timesteps=974400, episode_reward=-0.12 +/- 0.87\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.115   |\n","| time/              |          |\n","|    total_timesteps | 974400   |\n","| train/             |          |\n","|    actor_loss      | -6.8     |\n","|    critic_loss     | 28.6     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1574160  |\n","---------------------------------\n","Eval num_timesteps=976800, episode_reward=3.10 +/- 1.31\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.1      |\n","| time/              |          |\n","|    total_timesteps | 976800   |\n","| train/             |          |\n","|    actor_loss      | -9.66    |\n","|    critic_loss     | 38.8     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1576560  |\n","---------------------------------\n","Eval num_timesteps=979200, episode_reward=-60.08 +/- 54.46\n","Episode length: 260.60 +/- 195.47\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 261      |\n","|    mean_reward     | -60.1    |\n","| time/              |          |\n","|    total_timesteps | 979200   |\n","| train/             |          |\n","|    actor_loss      | -9.62    |\n","|    critic_loss     | 53.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1578960  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2000     |\n","|    fps             | 100      |\n","|    time_elapsed    | 9800     |\n","|    total_timesteps | 981336   |\n","| train/             |          |\n","|    actor_loss      | -10.7    |\n","|    critic_loss     | 43.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1581096  |\n","---------------------------------\n","Eval num_timesteps=981600, episode_reward=5.41 +/- 0.89\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.41     |\n","| time/              |          |\n","|    total_timesteps | 981600   |\n","| train/             |          |\n","|    actor_loss      | -10.7    |\n","|    critic_loss     | 35.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1581360  |\n","---------------------------------\n","Eval num_timesteps=984000, episode_reward=2.54 +/- 0.59\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.54     |\n","| time/              |          |\n","|    total_timesteps | 984000   |\n","| train/             |          |\n","|    actor_loss      | -9.43    |\n","|    critic_loss     | 19.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1583760  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2010     |\n","|    fps             | 100      |\n","|    time_elapsed    | 9853     |\n","|    total_timesteps | 986328   |\n","| train/             |          |\n","|    actor_loss      | -9.38    |\n","|    critic_loss     | 18.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1586088  |\n","---------------------------------\n","Eval num_timesteps=986400, episode_reward=-1.31 +/- 0.22\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.31    |\n","| time/              |          |\n","|    total_timesteps | 986400   |\n","| train/             |          |\n","|    actor_loss      | -8.93    |\n","|    critic_loss     | 48.1     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1586160  |\n","---------------------------------\n","Eval num_timesteps=988800, episode_reward=-1.60 +/- 0.20\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.6     |\n","| time/              |          |\n","|    total_timesteps | 988800   |\n","| train/             |          |\n","|    actor_loss      | -7.38    |\n","|    critic_loss     | 21       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1588560  |\n","---------------------------------\n","Eval num_timesteps=991200, episode_reward=-3.06 +/- 0.60\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -3.06    |\n","| time/              |          |\n","|    total_timesteps | 991200   |\n","| train/             |          |\n","|    actor_loss      | -4.67    |\n","|    critic_loss     | 26.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1590960  |\n","---------------------------------\n","Eval num_timesteps=993600, episode_reward=-114.85 +/- 4.07\n","Episode length: 109.00 +/- 29.39\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 109      |\n","|    mean_reward     | -115     |\n","| time/              |          |\n","|    total_timesteps | 993600   |\n","| train/             |          |\n","|    actor_loss      | -3.36    |\n","|    critic_loss     | 18.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1593360  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2020     |\n","|    fps             | 100      |\n","|    time_elapsed    | 9943     |\n","|    total_timesteps | 995736   |\n","| train/             |          |\n","|    actor_loss      | -1.25    |\n","|    critic_loss     | 15.7     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1595496  |\n","---------------------------------\n","Eval num_timesteps=996000, episode_reward=0.86 +/- 2.56\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.859    |\n","| time/              |          |\n","|    total_timesteps | 996000   |\n","| train/             |          |\n","|    actor_loss      | -1.86    |\n","|    critic_loss     | 14.6     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1595760  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2030     |\n","|    fps             | 100      |\n","|    time_elapsed    | 9968     |\n","|    total_timesteps | 998112   |\n","| train/             |          |\n","|    actor_loss      | -0.73    |\n","|    critic_loss     | 13.6     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1597872  |\n","---------------------------------\n","Eval num_timesteps=998400, episode_reward=-2.28 +/- 1.14\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -2.28    |\n","| time/              |          |\n","|    total_timesteps | 998400   |\n","| train/             |          |\n","|    actor_loss      | -0.499   |\n","|    critic_loss     | 10.9     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1598160  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2040     |\n","|    fps             | 100      |\n","|    time_elapsed    | 9989     |\n","|    total_timesteps | 1000080  |\n","| train/             |          |\n","|    actor_loss      | 1.92     |\n","|    critic_loss     | 25       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1599840  |\n","---------------------------------\n","Eval num_timesteps=1000800, episode_reward=-106.33 +/- 2.87\n","Episode length: 56.40 +/- 10.29\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 56.4     |\n","|    mean_reward     | -106     |\n","| time/              |          |\n","|    total_timesteps | 1000800  |\n","| train/             |          |\n","|    actor_loss      | 1.32     |\n","|    critic_loss     | 14.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1600560  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2050     |\n","|    fps             | 100      |\n","|    time_elapsed    | 9998     |\n","|    total_timesteps | 1001064  |\n","| train/             |          |\n","|    actor_loss      | 0.998    |\n","|    critic_loss     | 6.61     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1600824  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2060     |\n","|    fps             | 100      |\n","|    time_elapsed    | 10007    |\n","|    total_timesteps | 1002408  |\n","| train/             |          |\n","|    actor_loss      | 1.64     |\n","|    critic_loss     | 8.33     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1602168  |\n","---------------------------------\n","Eval num_timesteps=1003200, episode_reward=-9.79 +/- 3.12\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -9.79    |\n","| time/              |          |\n","|    total_timesteps | 1003200  |\n","| train/             |          |\n","|    actor_loss      | 1.79     |\n","|    critic_loss     | 4.78     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1602960  |\n","---------------------------------\n","Eval num_timesteps=1005600, episode_reward=8.38 +/- 6.31\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 8.38     |\n","| time/              |          |\n","|    total_timesteps | 1005600  |\n","| train/             |          |\n","|    actor_loss      | 2.77     |\n","|    critic_loss     | 7.96     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1605360  |\n","---------------------------------\n","Eval num_timesteps=1008000, episode_reward=-41.26 +/- 49.44\n","Episode length: 395.20 +/- 128.35\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 395      |\n","|    mean_reward     | -41.3    |\n","| time/              |          |\n","|    total_timesteps | 1008000  |\n","| train/             |          |\n","|    actor_loss      | 3.24     |\n","|    critic_loss     | 8.99     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1607760  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2070     |\n","|    fps             | 100      |\n","|    time_elapsed    | 10085    |\n","|    total_timesteps | 1010328  |\n","| train/             |          |\n","|    actor_loss      | 3.69     |\n","|    critic_loss     | 4.31     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1610088  |\n","---------------------------------\n","Eval num_timesteps=1010400, episode_reward=-60.06 +/- 50.69\n","Episode length: 342.20 +/- 128.84\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 342      |\n","|    mean_reward     | -60.1    |\n","| time/              |          |\n","|    total_timesteps | 1010400  |\n","| train/             |          |\n","|    actor_loss      | 4.33     |\n","|    critic_loss     | 11       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1610160  |\n","---------------------------------\n","Eval num_timesteps=1012800, episode_reward=2.23 +/- 3.00\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.23     |\n","| time/              |          |\n","|    total_timesteps | 1012800  |\n","| train/             |          |\n","|    actor_loss      | 4.51     |\n","|    critic_loss     | 17.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1612560  |\n","---------------------------------\n","Eval num_timesteps=1015200, episode_reward=-5.73 +/- 1.81\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -5.73    |\n","| time/              |          |\n","|    total_timesteps | 1015200  |\n","| train/             |          |\n","|    actor_loss      | 5.02     |\n","|    critic_loss     | 12.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1614960  |\n","---------------------------------\n","Eval num_timesteps=1017600, episode_reward=-5.61 +/- 2.61\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -5.61    |\n","| time/              |          |\n","|    total_timesteps | 1017600  |\n","| train/             |          |\n","|    actor_loss      | 5.31     |\n","|    critic_loss     | 22.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1617360  |\n","---------------------------------\n","Eval num_timesteps=1020000, episode_reward=-8.06 +/- 0.10\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -8.06    |\n","| time/              |          |\n","|    total_timesteps | 1020000  |\n","| train/             |          |\n","|    actor_loss      | 5.22     |\n","|    critic_loss     | 9.3      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1619760  |\n","---------------------------------\n","Eval num_timesteps=1022400, episode_reward=-8.71 +/- 2.63\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -8.71    |\n","| time/              |          |\n","|    total_timesteps | 1022400  |\n","| train/             |          |\n","|    actor_loss      | 5.12     |\n","|    critic_loss     | 9.69     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1622160  |\n","---------------------------------\n","Eval num_timesteps=1024800, episode_reward=-46.06 +/- 50.17\n","Episode length: 436.80 +/- 77.40\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 437      |\n","|    mean_reward     | -46.1    |\n","| time/              |          |\n","|    total_timesteps | 1024800  |\n","| train/             |          |\n","|    actor_loss      | 5.38     |\n","|    critic_loss     | 6.86     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1624560  |\n","---------------------------------\n","Eval num_timesteps=1027200, episode_reward=-68.98 +/- 57.34\n","Episode length: 437.60 +/- 50.95\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 438      |\n","|    mean_reward     | -69      |\n","| time/              |          |\n","|    total_timesteps | 1027200  |\n","| train/             |          |\n","|    actor_loss      | 5.39     |\n","|    critic_loss     | 3.42     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1626960  |\n","---------------------------------\n","Eval num_timesteps=1029600, episode_reward=-45.04 +/- 53.52\n","Episode length: 453.20 +/- 57.32\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 453      |\n","|    mean_reward     | -45      |\n","| time/              |          |\n","|    total_timesteps | 1029600  |\n","| train/             |          |\n","|    actor_loss      | 5.37     |\n","|    critic_loss     | 9.72     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1629360  |\n","---------------------------------\n","Eval num_timesteps=1032000, episode_reward=-37.16 +/- 50.42\n","Episode length: 456.00 +/- 53.89\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 456      |\n","|    mean_reward     | -37.2    |\n","| time/              |          |\n","|    total_timesteps | 1032000  |\n","| train/             |          |\n","|    actor_loss      | 4.97     |\n","|    critic_loss     | 3.75     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1631760  |\n","---------------------------------\n","Eval num_timesteps=1034400, episode_reward=-59.46 +/- 47.73\n","Episode length: 326.60 +/- 141.58\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 327      |\n","|    mean_reward     | -59.5    |\n","| time/              |          |\n","|    total_timesteps | 1034400  |\n","| train/             |          |\n","|    actor_loss      | 5.56     |\n","|    critic_loss     | 8.68     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1634160  |\n","---------------------------------\n","Eval num_timesteps=1036800, episode_reward=4.27 +/- 1.21\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.27     |\n","| time/              |          |\n","|    total_timesteps | 1036800  |\n","| train/             |          |\n","|    actor_loss      | 5.53     |\n","|    critic_loss     | 9.21     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1636560  |\n","---------------------------------\n","Eval num_timesteps=1039200, episode_reward=10.41 +/- 5.00\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 10.4     |\n","| time/              |          |\n","|    total_timesteps | 1039200  |\n","| train/             |          |\n","|    actor_loss      | 5.19     |\n","|    critic_loss     | 12.7     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1638960  |\n","---------------------------------\n","Eval num_timesteps=1041600, episode_reward=-4.60 +/- 6.16\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -4.6     |\n","| time/              |          |\n","|    total_timesteps | 1041600  |\n","| train/             |          |\n","|    actor_loss      | 4.94     |\n","|    critic_loss     | 7.26     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1641360  |\n","---------------------------------\n","Eval num_timesteps=1044000, episode_reward=-72.18 +/- 50.73\n","Episode length: 438.80 +/- 49.97\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 439      |\n","|    mean_reward     | -72.2    |\n","| time/              |          |\n","|    total_timesteps | 1044000  |\n","| train/             |          |\n","|    actor_loss      | 5.07     |\n","|    critic_loss     | 8.1      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1643760  |\n","---------------------------------\n","Eval num_timesteps=1046400, episode_reward=-45.37 +/- 53.50\n","Episode length: 474.40 +/- 31.35\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 474      |\n","|    mean_reward     | -45.4    |\n","| time/              |          |\n","|    total_timesteps | 1046400  |\n","| train/             |          |\n","|    actor_loss      | 5.74     |\n","|    critic_loss     | 7.6      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1646160  |\n","---------------------------------\n","Eval num_timesteps=1048800, episode_reward=-43.56 +/- 54.65\n","Episode length: 472.80 +/- 33.31\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 473      |\n","|    mean_reward     | -43.6    |\n","| time/              |          |\n","|    total_timesteps | 1048800  |\n","| train/             |          |\n","|    actor_loss      | 5.65     |\n","|    critic_loss     | 19.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1648560  |\n","---------------------------------\n","Eval num_timesteps=1051200, episode_reward=-48.11 +/- 47.24\n","Episode length: 435.20 +/- 79.36\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 435      |\n","|    mean_reward     | -48.1    |\n","| time/              |          |\n","|    total_timesteps | 1051200  |\n","| train/             |          |\n","|    actor_loss      | 4.9      |\n","|    critic_loss     | 3.37     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1650960  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2080     |\n","|    fps             | 100      |\n","|    time_elapsed    | 10518    |\n","|    total_timesteps | 1052544  |\n","| train/             |          |\n","|    actor_loss      | 5.38     |\n","|    critic_loss     | 7.66     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1652304  |\n","---------------------------------\n","Eval num_timesteps=1053600, episode_reward=2.94 +/- 2.75\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.94     |\n","| time/              |          |\n","|    total_timesteps | 1053600  |\n","| train/             |          |\n","|    actor_loss      | 4.94     |\n","|    critic_loss     | 17.5     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1653360  |\n","---------------------------------\n","Eval num_timesteps=1056000, episode_reward=-104.78 +/- 0.68\n","Episode length: 151.60 +/- 41.15\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 152      |\n","|    mean_reward     | -105     |\n","| time/              |          |\n","|    total_timesteps | 1056000  |\n","| train/             |          |\n","|    actor_loss      | 4.66     |\n","|    critic_loss     | 6.27     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1655760  |\n","---------------------------------\n","Eval num_timesteps=1058400, episode_reward=-42.28 +/- 50.73\n","Episode length: 449.60 +/- 61.73\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 450      |\n","|    mean_reward     | -42.3    |\n","| time/              |          |\n","|    total_timesteps | 1058400  |\n","| train/             |          |\n","|    actor_loss      | 5.07     |\n","|    critic_loss     | 12.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1658160  |\n","---------------------------------\n","Eval num_timesteps=1060800, episode_reward=-2.11 +/- 1.30\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -2.11    |\n","| time/              |          |\n","|    total_timesteps | 1060800  |\n","| train/             |          |\n","|    actor_loss      | 5.1      |\n","|    critic_loss     | 7.96     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1660560  |\n","---------------------------------\n","Eval num_timesteps=1063200, episode_reward=4.19 +/- 1.80\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.19     |\n","| time/              |          |\n","|    total_timesteps | 1063200  |\n","| train/             |          |\n","|    actor_loss      | 5.69     |\n","|    critic_loss     | 8.96     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1662960  |\n","---------------------------------\n","Eval num_timesteps=1065600, episode_reward=1.08 +/- 0.48\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.08     |\n","| time/              |          |\n","|    total_timesteps | 1065600  |\n","| train/             |          |\n","|    actor_loss      | 5.36     |\n","|    critic_loss     | 9.32     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1665360  |\n","---------------------------------\n","Eval num_timesteps=1068000, episode_reward=8.38 +/- 1.67\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 8.38     |\n","| time/              |          |\n","|    total_timesteps | 1068000  |\n","| train/             |          |\n","|    actor_loss      | 5.38     |\n","|    critic_loss     | 15.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1667760  |\n","---------------------------------\n","Eval num_timesteps=1070400, episode_reward=2.12 +/- 4.57\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.12     |\n","| time/              |          |\n","|    total_timesteps | 1070400  |\n","| train/             |          |\n","|    actor_loss      | 5.16     |\n","|    critic_loss     | 11.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1670160  |\n","---------------------------------\n","Eval num_timesteps=1072800, episode_reward=7.37 +/- 0.38\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.37     |\n","| time/              |          |\n","|    total_timesteps | 1072800  |\n","| train/             |          |\n","|    actor_loss      | 5.29     |\n","|    critic_loss     | 5.63     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1672560  |\n","---------------------------------\n","Eval num_timesteps=1075200, episode_reward=2.96 +/- 0.18\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.96     |\n","| time/              |          |\n","|    total_timesteps | 1075200  |\n","| train/             |          |\n","|    actor_loss      | 5.29     |\n","|    critic_loss     | 4.12     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1674960  |\n","---------------------------------\n","Eval num_timesteps=1077600, episode_reward=2.36 +/- 1.03\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.36     |\n","| time/              |          |\n","|    total_timesteps | 1077600  |\n","| train/             |          |\n","|    actor_loss      | 5.31     |\n","|    critic_loss     | 8.53     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1677360  |\n","---------------------------------\n","Eval num_timesteps=1080000, episode_reward=-64.73 +/- 50.70\n","Episode length: 474.80 +/- 20.58\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 475      |\n","|    mean_reward     | -64.7    |\n","| time/              |          |\n","|    total_timesteps | 1080000  |\n","| train/             |          |\n","|    actor_loss      | 4.8      |\n","|    critic_loss     | 6.04     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1679760  |\n","---------------------------------\n","Eval num_timesteps=1082400, episode_reward=-66.38 +/- 53.09\n","Episode length: 400.40 +/- 81.32\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 400      |\n","|    mean_reward     | -66.4    |\n","| time/              |          |\n","|    total_timesteps | 1082400  |\n","| train/             |          |\n","|    actor_loss      | 5.57     |\n","|    critic_loss     | 17       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1682160  |\n","---------------------------------\n","Eval num_timesteps=1084800, episode_reward=-71.52 +/- 49.84\n","Episode length: 441.20 +/- 48.01\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 441      |\n","|    mean_reward     | -71.5    |\n","| time/              |          |\n","|    total_timesteps | 1084800  |\n","| train/             |          |\n","|    actor_loss      | 5.45     |\n","|    critic_loss     | 5.67     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1684560  |\n","---------------------------------\n","Eval num_timesteps=1087200, episode_reward=-52.53 +/- 50.60\n","Episode length: 483.60 +/- 20.09\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 484      |\n","|    mean_reward     | -52.5    |\n","| time/              |          |\n","|    total_timesteps | 1087200  |\n","| train/             |          |\n","|    actor_loss      | 5.74     |\n","|    critic_loss     | 9.49     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1686960  |\n","---------------------------------\n","Eval num_timesteps=1089600, episode_reward=-43.27 +/- 49.95\n","Episode length: 427.60 +/- 88.67\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 428      |\n","|    mean_reward     | -43.3    |\n","| time/              |          |\n","|    total_timesteps | 1089600  |\n","| train/             |          |\n","|    actor_loss      | 5.52     |\n","|    critic_loss     | 10.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1689360  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2090     |\n","|    fps             | 100      |\n","|    time_elapsed    | 10914    |\n","|    total_timesteps | 1091736  |\n","| train/             |          |\n","|    actor_loss      | 5.37     |\n","|    critic_loss     | 5.86     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1691496  |\n","---------------------------------\n","Eval num_timesteps=1092000, episode_reward=-8.57 +/- 0.06\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -8.57    |\n","| time/              |          |\n","|    total_timesteps | 1092000  |\n","| train/             |          |\n","|    actor_loss      | 5.8      |\n","|    critic_loss     | 6.99     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1691760  |\n","---------------------------------\n","Eval num_timesteps=1094400, episode_reward=-101.30 +/- 3.01\n","Episode length: 369.60 +/- 72.99\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 370      |\n","|    mean_reward     | -101     |\n","| time/              |          |\n","|    total_timesteps | 1094400  |\n","| train/             |          |\n","|    actor_loss      | 5.65     |\n","|    critic_loss     | 3.53     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1694160  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2100     |\n","|    fps             | 100      |\n","|    time_elapsed    | 10965    |\n","|    total_timesteps | 1096680  |\n","| train/             |          |\n","|    actor_loss      | 5.54     |\n","|    critic_loss     | 18.5     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1696440  |\n","---------------------------------\n","Eval num_timesteps=1096800, episode_reward=-111.27 +/- 0.70\n","Episode length: 363.20 +/- 23.52\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 363      |\n","|    mean_reward     | -111     |\n","| time/              |          |\n","|    total_timesteps | 1096800  |\n","| train/             |          |\n","|    actor_loss      | 5.43     |\n","|    critic_loss     | 12.3     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1696560  |\n","---------------------------------\n","Eval num_timesteps=1099200, episode_reward=-107.84 +/- 1.00\n","Episode length: 291.40 +/- 44.58\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 291      |\n","|    mean_reward     | -108     |\n","| time/              |          |\n","|    total_timesteps | 1099200  |\n","| train/             |          |\n","|    actor_loss      | 5.16     |\n","|    critic_loss     | 4.32     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1698960  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2110     |\n","|    fps             | 99       |\n","|    time_elapsed    | 11006    |\n","|    total_timesteps | 1100424  |\n","| train/             |          |\n","|    actor_loss      | 6.28     |\n","|    critic_loss     | 22.7     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1700184  |\n","---------------------------------\n","Eval num_timesteps=1101600, episode_reward=-105.54 +/- 2.81\n","Episode length: 313.40 +/- 27.43\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 313      |\n","|    mean_reward     | -106     |\n","| time/              |          |\n","|    total_timesteps | 1101600  |\n","| train/             |          |\n","|    actor_loss      | 6.3      |\n","|    critic_loss     | 6.02     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1701360  |\n","---------------------------------\n","Eval num_timesteps=1104000, episode_reward=-57.24 +/- 51.97\n","Episode length: 437.60 +/- 50.95\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 438      |\n","|    mean_reward     | -57.2    |\n","| time/              |          |\n","|    total_timesteps | 1104000  |\n","| train/             |          |\n","|    actor_loss      | 6.78     |\n","|    critic_loss     | 3.67     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1703760  |\n","---------------------------------\n","Eval num_timesteps=1106400, episode_reward=-1.19 +/- 0.51\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.19    |\n","| time/              |          |\n","|    total_timesteps | 1106400  |\n","| train/             |          |\n","|    actor_loss      | 6.28     |\n","|    critic_loss     | 3.12     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1706160  |\n","---------------------------------\n","Eval num_timesteps=1108800, episode_reward=-9.30 +/- 1.91\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -9.3     |\n","| time/              |          |\n","|    total_timesteps | 1108800  |\n","| train/             |          |\n","|    actor_loss      | 6.61     |\n","|    critic_loss     | 6.3      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1708560  |\n","---------------------------------\n","Eval num_timesteps=1111200, episode_reward=6.18 +/- 1.36\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.18     |\n","| time/              |          |\n","|    total_timesteps | 1111200  |\n","| train/             |          |\n","|    actor_loss      | 6.07     |\n","|    critic_loss     | 6.16     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1710960  |\n","---------------------------------\n","Eval num_timesteps=1113600, episode_reward=4.42 +/- 0.29\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.42     |\n","| time/              |          |\n","|    total_timesteps | 1113600  |\n","| train/             |          |\n","|    actor_loss      | 6.24     |\n","|    critic_loss     | 21.5     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1713360  |\n","---------------------------------\n","Eval num_timesteps=1116000, episode_reward=0.21 +/- 0.56\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.21     |\n","| time/              |          |\n","|    total_timesteps | 1116000  |\n","| train/             |          |\n","|    actor_loss      | 5.61     |\n","|    critic_loss     | 10.3     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1715760  |\n","---------------------------------\n","Eval num_timesteps=1118400, episode_reward=4.36 +/- 2.47\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.36     |\n","| time/              |          |\n","|    total_timesteps | 1118400  |\n","| train/             |          |\n","|    actor_loss      | 5.93     |\n","|    critic_loss     | 5.68     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1718160  |\n","---------------------------------\n","Eval num_timesteps=1120800, episode_reward=0.37 +/- 2.45\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.375    |\n","| time/              |          |\n","|    total_timesteps | 1120800  |\n","| train/             |          |\n","|    actor_loss      | 5.64     |\n","|    critic_loss     | 3.65     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1720560  |\n","---------------------------------\n","Eval num_timesteps=1123200, episode_reward=-61.48 +/- 47.56\n","Episode length: 455.00 +/- 36.74\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 455      |\n","|    mean_reward     | -61.5    |\n","| time/              |          |\n","|    total_timesteps | 1123200  |\n","| train/             |          |\n","|    actor_loss      | 5.69     |\n","|    critic_loss     | 7.6      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1722960  |\n","---------------------------------\n","Eval num_timesteps=1125600, episode_reward=-105.28 +/- 0.80\n","Episode length: 382.40 +/- 76.42\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 382      |\n","|    mean_reward     | -105     |\n","| time/              |          |\n","|    total_timesteps | 1125600  |\n","| train/             |          |\n","|    actor_loss      | 5.5      |\n","|    critic_loss     | 4.24     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1725360  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2120     |\n","|    fps             | 99       |\n","|    time_elapsed    | 11275    |\n","|    total_timesteps | 1126368  |\n","| train/             |          |\n","|    actor_loss      | 5.7      |\n","|    critic_loss     | 3.8      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1726128  |\n","---------------------------------\n","Eval num_timesteps=1128000, episode_reward=-105.05 +/- 1.56\n","Episode length: 388.20 +/- 43.11\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 388      |\n","|    mean_reward     | -105     |\n","| time/              |          |\n","|    total_timesteps | 1128000  |\n","| train/             |          |\n","|    actor_loss      | 5.44     |\n","|    critic_loss     | 12.7     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1727760  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2130     |\n","|    fps             | 99       |\n","|    time_elapsed    | 11306    |\n","|    total_timesteps | 1129416  |\n","| train/             |          |\n","|    actor_loss      | 5.63     |\n","|    critic_loss     | 4.05     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1729176  |\n","---------------------------------\n","Eval num_timesteps=1130400, episode_reward=-109.08 +/- 0.69\n","Episode length: 293.80 +/- 50.46\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 294      |\n","|    mean_reward     | -109     |\n","| time/              |          |\n","|    total_timesteps | 1130400  |\n","| train/             |          |\n","|    actor_loss      | 5.38     |\n","|    critic_loss     | 3.36     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1730160  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2140     |\n","|    fps             | 99       |\n","|    time_elapsed    | 11328    |\n","|    total_timesteps | 1131888  |\n","| train/             |          |\n","|    actor_loss      | 5.55     |\n","|    critic_loss     | 10.3     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1731648  |\n","---------------------------------\n","Eval num_timesteps=1132800, episode_reward=-0.12 +/- 2.77\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.117   |\n","| time/              |          |\n","|    total_timesteps | 1132800  |\n","| train/             |          |\n","|    actor_loss      | 4.9      |\n","|    critic_loss     | 2.59     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1732560  |\n","---------------------------------\n","Eval num_timesteps=1135200, episode_reward=2.07 +/- 1.91\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.07     |\n","| time/              |          |\n","|    total_timesteps | 1135200  |\n","| train/             |          |\n","|    actor_loss      | 5.47     |\n","|    critic_loss     | 7.81     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1734960  |\n","---------------------------------\n","Eval num_timesteps=1137600, episode_reward=-37.70 +/- 51.44\n","Episode length: 451.20 +/- 59.77\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 451      |\n","|    mean_reward     | -37.7    |\n","| time/              |          |\n","|    total_timesteps | 1137600  |\n","| train/             |          |\n","|    actor_loss      | 5.24     |\n","|    critic_loss     | 3.01     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1737360  |\n","---------------------------------\n","Eval num_timesteps=1140000, episode_reward=-59.65 +/- 52.14\n","Episode length: 430.40 +/- 56.83\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 430      |\n","|    mean_reward     | -59.7    |\n","| time/              |          |\n","|    total_timesteps | 1140000  |\n","| train/             |          |\n","|    actor_loss      | 4.61     |\n","|    critic_loss     | 5.16     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1739760  |\n","---------------------------------\n","Eval num_timesteps=1142400, episode_reward=14.29 +/- 1.62\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 14.3     |\n","| time/              |          |\n","|    total_timesteps | 1142400  |\n","| train/             |          |\n","|    actor_loss      | 5.11     |\n","|    critic_loss     | 4.83     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1742160  |\n","---------------------------------\n","New best mean reward!\n","Eval num_timesteps=1144800, episode_reward=-36.63 +/- 50.42\n","Episode length: 466.80 +/- 40.66\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 467      |\n","|    mean_reward     | -36.6    |\n","| time/              |          |\n","|    total_timesteps | 1144800  |\n","| train/             |          |\n","|    actor_loss      | 4.28     |\n","|    critic_loss     | 4.59     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1744560  |\n","---------------------------------\n","Eval num_timesteps=1147200, episode_reward=-62.51 +/- 52.35\n","Episode length: 344.00 +/- 127.37\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 344      |\n","|    mean_reward     | -62.5    |\n","| time/              |          |\n","|    total_timesteps | 1147200  |\n","| train/             |          |\n","|    actor_loss      | 3.61     |\n","|    critic_loss     | 8.95     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1746960  |\n","---------------------------------\n","Eval num_timesteps=1149600, episode_reward=4.52 +/- 5.74\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.52     |\n","| time/              |          |\n","|    total_timesteps | 1149600  |\n","| train/             |          |\n","|    actor_loss      | 3.78     |\n","|    critic_loss     | 3.13     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1749360  |\n","---------------------------------\n","Eval num_timesteps=1152000, episode_reward=2.81 +/- 1.86\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.81     |\n","| time/              |          |\n","|    total_timesteps | 1152000  |\n","| train/             |          |\n","|    actor_loss      | 3.81     |\n","|    critic_loss     | 6.7      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1751760  |\n","---------------------------------\n","Eval num_timesteps=1154400, episode_reward=9.27 +/- 4.00\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 9.27     |\n","| time/              |          |\n","|    total_timesteps | 1154400  |\n","| train/             |          |\n","|    actor_loss      | 2.91     |\n","|    critic_loss     | 5.34     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1754160  |\n","---------------------------------\n","Eval num_timesteps=1156800, episode_reward=8.91 +/- 3.17\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 8.91     |\n","| time/              |          |\n","|    total_timesteps | 1156800  |\n","| train/             |          |\n","|    actor_loss      | 2.89     |\n","|    critic_loss     | 5.87     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1756560  |\n","---------------------------------\n","Eval num_timesteps=1159200, episode_reward=-108.31 +/- 0.13\n","Episode length: 332.60 +/- 22.54\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 333      |\n","|    mean_reward     | -108     |\n","| time/              |          |\n","|    total_timesteps | 1159200  |\n","| train/             |          |\n","|    actor_loss      | 2.86     |\n","|    critic_loss     | 16.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1758960  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2150     |\n","|    fps             | 99       |\n","|    time_elapsed    | 11612    |\n","|    total_timesteps | 1159704  |\n","| train/             |          |\n","|    actor_loss      | 2.49     |\n","|    critic_loss     | 2.52     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1759464  |\n","---------------------------------\n","Eval num_timesteps=1161600, episode_reward=-109.08 +/- 1.93\n","Episode length: 239.40 +/- 24.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 239      |\n","|    mean_reward     | -109     |\n","| time/              |          |\n","|    total_timesteps | 1161600  |\n","| train/             |          |\n","|    actor_loss      | 3.05     |\n","|    critic_loss     | 11.6     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1761360  |\n","---------------------------------\n","Eval num_timesteps=1164000, episode_reward=13.05 +/- 1.51\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 13.1     |\n","| time/              |          |\n","|    total_timesteps | 1164000  |\n","| train/             |          |\n","|    actor_loss      | 2.77     |\n","|    critic_loss     | 7.41     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1763760  |\n","---------------------------------\n","Eval num_timesteps=1166400, episode_reward=10.40 +/- 2.10\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 10.4     |\n","| time/              |          |\n","|    total_timesteps | 1166400  |\n","| train/             |          |\n","|    actor_loss      | 3.17     |\n","|    critic_loss     | 11       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1766160  |\n","---------------------------------\n","Eval num_timesteps=1168800, episode_reward=-56.05 +/- 55.24\n","Episode length: 333.20 +/- 136.19\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 333      |\n","|    mean_reward     | -56.1    |\n","| time/              |          |\n","|    total_timesteps | 1168800  |\n","| train/             |          |\n","|    actor_loss      | 3.1      |\n","|    critic_loss     | 6.92     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1768560  |\n","---------------------------------\n","Eval num_timesteps=1171200, episode_reward=-3.72 +/- 1.77\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -3.72    |\n","| time/              |          |\n","|    total_timesteps | 1171200  |\n","| train/             |          |\n","|    actor_loss      | 2.96     |\n","|    critic_loss     | 10.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1770960  |\n","---------------------------------\n","Eval num_timesteps=1173600, episode_reward=-12.66 +/- 0.47\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -12.7    |\n","| time/              |          |\n","|    total_timesteps | 1173600  |\n","| train/             |          |\n","|    actor_loss      | 2.98     |\n","|    critic_loss     | 14.6     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1773360  |\n","---------------------------------\n","Eval num_timesteps=1176000, episode_reward=-9.23 +/- 2.10\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -9.23    |\n","| time/              |          |\n","|    total_timesteps | 1176000  |\n","| train/             |          |\n","|    actor_loss      | 2.75     |\n","|    critic_loss     | 5.88     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1775760  |\n","---------------------------------\n","Eval num_timesteps=1178400, episode_reward=-7.24 +/- 0.07\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -7.24    |\n","| time/              |          |\n","|    total_timesteps | 1178400  |\n","| train/             |          |\n","|    actor_loss      | 2.95     |\n","|    critic_loss     | 7.5      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1778160  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2160     |\n","|    fps             | 99       |\n","|    time_elapsed    | 11819    |\n","|    total_timesteps | 1180728  |\n","| train/             |          |\n","|    actor_loss      | 2.97     |\n","|    critic_loss     | 9.75     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1780488  |\n","---------------------------------\n","Eval num_timesteps=1180800, episode_reward=-5.67 +/- 1.67\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -5.67    |\n","| time/              |          |\n","|    total_timesteps | 1180800  |\n","| train/             |          |\n","|    actor_loss      | 3.39     |\n","|    critic_loss     | 8.77     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1780560  |\n","---------------------------------\n","Eval num_timesteps=1183200, episode_reward=-0.70 +/- 0.20\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.702   |\n","| time/              |          |\n","|    total_timesteps | 1183200  |\n","| train/             |          |\n","|    actor_loss      | 2.78     |\n","|    critic_loss     | 9.97     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1782960  |\n","---------------------------------\n","Eval num_timesteps=1185600, episode_reward=-7.37 +/- 0.26\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -7.37    |\n","| time/              |          |\n","|    total_timesteps | 1185600  |\n","| train/             |          |\n","|    actor_loss      | 2.43     |\n","|    critic_loss     | 7.01     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1785360  |\n","---------------------------------\n","Eval num_timesteps=1188000, episode_reward=-4.30 +/- 1.76\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -4.3     |\n","| time/              |          |\n","|    total_timesteps | 1188000  |\n","| train/             |          |\n","|    actor_loss      | 2.74     |\n","|    critic_loss     | 9.64     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1787760  |\n","---------------------------------\n","Eval num_timesteps=1190400, episode_reward=0.44 +/- 0.83\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.442    |\n","| time/              |          |\n","|    total_timesteps | 1190400  |\n","| train/             |          |\n","|    actor_loss      | 2.34     |\n","|    critic_loss     | 6.6      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1790160  |\n","---------------------------------\n","Eval num_timesteps=1192800, episode_reward=0.23 +/- 0.76\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.232    |\n","| time/              |          |\n","|    total_timesteps | 1192800  |\n","| train/             |          |\n","|    actor_loss      | 2.36     |\n","|    critic_loss     | 2.65     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1792560  |\n","---------------------------------\n","Eval num_timesteps=1195200, episode_reward=-2.99 +/- 4.28\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -2.99    |\n","| time/              |          |\n","|    total_timesteps | 1195200  |\n","| train/             |          |\n","|    actor_loss      | 2.29     |\n","|    critic_loss     | 2.23     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1794960  |\n","---------------------------------\n","Eval num_timesteps=1197600, episode_reward=-9.06 +/- 2.00\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -9.06    |\n","| time/              |          |\n","|    total_timesteps | 1197600  |\n","| train/             |          |\n","|    actor_loss      | 2.04     |\n","|    critic_loss     | 3.08     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1797360  |\n","---------------------------------\n","Eval num_timesteps=1200000, episode_reward=2.37 +/- 2.13\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.37     |\n","| time/              |          |\n","|    total_timesteps | 1200000  |\n","| train/             |          |\n","|    actor_loss      | 1.49     |\n","|    critic_loss     | 7.38     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1799760  |\n","---------------------------------\n","Eval num_timesteps=1202400, episode_reward=6.53 +/- 1.66\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.53     |\n","| time/              |          |\n","|    total_timesteps | 1202400  |\n","| train/             |          |\n","|    actor_loss      | 1.75     |\n","|    critic_loss     | 2.91     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1802160  |\n","---------------------------------\n","Eval num_timesteps=1204800, episode_reward=-5.66 +/- 0.20\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -5.66    |\n","| time/              |          |\n","|    total_timesteps | 1204800  |\n","| train/             |          |\n","|    actor_loss      | 2.2      |\n","|    critic_loss     | 8.57     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1804560  |\n","---------------------------------\n","Eval num_timesteps=1207200, episode_reward=-10.08 +/- 0.83\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -10.1    |\n","| time/              |          |\n","|    total_timesteps | 1207200  |\n","| train/             |          |\n","|    actor_loss      | 2.12     |\n","|    critic_loss     | 12.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1806960  |\n","---------------------------------\n","Eval num_timesteps=1209600, episode_reward=0.28 +/- 1.42\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.279    |\n","| time/              |          |\n","|    total_timesteps | 1209600  |\n","| train/             |          |\n","|    actor_loss      | 1.94     |\n","|    critic_loss     | 6.44     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1809360  |\n","---------------------------------\n","Eval num_timesteps=1212000, episode_reward=-5.08 +/- 4.76\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -5.08    |\n","| time/              |          |\n","|    total_timesteps | 1212000  |\n","| train/             |          |\n","|    actor_loss      | 2.36     |\n","|    critic_loss     | 11.9     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1811760  |\n","---------------------------------\n","Eval num_timesteps=1214400, episode_reward=-1.90 +/- 2.37\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.9     |\n","| time/              |          |\n","|    total_timesteps | 1214400  |\n","| train/             |          |\n","|    actor_loss      | 1.9      |\n","|    critic_loss     | 2.99     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1814160  |\n","---------------------------------\n","Eval num_timesteps=1216800, episode_reward=-8.69 +/- 1.46\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -8.69    |\n","| time/              |          |\n","|    total_timesteps | 1216800  |\n","| train/             |          |\n","|    actor_loss      | 2.03     |\n","|    critic_loss     | 8.15     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1816560  |\n","---------------------------------\n","Eval num_timesteps=1219200, episode_reward=-6.38 +/- 2.36\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -6.38    |\n","| time/              |          |\n","|    total_timesteps | 1219200  |\n","| train/             |          |\n","|    actor_loss      | 2.31     |\n","|    critic_loss     | 2.92     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1818960  |\n","---------------------------------\n","Eval num_timesteps=1221600, episode_reward=1.15 +/- 3.76\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.15     |\n","| time/              |          |\n","|    total_timesteps | 1221600  |\n","| train/             |          |\n","|    actor_loss      | 2.03     |\n","|    critic_loss     | 6.88     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1821360  |\n","---------------------------------\n","Eval num_timesteps=1224000, episode_reward=7.44 +/- 1.33\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.44     |\n","| time/              |          |\n","|    total_timesteps | 1224000  |\n","| train/             |          |\n","|    actor_loss      | 1.88     |\n","|    critic_loss     | 10.1     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1823760  |\n","---------------------------------\n","Eval num_timesteps=1226400, episode_reward=10.21 +/- 1.82\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 10.2     |\n","| time/              |          |\n","|    total_timesteps | 1226400  |\n","| train/             |          |\n","|    actor_loss      | 1.85     |\n","|    critic_loss     | 4.67     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1826160  |\n","---------------------------------\n","Eval num_timesteps=1228800, episode_reward=-5.04 +/- 2.53\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -5.04    |\n","| time/              |          |\n","|    total_timesteps | 1228800  |\n","| train/             |          |\n","|    actor_loss      | 1.28     |\n","|    critic_loss     | 2.71     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1828560  |\n","---------------------------------\n","Eval num_timesteps=1231200, episode_reward=-14.01 +/- 2.37\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -14      |\n","| time/              |          |\n","|    total_timesteps | 1231200  |\n","| train/             |          |\n","|    actor_loss      | 1.61     |\n","|    critic_loss     | 1.62     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1830960  |\n","---------------------------------\n","Eval num_timesteps=1233600, episode_reward=2.08 +/- 0.16\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.08     |\n","| time/              |          |\n","|    total_timesteps | 1233600  |\n","| train/             |          |\n","|    actor_loss      | 1.74     |\n","|    critic_loss     | 10.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1833360  |\n","---------------------------------\n","Eval num_timesteps=1236000, episode_reward=5.54 +/- 2.13\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.54     |\n","| time/              |          |\n","|    total_timesteps | 1236000  |\n","| train/             |          |\n","|    actor_loss      | 1.93     |\n","|    critic_loss     | 5.31     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1835760  |\n","---------------------------------\n","Eval num_timesteps=1238400, episode_reward=5.87 +/- 0.30\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.87     |\n","| time/              |          |\n","|    total_timesteps | 1238400  |\n","| train/             |          |\n","|    actor_loss      | 2.29     |\n","|    critic_loss     | 4.93     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1838160  |\n","---------------------------------\n","Eval num_timesteps=1240800, episode_reward=0.60 +/- 1.96\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.595    |\n","| time/              |          |\n","|    total_timesteps | 1240800  |\n","| train/             |          |\n","|    actor_loss      | 1.82     |\n","|    critic_loss     | 8.64     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1840560  |\n","---------------------------------\n","Eval num_timesteps=1243200, episode_reward=5.56 +/- 0.47\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.56     |\n","| time/              |          |\n","|    total_timesteps | 1243200  |\n","| train/             |          |\n","|    actor_loss      | 1.91     |\n","|    critic_loss     | 2.1      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1842960  |\n","---------------------------------\n","Eval num_timesteps=1245600, episode_reward=2.22 +/- 0.78\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.22     |\n","| time/              |          |\n","|    total_timesteps | 1245600  |\n","| train/             |          |\n","|    actor_loss      | 2.04     |\n","|    critic_loss     | 5.34     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1845360  |\n","---------------------------------\n","Eval num_timesteps=1248000, episode_reward=6.51 +/- 3.75\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.51     |\n","| time/              |          |\n","|    total_timesteps | 1248000  |\n","| train/             |          |\n","|    actor_loss      | 1.22     |\n","|    critic_loss     | 7.7      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1847760  |\n","---------------------------------\n","Eval num_timesteps=1250400, episode_reward=14.07 +/- 0.58\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 14.1     |\n","| time/              |          |\n","|    total_timesteps | 1250400  |\n","| train/             |          |\n","|    actor_loss      | 1.78     |\n","|    critic_loss     | 5.62     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1850160  |\n","---------------------------------\n","Eval num_timesteps=1252800, episode_reward=11.27 +/- 0.26\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 11.3     |\n","| time/              |          |\n","|    total_timesteps | 1252800  |\n","| train/             |          |\n","|    actor_loss      | 1.33     |\n","|    critic_loss     | 9.8      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1852560  |\n","---------------------------------\n","Eval num_timesteps=1255200, episode_reward=7.82 +/- 1.40\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.82     |\n","| time/              |          |\n","|    total_timesteps | 1255200  |\n","| train/             |          |\n","|    actor_loss      | 1.18     |\n","|    critic_loss     | 11.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1854960  |\n","---------------------------------\n","Eval num_timesteps=1257600, episode_reward=10.00 +/- 0.47\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 10       |\n","| time/              |          |\n","|    total_timesteps | 1257600  |\n","| train/             |          |\n","|    actor_loss      | 0.818    |\n","|    critic_loss     | 8.18     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1857360  |\n","---------------------------------\n","Eval num_timesteps=1260000, episode_reward=2.74 +/- 0.32\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.74     |\n","| time/              |          |\n","|    total_timesteps | 1260000  |\n","| train/             |          |\n","|    actor_loss      | 0.189    |\n","|    critic_loss     | 7.72     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1859760  |\n","---------------------------------\n","Eval num_timesteps=1262400, episode_reward=11.76 +/- 0.92\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 11.8     |\n","| time/              |          |\n","|    total_timesteps | 1262400  |\n","| train/             |          |\n","|    actor_loss      | 0.652    |\n","|    critic_loss     | 8.3      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1862160  |\n","---------------------------------\n","Eval num_timesteps=1264800, episode_reward=8.47 +/- 2.01\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 8.47     |\n","| time/              |          |\n","|    total_timesteps | 1264800  |\n","| train/             |          |\n","|    actor_loss      | 1.06     |\n","|    critic_loss     | 4.04     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1864560  |\n","---------------------------------\n","Eval num_timesteps=1267200, episode_reward=0.74 +/- 0.26\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.737    |\n","| time/              |          |\n","|    total_timesteps | 1267200  |\n","| train/             |          |\n","|    actor_loss      | 0.0818   |\n","|    critic_loss     | 1.4      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1866960  |\n","---------------------------------\n","Eval num_timesteps=1269600, episode_reward=16.60 +/- 4.67\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 16.6     |\n","| time/              |          |\n","|    total_timesteps | 1269600  |\n","| train/             |          |\n","|    actor_loss      | 0.42     |\n","|    critic_loss     | 9.23     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1869360  |\n","---------------------------------\n","New best mean reward!\n","Eval num_timesteps=1272000, episode_reward=10.85 +/- 0.45\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 10.9     |\n","| time/              |          |\n","|    total_timesteps | 1272000  |\n","| train/             |          |\n","|    actor_loss      | 0.133    |\n","|    critic_loss     | 7.05     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1871760  |\n","---------------------------------\n","Eval num_timesteps=1274400, episode_reward=2.27 +/- 4.46\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.27     |\n","| time/              |          |\n","|    total_timesteps | 1274400  |\n","| train/             |          |\n","|    actor_loss      | -0.312   |\n","|    critic_loss     | 4.5      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1874160  |\n","---------------------------------\n","Eval num_timesteps=1276800, episode_reward=7.96 +/- 0.48\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.96     |\n","| time/              |          |\n","|    total_timesteps | 1276800  |\n","| train/             |          |\n","|    actor_loss      | -0.325   |\n","|    critic_loss     | 9.3      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1876560  |\n","---------------------------------\n","Eval num_timesteps=1279200, episode_reward=5.86 +/- 0.90\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.86     |\n","| time/              |          |\n","|    total_timesteps | 1279200  |\n","| train/             |          |\n","|    actor_loss      | -0.155   |\n","|    critic_loss     | 1.73     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1878960  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2170     |\n","|    fps             | 99       |\n","|    time_elapsed    | 12848    |\n","|    total_timesteps | 1281216  |\n","| train/             |          |\n","|    actor_loss      | -0.538   |\n","|    critic_loss     | 6.93     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1880976  |\n","---------------------------------\n","Eval num_timesteps=1281600, episode_reward=-46.04 +/- 49.15\n","Episode length: 339.60 +/- 196.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 340      |\n","|    mean_reward     | -46      |\n","| time/              |          |\n","|    total_timesteps | 1281600  |\n","| train/             |          |\n","|    actor_loss      | -0.297   |\n","|    critic_loss     | 3.11     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1881360  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2180     |\n","|    fps             | 99       |\n","|    time_elapsed    | 12874    |\n","|    total_timesteps | 1283760  |\n","| train/             |          |\n","|    actor_loss      | -0.0302  |\n","|    critic_loss     | 1.9      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1883520  |\n","---------------------------------\n","Eval num_timesteps=1284000, episode_reward=-1.14 +/- 0.21\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.14    |\n","| time/              |          |\n","|    total_timesteps | 1284000  |\n","| train/             |          |\n","|    actor_loss      | 0.0905   |\n","|    critic_loss     | 3.8      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1883760  |\n","---------------------------------\n","Eval num_timesteps=1286400, episode_reward=-63.77 +/- 53.11\n","Episode length: 293.00 +/- 169.01\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 293      |\n","|    mean_reward     | -63.8    |\n","| time/              |          |\n","|    total_timesteps | 1286400  |\n","| train/             |          |\n","|    actor_loss      | 0.467    |\n","|    critic_loss     | 1.58     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1886160  |\n","---------------------------------\n","Eval num_timesteps=1288800, episode_reward=11.67 +/- 3.65\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 11.7     |\n","| time/              |          |\n","|    total_timesteps | 1288800  |\n","| train/             |          |\n","|    actor_loss      | 0.735    |\n","|    critic_loss     | 1.25     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1888560  |\n","---------------------------------\n","Eval num_timesteps=1291200, episode_reward=10.67 +/- 0.47\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 10.7     |\n","| time/              |          |\n","|    total_timesteps | 1291200  |\n","| train/             |          |\n","|    actor_loss      | 1.11     |\n","|    critic_loss     | 8.8      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1890960  |\n","---------------------------------\n","Eval num_timesteps=1293600, episode_reward=7.22 +/- 0.26\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.22     |\n","| time/              |          |\n","|    total_timesteps | 1293600  |\n","| train/             |          |\n","|    actor_loss      | 1.19     |\n","|    critic_loss     | 5.9      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1893360  |\n","---------------------------------\n","Eval num_timesteps=1296000, episode_reward=9.80 +/- 1.59\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 9.8      |\n","| time/              |          |\n","|    total_timesteps | 1296000  |\n","| train/             |          |\n","|    actor_loss      | 1.43     |\n","|    critic_loss     | 4.89     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1895760  |\n","---------------------------------\n","Eval num_timesteps=1298400, episode_reward=-2.21 +/- 0.75\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -2.21    |\n","| time/              |          |\n","|    total_timesteps | 1298400  |\n","| train/             |          |\n","|    actor_loss      | 1.26     |\n","|    critic_loss     | 3.47     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1898160  |\n","---------------------------------\n","Eval num_timesteps=1300800, episode_reward=5.01 +/- 3.62\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.01     |\n","| time/              |          |\n","|    total_timesteps | 1300800  |\n","| train/             |          |\n","|    actor_loss      | 1.09     |\n","|    critic_loss     | 1.78     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1900560  |\n","---------------------------------\n","Eval num_timesteps=1303200, episode_reward=-1.17 +/- 2.12\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.17    |\n","| time/              |          |\n","|    total_timesteps | 1303200  |\n","| train/             |          |\n","|    actor_loss      | 1.13     |\n","|    critic_loss     | 3.02     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1902960  |\n","---------------------------------\n","Eval num_timesteps=1305600, episode_reward=-4.60 +/- 2.25\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -4.6     |\n","| time/              |          |\n","|    total_timesteps | 1305600  |\n","| train/             |          |\n","|    actor_loss      | 1.02     |\n","|    critic_loss     | 3.4      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1905360  |\n","---------------------------------\n","Eval num_timesteps=1308000, episode_reward=-0.96 +/- 5.61\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.959   |\n","| time/              |          |\n","|    total_timesteps | 1308000  |\n","| train/             |          |\n","|    actor_loss      | 0.953    |\n","|    critic_loss     | 1.54     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1907760  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2190     |\n","|    fps             | 99       |\n","|    time_elapsed    | 13137    |\n","|    total_timesteps | 1309392  |\n","| train/             |          |\n","|    actor_loss      | 1.02     |\n","|    critic_loss     | 8.49     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1909152  |\n","---------------------------------\n","Eval num_timesteps=1310400, episode_reward=-62.92 +/- 49.05\n","Episode length: 250.40 +/- 203.80\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 250      |\n","|    mean_reward     | -62.9    |\n","| time/              |          |\n","|    total_timesteps | 1310400  |\n","| train/             |          |\n","|    actor_loss      | 0.87     |\n","|    critic_loss     | 5.44     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1910160  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2200     |\n","|    fps             | 99       |\n","|    time_elapsed    | 13165    |\n","|    total_timesteps | 1312296  |\n","| train/             |          |\n","|    actor_loss      | 0.5      |\n","|    critic_loss     | 1.32     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1912056  |\n","---------------------------------\n","Eval num_timesteps=1312800, episode_reward=9.85 +/- 0.94\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 9.85     |\n","| time/              |          |\n","|    total_timesteps | 1312800  |\n","| train/             |          |\n","|    actor_loss      | 0.74     |\n","|    critic_loss     | 11.1     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1912560  |\n","---------------------------------\n","Eval num_timesteps=1315200, episode_reward=12.14 +/- 0.21\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 12.1     |\n","| time/              |          |\n","|    total_timesteps | 1315200  |\n","| train/             |          |\n","|    actor_loss      | 0.777    |\n","|    critic_loss     | 5.05     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1914960  |\n","---------------------------------\n","Eval num_timesteps=1317600, episode_reward=8.58 +/- 3.63\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 8.58     |\n","| time/              |          |\n","|    total_timesteps | 1317600  |\n","| train/             |          |\n","|    actor_loss      | 0.961    |\n","|    critic_loss     | 6.88     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1917360  |\n","---------------------------------\n","Eval num_timesteps=1320000, episode_reward=6.80 +/- 0.14\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.8      |\n","| time/              |          |\n","|    total_timesteps | 1320000  |\n","| train/             |          |\n","|    actor_loss      | 0.889    |\n","|    critic_loss     | 6.82     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1919760  |\n","---------------------------------\n","Eval num_timesteps=1322400, episode_reward=1.40 +/- 3.29\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.4      |\n","| time/              |          |\n","|    total_timesteps | 1322400  |\n","| train/             |          |\n","|    actor_loss      | 0.885    |\n","|    critic_loss     | 3.98     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1922160  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2210     |\n","|    fps             | 99       |\n","|    time_elapsed    | 13288    |\n","|    total_timesteps | 1324464  |\n","| train/             |          |\n","|    actor_loss      | 0.727    |\n","|    critic_loss     | 2.78     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1924224  |\n","---------------------------------\n","Eval num_timesteps=1324800, episode_reward=-61.90 +/- 51.59\n","Episode length: 250.40 +/- 203.80\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 250      |\n","|    mean_reward     | -61.9    |\n","| time/              |          |\n","|    total_timesteps | 1324800  |\n","| train/             |          |\n","|    actor_loss      | 1.11     |\n","|    critic_loss     | 9.31     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1924560  |\n","---------------------------------\n","Eval num_timesteps=1327200, episode_reward=8.51 +/- 1.41\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 8.51     |\n","| time/              |          |\n","|    total_timesteps | 1327200  |\n","| train/             |          |\n","|    actor_loss      | 0.903    |\n","|    critic_loss     | 1.56     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1926960  |\n","---------------------------------\n","Eval num_timesteps=1329600, episode_reward=-6.90 +/- 1.01\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -6.9     |\n","| time/              |          |\n","|    total_timesteps | 1329600  |\n","| train/             |          |\n","|    actor_loss      | 1.16     |\n","|    critic_loss     | 2.66     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1929360  |\n","---------------------------------\n","Eval num_timesteps=1332000, episode_reward=5.02 +/- 0.81\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.02     |\n","| time/              |          |\n","|    total_timesteps | 1332000  |\n","| train/             |          |\n","|    actor_loss      | 1.23     |\n","|    critic_loss     | 1.01     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1931760  |\n","---------------------------------\n","Eval num_timesteps=1334400, episode_reward=7.94 +/- 1.58\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.94     |\n","| time/              |          |\n","|    total_timesteps | 1334400  |\n","| train/             |          |\n","|    actor_loss      | 0.675    |\n","|    critic_loss     | 4.53     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1934160  |\n","---------------------------------\n","Eval num_timesteps=1336800, episode_reward=4.32 +/- 0.95\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.32     |\n","| time/              |          |\n","|    total_timesteps | 1336800  |\n","| train/             |          |\n","|    actor_loss      | 0.949    |\n","|    critic_loss     | 4.23     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1936560  |\n","---------------------------------\n","Eval num_timesteps=1339200, episode_reward=11.79 +/- 2.75\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 11.8     |\n","| time/              |          |\n","|    total_timesteps | 1339200  |\n","| train/             |          |\n","|    actor_loss      | 0.705    |\n","|    critic_loss     | 1.75     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1938960  |\n","---------------------------------\n","Eval num_timesteps=1341600, episode_reward=-6.35 +/- 3.06\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -6.35    |\n","| time/              |          |\n","|    total_timesteps | 1341600  |\n","| train/             |          |\n","|    actor_loss      | 0.495    |\n","|    critic_loss     | 3.96     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1941360  |\n","---------------------------------\n","Eval num_timesteps=1344000, episode_reward=-68.05 +/- 43.47\n","Episode length: 266.00 +/- 191.06\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 266      |\n","|    mean_reward     | -68      |\n","| time/              |          |\n","|    total_timesteps | 1344000  |\n","| train/             |          |\n","|    actor_loss      | 0.799    |\n","|    critic_loss     | 1.28     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1943760  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2220     |\n","|    fps             | 99       |\n","|    time_elapsed    | 13506    |\n","|    total_timesteps | 1345944  |\n","| train/             |          |\n","|    actor_loss      | 0.72     |\n","|    critic_loss     | 1.21     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1945704  |\n","---------------------------------\n","Eval num_timesteps=1346400, episode_reward=-106.80 +/- 2.46\n","Episode length: 134.20 +/- 25.96\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 134      |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 1346400  |\n","| train/             |          |\n","|    actor_loss      | 1.17     |\n","|    critic_loss     | 2.03     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1946160  |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2230     |\n","|    fps             | 99       |\n","|    time_elapsed    | 13522    |\n","|    total_timesteps | 1347744  |\n","| train/             |          |\n","|    actor_loss      | 0.857    |\n","|    critic_loss     | 0.901    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1947504  |\n","---------------------------------\n","Eval num_timesteps=1348800, episode_reward=-103.81 +/- 0.25\n","Episode length: 106.40 +/- 15.19\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 106      |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 1348800  |\n","| train/             |          |\n","|    actor_loss      | 1.11     |\n","|    critic_loss     | 1.2      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1948560  |\n","---------------------------------\n","Eval num_timesteps=1351200, episode_reward=14.59 +/- 2.75\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 14.6     |\n","| time/              |          |\n","|    total_timesteps | 1351200  |\n","| train/             |          |\n","|    actor_loss      | 1.58     |\n","|    critic_loss     | 3.35     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1950960  |\n","---------------------------------\n","Eval num_timesteps=1353600, episode_reward=12.11 +/- 0.88\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 12.1     |\n","| time/              |          |\n","|    total_timesteps | 1353600  |\n","| train/             |          |\n","|    actor_loss      | 1.36     |\n","|    critic_loss     | 4.02     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1953360  |\n","---------------------------------\n","Eval num_timesteps=1356000, episode_reward=4.17 +/- 0.26\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.17     |\n","| time/              |          |\n","|    total_timesteps | 1356000  |\n","| train/             |          |\n","|    actor_loss      | 1.42     |\n","|    critic_loss     | 1.33     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1955760  |\n","---------------------------------\n","Eval num_timesteps=1358400, episode_reward=11.35 +/- 1.95\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 11.3     |\n","| time/              |          |\n","|    total_timesteps | 1358400  |\n","| train/             |          |\n","|    actor_loss      | 1.27     |\n","|    critic_loss     | 10.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1958160  |\n","---------------------------------\n","Eval num_timesteps=1360800, episode_reward=11.00 +/- 1.51\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 11       |\n","| time/              |          |\n","|    total_timesteps | 1360800  |\n","| train/             |          |\n","|    actor_loss      | 0.678    |\n","|    critic_loss     | 6.6      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1960560  |\n","---------------------------------\n","Eval num_timesteps=1363200, episode_reward=9.67 +/- 0.91\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 9.67     |\n","| time/              |          |\n","|    total_timesteps | 1363200  |\n","| train/             |          |\n","|    actor_loss      | 0.77     |\n","|    critic_loss     | 4.37     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1962960  |\n","---------------------------------\n","Eval num_timesteps=1365600, episode_reward=12.81 +/- 0.20\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 12.8     |\n","| time/              |          |\n","|    total_timesteps | 1365600  |\n","| train/             |          |\n","|    actor_loss      | 0.758    |\n","|    critic_loss     | 5.29     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 1965360  |\n","---------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n"]},{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-22-7728c16c4aa9>\", line 14, in <cell line: 14>\n","    model.learn(total_timesteps=5000000, log_interval=10,callback=eval_callback)\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/ddpg/ddpg.py\", line 123, in learn\n","    return super().learn(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/td3/td3.py\", line 222, in learn\n","    return super().learn(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 312, in learn\n","    rollout = self.collect_rollouts(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 552, in collect_rollouts\n","    if callback.on_step() is False:\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 104, in on_step\n","    return self._on_step()\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 471, in _on_step\n","    np.savez(\n","  File \"<__array_function__ internals>\", line 180, in savez\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 615, in savez\n","    _savez(file, args, kwds, False)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 712, in _savez\n","    zipf = zipfile_factory(file, mode=\"w\", compression=compression)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 103, in zipfile_factory\n","    return zipfile.ZipFile(file, *args, **kwargs)\n","  File \"/usr/lib/python3.10/zipfile.py\", line 1251, in __init__\n","    self.fp = io.open(file, filemode)\n","OSError: [Errno 107] Transport endpoint is not connected: './multiwalker_ddpg2_5_log_eval/evaluations.npz'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'OSError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-22-7728c16c4aa9>\", line 14, in <cell line: 14>\n","    model.learn(total_timesteps=5000000, log_interval=10,callback=eval_callback)\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/ddpg/ddpg.py\", line 123, in learn\n","    return super().learn(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/td3/td3.py\", line 222, in learn\n","    return super().learn(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 312, in learn\n","    rollout = self.collect_rollouts(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 552, in collect_rollouts\n","    if callback.on_step() is False:\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 104, in on_step\n","    return self._on_step()\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 471, in _on_step\n","    np.savez(\n","  File \"<__array_function__ internals>\", line 180, in savez\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 615, in savez\n","    _savez(file, args, kwds, False)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 712, in _savez\n","    zipf = zipfile_factory(file, mode=\"w\", compression=compression)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 103, in zipfile_factory\n","    return zipfile.ZipFile(file, *args, **kwargs)\n","  File \"/usr/lib/python3.10/zipfile.py\", line 1251, in __init__\n","    self.fp = io.open(file, filemode)\n","OSError: [Errno 107] Transport endpoint is not connected: './multiwalker_ddpg2_5_log_eval/evaluations.npz'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'OSError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n","    if (await self.run_code(code, result,  async_=asy)):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n","    self.showtraceback(running_compiled_code=True)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n","    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-22-7728c16c4aa9>\", line 14, in <cell line: 14>\n","    model.learn(total_timesteps=5000000, log_interval=10,callback=eval_callback)\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/ddpg/ddpg.py\", line 123, in learn\n","    return super().learn(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/td3/td3.py\", line 222, in learn\n","    return super().learn(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 312, in learn\n","    rollout = self.collect_rollouts(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 552, in collect_rollouts\n","    if callback.on_step() is False:\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 104, in on_step\n","    return self._on_step()\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 471, in _on_step\n","    np.savez(\n","  File \"<__array_function__ internals>\", line 180, in savez\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 615, in savez\n","    _savez(file, args, kwds, False)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 712, in _savez\n","    zipf = zipfile_factory(file, mode=\"w\", compression=compression)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 103, in zipfile_factory\n","    return zipfile.ZipFile(file, *args, **kwargs)\n","  File \"/usr/lib/python3.10/zipfile.py\", line 1251, in __init__\n","    self.fp = io.open(file, filemode)\n","OSError: [Errno 107] Transport endpoint is not connected: './multiwalker_ddpg2_5_log_eval/evaluations.npz'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'OSError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n","    if (await self.run_code(code, result,  async_=asy)):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n","    self.showtraceback(running_compiled_code=True)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n","    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n","    return runner(coro)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n","    coro.send(None)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n","    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3492, in run_ast_nodes\n","    self.showtraceback()\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1142, in structured_traceback\n","    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyPxNx/qmJQeg9+r7JyQ4nKn"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}