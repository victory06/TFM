{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"d8JAmEUyj9De"},"outputs":[],"source":["# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n","mount='/content/gdrive'\n","drive_root = mount + \"/My Drive/TFM\"\n","\n","try:\n","  from google.colab import drive\n","  IN_COLAB=True\n","except:\n","  IN_COLAB=False"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41969,"status":"ok","timestamp":1699028246585,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-60},"id":"DrEo9QnxkAne","outputId":"9fb9c7b2-5846-4c72-d212-4e5d4f858633"},"outputs":[{"output_type":"stream","name":"stdout","text":["We're running Colab\n","Colab: mounting Google drive on  /content/gdrive\n","Mounted at /content/gdrive\n","\n","Colab: making sure  /content/gdrive/My Drive/TFM  exists.\n","\n","Colab: Changing directory to  /content/gdrive/My Drive/TFM\n","/content/gdrive/My Drive/TFM\n","Archivos en el directorio: \n","['multiwalker_sac3_04_log_eval', 'TFM_Multiwalker_SAC_recompensas_00_gym_cap.ipynb', 'Entrenamientos antiguos sin logs', '.ipynb_checkpoints', 'Entrenamientos_log_no_eval', 'PPO_policies', 'DQN_new_pettingzoo_gym_cap.ipynb', 'multi_car_racing', 'policy_log_eval', 'DQN_policies', 'results_rllib', 'MCR_TFM.ipynb', 'multiwalker_ddpg_log_eval', 'multiwalker_sac_log_eval', 'multiwalker_ddpg.zip', 'multiwalker_ppo_log_eval', 'multiwalker_ppo.zip', 'multiwalker_td3_log_eval', 'multiwalker_sac2_log_eval', 'multiwalker_td3_2_log_eval', 'multiwalker_sac3_log_eval', 'multiwalker_sac3.zip', 'multiwalker_ppo_2_log_eval', 'multiwalker_ddpg2_log_eval', 'multiwalker_ppo_2.zip', 'TFM_Multiwalker_DDPG_PPO_gym_cap.ipynb', 'multiwalker_td3_3_log_eval', 'TFM_Multiwalker_TD3_gym_cap.ipynb', 'TFM_Multiwalker_SAC_gym_cap.ipynb', 'TFM_PPO_KAZ_gym_cap.ipynb', 'TFM_PPO_new_KAZ_gym_cap.ipynb', 'TFM_Multiwalker_DDPG_recompensas_gym_cap.ipynb', 'multiwalker_ddpg2_5_log_eval', 'multiwalker_ppo_rew_08_log_eval', 'multiwalker_ppo_rew_08.zip', 'multiwalker_ppo_08_2_log_eval', 'multiwalker_ddpg2_6_log_eval', 'multiwalker_ppo_08_2.zip', 'multiwalker_sac_08_log_eval', 'TFM_Multiwalker_DDPG_gym_cap.ipynb', 'multiwalker_sac2_08_log_eval', 'multiwalker_ppo_rew_04_log_eval', 'multiwalker_ppo_rew_04.zip', 'multiwalker_ppo_04_2_log_eval', 'multiwalker_ppo_04_2.zip', 'multiwalker_ppo_rew_0_log_eval', 'multiwalker_sac3_08_log_eval', 'multiwalker_ppo_rew_0.zip', 'multiwalker_ppo_0_2_log_eval', 'multiwalker_sac3_08.zip', 'TFM_Multiwalker_SAC_recompensas_gym_cap.ipynb', 'multiwalker_ppo_0_2.zip', 'TFM_Multiwalker_PPO_recompensas_gym_cap.ipynb', 'multiwalker_sac_00_log_eval', 'multiwalker_sac_04_log_eval', 'multiwalker_sac_00.zip', 'multiwalker_sac2_00_log_eval', 'multiwalker_sac2_04_log_eval', 'multiwalker_sac2_04.zip', '=2.13', 'multiwalker_sac2_00.zip', 'TFM_Multiwalker_SAC_recompensas_04_gym_cap.ipynb']\n"]}],"source":["# Switch to the directory on the Google Drive that you want to use\n","import os\n","if IN_COLAB:\n","  print(\"We're running Colab\")\n","\n","  if IN_COLAB:\n","    # Mount the Google Drive at mount\n","    print(\"Colab: mounting Google drive on \", mount)\n","\n","    drive.mount(mount)\n","\n","    # Create drive_root if it doesn't exist\n","    create_drive_root = True\n","    if create_drive_root:\n","      print(\"\\nColab: making sure \", drive_root, \" exists.\")\n","      os.makedirs(drive_root, exist_ok=True)\n","\n","    # Change to the directory\n","    print(\"\\nColab: Changing directory to \", drive_root)\n","    %cd $drive_root\n","# Verify we're in the correct working directory\n","%pwd\n","print(\"Archivos en el directorio: \")\n","print(os.listdir())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xAZDg478kEbs","outputId":"9bc71e03-73de-425d-871b-44ebdbb896d0","executionInfo":{"status":"ok","timestamp":1699028473943,"user_tz":-60,"elapsed":227367,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gym==0.17.3\n","  Downloading gym-0.17.3.tar.gz (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym==0.17.3) (1.11.3)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.17.3) (1.23.5)\n","Collecting pyglet<=1.5.0,>=1.4.0 (from gym==0.17.3)\n","  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cloudpickle<1.7.0,>=1.2.0 (from gym==0.17.3)\n","  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3) (0.18.3)\n","Building wheels for collected packages: gym\n","  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym: filename=gym-0.17.3-py3-none-any.whl size=1654617 sha256=c77150256ba788da92705327f9ffa69026e0c8a92a09e27c644a3d1999e96651\n","  Stored in directory: /root/.cache/pip/wheels/af/4b/74/fcfc8238472c34d7f96508a63c962ff3ac9485a9a4137afd4e\n","Successfully built gym\n","Installing collected packages: pyglet, cloudpickle, gym\n","  Attempting uninstall: cloudpickle\n","    Found existing installation: cloudpickle 2.2.1\n","    Uninstalling cloudpickle-2.2.1:\n","      Successfully uninstalled cloudpickle-2.2.1\n","  Attempting uninstall: gym\n","    Found existing installation: gym 0.25.2\n","    Uninstalling gym-0.25.2:\n","      Successfully uninstalled gym-0.25.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","bigframes 0.10.0 requires cloudpickle>=2.0.0, but you have cloudpickle 1.6.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed cloudpickle-1.6.0 gym-0.17.3 pyglet-1.5.0\n","Collecting git+https://github.com/Kojoley/atari-py.git\n","  Cloning https://github.com/Kojoley/atari-py.git to /tmp/pip-req-build-t172myff\n","  Running command git clone --filter=blob:none --quiet https://github.com/Kojoley/atari-py.git /tmp/pip-req-build-t172myff\n","  Resolved https://github.com/Kojoley/atari-py.git to commit 86a1e05c0a95e9e6233c3a413521fdb34ca8a089\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from atari-py==1.2.2) (1.23.5)\n","Building wheels for collected packages: atari-py\n","  Building wheel for atari-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for atari-py: filename=atari_py-1.2.2-cp310-cp310-linux_x86_64.whl size=4738733 sha256=5bab576562d68d1adb248484d41b61020753343c44a727b8f6ee1e9f94e6329e\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-n6ed0goa/wheels/6c/9b/f1/8a80a63a233d6f4eb2e2ee8c7357f4875f21c3ffc7f2613f9f\n","Successfully built atari-py\n","Installing collected packages: atari-py\n","Successfully installed atari-py-1.2.2\n","Collecting keras-rl2==1.0.5\n","  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from keras-rl2==1.0.5) (2.14.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (23.5.26)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (16.0.6)\n","Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.2.0)\n","Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.23.5)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (23.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (4.5.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.34.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.59.0)\n","Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.14.1)\n","Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.14.0)\n","Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.14.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2==1.0.5) (0.41.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.2.2)\n","Installing collected packages: keras-rl2\n","Successfully installed keras-rl2-1.0.5\n","Collecting tensorflow==2.8\n","  Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl (497.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.6.3)\n","Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (23.5.26)\n","Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.9.0)\n","Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.8)\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (16.0.6)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.23.5)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.3.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (4.5.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.14.1)\n","Collecting tensorboard<2.9,>=2.8 (from tensorflow==2.8)\n","  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m108.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tf-estimator-nightly==2.8.0.dev2021122109 (from tensorflow==2.8)\n","  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting keras<2.9,>=2.8.0rc0 (from tensorflow==2.8)\n","  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.34.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.59.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.8) (0.41.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.17.3)\n","Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.31.0)\n","Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n","  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n","  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.2.2)\n","Installing collected packages: tf-estimator-nightly, tensorboard-plugin-wit, keras, tensorboard-data-server, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.14.0\n","    Uninstalling keras-2.14.0:\n","      Successfully uninstalled keras-2.14.0\n","  Attempting uninstall: tensorboard-data-server\n","    Found existing installation: tensorboard-data-server 0.7.2\n","    Uninstalling tensorboard-data-server-0.7.2:\n","      Successfully uninstalled tensorboard-data-server-0.7.2\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 1.0.0\n","    Uninstalling google-auth-oauthlib-1.0.0:\n","      Successfully uninstalled google-auth-oauthlib-1.0.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.14.1\n","    Uninstalling tensorboard-2.14.1:\n","      Successfully uninstalled tensorboard-2.14.1\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.14.0\n","    Uninstalling tensorflow-2.14.0:\n","      Successfully uninstalled tensorflow-2.14.0\n","Successfully installed google-auth-oauthlib-0.4.6 keras-2.8.0 keras-preprocessing-1.1.2 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tf-estimator-nightly-2.8.0.dev2021122109\n","Collecting gym-cap\n","  Downloading gym_cap-0.1.4.12-py3-none-any.whl (32 kB)\n","Requirement already satisfied: gym>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from gym-cap) (0.17.3)\n","Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from gym-cap) (1.23.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym>=0.16.0->gym-cap) (1.11.3)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.16.0->gym-cap) (1.5.0)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.16.0->gym-cap) (1.6.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.16.0->gym-cap) (0.18.3)\n","Installing collected packages: gym-cap\n","Successfully installed gym-cap-0.1.4.12\n","Collecting pettingzoo[butterfly]\n","  Downloading pettingzoo-1.24.1-py3-none-any.whl (840 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.8/840.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[butterfly]) (1.23.5)\n","Collecting gymnasium>=0.28.0 (from pettingzoo[butterfly])\n","  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pygame==2.3.0 (from pettingzoo[butterfly])\n","  Downloading pygame-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pymunk==6.2.0 (from pettingzoo[butterfly])\n","  Downloading pymunk-6.2.0.zip (7.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: cffi>1.14.0 in /usr/local/lib/python3.10/dist-packages (from pymunk==6.2.0->pettingzoo[butterfly]) (1.16.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[butterfly]) (1.6.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[butterfly]) (4.5.0)\n","Collecting farama-notifications>=0.0.1 (from gymnasium>=0.28.0->pettingzoo[butterfly])\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>1.14.0->pymunk==6.2.0->pettingzoo[butterfly]) (2.21)\n","Building wheels for collected packages: pymunk\n","  Building wheel for pymunk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pymunk: filename=pymunk-6.2.0-cp310-cp310-linux_x86_64.whl size=801634 sha256=ef6d50f4e110117346e58caf683129f77c102cdc8143ee64a4e2603b122a2d9f\n","  Stored in directory: /root/.cache/pip/wheels/2e/81/a2/f941a9ff417bb4020c37ae218fb7117d12d3fc019ea493d66f\n","Successfully built pymunk\n","Installing collected packages: farama-notifications, pygame, gymnasium, pymunk, pettingzoo\n","  Attempting uninstall: pygame\n","    Found existing installation: pygame 2.5.2\n","    Uninstalling pygame-2.5.2:\n","      Successfully uninstalled pygame-2.5.2\n","Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1 pettingzoo-1.24.1 pygame-2.3.0 pymunk-6.2.0\n"]}],"source":["if IN_COLAB:\n","  %pip install gym==0.17.3\n","  %pip install git+https://github.com/Kojoley/atari-py.git\n","  %pip install keras-rl2==1.0.5\n","  %pip install tensorflow==2.8\n","  %pip install gym-cap\n","  %pip install pettingzoo[butterfly]\n","else:\n","  %pip install gym==0.17.3\n","  %pip install git+https://github.com/Kojoley/atari-py.git\n","  %pip install pyglet==1.5.0\n","  %pip install h5py==3.1.0\n","  %pip install Pillow==9.5.0\n","  %pip install keras-rl2==1.0.5\n","  %pip install Keras==2.2.4\n","  %pip install tensorflow==2.5.3\n","  %pip install torch==2.0.1\n","  %pip install agents==1.4.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G8cw-IX3laE9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699028473944,"user_tz":-60,"elapsed":41,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"75ad7d1a-c2eb-4a0b-bb40-73bcdce85331"},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'multi_car_racing' already exists and is not an empty directory.\n"]}],"source":["!git clone https://github.com/igilitschenski/multi_car_racing.git\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IqbMo3gK7vBG"},"outputs":[],"source":["!cd multi_car_racing\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jekec6f98b3A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699028479954,"user_tz":-60,"elapsed":5653,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"2d83d8fb-a4e7-4a60-d569-79de9576e9db"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting swig\n","  Downloading swig-4.1.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: swig\n","Successfully installed swig-4.1.1\n"]}],"source":["!pip install swig"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KKxRPBFx85k6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699028538147,"user_tz":-60,"elapsed":58201,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"207bf891-c6b8-4f77-bbd9-6ab7431c1e27"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting box2d\n","  Downloading Box2D-2.3.2.tar.gz (427 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.9/427.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: box2d\n","  Building wheel for box2d (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d: filename=Box2D-2.3.2-cp310-cp310-linux_x86_64.whl size=2391302 sha256=3c6c759454928010bbe772ec54877b6b0cc475d5a31394978607229123879547\n","  Stored in directory: /root/.cache/pip/wheels/eb/cb/be/e663f3ce9aba6580611c0febaf7cd3cf7603f87047de2a52f9\n","Successfully built box2d\n","Installing collected packages: box2d\n","Successfully installed box2d-2.3.2\n"]}],"source":["!pip install box2d"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ijp5V0i09MRF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699028596957,"user_tz":-60,"elapsed":58858,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"7a2bcc5c-2020-47af-c3fc-71760579f898"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting box2d-py\n","  Downloading box2d-py-2.3.8.tar.gz (374 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.5/374.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: box2d-py\n","  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-py: filename=box2d_py-2.3.8-cp310-cp310-linux_x86_64.whl size=2373077 sha256=528aede2b8b0cf20000676eb275a74e913cbc95ab40cf5c872456ad590279349\n","  Stored in directory: /root/.cache/pip/wheels/47/01/d2/6a780da77ccb98b1d2facdd520a8d10838a03b590f6f8d50c0\n","Successfully built box2d-py\n","Installing collected packages: box2d-py\n","Successfully installed box2d-py-2.3.8\n"]}],"source":["!pip install box2d-py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BwjugqI99g0I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699028622548,"user_tz":-60,"elapsed":25640,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"99e81060-2150-4cb9-af8f-e8d727c3d94b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Obtaining file:///content/gdrive/MyDrive/TFM/multi_car_racing\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: box2d-py~=2.3.5 in /usr/local/lib/python3.10/dist-packages (from gym-multi-car-racing==0.0.1) (2.3.8)\n","Collecting shapely~=1.7.0 (from gym-multi-car-racing==0.0.1)\n","  Downloading Shapely-1.7.1.tar.gz (383 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.2/383.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from gym-multi-car-racing==0.0.1) (1.23.5)\n","Requirement already satisfied: gym~=0.17.2 in /usr/local/lib/python3.10/dist-packages (from gym-multi-car-racing==0.0.1) (0.17.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym~=0.17.2->gym-multi-car-racing==0.0.1) (1.11.3)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from gym~=0.17.2->gym-multi-car-racing==0.0.1) (1.5.0)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym~=0.17.2->gym-multi-car-racing==0.0.1) (1.6.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym~=0.17.2->gym-multi-car-racing==0.0.1) (0.18.3)\n","Building wheels for collected packages: shapely\n","  Building wheel for shapely (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for shapely: filename=Shapely-1.7.1-cp310-cp310-linux_x86_64.whl size=997159 sha256=37a1b471b43696a852aa36ae86bace0dfb114ad1f0d8319da66bfbd72f951421\n","  Stored in directory: /root/.cache/pip/wheels/2e/fa/97/c85f587c35afcaf4a81c481741d36592518d1e50445572f0d4\n","Successfully built shapely\n","Installing collected packages: shapely, gym-multi-car-racing\n","  Attempting uninstall: shapely\n","    Found existing installation: shapely 2.0.2\n","    Uninstalling shapely-2.0.2:\n","      Successfully uninstalled shapely-2.0.2\n","  Running setup.py develop for gym-multi-car-racing\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires fastapi, which is not installed.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed gym-multi-car-racing-0.0.1 shapely-1.7.1\n"]}],"source":["!pip install -e multi_car_racing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IrAvXzCW-Z3e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699028631483,"user_tz":-60,"elapsed":8987,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"e9be18e7-275a-4436-f7b1-ea8e57d774f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  freeglut3 libegl-dev libgl-dev libgl1-mesa-dev libgles-dev libgles1\n","  libglu1-mesa libglu1-mesa-dev libglvnd-core-dev libglvnd-dev libglx-dev\n","  libice-dev libopengl-dev libsm-dev libxt-dev\n","Suggested packages:\n","  libice-doc libsm-doc libxt-doc\n","The following NEW packages will be installed:\n","  freeglut3 freeglut3-dev libegl-dev libgl-dev libgl1-mesa-dev libgles-dev\n","  libgles1 libglu1-mesa libglu1-mesa-dev libglvnd-core-dev libglvnd-dev\n","  libglx-dev libice-dev libopengl-dev libsm-dev libxt-dev\n","0 upgraded, 16 newly installed, 0 to remove and 19 not upgraded.\n","Need to get 1,261 kB of archives.\n","After this operation, 6,753 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3 amd64 2.8.1-6 [74.0 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglx-dev amd64 1.4.0-1 [14.1 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgl-dev amd64 1.4.0-1 [101 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-core-dev amd64 1.4.0-1 [12.7 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libegl-dev amd64 1.4.0-1 [18.0 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles1 amd64 1.4.0-1 [11.5 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles-dev amd64 1.4.0-1 [49.4 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libopengl-dev amd64 1.4.0-1 [3,400 B]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-dev amd64 1.4.0-1 [3,162 B]\n","Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgl1-mesa-dev amd64 23.0.4-0ubuntu1~22.04.1 [6,510 B]\n","Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n","Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa-dev amd64 9.0.2-1 [231 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 libice-dev amd64 2:1.0.10-1build2 [51.4 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsm-dev amd64 2:1.2.3-1build2 [18.1 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3-dev amd64 2.8.1-6 [126 kB]\n","Fetched 1,261 kB in 1s (1,744 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 16.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package freeglut3:amd64.\n","(Reading database ... 120874 files and directories currently installed.)\n","Preparing to unpack .../00-freeglut3_2.8.1-6_amd64.deb ...\n","Unpacking freeglut3:amd64 (2.8.1-6) ...\n","Selecting previously unselected package libglx-dev:amd64.\n","Preparing to unpack .../01-libglx-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglx-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgl-dev:amd64.\n","Preparing to unpack .../02-libgl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libgl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libglvnd-core-dev:amd64.\n","Preparing to unpack .../03-libglvnd-core-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglvnd-core-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libegl-dev:amd64.\n","Preparing to unpack .../04-libegl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libegl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgles1:amd64.\n","Preparing to unpack .../05-libgles1_1.4.0-1_amd64.deb ...\n","Unpacking libgles1:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgles-dev:amd64.\n","Preparing to unpack .../06-libgles-dev_1.4.0-1_amd64.deb ...\n","Unpacking libgles-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libopengl-dev:amd64.\n","Preparing to unpack .../07-libopengl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libopengl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libglvnd-dev:amd64.\n","Preparing to unpack .../08-libglvnd-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglvnd-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgl1-mesa-dev:amd64.\n","Preparing to unpack .../09-libgl1-mesa-dev_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n","Unpacking libgl1-mesa-dev:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Selecting previously unselected package libglu1-mesa:amd64.\n","Preparing to unpack .../10-libglu1-mesa_9.0.2-1_amd64.deb ...\n","Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n","Selecting previously unselected package libglu1-mesa-dev:amd64.\n","Preparing to unpack .../11-libglu1-mesa-dev_9.0.2-1_amd64.deb ...\n","Unpacking libglu1-mesa-dev:amd64 (9.0.2-1) ...\n","Selecting previously unselected package libice-dev:amd64.\n","Preparing to unpack .../12-libice-dev_2%3a1.0.10-1build2_amd64.deb ...\n","Unpacking libice-dev:amd64 (2:1.0.10-1build2) ...\n","Selecting previously unselected package libsm-dev:amd64.\n","Preparing to unpack .../13-libsm-dev_2%3a1.2.3-1build2_amd64.deb ...\n","Unpacking libsm-dev:amd64 (2:1.2.3-1build2) ...\n","Selecting previously unselected package libxt-dev:amd64.\n","Preparing to unpack .../14-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n","Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n","Selecting previously unselected package freeglut3-dev:amd64.\n","Preparing to unpack .../15-freeglut3-dev_2.8.1-6_amd64.deb ...\n","Unpacking freeglut3-dev:amd64 (2.8.1-6) ...\n","Setting up freeglut3:amd64 (2.8.1-6) ...\n","Setting up libglvnd-core-dev:amd64 (1.4.0-1) ...\n","Setting up libice-dev:amd64 (2:1.0.10-1build2) ...\n","Setting up libsm-dev:amd64 (2:1.2.3-1build2) ...\n","Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n","Setting up libgles1:amd64 (1.4.0-1) ...\n","Setting up libglx-dev:amd64 (1.4.0-1) ...\n","Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n","Setting up libopengl-dev:amd64 (1.4.0-1) ...\n","Setting up libgl-dev:amd64 (1.4.0-1) ...\n","Setting up libegl-dev:amd64 (1.4.0-1) ...\n","Setting up libglu1-mesa-dev:amd64 (9.0.2-1) ...\n","Setting up libgles-dev:amd64 (1.4.0-1) ...\n","Setting up libglvnd-dev:amd64 (1.4.0-1) ...\n","Setting up libgl1-mesa-dev:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Setting up freeglut3-dev:amd64 (2.8.1-6) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n"]}],"source":["!sudo apt-get install freeglut3-dev"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cgGdQ6n9EERW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699028650916,"user_tz":-60,"elapsed":19474,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"1a8a850a-5487-430a-c005-6e7281f606e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n","  xserver-common\n","The following NEW packages will be installed:\n","  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n","  xserver-common xvfb\n","0 upgraded, 9 newly installed, 0 to remove and 19 not upgraded.\n","Need to get 7,814 kB of archives.\n","After this operation, 11.9 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.2 [28.1 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.2 [864 kB]\n","Fetched 7,814 kB in 2s (4,473 kB/s)\n","Selecting previously unselected package libfontenc1:amd64.\n","(Reading database ... 121332 files and directories currently installed.)\n","Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n","Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n","Selecting previously unselected package libxfont2:amd64.\n","Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n","Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n","Selecting previously unselected package libxkbfile1:amd64.\n","Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n","Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n","Selecting previously unselected package x11-xkb-utils.\n","Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n","Unpacking x11-xkb-utils (7.7+5build4) ...\n","Selecting previously unselected package xfonts-encodings.\n","Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n","Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n","Selecting previously unselected package xfonts-utils.\n","Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n","Unpacking xfonts-utils (1:7.7+6build2) ...\n","Selecting previously unselected package xfonts-base.\n","Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n","Unpacking xfonts-base (1:1.0.5) ...\n","Selecting previously unselected package xserver-common.\n","Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.2_all.deb ...\n","Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n","Selecting previously unselected package xvfb.\n","Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.2_amd64.deb ...\n","Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.2) ...\n","Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n","Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n","Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n","Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n","Setting up x11-xkb-utils (7.7+5build4) ...\n","Setting up xfonts-utils (1:7.7+6build2) ...\n","Setting up xfonts-base (1:1.0.5) ...\n","Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n","Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.2) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","Collecting pyvirtualdisplay\n","  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n","Installing collected packages: pyvirtualdisplay\n","Successfully installed pyvirtualdisplay-3.0\n","Collecting piglet\n","  Downloading piglet-1.0.0-py2.py3-none-any.whl (2.2 kB)\n","Collecting piglet-templates (from piglet)\n","  Downloading piglet_templates-1.3.0-py3-none-any.whl (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.5/67.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from piglet-templates->piglet) (3.1.1)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from piglet-templates->piglet) (23.1.0)\n","Requirement already satisfied: astunparse in /usr/local/lib/python3.10/dist-packages (from piglet-templates->piglet) (1.6.3)\n","Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from piglet-templates->piglet) (2.1.3)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse->piglet-templates->piglet) (0.41.2)\n","Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from astunparse->piglet-templates->piglet) (1.16.0)\n","Installing collected packages: piglet-templates, piglet\n","Successfully installed piglet-1.0.0 piglet-templates-1.3.0\n"]}],"source":["!apt install xvfb -y\n","!pip install pyvirtualdisplay\n","!pip install piglet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5OaWkBSmhm6R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699028664956,"user_tz":-60,"elapsed":14086,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"1d64149e-7c5b-4a99-b465-8c579dc45874"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting supersuit\n","  Downloading SuperSuit-3.9.0-py3-none-any.whl (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from supersuit) (1.23.5)\n","Requirement already satisfied: gymnasium>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from supersuit) (0.29.1)\n","Collecting tinyscaler>=1.2.6 (from supersuit)\n","  Downloading tinyscaler-1.2.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (517 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.1/517.1 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (1.6.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (0.0.4)\n","Installing collected packages: tinyscaler, supersuit\n","Successfully installed supersuit-3.9.0 tinyscaler-1.2.7\n","Collecting stable_baselines3\n","  Downloading stable_baselines3-2.1.0-py3-none-any.whl (178 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (0.29.1)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.23.5)\n","Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.1.0+cu118)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.6.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.5.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (3.7.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (0.0.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.12.4)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2.1.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.1.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (4.43.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (23.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable_baselines3) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable_baselines3) (1.3.0)\n","Installing collected packages: stable_baselines3\n","Successfully installed stable_baselines3-2.1.0\n"]}],"source":["!pip install supersuit\n","!pip install stable_baselines3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"thmOvcHdjKHw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699028671610,"user_tz":-60,"elapsed":6702,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"13225a1c-ea4f-4815-92d4-031fd0db5469"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting shimmy>=0.2.1\n","  Downloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from shimmy>=0.2.1) (1.23.5)\n","Requirement already satisfied: gymnasium>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from shimmy>=0.2.1) (0.29.1)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy>=0.2.1) (1.6.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy>=0.2.1) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy>=0.2.1) (0.0.4)\n","Installing collected packages: shimmy\n","Successfully installed shimmy-1.3.0\n"]}],"source":["!pip install 'shimmy>=0.2.1'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k0iVvep_spQz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699028693122,"user_tz":-60,"elapsed":21519,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"9004e1cf-9f23-4079-857d-22649bcfddb5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tf-agents-nightly\n","  Downloading tf_agents_nightly-0.19.0.dev20231010-py3-none-any.whl (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.4.0)\n","Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.6.0)\n","Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (0.5.0)\n","Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (0.17.3)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.23.5)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (9.4.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.16.0)\n","Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (3.20.3)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.14.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (4.5.0)\n","Collecting pygame==2.1.3 (from tf-agents-nightly)\n","  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tfp-nightly (from tf-agents-nightly)\n","  Downloading tfp_nightly-0.23.0.dev20231103-py2.py3-none-any.whl (6.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents-nightly) (1.11.3)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents-nightly) (1.5.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tfp-nightly->tf-agents-nightly) (4.4.2)\n","Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tfp-nightly->tf-agents-nightly) (0.5.4)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tfp-nightly->tf-agents-nightly) (0.1.8)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<=0.23.0,>=0.17.0->tf-agents-nightly) (0.18.3)\n","Installing collected packages: tfp-nightly, pygame, tf-agents-nightly\n","  Attempting uninstall: pygame\n","    Found existing installation: pygame 2.3.0\n","    Uninstalling pygame-2.3.0:\n","      Successfully uninstalled pygame-2.3.0\n","Successfully installed pygame-2.1.3 tf-agents-nightly-0.19.0.dev20231010 tfp-nightly-0.23.0.dev20231103\n"]}],"source":["!pip install tf-agents-nightly"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UlXxViz9tdvH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699028964267,"user_tz":-60,"elapsed":271193,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"d3f9481a-a992-46c9-8c6b-db3e07bf2838"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: tensorflow 2.8.0\n","Uninstalling tensorflow-2.8.0:\n","  Would remove:\n","    /usr/local/bin/estimator_ckpt_converter\n","    /usr/local/bin/import_pb_to_tensorboard\n","    /usr/local/bin/saved_model_cli\n","    /usr/local/bin/tensorboard\n","    /usr/local/bin/tf_upgrade_v2\n","    /usr/local/bin/tflite_convert\n","    /usr/local/bin/toco\n","    /usr/local/bin/toco_from_protos\n","    /usr/local/lib/python3.10/dist-packages/tensorflow-2.8.0.dist-info/*\n","    /usr/local/lib/python3.10/dist-packages/tensorflow/*\n","Proceed (Y/n)? Y\n","  Successfully uninstalled tensorflow-2.8.0\n"]}],"source":["!pip uninstall tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dsWlVQ6MtKLj"},"outputs":[],"source":["!pip install tensorflow>=2.13"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wE5AiVtFtZDc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699029011309,"user_tz":-60,"elapsed":4426,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"9a46b125-faac-494e-caf4-006658742bfc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Name: tensorflow\n","Version: 2.14.0\n","Summary: TensorFlow is an open source machine learning framework for everyone.\n","Home-page: https://www.tensorflow.org/\n","Author: Google Inc.\n","Author-email: packages@tensorflow.org\n","License: Apache 2.0\n","Location: /usr/local/lib/python3.10/dist-packages\n","Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, libclang, ml-dtypes, numpy, opt-einsum, packaging, protobuf, setuptools, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n","Required-by: dopamine-rl, keras-rl2\n"]}],"source":["!pip show tensorflow"]},{"cell_type":"code","source":["!pip install pettingzoo[sisl]"],"metadata":{"id":"PZa1qybXZKSX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699029079175,"user_tz":-60,"elapsed":67873,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"2d71cfa2-d123-4c5a-e577-713cc15aa6d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pettingzoo[sisl] in /usr/local/lib/python3.10/dist-packages (1.24.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[sisl]) (1.23.5)\n","Requirement already satisfied: gymnasium>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[sisl]) (0.29.1)\n","Collecting pygame==2.3.0 (from pettingzoo[sisl])\n","  Using cached pygame-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\n","Requirement already satisfied: pymunk==6.2.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[sisl]) (6.2.0)\n","Collecting box2d-py==2.3.5 (from pettingzoo[sisl])\n","  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[sisl]) (1.11.3)\n","Requirement already satisfied: cffi>1.14.0 in /usr/local/lib/python3.10/dist-packages (from pymunk==6.2.0->pettingzoo[sisl]) (1.16.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[sisl]) (1.6.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[sisl]) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[sisl]) (0.0.4)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>1.14.0->pymunk==6.2.0->pettingzoo[sisl]) (2.21)\n","Building wheels for collected packages: box2d-py\n","  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2373072 sha256=d17f587fda2899abfba420d17a2022883c929ca7d1449b62b75a3980f8b36204\n","  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n","Successfully built box2d-py\n","Installing collected packages: box2d-py, pygame\n","  Attempting uninstall: box2d-py\n","    Found existing installation: box2d-py 2.3.8\n","    Uninstalling box2d-py-2.3.8:\n","      Successfully uninstalled box2d-py-2.3.8\n","  Attempting uninstall: pygame\n","    Found existing installation: pygame 2.1.3\n","    Uninstalling pygame-2.1.3:\n","      Successfully uninstalled pygame-2.1.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tf-agents-nightly 0.19.0.dev20231010 requires pygame==2.1.3, but you have pygame 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed box2d-py-2.3.5 pygame-2.3.0\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"NFzawQ1QZFwj"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JpA_YhKzCeC2"},"outputs":[],"source":["############################# Código para entrenar Multiwalker ######################"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tnz3fJDA33w8"},"outputs":[],"source":["# from stable_baselines3.dqn import MlpPolicy,CnnPolicy\n","from stable_baselines3 import SAC\n","from pettingzoo.sisl import multiwalker_v9\n","import supersuit as ss\n","from stable_baselines3.common.monitor import Monitor"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":218,"status":"ok","timestamp":1697644598142,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"6-kdhb3CI5VC","outputId":"4db87993-1f4b-464a-f0f0-20ed31aa0c34"},"outputs":[{"output_type":"stream","name":"stdout","text":["drive  gdrive  sample_data\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":395,"status":"ok","timestamp":1697644599460,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"04CbnRTvI9L2","outputId":"347477dc-2a63-4315-a08f-182fdf724421"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/TFM\n"]}],"source":["cd /content/drive/MyDrive/TFM"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":305,"status":"ok","timestamp":1697644601324,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"bUEH8254I_kb","outputId":"406e2b97-e693-46da-9e49-9d56c991f4e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["'=2.13'\t\t\t\t     multiwalker_sac_log_eval\n"," DQN_new_pettingzoo_gym_cap.ipynb    policy_log_eval\n"," DQN_policies\t\t\t     PPO_policies\n","'Entrenamientos antiguos sin logs'   results_rllib\n"," Entrenamientos_log_no_eval\t     TFM_Multiwalker_DDPG_gym_cap.ipynb\n"," MCR_TFM.ipynb\t\t\t     TFM_Multiwalker_SAC_gym_cap.ipynb\n"," multi_car_racing\t\t     TFM_PPO_new_pettingzoo_gym_cap.ipynb\n"," multiwalker_ddpg_log_eval\t     TFM_PPO_pettingzoo_gym_cap.ipynb\n"," multiwalker_ddpg.zip\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2HubK-2G3_vH"},"outputs":[],"source":["env = multiwalker_v9.parallel_env(render_mode=\"rgb_array\",shared_reward=0)\n","\n","# #---------------------------------\n","# log_dir = \"./policy_log\"\n","# os.makedirs(log_dir, exist_ok=True)\n","# env = Monitor(env, log_dir)\n","# #---------------------------------\n","# env = ss.color_reduction_v0(env, mode='B')\n","# env = ss.black_death_v3(env)\n","# env = ss.resize_v1(env, x_size=84, y_size=84)\n","env = ss.frame_stack_v1(env, 3)\n","env = ss.pettingzoo_env_to_vec_env_v1(env)\n","env = ss.concat_vec_envs_v1(env, 8, num_cpus=4, base_class='stable_baselines3')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"foI4bTFGbQo1"},"outputs":[],"source":["from stable_baselines3.common.callbacks import EvalCallback\n","eval_callback = EvalCallback(env, best_model_save_path=\"./multiwalker_sac_00_log_eval/\",\n","                             log_path=\"./multiwalker_sac_00_log_eval/\", eval_freq=100,\n","                             deterministic=False, render=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ib9VPIVgec47","outputId":"7e4baa28-070d-4d67-d056-5009ad589ecf","executionInfo":{"status":"ok","timestamp":1699013128287,"user_tz":-60,"elapsed":13140389,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Logging to multiwalker_sac_00_log_eval/\n","Using cuda device\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 10       |\n","|    fps             | 287      |\n","|    time_elapsed    | 5        |\n","|    total_timesteps | 1704     |\n","| train/             |          |\n","|    actor_loss      | -5.3     |\n","|    critic_loss     | 0.238    |\n","|    ent_coef        | 0.981    |\n","|    ent_coef_loss   | -0.131   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 66       |\n","---------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 139795   |\n","---------------------------------\n","Eval num_timesteps=3357600, episode_reward=90.76 +/- 1.20\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 90.8     |\n","| time/              |          |\n","|    total_timesteps | 3357600  |\n","| train/             |          |\n","|    actor_loss      | -2.82    |\n","|    critic_loss     | 0.282    |\n","|    ent_coef        | 0.00323  |\n","|    ent_coef_loss   | -3.47    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 139895   |\n","---------------------------------\n","Eval num_timesteps=3360000, episode_reward=4.96 +/- 72.61\n","Episode length: 399.20 +/- 82.30\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 399      |\n","|    mean_reward     | 4.96     |\n","| time/              |          |\n","|    total_timesteps | 3360000  |\n","| train/             |          |\n","|    actor_loss      | -2.89    |\n","|    critic_loss     | 0.712    |\n","|    ent_coef        | 0.00319  |\n","|    ent_coef_loss   | 1.47     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 139995   |\n","---------------------------------\n","Eval num_timesteps=3362400, episode_reward=42.79 +/- 71.09\n","Episode length: 433.60 +/- 81.32\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 434      |\n","|    mean_reward     | 42.8     |\n","| time/              |          |\n","|    total_timesteps | 3362400  |\n","| train/             |          |\n","|    actor_loss      | -1.55    |\n","|    critic_loss     | 1.29     |\n","|    ent_coef        | 0.0032   |\n","|    ent_coef_loss   | 1.63     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140095   |\n","---------------------------------\n","Eval num_timesteps=3364800, episode_reward=-98.60 +/- 6.15\n","Episode length: 75.00 +/- 2.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 75       |\n","|    mean_reward     | -98.6    |\n","| time/              |          |\n","|    total_timesteps | 3364800  |\n","| train/             |          |\n","|    actor_loss      | -2.45    |\n","|    critic_loss     | 1        |\n","|    ent_coef        | 0.00315  |\n","|    ent_coef_loss   | -3.02    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5670     |\n","|    fps             | 306      |\n","|    time_elapsed    | 10967    |\n","|    total_timesteps | 3366912  |\n","| train/             |          |\n","|    actor_loss      | -3.19    |\n","|    critic_loss     | 0.603    |\n","|    ent_coef        | 0.00312  |\n","|    ent_coef_loss   | -0.0445  |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140283   |\n","---------------------------------\n","Eval num_timesteps=3367200, episode_reward=15.64 +/- 59.31\n","Episode length: 495.20 +/- 3.92\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 495      |\n","|    mean_reward     | 15.6     |\n","| time/              |          |\n","|    total_timesteps | 3367200  |\n","| train/             |          |\n","|    actor_loss      | -2.89    |\n","|    critic_loss     | 0.63     |\n","|    ent_coef        | 0.00312  |\n","|    ent_coef_loss   | 2.31     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140295   |\n","---------------------------------\n","Eval num_timesteps=3369600, episode_reward=44.39 +/- 71.56\n","Episode length: 437.60 +/- 76.42\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 438      |\n","|    mean_reward     | 44.4     |\n","| time/              |          |\n","|    total_timesteps | 3369600  |\n","| train/             |          |\n","|    actor_loss      | -3       |\n","|    critic_loss     | 0.516    |\n","|    ent_coef        | 0.00315  |\n","|    ent_coef_loss   | -2.34    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140395   |\n","---------------------------------\n","Eval num_timesteps=3372000, episode_reward=57.16 +/- 11.89\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 57.2     |\n","| time/              |          |\n","|    total_timesteps | 3372000  |\n","| train/             |          |\n","|    actor_loss      | -2.29    |\n","|    critic_loss     | 1.25     |\n","|    ent_coef        | 0.00316  |\n","|    ent_coef_loss   | 0.772    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140495   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5680     |\n","|    fps             | 306      |\n","|    time_elapsed    | 10994    |\n","|    total_timesteps | 3373872  |\n","| train/             |          |\n","|    actor_loss      | -2.75    |\n","|    critic_loss     | 0.341    |\n","|    ent_coef        | 0.00314  |\n","|    ent_coef_loss   | -0.0104  |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140573   |\n","---------------------------------\n","Eval num_timesteps=3374400, episode_reward=41.26 +/- 58.76\n","Episode length: 475.20 +/- 30.37\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 475      |\n","|    mean_reward     | 41.3     |\n","| time/              |          |\n","|    total_timesteps | 3374400  |\n","| train/             |          |\n","|    actor_loss      | -2.36    |\n","|    critic_loss     | 0.567    |\n","|    ent_coef        | 0.00313  |\n","|    ent_coef_loss   | -1.23    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140595   |\n","---------------------------------\n","Eval num_timesteps=3376800, episode_reward=11.22 +/- 95.41\n","Episode length: 350.00 +/- 183.71\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 350      |\n","|    mean_reward     | 11.2     |\n","| time/              |          |\n","|    total_timesteps | 3376800  |\n","| train/             |          |\n","|    actor_loss      | -3.27    |\n","|    critic_loss     | 0.585    |\n","|    ent_coef        | 0.00315  |\n","|    ent_coef_loss   | 2.05     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140695   |\n","---------------------------------\n","Eval num_timesteps=3379200, episode_reward=-2.16 +/- 51.76\n","Episode length: 474.80 +/- 20.58\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 475      |\n","|    mean_reward     | -2.16    |\n","| time/              |          |\n","|    total_timesteps | 3379200  |\n","| train/             |          |\n","|    actor_loss      | -3.01    |\n","|    critic_loss     | 0.193    |\n","|    ent_coef        | 0.00325  |\n","|    ent_coef_loss   | -1.77    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140795   |\n","---------------------------------\n","Eval num_timesteps=3381600, episode_reward=-56.56 +/- 22.48\n","Episode length: 305.60 +/- 61.73\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 306      |\n","|    mean_reward     | -56.6    |\n","| time/              |          |\n","|    total_timesteps | 3381600  |\n","| train/             |          |\n","|    actor_loss      | -3.12    |\n","|    critic_loss     | 2.69     |\n","|    ent_coef        | 0.0032   |\n","|    ent_coef_loss   | 2.51     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140895   |\n","---------------------------------\n","Eval num_timesteps=3384000, episode_reward=100.90 +/- 0.56\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 101      |\n","| time/              |          |\n","|    total_timesteps | 3384000  |\n","| train/             |          |\n","|    actor_loss      | -2.69    |\n","|    critic_loss     | 0.323    |\n","|    ent_coef        | 0.00324  |\n","|    ent_coef_loss   | -2.75    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5690     |\n","|    fps             | 306      |\n","|    time_elapsed    | 11033    |\n","|    total_timesteps | 3385800  |\n","| train/             |          |\n","|    actor_loss      | -2.68    |\n","|    critic_loss     | 35.3     |\n","|    ent_coef        | 0.00319  |\n","|    ent_coef_loss   | -3.28    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141070   |\n","---------------------------------\n","Eval num_timesteps=3386400, episode_reward=-19.59 +/- 100.73\n","Episode length: 246.80 +/- 206.74\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 247      |\n","|    mean_reward     | -19.6    |\n","| time/              |          |\n","|    total_timesteps | 3386400  |\n","| train/             |          |\n","|    actor_loss      | -2.88    |\n","|    critic_loss     | 0.65     |\n","|    ent_coef        | 0.00317  |\n","|    ent_coef_loss   | -3.28    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141095   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5700     |\n","|    fps             | 306      |\n","|    time_elapsed    | 11043    |\n","|    total_timesteps | 3388440  |\n","| train/             |          |\n","|    actor_loss      | -2.77    |\n","|    critic_loss     | 1.1      |\n","|    ent_coef        | 0.0031   |\n","|    ent_coef_loss   | -1.03    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141180   |\n","---------------------------------\n","Eval num_timesteps=3388800, episode_reward=107.56 +/- 2.50\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 108      |\n","| time/              |          |\n","|    total_timesteps | 3388800  |\n","| train/             |          |\n","|    actor_loss      | -2.78    |\n","|    critic_loss     | 0.396    |\n","|    ent_coef        | 0.00308  |\n","|    ent_coef_loss   | -4.79    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141195   |\n","---------------------------------\n","Eval num_timesteps=3391200, episode_reward=-29.38 +/- 7.88\n","Episode length: 457.60 +/- 20.09\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 458      |\n","|    mean_reward     | -29.4    |\n","| time/              |          |\n","|    total_timesteps | 3391200  |\n","| train/             |          |\n","|    actor_loss      | -1.94    |\n","|    critic_loss     | 0.706    |\n","|    ent_coef        | 0.00302  |\n","|    ent_coef_loss   | 5.18     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5710     |\n","|    fps             | 306      |\n","|    time_elapsed    | 11058    |\n","|    total_timesteps | 3391824  |\n","| train/             |          |\n","|    actor_loss      | -2.93    |\n","|    critic_loss     | 0.395    |\n","|    ent_coef        | 0.00303  |\n","|    ent_coef_loss   | -0.928   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141321   |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    episodes        | 5720    |\n","|    fps             | 306     |\n","|    time_elapsed    | 11058   |\n","|    total_timesteps | 3391824 |\n","--------------------------------\n","Eval num_timesteps=3393600, episode_reward=94.52 +/- 0.15\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 94.5     |\n","| time/              |          |\n","|    total_timesteps | 3393600  |\n","| train/             |          |\n","|    actor_loss      | -2.87    |\n","|    critic_loss     | 0.409    |\n","|    ent_coef        | 0.00305  |\n","|    ent_coef_loss   | 0.715    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141395   |\n","---------------------------------\n","Eval num_timesteps=3396000, episode_reward=-12.52 +/- 101.67\n","Episode length: 259.40 +/- 196.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 259      |\n","|    mean_reward     | -12.5    |\n","| time/              |          |\n","|    total_timesteps | 3396000  |\n","| train/             |          |\n","|    actor_loss      | -2.6     |\n","|    critic_loss     | 10.9     |\n","|    ent_coef        | 0.00307  |\n","|    ent_coef_loss   | -3.64    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141495   |\n","---------------------------------\n","Eval num_timesteps=3398400, episode_reward=-21.80 +/- 93.48\n","Episode length: 264.20 +/- 192.53\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 264      |\n","|    mean_reward     | -21.8    |\n","| time/              |          |\n","|    total_timesteps | 3398400  |\n","| train/             |          |\n","|    actor_loss      | -2.7     |\n","|    critic_loss     | 1.04     |\n","|    ent_coef        | 0.003    |\n","|    ent_coef_loss   | 0.819    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141595   |\n","---------------------------------\n","Eval num_timesteps=3400800, episode_reward=-57.93 +/- 7.87\n","Episode length: 300.40 +/- 2.94\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 300      |\n","|    mean_reward     | -57.9    |\n","| time/              |          |\n","|    total_timesteps | 3400800  |\n","| train/             |          |\n","|    actor_loss      | -3.24    |\n","|    critic_loss     | 33       |\n","|    ent_coef        | 0.00295  |\n","|    ent_coef_loss   | -4.4     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5730     |\n","|    fps             | 306      |\n","|    time_elapsed    | 11090    |\n","|    total_timesteps | 3401448  |\n","| train/             |          |\n","|    actor_loss      | -3.01    |\n","|    critic_loss     | 0.783    |\n","|    ent_coef        | 0.00293  |\n","|    ent_coef_loss   | -0.801   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141722   |\n","---------------------------------\n","Eval num_timesteps=3403200, episode_reward=-80.21 +/- 10.71\n","Episode length: 226.80 +/- 37.72\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 227      |\n","|    mean_reward     | -80.2    |\n","| time/              |          |\n","|    total_timesteps | 3403200  |\n","| train/             |          |\n","|    actor_loss      | -2.71    |\n","|    critic_loss     | 1.85     |\n","|    ent_coef        | 0.00296  |\n","|    ent_coef_loss   | 9.04     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141795   |\n","---------------------------------\n","Eval num_timesteps=3405600, episode_reward=-46.13 +/- 31.67\n","Episode length: 293.80 +/- 131.29\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 294      |\n","|    mean_reward     | -46.1    |\n","| time/              |          |\n","|    total_timesteps | 3405600  |\n","| train/             |          |\n","|    actor_loss      | -2.02    |\n","|    critic_loss     | 2.63     |\n","|    ent_coef        | 0.00312  |\n","|    ent_coef_loss   | -1.04    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141895   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5740     |\n","|    fps             | 306      |\n","|    time_elapsed    | 11103    |\n","|    total_timesteps | 3405624  |\n","| train/             |          |\n","|    actor_loss      | -3.1     |\n","|    critic_loss     | 0.626    |\n","|    ent_coef        | 0.00312  |\n","|    ent_coef_loss   | -4.85    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141896   |\n","---------------------------------\n","Eval num_timesteps=3408000, episode_reward=-3.16 +/- 87.84\n","Episode length: 352.80 +/- 180.28\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 353      |\n","|    mean_reward     | -3.16    |\n","| time/              |          |\n","|    total_timesteps | 3408000  |\n","| train/             |          |\n","|    actor_loss      | -3.1     |\n","|    critic_loss     | 0.589    |\n","|    ent_coef        | 0.00308  |\n","|    ent_coef_loss   | -1.19    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141995   |\n","---------------------------------\n","Eval num_timesteps=3410400, episode_reward=-63.49 +/- 18.48\n","Episode length: 406.20 +/- 60.26\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 406      |\n","|    mean_reward     | -63.5    |\n","| time/              |          |\n","|    total_timesteps | 3410400  |\n","| train/             |          |\n","|    actor_loss      | -2.83    |\n","|    critic_loss     | 0.462    |\n","|    ent_coef        | 0.00309  |\n","|    ent_coef_loss   | 0.842    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 142095   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5750     |\n","|    fps             | 306      |\n","|    time_elapsed    | 11119    |\n","|    total_timesteps | 3410880  |\n","| train/             |          |\n","|    actor_loss      | -3.11    |\n","|    critic_loss     | 1.3      |\n","|    ent_coef        | 0.0031   |\n","|    ent_coef_loss   | 3.8      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 142115   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5760     |\n","|    fps             | 306      |\n","|    time_elapsed    | 11120    |\n","|    total_timesteps | 3411720  |\n","| train/             |          |\n","|    actor_loss      | -3.55    |\n","|    critic_loss     | 0.441    |\n","|    ent_coef        | 0.00311  |\n","|    ent_coef_loss   | -1.75    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 142150   |\n","---------------------------------\n","Eval num_timesteps=3412800, episode_reward=-1.96 +/- 85.26\n","Episode length: 347.60 +/- 186.65\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 348      |\n","|    mean_reward     | -1.96    |\n","| time/              |          |\n","|    total_timesteps | 3412800  |\n","| train/             |          |\n","|    actor_loss      | -2.26    |\n","|    critic_loss     | 1.02     |\n","|    ent_coef        | 0.00313  |\n","|    ent_coef_loss   | 1.32     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 142195   |\n","---------------------------------\n","Eval num_timesteps=3415200, episode_reward=78.34 +/- 1.88\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 78.3     |\n","| time/              |          |\n","|    total_timesteps | 3415200  |\n","| train/             |          |\n","|    actor_loss      | -3.22    |\n","|    critic_loss     | 0.309    |\n","|    ent_coef        | 0.00312  |\n","|    ent_coef_loss   | 2.04     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 142295   |\n","---------------------------------\n","Eval num_timesteps=3417600, episode_reward=-59.11 +/- 22.56\n","Episode length: 284.60 +/- 61.73\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 285      |\n","|    mean_reward     | -59.1    |\n","| time/              |          |\n","|    total_timesteps | 3417600  |\n","| train/             |          |\n","|    actor_loss      | -3.09    |\n","|    critic_loss     | 0.765    |\n","|    ent_coef        | 0.00317  |\n","|    ent_coef_loss   | -1.69    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 142395   |\n","---------------------------------\n","Eval num_timesteps=3420000, episode_reward=7.54 +/- 59.25\n","Episode length: 430.40 +/- 56.83\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 430      |\n","|    mean_reward     | 7.54     |\n","| time/              |          |\n","|    total_timesteps | 3420000  |\n","| train/             |          |\n","|    actor_loss      | -2.81    |\n","|    critic_loss     | 0.444    |\n","|    ent_coef        | 0.00317  |\n","|    ent_coef_loss   | 4.04     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 142495   |\n","---------------------------------\n","Eval num_timesteps=3422400, episode_reward=-92.19 +/- 9.15\n","Episode length: 162.60 +/- 29.88\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 163      |\n","|    mean_reward     | -92.2    |\n","| time/              |          |\n","|    total_timesteps | 3422400  |\n","| train/             |          |\n","|    actor_loss      | -2.59    |\n","|    critic_loss     | 0.571    |\n","|    ent_coef        | 0.00326  |\n","|    ent_coef_loss   | 2.22     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 142595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5770     |\n","|    fps             | 306      |\n","|    time_elapsed    | 11155    |\n","|    total_timesteps | 3423408  |\n","| train/             |          |\n","|    actor_loss      | -2.69    |\n","|    critic_loss     | 0.765    |\n","|    ent_coef        | 0.00328  |\n","|    ent_coef_loss   | 1.2      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 142637   |\n","---------------------------------\n","Eval num_timesteps=3424800, episode_reward=17.09 +/- 77.26\n","Episode length: 379.60 +/- 147.46\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 380      |\n","|    mean_reward     | 17.1     |\n","| time/              |          |\n","|    total_timesteps | 3424800  |\n","| train/             |          |\n","|    actor_loss      | -2.87    |\n","|    critic_loss     | 1.79     |\n","|    ent_coef        | 0.0033   |\n","|    ent_coef_loss   | 2.19     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 142695   |\n","---------------------------------\n","Eval num_timesteps=3427200, episode_reward=53.76 +/- 3.20\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 53.8     |\n","| time/              |          |\n","|    total_timesteps | 3427200  |\n","| train/             |          |\n","|    actor_loss      | -2.95    |\n","|    critic_loss     | 0.664    |\n","|    ent_coef        | 0.00344  |\n","|    ent_coef_loss   | 9.38     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 142795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5780     |\n","|    fps             | 306      |\n","|    time_elapsed    | 11172    |\n","|    total_timesteps | 3428376  |\n","| train/             |          |\n","|    actor_loss      | -2.49    |\n","|    critic_loss     | 11.6     |\n","|    ent_coef        | 0.00354  |\n","|    ent_coef_loss   | 5.36     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 142844   |\n","---------------------------------\n","Eval num_timesteps=3429600, episode_reward=68.30 +/- 17.86\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 68.3     |\n","| time/              |          |\n","|    total_timesteps | 3429600  |\n","| train/             |          |\n","|    actor_loss      | -2.85    |\n","|    critic_loss     | 0.486    |\n","|    ent_coef        | 0.00361  |\n","|    ent_coef_loss   | 1.45     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 142895   |\n","---------------------------------\n","Eval num_timesteps=3432000, episode_reward=1.53 +/- 56.66\n","Episode length: 460.40 +/- 48.50\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 460      |\n","|    mean_reward     | 1.53     |\n","| time/              |          |\n","|    total_timesteps | 3432000  |\n","| train/             |          |\n","|    actor_loss      | -2.89    |\n","|    critic_loss     | 0.431    |\n","|    ent_coef        | 0.00365  |\n","|    ent_coef_loss   | 3.41     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 142995   |\n","---------------------------------\n","Eval num_timesteps=3434400, episode_reward=95.86 +/- 0.27\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 95.9     |\n","| time/              |          |\n","|    total_timesteps | 3434400  |\n","| train/             |          |\n","|    actor_loss      | -1.86    |\n","|    critic_loss     | 1.57     |\n","|    ent_coef        | 0.00366  |\n","|    ent_coef_loss   | -2.35    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 143095   |\n","---------------------------------\n","Eval num_timesteps=3436800, episode_reward=79.04 +/- 0.46\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 79       |\n","| time/              |          |\n","|    total_timesteps | 3436800  |\n","| train/             |          |\n","|    actor_loss      | -2.68    |\n","|    critic_loss     | 0.573    |\n","|    ent_coef        | 0.00365  |\n","|    ent_coef_loss   | -2.86    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 143195   |\n","---------------------------------\n","Eval num_timesteps=3439200, episode_reward=96.37 +/- 3.07\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 96.4     |\n","| time/              |          |\n","|    total_timesteps | 3439200  |\n","| train/             |          |\n","|    actor_loss      | -3.07    |\n","|    critic_loss     | 4.21     |\n","|    ent_coef        | 0.00365  |\n","|    ent_coef_loss   | -1.56    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 143295   |\n","---------------------------------\n","Eval num_timesteps=3441600, episode_reward=86.46 +/- 3.79\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 86.5     |\n","| time/              |          |\n","|    total_timesteps | 3441600  |\n","| train/             |          |\n","|    actor_loss      | -3.51    |\n","|    critic_loss     | 0.309    |\n","|    ent_coef        | 0.00366  |\n","|    ent_coef_loss   | -1.58    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 143395   |\n","---------------------------------\n","Eval num_timesteps=3444000, episode_reward=19.27 +/- 59.74\n","Episode length: 465.20 +/- 42.62\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 465      |\n","|    mean_reward     | 19.3     |\n","| time/              |          |\n","|    total_timesteps | 3444000  |\n","| train/             |          |\n","|    actor_loss      | -3.45    |\n","|    critic_loss     | 0.488    |\n","|    ent_coef        | 0.00365  |\n","|    ent_coef_loss   | -0.115   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 143495   |\n","---------------------------------\n","Eval num_timesteps=3446400, episode_reward=27.97 +/- 9.59\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 28       |\n","| time/              |          |\n","|    total_timesteps | 3446400  |\n","| train/             |          |\n","|    actor_loss      | -3.52    |\n","|    critic_loss     | 0.284    |\n","|    ent_coef        | 0.00373  |\n","|    ent_coef_loss   | -2.2     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 143595   |\n","---------------------------------\n","Eval num_timesteps=3448800, episode_reward=-39.16 +/- 53.65\n","Episode length: 376.40 +/- 151.38\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 376      |\n","|    mean_reward     | -39.2    |\n","| time/              |          |\n","|    total_timesteps | 3448800  |\n","| train/             |          |\n","|    actor_loss      | -3.71    |\n","|    critic_loss     | 0.372    |\n","|    ent_coef        | 0.00374  |\n","|    ent_coef_loss   | -4.36    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 143695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5790     |\n","|    fps             | 306      |\n","|    time_elapsed    | 11254    |\n","|    total_timesteps | 3450480  |\n","| train/             |          |\n","|    actor_loss      | -2.06    |\n","|    critic_loss     | 1.7      |\n","|    ent_coef        | 0.00377  |\n","|    ent_coef_loss   | 0.431    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 143765   |\n","---------------------------------\n","Eval num_timesteps=3451200, episode_reward=-65.97 +/- 6.20\n","Episode length: 301.20 +/- 21.07\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 301      |\n","|    mean_reward     | -66      |\n","| time/              |          |\n","|    total_timesteps | 3451200  |\n","| train/             |          |\n","|    actor_loss      | -2.44    |\n","|    critic_loss     | 0.921    |\n","|    ent_coef        | 0.00377  |\n","|    ent_coef_loss   | 0.543    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 143795   |\n","---------------------------------\n","Eval num_timesteps=3453600, episode_reward=7.89 +/- 100.02\n","Episode length: 371.60 +/- 104.84\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 372      |\n","|    mean_reward     | 7.89     |\n","| time/              |          |\n","|    total_timesteps | 3453600  |\n","| train/             |          |\n","|    actor_loss      | -1.86    |\n","|    critic_loss     | 5.57     |\n","|    ent_coef        | 0.00367  |\n","|    ent_coef_loss   | -2.64    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 143895   |\n","---------------------------------\n","Eval num_timesteps=3456000, episode_reward=-67.24 +/- 17.42\n","Episode length: 235.80 +/- 45.56\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 236      |\n","|    mean_reward     | -67.2    |\n","| time/              |          |\n","|    total_timesteps | 3456000  |\n","| train/             |          |\n","|    actor_loss      | -2.1     |\n","|    critic_loss     | 0.678    |\n","|    ent_coef        | 0.0036   |\n","|    ent_coef_loss   | 0.962    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 143995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5800     |\n","|    fps             | 306      |\n","|    time_elapsed    | 11273    |\n","|    total_timesteps | 3456408  |\n","| train/             |          |\n","|    actor_loss      | -3       |\n","|    critic_loss     | 0.478    |\n","|    ent_coef        | 0.0036   |\n","|    ent_coef_loss   | 0.544    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 144012   |\n","---------------------------------\n","Eval num_timesteps=3458400, episode_reward=3.64 +/- 62.77\n","Episode length: 461.60 +/- 31.35\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 462      |\n","|    mean_reward     | 3.64     |\n","| time/              |          |\n","|    total_timesteps | 3458400  |\n","| train/             |          |\n","|    actor_loss      | -3.41    |\n","|    critic_loss     | 0.733    |\n","|    ent_coef        | 0.00359  |\n","|    ent_coef_loss   | 0.548    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 144095   |\n","---------------------------------\n","Eval num_timesteps=3460800, episode_reward=71.42 +/- 1.79\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 71.4     |\n","| time/              |          |\n","|    total_timesteps | 3460800  |\n","| train/             |          |\n","|    actor_loss      | -3.19    |\n","|    critic_loss     | 0.669    |\n","|    ent_coef        | 0.00362  |\n","|    ent_coef_loss   | -3.15    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 144195   |\n","---------------------------------\n","Eval num_timesteps=3463200, episode_reward=-28.25 +/- 10.15\n","Episode length: 451.20 +/- 23.03\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 451      |\n","|    mean_reward     | -28.2    |\n","| time/              |          |\n","|    total_timesteps | 3463200  |\n","| train/             |          |\n","|    actor_loss      | -2.63    |\n","|    critic_loss     | 1.85     |\n","|    ent_coef        | 0.0036   |\n","|    ent_coef_loss   | 0.643    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 144295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5810     |\n","|    fps             | 306      |\n","|    time_elapsed    | 11299    |\n","|    total_timesteps | 3464568  |\n","| train/             |          |\n","|    actor_loss      | -2.49    |\n","|    critic_loss     | 0.837    |\n","|    ent_coef        | 0.00361  |\n","|    ent_coef_loss   | 0.999    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 144352   |\n","---------------------------------\n","Eval num_timesteps=3465600, episode_reward=-94.05 +/- 15.90\n","Episode length: 205.60 +/- 99.94\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 206      |\n","|    mean_reward     | -94.1    |\n","| time/              |          |\n","|    total_timesteps | 3465600  |\n","| train/             |          |\n","|    actor_loss      | -3.19    |\n","|    critic_loss     | 1.21     |\n","|    ent_coef        | 0.00363  |\n","|    ent_coef_loss   | 5.78     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 144395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5820     |\n","|    fps             | 306      |\n","|    time_elapsed    | 11303    |\n","|    total_timesteps | 3466584  |\n","| train/             |          |\n","|    actor_loss      | -3.19    |\n","|    critic_loss     | 0.849    |\n","|    ent_coef        | 0.00368  |\n","|    ent_coef_loss   | -1.75    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 144436   |\n","---------------------------------\n","Eval num_timesteps=3468000, episode_reward=-1.14 +/- 84.18\n","Episode length: 386.60 +/- 92.59\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 387      |\n","|    mean_reward     | -1.14    |\n","| time/              |          |\n","|    total_timesteps | 3468000  |\n","| train/             |          |\n","|    actor_loss      | -2.42    |\n","|    critic_loss     | 0.988    |\n","|    ent_coef        | 0.0037   |\n","|    ent_coef_loss   | 2.19     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 144495   |\n","---------------------------------\n","Eval num_timesteps=3470400, episode_reward=96.19 +/- 2.54\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 96.2     |\n","| time/              |          |\n","|    total_timesteps | 3470400  |\n","| train/             |          |\n","|    actor_loss      | -2.08    |\n","|    critic_loss     | 1.36     |\n","|    ent_coef        | 0.00366  |\n","|    ent_coef_loss   | 1.65     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 144595   |\n","---------------------------------\n","Eval num_timesteps=3472800, episode_reward=72.84 +/- 2.27\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 72.8     |\n","| time/              |          |\n","|    total_timesteps | 3472800  |\n","| train/             |          |\n","|    actor_loss      | -3.19    |\n","|    critic_loss     | 1.43     |\n","|    ent_coef        | 0.00362  |\n","|    ent_coef_loss   | -4.15    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 144695   |\n","---------------------------------\n","Eval num_timesteps=3475200, episode_reward=91.06 +/- 5.68\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 91.1     |\n","| time/              |          |\n","|    total_timesteps | 3475200  |\n","| train/             |          |\n","|    actor_loss      | -2.42    |\n","|    critic_loss     | 0.914    |\n","|    ent_coef        | 0.00366  |\n","|    ent_coef_loss   | -1.53    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 144795   |\n","---------------------------------\n","Eval num_timesteps=3477600, episode_reward=83.40 +/- 0.75\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 83.4     |\n","| time/              |          |\n","|    total_timesteps | 3477600  |\n","| train/             |          |\n","|    actor_loss      | -2.4     |\n","|    critic_loss     | 2.26     |\n","|    ent_coef        | 0.00358  |\n","|    ent_coef_loss   | -2.63    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 144895   |\n","---------------------------------\n","Eval num_timesteps=3480000, episode_reward=82.94 +/- 5.60\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 82.9     |\n","| time/              |          |\n","|    total_timesteps | 3480000  |\n","| train/             |          |\n","|    actor_loss      | -2.37    |\n","|    critic_loss     | 0.727    |\n","|    ent_coef        | 0.00353  |\n","|    ent_coef_loss   | -1.97    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 144995   |\n","---------------------------------\n","Eval num_timesteps=3482400, episode_reward=19.48 +/- 65.30\n","Episode length: 461.60 +/- 31.35\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 462      |\n","|    mean_reward     | 19.5     |\n","| time/              |          |\n","|    total_timesteps | 3482400  |\n","| train/             |          |\n","|    actor_loss      | -3.43    |\n","|    critic_loss     | 0.303    |\n","|    ent_coef        | 0.00352  |\n","|    ent_coef_loss   | -5.02    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 145095   |\n","---------------------------------\n","Eval num_timesteps=3484800, episode_reward=-106.76 +/- 3.96\n","Episode length: 170.00 +/- 22.05\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 170      |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 3484800  |\n","| train/             |          |\n","|    actor_loss      | -3.18    |\n","|    critic_loss     | 0.949    |\n","|    ent_coef        | 0.00349  |\n","|    ent_coef_loss   | -0.561   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 145195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5830     |\n","|    fps             | 306      |\n","|    time_elapsed    | 11369    |\n","|    total_timesteps | 3485688  |\n","| train/             |          |\n","|    actor_loss      | -2.98    |\n","|    critic_loss     | 1.83     |\n","|    ent_coef        | 0.00351  |\n","|    ent_coef_loss   | -1.14    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 145232   |\n","---------------------------------\n","Eval num_timesteps=3487200, episode_reward=45.33 +/- 55.31\n","Episode length: 497.60 +/- 2.94\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 498      |\n","|    mean_reward     | 45.3     |\n","| time/              |          |\n","|    total_timesteps | 3487200  |\n","| train/             |          |\n","|    actor_loss      | -2.38    |\n","|    critic_loss     | 1.28     |\n","|    ent_coef        | 0.00347  |\n","|    ent_coef_loss   | 4.15     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 145295   |\n","---------------------------------\n","Eval num_timesteps=3489600, episode_reward=86.04 +/- 1.67\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 86       |\n","| time/              |          |\n","|    total_timesteps | 3489600  |\n","| train/             |          |\n","|    actor_loss      | -3.65    |\n","|    critic_loss     | 0.455    |\n","|    ent_coef        | 0.00347  |\n","|    ent_coef_loss   | -3.96    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 145395   |\n","---------------------------------\n","Eval num_timesteps=3492000, episode_reward=81.85 +/- 4.99\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 81.8     |\n","| time/              |          |\n","|    total_timesteps | 3492000  |\n","| train/             |          |\n","|    actor_loss      | -3.32    |\n","|    critic_loss     | 3.94     |\n","|    ent_coef        | 0.00351  |\n","|    ent_coef_loss   | -1.44    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 145495   |\n","---------------------------------\n","Eval num_timesteps=3494400, episode_reward=52.93 +/- 52.38\n","Episode length: 495.20 +/- 5.88\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 495      |\n","|    mean_reward     | 52.9     |\n","| time/              |          |\n","|    total_timesteps | 3494400  |\n","| train/             |          |\n","|    actor_loss      | -2.66    |\n","|    critic_loss     | 3.03     |\n","|    ent_coef        | 0.0035   |\n","|    ent_coef_loss   | 0.312    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 145595   |\n","---------------------------------\n","Eval num_timesteps=3496800, episode_reward=-73.28 +/- 24.52\n","Episode length: 254.20 +/- 79.36\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 254      |\n","|    mean_reward     | -73.3    |\n","| time/              |          |\n","|    total_timesteps | 3496800  |\n","| train/             |          |\n","|    actor_loss      | -3.21    |\n","|    critic_loss     | 1.91     |\n","|    ent_coef        | 0.00352  |\n","|    ent_coef_loss   | -2.01    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 145695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5840     |\n","|    fps             | 306      |\n","|    time_elapsed    | 11410    |\n","|    total_timesteps | 3498696  |\n","| train/             |          |\n","|    actor_loss      | -2.7     |\n","|    critic_loss     | 16.5     |\n","|    ent_coef        | 0.00349  |\n","|    ent_coef_loss   | 1.99     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 145774   |\n","---------------------------------\n","Eval num_timesteps=3499200, episode_reward=83.70 +/- 3.88\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 83.7     |\n","| time/              |          |\n","|    total_timesteps | 3499200  |\n","| train/             |          |\n","|    actor_loss      | -2.68    |\n","|    critic_loss     | 0.911    |\n","|    ent_coef        | 0.00349  |\n","|    ent_coef_loss   | -1.75    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 145795   |\n","---------------------------------\n","Eval num_timesteps=3501600, episode_reward=-30.65 +/- 62.08\n","Episode length: 397.40 +/- 83.77\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 397      |\n","|    mean_reward     | -30.6    |\n","| time/              |          |\n","|    total_timesteps | 3501600  |\n","| train/             |          |\n","|    actor_loss      | -1.87    |\n","|    critic_loss     | 1.25     |\n","|    ent_coef        | 0.00351  |\n","|    ent_coef_loss   | 9.21     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 145895   |\n","---------------------------------\n","Eval num_timesteps=3504000, episode_reward=91.82 +/- 1.13\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 91.8     |\n","| time/              |          |\n","|    total_timesteps | 3504000  |\n","| train/             |          |\n","|    actor_loss      | -3.16    |\n","|    critic_loss     | 0.783    |\n","|    ent_coef        | 0.0035   |\n","|    ent_coef_loss   | 1.11     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 145995   |\n","---------------------------------\n","Eval num_timesteps=3506400, episode_reward=99.75 +/- 1.53\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 99.8     |\n","| time/              |          |\n","|    total_timesteps | 3506400  |\n","| train/             |          |\n","|    actor_loss      | -3.14    |\n","|    critic_loss     | 42.5     |\n","|    ent_coef        | 0.00348  |\n","|    ent_coef_loss   | -1.97    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 146095   |\n","---------------------------------\n","Eval num_timesteps=3508800, episode_reward=88.34 +/- 2.17\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 88.3     |\n","| time/              |          |\n","|    total_timesteps | 3508800  |\n","| train/             |          |\n","|    actor_loss      | -2.69    |\n","|    critic_loss     | 1.27     |\n","|    ent_coef        | 0.00344  |\n","|    ent_coef_loss   | 2.1      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 146195   |\n","---------------------------------\n","Eval num_timesteps=3511200, episode_reward=20.02 +/- 99.63\n","Episode length: 340.80 +/- 194.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 341      |\n","|    mean_reward     | 20       |\n","| time/              |          |\n","|    total_timesteps | 3511200  |\n","| train/             |          |\n","|    actor_loss      | -2.95    |\n","|    critic_loss     | 42.7     |\n","|    ent_coef        | 0.00343  |\n","|    ent_coef_loss   | -2.53    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 146295   |\n","---------------------------------\n","Eval num_timesteps=3513600, episode_reward=80.49 +/- 4.31\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 80.5     |\n","| time/              |          |\n","|    total_timesteps | 3513600  |\n","| train/             |          |\n","|    actor_loss      | -3.02    |\n","|    critic_loss     | 0.926    |\n","|    ent_coef        | 0.00345  |\n","|    ent_coef_loss   | -0.194   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 146395   |\n","---------------------------------\n","Eval num_timesteps=3516000, episode_reward=64.93 +/- 2.01\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 64.9     |\n","| time/              |          |\n","|    total_timesteps | 3516000  |\n","| train/             |          |\n","|    actor_loss      | -3.36    |\n","|    critic_loss     | 0.579    |\n","|    ent_coef        | 0.00344  |\n","|    ent_coef_loss   | 1.42     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 146495   |\n","---------------------------------\n","Eval num_timesteps=3518400, episode_reward=80.63 +/- 3.36\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 80.6     |\n","| time/              |          |\n","|    total_timesteps | 3518400  |\n","| train/             |          |\n","|    actor_loss      | -3.11    |\n","|    critic_loss     | 0.612    |\n","|    ent_coef        | 0.00354  |\n","|    ent_coef_loss   | -1.66    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 146595   |\n","---------------------------------\n","Eval num_timesteps=3520800, episode_reward=-105.92 +/- 4.68\n","Episode length: 157.80 +/- 16.17\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 158      |\n","|    mean_reward     | -106     |\n","| time/              |          |\n","|    total_timesteps | 3520800  |\n","| train/             |          |\n","|    actor_loss      | -2.56    |\n","|    critic_loss     | 0.69     |\n","|    ent_coef        | 0.00353  |\n","|    ent_coef_loss   | -0.323   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 146695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5850     |\n","|    fps             | 306      |\n","|    time_elapsed    | 11493    |\n","|    total_timesteps | 3521784  |\n","| train/             |          |\n","|    actor_loss      | -3.31    |\n","|    critic_loss     | 3.96     |\n","|    ent_coef        | 0.00354  |\n","|    ent_coef_loss   | -0.281   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 146736   |\n","---------------------------------\n","Eval num_timesteps=3523200, episode_reward=-59.95 +/- 14.83\n","Episode length: 300.80 +/- 67.12\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 301      |\n","|    mean_reward     | -59.9    |\n","| time/              |          |\n","|    total_timesteps | 3523200  |\n","| train/             |          |\n","|    actor_loss      | -2.23    |\n","|    critic_loss     | 0.759    |\n","|    ent_coef        | 0.00355  |\n","|    ent_coef_loss   | -2.66    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 146795   |\n","---------------------------------\n","Eval num_timesteps=3525600, episode_reward=104.80 +/- 0.70\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 105      |\n","| time/              |          |\n","|    total_timesteps | 3525600  |\n","| train/             |          |\n","|    actor_loss      | -3.47    |\n","|    critic_loss     | 0.644    |\n","|    ent_coef        | 0.00354  |\n","|    ent_coef_loss   | 0.755    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 146895   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5860     |\n","|    fps             | 306      |\n","|    time_elapsed    | 11510    |\n","|    total_timesteps | 3527352  |\n","| train/             |          |\n","|    actor_loss      | -2.39    |\n","|    critic_loss     | 0.875    |\n","|    ent_coef        | 0.00352  |\n","|    ent_coef_loss   | -0.279   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 146968   |\n","---------------------------------\n","Eval num_timesteps=3528000, episode_reward=103.90 +/- 0.10\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 104      |\n","| time/              |          |\n","|    total_timesteps | 3528000  |\n","| train/             |          |\n","|    actor_loss      | -2.6     |\n","|    critic_loss     | 0.975    |\n","|    ent_coef        | 0.00351  |\n","|    ent_coef_loss   | -1.76    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 146995   |\n","---------------------------------\n","Eval num_timesteps=3530400, episode_reward=94.11 +/- 1.76\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 94.1     |\n","| time/              |          |\n","|    total_timesteps | 3530400  |\n","| train/             |          |\n","|    actor_loss      | -2.51    |\n","|    critic_loss     | 1.07     |\n","|    ent_coef        | 0.00349  |\n","|    ent_coef_loss   | 5.77     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 147095   |\n","---------------------------------\n","Eval num_timesteps=3532800, episode_reward=106.59 +/- 0.49\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 107      |\n","| time/              |          |\n","|    total_timesteps | 3532800  |\n","| train/             |          |\n","|    actor_loss      | -2.62    |\n","|    critic_loss     | 6.23     |\n","|    ent_coef        | 0.00346  |\n","|    ent_coef_loss   | 0.0616   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 147195   |\n","---------------------------------\n","Eval num_timesteps=3535200, episode_reward=97.15 +/- 3.17\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 97.2     |\n","| time/              |          |\n","|    total_timesteps | 3535200  |\n","| train/             |          |\n","|    actor_loss      | -3.44    |\n","|    critic_loss     | 0.801    |\n","|    ent_coef        | 0.0034   |\n","|    ent_coef_loss   | -1.02    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 147295   |\n","---------------------------------\n","Eval num_timesteps=3537600, episode_reward=107.90 +/- 0.63\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 108      |\n","| time/              |          |\n","|    total_timesteps | 3537600  |\n","| train/             |          |\n","|    actor_loss      | -2.58    |\n","|    critic_loss     | 0.778    |\n","|    ent_coef        | 0.00334  |\n","|    ent_coef_loss   | -2.74    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 147395   |\n","---------------------------------\n","Eval num_timesteps=3540000, episode_reward=108.02 +/- 1.64\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 108      |\n","| time/              |          |\n","|    total_timesteps | 3540000  |\n","| train/             |          |\n","|    actor_loss      | -3.03    |\n","|    critic_loss     | 0.615    |\n","|    ent_coef        | 0.00334  |\n","|    ent_coef_loss   | 1.59     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 147495   |\n","---------------------------------\n","Eval num_timesteps=3542400, episode_reward=93.46 +/- 1.66\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 93.5     |\n","| time/              |          |\n","|    total_timesteps | 3542400  |\n","| train/             |          |\n","|    actor_loss      | -3.88    |\n","|    critic_loss     | 27.5     |\n","|    ent_coef        | 0.00342  |\n","|    ent_coef_loss   | 0.25     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 147595   |\n","---------------------------------\n","Eval num_timesteps=3544800, episode_reward=100.29 +/- 1.09\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 100      |\n","| time/              |          |\n","|    total_timesteps | 3544800  |\n","| train/             |          |\n","|    actor_loss      | -3.58    |\n","|    critic_loss     | 0.504    |\n","|    ent_coef        | 0.00343  |\n","|    ent_coef_loss   | -3.02    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 147695   |\n","---------------------------------\n","Eval num_timesteps=3547200, episode_reward=-51.02 +/- 17.53\n","Episode length: 302.60 +/- 51.93\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 303      |\n","|    mean_reward     | -51      |\n","| time/              |          |\n","|    total_timesteps | 3547200  |\n","| train/             |          |\n","|    actor_loss      | -3.68    |\n","|    critic_loss     | 0.447    |\n","|    ent_coef        | 0.0034   |\n","|    ent_coef_loss   | -4.55    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 147795   |\n","---------------------------------\n","Eval num_timesteps=3549600, episode_reward=111.63 +/- 0.92\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 112      |\n","| time/              |          |\n","|    total_timesteps | 3549600  |\n","| train/             |          |\n","|    actor_loss      | -3.47    |\n","|    critic_loss     | 0.401    |\n","|    ent_coef        | 0.00332  |\n","|    ent_coef_loss   | 0.954    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 147895   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5870     |\n","|    fps             | 306      |\n","|    time_elapsed    | 11592    |\n","|    total_timesteps | 3551136  |\n","| train/             |          |\n","|    actor_loss      | -3.76    |\n","|    critic_loss     | 0.685    |\n","|    ent_coef        | 0.00334  |\n","|    ent_coef_loss   | 5.14     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 147959   |\n","---------------------------------\n","Eval num_timesteps=3552000, episode_reward=95.53 +/- 2.36\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 95.5     |\n","| time/              |          |\n","|    total_timesteps | 3552000  |\n","| train/             |          |\n","|    actor_loss      | -3.04    |\n","|    critic_loss     | 43.6     |\n","|    ent_coef        | 0.00336  |\n","|    ent_coef_loss   | -2.14    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 147995   |\n","---------------------------------\n","Eval num_timesteps=3554400, episode_reward=109.30 +/- 4.11\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 109      |\n","| time/              |          |\n","|    total_timesteps | 3554400  |\n","| train/             |          |\n","|    actor_loss      | -2.04    |\n","|    critic_loss     | 23.3     |\n","|    ent_coef        | 0.00331  |\n","|    ent_coef_loss   | 0.578    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148095   |\n","---------------------------------\n","Eval num_timesteps=3556800, episode_reward=106.64 +/- 0.83\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 107      |\n","| time/              |          |\n","|    total_timesteps | 3556800  |\n","| train/             |          |\n","|    actor_loss      | -2.65    |\n","|    critic_loss     | 0.61     |\n","|    ent_coef        | 0.00333  |\n","|    ent_coef_loss   | 1.21     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148195   |\n","---------------------------------\n","Eval num_timesteps=3559200, episode_reward=87.01 +/- 0.55\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 87       |\n","| time/              |          |\n","|    total_timesteps | 3559200  |\n","| train/             |          |\n","|    actor_loss      | -3.02    |\n","|    critic_loss     | 0.971    |\n","|    ent_coef        | 0.00332  |\n","|    ent_coef_loss   | 0.863    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148295   |\n","---------------------------------\n","Eval num_timesteps=3561600, episode_reward=96.39 +/- 3.19\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 96.4     |\n","| time/              |          |\n","|    total_timesteps | 3561600  |\n","| train/             |          |\n","|    actor_loss      | -2.58    |\n","|    critic_loss     | 0.441    |\n","|    ent_coef        | 0.00339  |\n","|    ent_coef_loss   | 1.88     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148395   |\n","---------------------------------\n","Eval num_timesteps=3564000, episode_reward=113.05 +/- 1.43\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 113      |\n","| time/              |          |\n","|    total_timesteps | 3564000  |\n","| train/             |          |\n","|    actor_loss      | -2.43    |\n","|    critic_loss     | 0.677    |\n","|    ent_coef        | 0.00335  |\n","|    ent_coef_loss   | -0.735   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148495   |\n","---------------------------------\n","Eval num_timesteps=3566400, episode_reward=101.24 +/- 2.12\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 101      |\n","| time/              |          |\n","|    total_timesteps | 3566400  |\n","| train/             |          |\n","|    actor_loss      | -3.75    |\n","|    critic_loss     | 0.546    |\n","|    ent_coef        | 0.00332  |\n","|    ent_coef_loss   | -3.95    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148595   |\n","---------------------------------\n","Eval num_timesteps=3568800, episode_reward=-17.06 +/- 98.61\n","Episode length: 247.40 +/- 206.25\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 247      |\n","|    mean_reward     | -17.1    |\n","| time/              |          |\n","|    total_timesteps | 3568800  |\n","| train/             |          |\n","|    actor_loss      | -3.02    |\n","|    critic_loss     | 1.39     |\n","|    ent_coef        | 0.00328  |\n","|    ent_coef_loss   | 1.75     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5880     |\n","|    fps             | 306      |\n","|    time_elapsed    | 11659    |\n","|    total_timesteps | 3570552  |\n","| train/             |          |\n","|    actor_loss      | -2.75    |\n","|    critic_loss     | 0.314    |\n","|    ent_coef        | 0.00328  |\n","|    ent_coef_loss   | -4.11    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148768   |\n","---------------------------------\n","Eval num_timesteps=3571200, episode_reward=99.30 +/- 1.38\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 99.3     |\n","| time/              |          |\n","|    total_timesteps | 3571200  |\n","| train/             |          |\n","|    actor_loss      | -2.64    |\n","|    critic_loss     | 0.897    |\n","|    ent_coef        | 0.00325  |\n","|    ent_coef_loss   | -1.31    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148795   |\n","---------------------------------\n","Eval num_timesteps=3573600, episode_reward=-77.06 +/- 27.00\n","Episode length: 171.60 +/- 119.54\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 172      |\n","|    mean_reward     | -77.1    |\n","| time/              |          |\n","|    total_timesteps | 3573600  |\n","| train/             |          |\n","|    actor_loss      | -2.15    |\n","|    critic_loss     | 37.4     |\n","|    ent_coef        | 0.00325  |\n","|    ent_coef_loss   | 3.44     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148895   |\n","---------------------------------\n","Eval num_timesteps=3576000, episode_reward=-18.67 +/- 101.27\n","Episode length: 239.60 +/- 212.62\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 240      |\n","|    mean_reward     | -18.7    |\n","| time/              |          |\n","|    total_timesteps | 3576000  |\n","| train/             |          |\n","|    actor_loss      | -2.99    |\n","|    critic_loss     | 38.9     |\n","|    ent_coef        | 0.00329  |\n","|    ent_coef_loss   | 3.51     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148995   |\n","---------------------------------\n","Eval num_timesteps=3578400, episode_reward=-96.12 +/- 6.61\n","Episode length: 76.80 +/- 0.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 76.8     |\n","|    mean_reward     | -96.1    |\n","| time/              |          |\n","|    total_timesteps | 3578400  |\n","| train/             |          |\n","|    actor_loss      | -4.09    |\n","|    critic_loss     | 0.496    |\n","|    ent_coef        | 0.00334  |\n","|    ent_coef_loss   | 1.89     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149095   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5890     |\n","|    fps             | 306      |\n","|    time_elapsed    | 11686    |\n","|    total_timesteps | 3578424  |\n","| train/             |          |\n","|    actor_loss      | -2.69    |\n","|    critic_loss     | 2.18     |\n","|    ent_coef        | 0.00334  |\n","|    ent_coef_loss   | 1.15     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149096   |\n","---------------------------------\n","Eval num_timesteps=3580800, episode_reward=112.48 +/- 0.48\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 112      |\n","| time/              |          |\n","|    total_timesteps | 3580800  |\n","| train/             |          |\n","|    actor_loss      | -2.92    |\n","|    critic_loss     | 0.856    |\n","|    ent_coef        | 0.00343  |\n","|    ent_coef_loss   | 1.82     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5900     |\n","|    fps             | 306      |\n","|    time_elapsed    | 11695    |\n","|    total_timesteps | 3582744  |\n","| train/             |          |\n","|    actor_loss      | -2.64    |\n","|    critic_loss     | 0.909    |\n","|    ent_coef        | 0.00348  |\n","|    ent_coef_loss   | -4.09    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149276   |\n","---------------------------------\n","Eval num_timesteps=3583200, episode_reward=46.66 +/- 68.37\n","Episode length: 443.60 +/- 69.08\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 444      |\n","|    mean_reward     | 46.7     |\n","| time/              |          |\n","|    total_timesteps | 3583200  |\n","| train/             |          |\n","|    actor_loss      | -2.95    |\n","|    critic_loss     | 39.2     |\n","|    ent_coef        | 0.00346  |\n","|    ent_coef_loss   | 0.857    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149295   |\n","---------------------------------\n","Eval num_timesteps=3585600, episode_reward=103.54 +/- 1.84\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 104      |\n","| time/              |          |\n","|    total_timesteps | 3585600  |\n","| train/             |          |\n","|    actor_loss      | -3.17    |\n","|    critic_loss     | 2.91     |\n","|    ent_coef        | 0.00353  |\n","|    ent_coef_loss   | 1.72     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149395   |\n","---------------------------------\n","Eval num_timesteps=3588000, episode_reward=96.22 +/- 2.02\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 96.2     |\n","| time/              |          |\n","|    total_timesteps | 3588000  |\n","| train/             |          |\n","|    actor_loss      | -2.73    |\n","|    critic_loss     | 1.17     |\n","|    ent_coef        | 0.00355  |\n","|    ent_coef_loss   | -1.01    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149495   |\n","---------------------------------\n","Eval num_timesteps=3590400, episode_reward=-19.93 +/- 98.29\n","Episode length: 244.40 +/- 208.70\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 244      |\n","|    mean_reward     | -19.9    |\n","| time/              |          |\n","|    total_timesteps | 3590400  |\n","| train/             |          |\n","|    actor_loss      | -3.54    |\n","|    critic_loss     | 3.08     |\n","|    ent_coef        | 0.00352  |\n","|    ent_coef_loss   | -0.236   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5910     |\n","|    fps             | 306      |\n","|    time_elapsed    | 11731    |\n","|    total_timesteps | 3592176  |\n","| train/             |          |\n","|    actor_loss      | -3.21    |\n","|    critic_loss     | 1.74     |\n","|    ent_coef        | 0.00352  |\n","|    ent_coef_loss   | -3.2     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149669   |\n","---------------------------------\n","Eval num_timesteps=3592800, episode_reward=88.36 +/- 0.90\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 88.4     |\n","| time/              |          |\n","|    total_timesteps | 3592800  |\n","| train/             |          |\n","|    actor_loss      | -2.32    |\n","|    critic_loss     | 0.757    |\n","|    ent_coef        | 0.00352  |\n","|    ent_coef_loss   | 2.53     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149695   |\n","---------------------------------\n","Eval num_timesteps=3595200, episode_reward=88.68 +/- 3.19\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 88.7     |\n","| time/              |          |\n","|    total_timesteps | 3595200  |\n","| train/             |          |\n","|    actor_loss      | -2.62    |\n","|    critic_loss     | 0.965    |\n","|    ent_coef        | 0.00352  |\n","|    ent_coef_loss   | 1.99     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149795   |\n","---------------------------------\n","Eval num_timesteps=3597600, episode_reward=100.81 +/- 1.86\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 101      |\n","| time/              |          |\n","|    total_timesteps | 3597600  |\n","| train/             |          |\n","|    actor_loss      | -3.27    |\n","|    critic_loss     | 0.708    |\n","|    ent_coef        | 0.00352  |\n","|    ent_coef_loss   | -1.3     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149895   |\n","---------------------------------\n","Eval num_timesteps=3600000, episode_reward=33.37 +/- 73.77\n","Episode length: 416.80 +/- 101.90\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 417      |\n","|    mean_reward     | 33.4     |\n","| time/              |          |\n","|    total_timesteps | 3600000  |\n","| train/             |          |\n","|    actor_loss      | -3.57    |\n","|    critic_loss     | 0.587    |\n","|    ent_coef        | 0.00352  |\n","|    ent_coef_loss   | -2.45    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149995   |\n","---------------------------------\n","Eval num_timesteps=3602400, episode_reward=26.02 +/- 63.41\n","Episode length: 435.20 +/- 52.91\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 435      |\n","|    mean_reward     | 26       |\n","| time/              |          |\n","|    total_timesteps | 3602400  |\n","| train/             |          |\n","|    actor_loss      | -3.73    |\n","|    critic_loss     | 0.65     |\n","|    ent_coef        | 0.00348  |\n","|    ent_coef_loss   | -5.95    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150095   |\n","---------------------------------\n","Eval num_timesteps=3604800, episode_reward=99.14 +/- 3.42\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 99.1     |\n","| time/              |          |\n","|    total_timesteps | 3604800  |\n","| train/             |          |\n","|    actor_loss      | -2.84    |\n","|    critic_loss     | 0.919    |\n","|    ent_coef        | 0.00336  |\n","|    ent_coef_loss   | -0.846   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150195   |\n","---------------------------------\n","Eval num_timesteps=3607200, episode_reward=95.91 +/- 4.62\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 95.9     |\n","| time/              |          |\n","|    total_timesteps | 3607200  |\n","| train/             |          |\n","|    actor_loss      | -3.61    |\n","|    critic_loss     | 0.545    |\n","|    ent_coef        | 0.00329  |\n","|    ent_coef_loss   | -6.1     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150295   |\n","---------------------------------\n","Eval num_timesteps=3609600, episode_reward=105.74 +/- 4.38\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 106      |\n","| time/              |          |\n","|    total_timesteps | 3609600  |\n","| train/             |          |\n","|    actor_loss      | -2.85    |\n","|    critic_loss     | 2.13     |\n","|    ent_coef        | 0.00319  |\n","|    ent_coef_loss   | 2.31     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5920     |\n","|    fps             | 306      |\n","|    time_elapsed    | 11800    |\n","|    total_timesteps | 3611664  |\n","| train/             |          |\n","|    actor_loss      | -3.75    |\n","|    critic_loss     | 1.05     |\n","|    ent_coef        | 0.00317  |\n","|    ent_coef_loss   | -0.101   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150481   |\n","---------------------------------\n","Eval num_timesteps=3612000, episode_reward=90.90 +/- 2.74\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 90.9     |\n","| time/              |          |\n","|    total_timesteps | 3612000  |\n","| train/             |          |\n","|    actor_loss      | -3.96    |\n","|    critic_loss     | 1.66     |\n","|    ent_coef        | 0.00317  |\n","|    ent_coef_loss   | 3.61     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150495   |\n","---------------------------------\n","Eval num_timesteps=3614400, episode_reward=36.99 +/- 53.59\n","Episode length: 488.60 +/- 9.31\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 489      |\n","|    mean_reward     | 37       |\n","| time/              |          |\n","|    total_timesteps | 3614400  |\n","| train/             |          |\n","|    actor_loss      | -1.53    |\n","|    critic_loss     | 2.1      |\n","|    ent_coef        | 0.00317  |\n","|    ent_coef_loss   | 3.72     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150595   |\n","---------------------------------\n","Eval num_timesteps=3616800, episode_reward=104.02 +/- 2.69\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 104      |\n","| time/              |          |\n","|    total_timesteps | 3616800  |\n","| train/             |          |\n","|    actor_loss      | -4.15    |\n","|    critic_loss     | 0.406    |\n","|    ent_coef        | 0.00315  |\n","|    ent_coef_loss   | -3.87    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150695   |\n","---------------------------------\n","Eval num_timesteps=3619200, episode_reward=95.35 +/- 2.54\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 95.4     |\n","| time/              |          |\n","|    total_timesteps | 3619200  |\n","| train/             |          |\n","|    actor_loss      | -3.85    |\n","|    critic_loss     | 1.28     |\n","|    ent_coef        | 0.00309  |\n","|    ent_coef_loss   | -2.08    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150795   |\n","---------------------------------\n","Eval num_timesteps=3621600, episode_reward=93.73 +/- 3.45\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 93.7     |\n","| time/              |          |\n","|    total_timesteps | 3621600  |\n","| train/             |          |\n","|    actor_loss      | -4.08    |\n","|    critic_loss     | 0.927    |\n","|    ent_coef        | 0.00314  |\n","|    ent_coef_loss   | -3.01    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150895   |\n","---------------------------------\n","Eval num_timesteps=3624000, episode_reward=7.60 +/- 94.24\n","Episode length: 345.20 +/- 189.59\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 345      |\n","|    mean_reward     | 7.6      |\n","| time/              |          |\n","|    total_timesteps | 3624000  |\n","| train/             |          |\n","|    actor_loss      | -3.34    |\n","|    critic_loss     | 0.877    |\n","|    ent_coef        | 0.0031   |\n","|    ent_coef_loss   | 0.546    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150995   |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    episodes        | 5930    |\n","|    fps             | 305     |\n","|    time_elapsed    | 11846   |\n","|    total_timesteps | 3624000 |\n","--------------------------------\n","Eval num_timesteps=3626400, episode_reward=84.59 +/- 1.98\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 84.6     |\n","| time/              |          |\n","|    total_timesteps | 3626400  |\n","| train/             |          |\n","|    actor_loss      | -3.3     |\n","|    critic_loss     | 1.01     |\n","|    ent_coef        | 0.00311  |\n","|    ent_coef_loss   | 3.64     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151095   |\n","---------------------------------\n","Eval num_timesteps=3628800, episode_reward=-12.15 +/- 99.15\n","Episode length: 264.20 +/- 192.53\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 264      |\n","|    mean_reward     | -12.2    |\n","| time/              |          |\n","|    total_timesteps | 3628800  |\n","| train/             |          |\n","|    actor_loss      | -3.12    |\n","|    critic_loss     | 3.27     |\n","|    ent_coef        | 0.00316  |\n","|    ent_coef_loss   | -3.56    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151195   |\n","---------------------------------\n","Eval num_timesteps=3631200, episode_reward=102.93 +/- 1.58\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 103      |\n","| time/              |          |\n","|    total_timesteps | 3631200  |\n","| train/             |          |\n","|    actor_loss      | -3.22    |\n","|    critic_loss     | 2.57     |\n","|    ent_coef        | 0.0031   |\n","|    ent_coef_loss   | 0.897    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151295   |\n","---------------------------------\n","Eval num_timesteps=3633600, episode_reward=99.66 +/- 0.41\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 99.7     |\n","| time/              |          |\n","|    total_timesteps | 3633600  |\n","| train/             |          |\n","|    actor_loss      | -2.77    |\n","|    critic_loss     | 1.15     |\n","|    ent_coef        | 0.00312  |\n","|    ent_coef_loss   | -0.736   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151395   |\n","---------------------------------\n","Eval num_timesteps=3636000, episode_reward=3.81 +/- 71.88\n","Episode length: 392.60 +/- 87.69\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 393      |\n","|    mean_reward     | 3.81     |\n","| time/              |          |\n","|    total_timesteps | 3636000  |\n","| train/             |          |\n","|    actor_loss      | -3.21    |\n","|    critic_loss     | 0.557    |\n","|    ent_coef        | 0.00309  |\n","|    ent_coef_loss   | -0.68    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151495   |\n","---------------------------------\n","Eval num_timesteps=3638400, episode_reward=96.64 +/- 0.40\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 96.6     |\n","| time/              |          |\n","|    total_timesteps | 3638400  |\n","| train/             |          |\n","|    actor_loss      | -3.46    |\n","|    critic_loss     | 1.13     |\n","|    ent_coef        | 0.0031   |\n","|    ent_coef_loss   | -1.46    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151595   |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    episodes        | 5940    |\n","|    fps             | 305     |\n","|    time_elapsed    | 11898   |\n","|    total_timesteps | 3638400 |\n","--------------------------------\n","Eval num_timesteps=3640800, episode_reward=-12.94 +/- 86.23\n","Episode length: 318.80 +/- 147.95\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 319      |\n","|    mean_reward     | -12.9    |\n","| time/              |          |\n","|    total_timesteps | 3640800  |\n","| train/             |          |\n","|    actor_loss      | -3.41    |\n","|    critic_loss     | 4.42     |\n","|    ent_coef        | 0.0031   |\n","|    ent_coef_loss   | 2.24     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151695   |\n","---------------------------------\n","Eval num_timesteps=3643200, episode_reward=84.79 +/- 0.23\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 84.8     |\n","| time/              |          |\n","|    total_timesteps | 3643200  |\n","| train/             |          |\n","|    actor_loss      | -4.21    |\n","|    critic_loss     | 0.714    |\n","|    ent_coef        | 0.00314  |\n","|    ent_coef_loss   | 2.73     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151795   |\n","---------------------------------\n","Eval num_timesteps=3645600, episode_reward=102.18 +/- 0.93\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 102      |\n","| time/              |          |\n","|    total_timesteps | 3645600  |\n","| train/             |          |\n","|    actor_loss      | -3.51    |\n","|    critic_loss     | 2.57     |\n","|    ent_coef        | 0.00317  |\n","|    ent_coef_loss   | -0.961   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151895   |\n","---------------------------------\n","Eval num_timesteps=3648000, episode_reward=44.18 +/- 51.22\n","Episode length: 490.00 +/- 12.25\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 490      |\n","|    mean_reward     | 44.2     |\n","| time/              |          |\n","|    total_timesteps | 3648000  |\n","| train/             |          |\n","|    actor_loss      | -3.23    |\n","|    critic_loss     | 0.871    |\n","|    ent_coef        | 0.00317  |\n","|    ent_coef_loss   | 3.91     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151995   |\n","---------------------------------\n","Eval num_timesteps=3650400, episode_reward=90.06 +/- 2.04\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 90.1     |\n","| time/              |          |\n","|    total_timesteps | 3650400  |\n","| train/             |          |\n","|    actor_loss      | -3.42    |\n","|    critic_loss     | 0.68     |\n","|    ent_coef        | 0.00327  |\n","|    ent_coef_loss   | 2.37     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152095   |\n","---------------------------------\n","Eval num_timesteps=3652800, episode_reward=108.64 +/- 0.40\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 109      |\n","| time/              |          |\n","|    total_timesteps | 3652800  |\n","| train/             |          |\n","|    actor_loss      | -3.18    |\n","|    critic_loss     | 0.593    |\n","|    ent_coef        | 0.00331  |\n","|    ent_coef_loss   | -4       |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152195   |\n","---------------------------------\n","Eval num_timesteps=3655200, episode_reward=103.27 +/- 2.53\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 103      |\n","| time/              |          |\n","|    total_timesteps | 3655200  |\n","| train/             |          |\n","|    actor_loss      | -3.36    |\n","|    critic_loss     | 0.731    |\n","|    ent_coef        | 0.00328  |\n","|    ent_coef_loss   | -0.306   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152295   |\n","---------------------------------\n","Eval num_timesteps=3657600, episode_reward=-17.55 +/- 93.85\n","Episode length: 336.20 +/- 133.74\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 336      |\n","|    mean_reward     | -17.6    |\n","| time/              |          |\n","|    total_timesteps | 3657600  |\n","| train/             |          |\n","|    actor_loss      | -3.31    |\n","|    critic_loss     | 43.9     |\n","|    ent_coef        | 0.00328  |\n","|    ent_coef_loss   | 1.43     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152395   |\n","---------------------------------\n","Eval num_timesteps=3660000, episode_reward=116.33 +/- 0.68\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 116      |\n","| time/              |          |\n","|    total_timesteps | 3660000  |\n","| train/             |          |\n","|    actor_loss      | -2.98    |\n","|    critic_loss     | 1.09     |\n","|    ent_coef        | 0.00326  |\n","|    ent_coef_loss   | -0.461   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152495   |\n","---------------------------------\n","Eval num_timesteps=3662400, episode_reward=107.39 +/- 0.82\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 107      |\n","| time/              |          |\n","|    total_timesteps | 3662400  |\n","| train/             |          |\n","|    actor_loss      | -3.7     |\n","|    critic_loss     | 1.1      |\n","|    ent_coef        | 0.0033   |\n","|    ent_coef_loss   | 0.972    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152595   |\n","---------------------------------\n","Eval num_timesteps=3664800, episode_reward=-76.33 +/- 29.44\n","Episode length: 176.40 +/- 63.20\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 176      |\n","|    mean_reward     | -76.3    |\n","| time/              |          |\n","|    total_timesteps | 3664800  |\n","| train/             |          |\n","|    actor_loss      | -3.55    |\n","|    critic_loss     | 35.2     |\n","|    ent_coef        | 0.00332  |\n","|    ent_coef_loss   | 3.04     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5950     |\n","|    fps             | 305      |\n","|    time_elapsed    | 11991    |\n","|    total_timesteps | 3665616  |\n","| train/             |          |\n","|    actor_loss      | -2.96    |\n","|    critic_loss     | 0.969    |\n","|    ent_coef        | 0.00334  |\n","|    ent_coef_loss   | 2.57     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152729   |\n","---------------------------------\n","Eval num_timesteps=3667200, episode_reward=-77.17 +/- 31.97\n","Episode length: 230.20 +/- 97.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 230      |\n","|    mean_reward     | -77.2    |\n","| time/              |          |\n","|    total_timesteps | 3667200  |\n","| train/             |          |\n","|    actor_loss      | -3.54    |\n","|    critic_loss     | 0.894    |\n","|    ent_coef        | 0.00333  |\n","|    ent_coef_loss   | -1.51    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152795   |\n","---------------------------------\n","Eval num_timesteps=3669600, episode_reward=93.08 +/- 3.01\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 93.1     |\n","| time/              |          |\n","|    total_timesteps | 3669600  |\n","| train/             |          |\n","|    actor_loss      | -1.51    |\n","|    critic_loss     | 2.72     |\n","|    ent_coef        | 0.00328  |\n","|    ent_coef_loss   | -1.35    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152895   |\n","---------------------------------\n","Eval num_timesteps=3672000, episode_reward=88.46 +/- 2.23\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 88.5     |\n","| time/              |          |\n","|    total_timesteps | 3672000  |\n","| train/             |          |\n","|    actor_loss      | -3.09    |\n","|    critic_loss     | 0.699    |\n","|    ent_coef        | 0.00326  |\n","|    ent_coef_loss   | 1.6      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152995   |\n","---------------------------------\n","Eval num_timesteps=3674400, episode_reward=33.40 +/- 56.78\n","Episode length: 483.60 +/- 20.09\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 484      |\n","|    mean_reward     | 33.4     |\n","| time/              |          |\n","|    total_timesteps | 3674400  |\n","| train/             |          |\n","|    actor_loss      | -3.88    |\n","|    critic_loss     | 1.06     |\n","|    ent_coef        | 0.00331  |\n","|    ent_coef_loss   | 1.24     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153095   |\n","---------------------------------\n","Eval num_timesteps=3676800, episode_reward=-26.35 +/- 12.68\n","Episode length: 412.40 +/- 24.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 412      |\n","|    mean_reward     | -26.4    |\n","| time/              |          |\n","|    total_timesteps | 3676800  |\n","| train/             |          |\n","|    actor_loss      | -3.26    |\n","|    critic_loss     | 0.727    |\n","|    ent_coef        | 0.00337  |\n","|    ent_coef_loss   | 1.43     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5960     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12030    |\n","|    total_timesteps | 3678432  |\n","| train/             |          |\n","|    actor_loss      | -3.77    |\n","|    critic_loss     | 0.269    |\n","|    ent_coef        | 0.00338  |\n","|    ent_coef_loss   | -0.803   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153263   |\n","---------------------------------\n","Eval num_timesteps=3679200, episode_reward=-28.38 +/- 89.30\n","Episode length: 295.40 +/- 167.06\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 295      |\n","|    mean_reward     | -28.4    |\n","| time/              |          |\n","|    total_timesteps | 3679200  |\n","| train/             |          |\n","|    actor_loss      | -3.92    |\n","|    critic_loss     | 0.294    |\n","|    ent_coef        | 0.00338  |\n","|    ent_coef_loss   | -2.24    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5970     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12037    |\n","|    total_timesteps | 3680832  |\n","| train/             |          |\n","|    actor_loss      | -3.87    |\n","|    critic_loss     | 1.05     |\n","|    ent_coef        | 0.00335  |\n","|    ent_coef_loss   | 1.57     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153363   |\n","---------------------------------\n","Eval num_timesteps=3681600, episode_reward=27.34 +/- 83.98\n","Episode length: 400.00 +/- 122.47\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 400      |\n","|    mean_reward     | 27.3     |\n","| time/              |          |\n","|    total_timesteps | 3681600  |\n","| train/             |          |\n","|    actor_loss      | -2.74    |\n","|    critic_loss     | 1.87     |\n","|    ent_coef        | 0.00334  |\n","|    ent_coef_loss   | 0.915    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153395   |\n","---------------------------------\n","Eval num_timesteps=3684000, episode_reward=-86.07 +/- 7.41\n","Episode length: 250.60 +/- 7.84\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 251      |\n","|    mean_reward     | -86.1    |\n","| time/              |          |\n","|    total_timesteps | 3684000  |\n","| train/             |          |\n","|    actor_loss      | -4.12    |\n","|    critic_loss     | 0.468    |\n","|    ent_coef        | 0.00346  |\n","|    ent_coef_loss   | -0.948   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153495   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5980     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12051    |\n","|    total_timesteps | 3684048  |\n","| train/             |          |\n","|    actor_loss      | -2.37    |\n","|    critic_loss     | 0.519    |\n","|    ent_coef        | 0.00346  |\n","|    ent_coef_loss   | 1.26     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153497   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5990     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12052    |\n","|    total_timesteps | 3685056  |\n","| train/             |          |\n","|    actor_loss      | -3.31    |\n","|    critic_loss     | 38.5     |\n","|    ent_coef        | 0.00345  |\n","|    ent_coef_loss   | -0.714   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153539   |\n","---------------------------------\n","Eval num_timesteps=3686400, episode_reward=30.03 +/- 83.74\n","Episode length: 398.40 +/- 124.43\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 398      |\n","|    mean_reward     | 30       |\n","| time/              |          |\n","|    total_timesteps | 3686400  |\n","| train/             |          |\n","|    actor_loss      | -3.15    |\n","|    critic_loss     | 1.53     |\n","|    ent_coef        | 0.00353  |\n","|    ent_coef_loss   | -2.42    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6000     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12060    |\n","|    total_timesteps | 3688704  |\n","| train/             |          |\n","|    actor_loss      | -4.54    |\n","|    critic_loss     | 0.468    |\n","|    ent_coef        | 0.00348  |\n","|    ent_coef_loss   | -0.853   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153691   |\n","---------------------------------\n","Eval num_timesteps=3688800, episode_reward=-88.32 +/- 5.31\n","Episode length: 265.40 +/- 60.75\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 265      |\n","|    mean_reward     | -88.3    |\n","| time/              |          |\n","|    total_timesteps | 3688800  |\n","| train/             |          |\n","|    actor_loss      | -3.34    |\n","|    critic_loss     | 1.21     |\n","|    ent_coef        | 0.00348  |\n","|    ent_coef_loss   | -0.0433  |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153695   |\n","---------------------------------\n","Eval num_timesteps=3691200, episode_reward=22.86 +/- 68.12\n","Episode length: 404.60 +/- 77.89\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 405      |\n","|    mean_reward     | 22.9     |\n","| time/              |          |\n","|    total_timesteps | 3691200  |\n","| train/             |          |\n","|    actor_loss      | -3.3     |\n","|    critic_loss     | 1.6      |\n","|    ent_coef        | 0.00346  |\n","|    ent_coef_loss   | -2.58    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153795   |\n","---------------------------------\n","Eval num_timesteps=3693600, episode_reward=-72.47 +/- 5.99\n","Episode length: 205.80 +/- 1.47\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 206      |\n","|    mean_reward     | -72.5    |\n","| time/              |          |\n","|    total_timesteps | 3693600  |\n","| train/             |          |\n","|    actor_loss      | -3.41    |\n","|    critic_loss     | 0.446    |\n","|    ent_coef        | 0.00344  |\n","|    ent_coef_loss   | -0.217   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153895   |\n","---------------------------------\n","Eval num_timesteps=3696000, episode_reward=100.15 +/- 3.02\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 100      |\n","| time/              |          |\n","|    total_timesteps | 3696000  |\n","| train/             |          |\n","|    actor_loss      | -3.34    |\n","|    critic_loss     | 1.74     |\n","|    ent_coef        | 0.00343  |\n","|    ent_coef_loss   | 0.297    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153995   |\n","---------------------------------\n","Eval num_timesteps=3698400, episode_reward=105.33 +/- 1.16\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 105      |\n","| time/              |          |\n","|    total_timesteps | 3698400  |\n","| train/             |          |\n","|    actor_loss      | -2.94    |\n","|    critic_loss     | 0.994    |\n","|    ent_coef        | 0.00345  |\n","|    ent_coef_loss   | -3       |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154095   |\n","---------------------------------\n","Eval num_timesteps=3700800, episode_reward=87.44 +/- 25.75\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 87.4     |\n","| time/              |          |\n","|    total_timesteps | 3700800  |\n","| train/             |          |\n","|    actor_loss      | -4.19    |\n","|    critic_loss     | 0.2      |\n","|    ent_coef        | 0.00344  |\n","|    ent_coef_loss   | 0.478    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154195   |\n","---------------------------------\n","Eval num_timesteps=3703200, episode_reward=54.86 +/- 71.30\n","Episode length: 456.40 +/- 53.40\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 456      |\n","|    mean_reward     | 54.9     |\n","| time/              |          |\n","|    total_timesteps | 3703200  |\n","| train/             |          |\n","|    actor_loss      | -4.06    |\n","|    critic_loss     | 0.312    |\n","|    ent_coef        | 0.00342  |\n","|    ent_coef_loss   | 1.31     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154295   |\n","---------------------------------\n","Eval num_timesteps=3705600, episode_reward=114.96 +/- 0.18\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 115      |\n","| time/              |          |\n","|    total_timesteps | 3705600  |\n","| train/             |          |\n","|    actor_loss      | -2.53    |\n","|    critic_loss     | 1.21     |\n","|    ent_coef        | 0.00344  |\n","|    ent_coef_loss   | 2.62     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154395   |\n","---------------------------------\n","Eval num_timesteps=3708000, episode_reward=52.73 +/- 66.57\n","Episode length: 476.00 +/- 29.39\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 476      |\n","|    mean_reward     | 52.7     |\n","| time/              |          |\n","|    total_timesteps | 3708000  |\n","| train/             |          |\n","|    actor_loss      | -3       |\n","|    critic_loss     | 1.74     |\n","|    ent_coef        | 0.00348  |\n","|    ent_coef_loss   | -1.28    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154495   |\n","---------------------------------\n","Eval num_timesteps=3710400, episode_reward=105.17 +/- 2.68\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 105      |\n","| time/              |          |\n","|    total_timesteps | 3710400  |\n","| train/             |          |\n","|    actor_loss      | -1.92    |\n","|    critic_loss     | 1.16     |\n","|    ent_coef        | 0.00349  |\n","|    ent_coef_loss   | 1.51     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154595   |\n","---------------------------------\n","Eval num_timesteps=3712800, episode_reward=94.78 +/- 2.35\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 94.8     |\n","| time/              |          |\n","|    total_timesteps | 3712800  |\n","| train/             |          |\n","|    actor_loss      | -2.59    |\n","|    critic_loss     | 1.57     |\n","|    ent_coef        | 0.00349  |\n","|    ent_coef_loss   | 4.69     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154695   |\n","---------------------------------\n","Eval num_timesteps=3715200, episode_reward=106.91 +/- 1.49\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 107      |\n","| time/              |          |\n","|    total_timesteps | 3715200  |\n","| train/             |          |\n","|    actor_loss      | -2.88    |\n","|    critic_loss     | 40.9     |\n","|    ent_coef        | 0.00347  |\n","|    ent_coef_loss   | 3.64     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154795   |\n","---------------------------------\n","Eval num_timesteps=3717600, episode_reward=5.05 +/- 86.81\n","Episode length: 334.00 +/- 203.31\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 334      |\n","|    mean_reward     | 5.05     |\n","| time/              |          |\n","|    total_timesteps | 3717600  |\n","| train/             |          |\n","|    actor_loss      | -3.87    |\n","|    critic_loss     | 41.2     |\n","|    ent_coef        | 0.00349  |\n","|    ent_coef_loss   | -0.187   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154895   |\n","---------------------------------\n","Eval num_timesteps=3720000, episode_reward=86.20 +/- 3.51\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 86.2     |\n","| time/              |          |\n","|    total_timesteps | 3720000  |\n","| train/             |          |\n","|    actor_loss      | -1.82    |\n","|    critic_loss     | 1.64     |\n","|    ent_coef        | 0.00353  |\n","|    ent_coef_loss   | 3.8      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154995   |\n","---------------------------------\n","Eval num_timesteps=3722400, episode_reward=-80.94 +/- 22.87\n","Episode length: 326.80 +/- 168.03\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 327      |\n","|    mean_reward     | -80.9    |\n","| time/              |          |\n","|    total_timesteps | 3722400  |\n","| train/             |          |\n","|    actor_loss      | -3.68    |\n","|    critic_loss     | 1.26     |\n","|    ent_coef        | 0.00362  |\n","|    ent_coef_loss   | 1.36     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155095   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6010     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12177    |\n","|    total_timesteps | 3723264  |\n","| train/             |          |\n","|    actor_loss      | -3.59    |\n","|    critic_loss     | 2.69     |\n","|    ent_coef        | 0.00365  |\n","|    ent_coef_loss   | -3.58    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155131   |\n","---------------------------------\n","Eval num_timesteps=3724800, episode_reward=-101.47 +/- 4.73\n","Episode length: 112.80 +/- 13.72\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 113      |\n","|    mean_reward     | -101     |\n","| time/              |          |\n","|    total_timesteps | 3724800  |\n","| train/             |          |\n","|    actor_loss      | -3.96    |\n","|    critic_loss     | 1.43     |\n","|    ent_coef        | 0.0037   |\n","|    ent_coef_loss   | 0.991    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6020     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12180    |\n","|    total_timesteps | 3725208  |\n","| train/             |          |\n","|    actor_loss      | -3.12    |\n","|    critic_loss     | 2.73     |\n","|    ent_coef        | 0.00371  |\n","|    ent_coef_loss   | -2.71    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155212   |\n","---------------------------------\n","Eval num_timesteps=3727200, episode_reward=93.54 +/- 0.42\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 93.5     |\n","| time/              |          |\n","|    total_timesteps | 3727200  |\n","| train/             |          |\n","|    actor_loss      | -3.71    |\n","|    critic_loss     | 0.391    |\n","|    ent_coef        | 0.0037   |\n","|    ent_coef_loss   | -1.71    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155295   |\n","---------------------------------\n","Eval num_timesteps=3729600, episode_reward=99.15 +/- 1.01\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 99.2     |\n","| time/              |          |\n","|    total_timesteps | 3729600  |\n","| train/             |          |\n","|    actor_loss      | -3.69    |\n","|    critic_loss     | 1.36     |\n","|    ent_coef        | 0.00368  |\n","|    ent_coef_loss   | -0.763   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155395   |\n","---------------------------------\n","Eval num_timesteps=3732000, episode_reward=87.83 +/- 1.81\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 87.8     |\n","| time/              |          |\n","|    total_timesteps | 3732000  |\n","| train/             |          |\n","|    actor_loss      | -2.76    |\n","|    critic_loss     | 2.64     |\n","|    ent_coef        | 0.00373  |\n","|    ent_coef_loss   | 5.33     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155495   |\n","---------------------------------\n","Eval num_timesteps=3734400, episode_reward=-48.31 +/- 11.88\n","Episode length: 428.20 +/- 72.01\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 428      |\n","|    mean_reward     | -48.3    |\n","| time/              |          |\n","|    total_timesteps | 3734400  |\n","| train/             |          |\n","|    actor_loss      | -3.1     |\n","|    critic_loss     | 2.01     |\n","|    ent_coef        | 0.00382  |\n","|    ent_coef_loss   | 0.383    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6030     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12214    |\n","|    total_timesteps | 3734712  |\n","| train/             |          |\n","|    actor_loss      | -3.26    |\n","|    critic_loss     | 44.1     |\n","|    ent_coef        | 0.00382  |\n","|    ent_coef_loss   | -1.5     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155608   |\n","---------------------------------\n","Eval num_timesteps=3736800, episode_reward=-64.40 +/- 14.78\n","Episode length: 344.80 +/- 96.51\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 345      |\n","|    mean_reward     | -64.4    |\n","| time/              |          |\n","|    total_timesteps | 3736800  |\n","| train/             |          |\n","|    actor_loss      | -3.95    |\n","|    critic_loss     | 0.845    |\n","|    ent_coef        | 0.00382  |\n","|    ent_coef_loss   | 5.04     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6040     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12225    |\n","|    total_timesteps | 3737688  |\n","| train/             |          |\n","|    actor_loss      | -4.09    |\n","|    critic_loss     | 0.501    |\n","|    ent_coef        | 0.00385  |\n","|    ent_coef_loss   | -1.25    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155732   |\n","---------------------------------\n","Eval num_timesteps=3739200, episode_reward=106.34 +/- 4.78\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 106      |\n","| time/              |          |\n","|    total_timesteps | 3739200  |\n","| train/             |          |\n","|    actor_loss      | -1.91    |\n","|    critic_loss     | 1.72     |\n","|    ent_coef        | 0.00382  |\n","|    ent_coef_loss   | 0.662    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155795   |\n","---------------------------------\n","Eval num_timesteps=3741600, episode_reward=35.12 +/- 67.48\n","Episode length: 448.80 +/- 62.71\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 449      |\n","|    mean_reward     | 35.1     |\n","| time/              |          |\n","|    total_timesteps | 3741600  |\n","| train/             |          |\n","|    actor_loss      | -4.11    |\n","|    critic_loss     | 0.881    |\n","|    ent_coef        | 0.00384  |\n","|    ent_coef_loss   | 2.64     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155895   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6050     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12243    |\n","|    total_timesteps | 3743592  |\n","| train/             |          |\n","|    actor_loss      | -3.03    |\n","|    critic_loss     | 1.31     |\n","|    ent_coef        | 0.00388  |\n","|    ent_coef_loss   | -0.214   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155978   |\n","---------------------------------\n","Eval num_timesteps=3744000, episode_reward=88.98 +/- 3.16\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 89       |\n","| time/              |          |\n","|    total_timesteps | 3744000  |\n","| train/             |          |\n","|    actor_loss      | -3.13    |\n","|    critic_loss     | 0.534    |\n","|    ent_coef        | 0.00387  |\n","|    ent_coef_loss   | -1.53    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155995   |\n","---------------------------------\n","Eval num_timesteps=3746400, episode_reward=103.37 +/- 0.51\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 103      |\n","| time/              |          |\n","|    total_timesteps | 3746400  |\n","| train/             |          |\n","|    actor_loss      | -3.65    |\n","|    critic_loss     | 1.62     |\n","|    ent_coef        | 0.0038   |\n","|    ent_coef_loss   | 2.44     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156095   |\n","---------------------------------\n","Eval num_timesteps=3748800, episode_reward=108.49 +/- 1.20\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 108      |\n","| time/              |          |\n","|    total_timesteps | 3748800  |\n","| train/             |          |\n","|    actor_loss      | -3.44    |\n","|    critic_loss     | 8.31     |\n","|    ent_coef        | 0.0038   |\n","|    ent_coef_loss   | -1.2     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6060     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12268    |\n","|    total_timesteps | 3750648  |\n","| train/             |          |\n","|    actor_loss      | -3.14    |\n","|    critic_loss     | 0.686    |\n","|    ent_coef        | 0.00376  |\n","|    ent_coef_loss   | 0.171    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156272   |\n","---------------------------------\n","Eval num_timesteps=3751200, episode_reward=115.40 +/- 1.20\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 115      |\n","| time/              |          |\n","|    total_timesteps | 3751200  |\n","| train/             |          |\n","|    actor_loss      | -3.98    |\n","|    critic_loss     | 0.395    |\n","|    ent_coef        | 0.00374  |\n","|    ent_coef_loss   | -4.26    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156295   |\n","---------------------------------\n","Eval num_timesteps=3753600, episode_reward=9.31 +/- 77.91\n","Episode length: 374.60 +/- 102.39\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 375      |\n","|    mean_reward     | 9.31     |\n","| time/              |          |\n","|    total_timesteps | 3753600  |\n","| train/             |          |\n","|    actor_loss      | -3.6     |\n","|    critic_loss     | 1.24     |\n","|    ent_coef        | 0.00374  |\n","|    ent_coef_loss   | 0.61     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156395   |\n","---------------------------------\n","Eval num_timesteps=3756000, episode_reward=102.23 +/- 1.02\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 102      |\n","| time/              |          |\n","|    total_timesteps | 3756000  |\n","| train/             |          |\n","|    actor_loss      | -3.74    |\n","|    critic_loss     | 1.45     |\n","|    ent_coef        | 0.00385  |\n","|    ent_coef_loss   | 1.16     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156495   |\n","---------------------------------\n","Eval num_timesteps=3758400, episode_reward=-4.45 +/- 82.52\n","Episode length: 367.40 +/- 108.27\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 367      |\n","|    mean_reward     | -4.45    |\n","| time/              |          |\n","|    total_timesteps | 3758400  |\n","| train/             |          |\n","|    actor_loss      | -3.3     |\n","|    critic_loss     | 0.837    |\n","|    ent_coef        | 0.00383  |\n","|    ent_coef_loss   | 5.52     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156595   |\n","---------------------------------\n","Eval num_timesteps=3760800, episode_reward=110.45 +/- 1.39\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 110      |\n","| time/              |          |\n","|    total_timesteps | 3760800  |\n","| train/             |          |\n","|    actor_loss      | -2.49    |\n","|    critic_loss     | 1.48     |\n","|    ent_coef        | 0.00386  |\n","|    ent_coef_loss   | -2.35    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156695   |\n","---------------------------------\n","Eval num_timesteps=3763200, episode_reward=109.57 +/- 1.76\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 110      |\n","| time/              |          |\n","|    total_timesteps | 3763200  |\n","| train/             |          |\n","|    actor_loss      | -3.89    |\n","|    critic_loss     | 0.245    |\n","|    ent_coef        | 0.0038   |\n","|    ent_coef_loss   | -6.27    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156795   |\n","---------------------------------\n","Eval num_timesteps=3765600, episode_reward=26.61 +/- 51.29\n","Episode length: 472.40 +/- 22.54\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 472      |\n","|    mean_reward     | 26.6     |\n","| time/              |          |\n","|    total_timesteps | 3765600  |\n","| train/             |          |\n","|    actor_loss      | -2.44    |\n","|    critic_loss     | 1.41     |\n","|    ent_coef        | 0.00372  |\n","|    ent_coef_loss   | 6.61     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156895   |\n","---------------------------------\n","Eval num_timesteps=3768000, episode_reward=28.90 +/- 68.55\n","Episode length: 420.20 +/- 65.16\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 420      |\n","|    mean_reward     | 28.9     |\n","| time/              |          |\n","|    total_timesteps | 3768000  |\n","| train/             |          |\n","|    actor_loss      | -2.72    |\n","|    critic_loss     | 44.9     |\n","|    ent_coef        | 0.00371  |\n","|    ent_coef_loss   | 1.24     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156995   |\n","---------------------------------\n","Eval num_timesteps=3770400, episode_reward=-14.25 +/- 95.02\n","Episode length: 312.20 +/- 153.34\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 312      |\n","|    mean_reward     | -14.2    |\n","| time/              |          |\n","|    total_timesteps | 3770400  |\n","| train/             |          |\n","|    actor_loss      | -3.3     |\n","|    critic_loss     | 24.4     |\n","|    ent_coef        | 0.00368  |\n","|    ent_coef_loss   | 1.45     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157095   |\n","---------------------------------\n","Eval num_timesteps=3772800, episode_reward=104.51 +/- 1.35\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 105      |\n","| time/              |          |\n","|    total_timesteps | 3772800  |\n","| train/             |          |\n","|    actor_loss      | -3.33    |\n","|    critic_loss     | 0.826    |\n","|    ent_coef        | 0.00364  |\n","|    ent_coef_loss   | 0.157    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157195   |\n","---------------------------------\n","Eval num_timesteps=3775200, episode_reward=-72.72 +/- 15.95\n","Episode length: 197.40 +/- 72.99\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 197      |\n","|    mean_reward     | -72.7    |\n","| time/              |          |\n","|    total_timesteps | 3775200  |\n","| train/             |          |\n","|    actor_loss      | -3.18    |\n","|    critic_loss     | 1.48     |\n","|    ent_coef        | 0.00361  |\n","|    ent_coef_loss   | -0.835   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6070     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12358    |\n","|    total_timesteps | 3777504  |\n","| train/             |          |\n","|    actor_loss      | -3.1     |\n","|    critic_loss     | 0.628    |\n","|    ent_coef        | 0.00362  |\n","|    ent_coef_loss   | 2.66     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157391   |\n","---------------------------------\n","Eval num_timesteps=3777600, episode_reward=26.32 +/- 61.62\n","Episode length: 465.60 +/- 42.13\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 466      |\n","|    mean_reward     | 26.3     |\n","| time/              |          |\n","|    total_timesteps | 3777600  |\n","| train/             |          |\n","|    actor_loss      | -3       |\n","|    critic_loss     | 3.37     |\n","|    ent_coef        | 0.00363  |\n","|    ent_coef_loss   | 2.97     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6080     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12363    |\n","|    total_timesteps | 3778248  |\n","| train/             |          |\n","|    actor_loss      | -3.32    |\n","|    critic_loss     | 44.5     |\n","|    ent_coef        | 0.00365  |\n","|    ent_coef_loss   | -0.657   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157422   |\n","---------------------------------\n","Eval num_timesteps=3780000, episode_reward=103.98 +/- 1.98\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 104      |\n","| time/              |          |\n","|    total_timesteps | 3780000  |\n","| train/             |          |\n","|    actor_loss      | -1.53    |\n","|    critic_loss     | 1.08     |\n","|    ent_coef        | 0.0036   |\n","|    ent_coef_loss   | 0.553    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157495   |\n","---------------------------------\n","Eval num_timesteps=3782400, episode_reward=112.52 +/- 2.30\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 113      |\n","| time/              |          |\n","|    total_timesteps | 3782400  |\n","| train/             |          |\n","|    actor_loss      | -3.59    |\n","|    critic_loss     | 0.847    |\n","|    ent_coef        | 0.00357  |\n","|    ent_coef_loss   | -3.12    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6090     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12381    |\n","|    total_timesteps | 3783792  |\n","| train/             |          |\n","|    actor_loss      | -3.52    |\n","|    critic_loss     | 5.32     |\n","|    ent_coef        | 0.00353  |\n","|    ent_coef_loss   | -2.69    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157653   |\n","---------------------------------\n","Eval num_timesteps=3784800, episode_reward=-98.41 +/- 7.47\n","Episode length: 107.60 +/- 11.76\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 108      |\n","|    mean_reward     | -98.4    |\n","| time/              |          |\n","|    total_timesteps | 3784800  |\n","| train/             |          |\n","|    actor_loss      | -2.96    |\n","|    critic_loss     | 0.871    |\n","|    ent_coef        | 0.0035   |\n","|    ent_coef_loss   | -3.16    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157695   |\n","---------------------------------\n","Eval num_timesteps=3787200, episode_reward=33.22 +/- 68.06\n","Episode length: 432.80 +/- 54.87\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 433      |\n","|    mean_reward     | 33.2     |\n","| time/              |          |\n","|    total_timesteps | 3787200  |\n","| train/             |          |\n","|    actor_loss      | -3.25    |\n","|    critic_loss     | 1.32     |\n","|    ent_coef        | 0.00348  |\n","|    ent_coef_loss   | 1.01     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6100     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12393    |\n","|    total_timesteps | 3787800  |\n","| train/             |          |\n","|    actor_loss      | -1.81    |\n","|    critic_loss     | 1.54     |\n","|    ent_coef        | 0.00347  |\n","|    ent_coef_loss   | 3.71     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157820   |\n","---------------------------------\n","Eval num_timesteps=3789600, episode_reward=19.62 +/- 101.63\n","Episode length: 356.80 +/- 175.38\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 357      |\n","|    mean_reward     | 19.6     |\n","| time/              |          |\n","|    total_timesteps | 3789600  |\n","| train/             |          |\n","|    actor_loss      | -3.89    |\n","|    critic_loss     | 0.92     |\n","|    ent_coef        | 0.00349  |\n","|    ent_coef_loss   | 4.31     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157895   |\n","---------------------------------\n","Eval num_timesteps=3792000, episode_reward=102.24 +/- 0.16\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 102      |\n","| time/              |          |\n","|    total_timesteps | 3792000  |\n","| train/             |          |\n","|    actor_loss      | -3.06    |\n","|    critic_loss     | 1.08     |\n","|    ent_coef        | 0.00362  |\n","|    ent_coef_loss   | 0.481    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157995   |\n","---------------------------------\n","Eval num_timesteps=3794400, episode_reward=92.21 +/- 1.10\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 92.2     |\n","| time/              |          |\n","|    total_timesteps | 3794400  |\n","| train/             |          |\n","|    actor_loss      | -3.16    |\n","|    critic_loss     | 1.32     |\n","|    ent_coef        | 0.00364  |\n","|    ent_coef_loss   | 2.6      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158095   |\n","---------------------------------\n","Eval num_timesteps=3796800, episode_reward=110.04 +/- 1.71\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 110      |\n","| time/              |          |\n","|    total_timesteps | 3796800  |\n","| train/             |          |\n","|    actor_loss      | -3.79    |\n","|    critic_loss     | 0.696    |\n","|    ent_coef        | 0.00367  |\n","|    ent_coef_loss   | 1.82     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158195   |\n","---------------------------------\n","Eval num_timesteps=3799200, episode_reward=-16.53 +/- 82.20\n","Episode length: 374.60 +/- 102.39\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 375      |\n","|    mean_reward     | -16.5    |\n","| time/              |          |\n","|    total_timesteps | 3799200  |\n","| train/             |          |\n","|    actor_loss      | -3.42    |\n","|    critic_loss     | 1.51     |\n","|    ent_coef        | 0.00374  |\n","|    ent_coef_loss   | 4.05     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158295   |\n","---------------------------------\n","Eval num_timesteps=3801600, episode_reward=1.78 +/- 81.75\n","Episode length: 352.40 +/- 120.51\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 352      |\n","|    mean_reward     | 1.78     |\n","| time/              |          |\n","|    total_timesteps | 3801600  |\n","| train/             |          |\n","|    actor_loss      | -3.16    |\n","|    critic_loss     | 0.782    |\n","|    ent_coef        | 0.00383  |\n","|    ent_coef_loss   | 6.53     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158395   |\n","---------------------------------\n","Eval num_timesteps=3804000, episode_reward=106.61 +/- 1.31\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 107      |\n","| time/              |          |\n","|    total_timesteps | 3804000  |\n","| train/             |          |\n","|    actor_loss      | -3.14    |\n","|    critic_loss     | 0.488    |\n","|    ent_coef        | 0.00395  |\n","|    ent_coef_loss   | 0.226    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158495   |\n","---------------------------------\n","Eval num_timesteps=3806400, episode_reward=99.41 +/- 1.94\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 99.4     |\n","| time/              |          |\n","|    total_timesteps | 3806400  |\n","| train/             |          |\n","|    actor_loss      | -3.23    |\n","|    critic_loss     | 1.34     |\n","|    ent_coef        | 0.00397  |\n","|    ent_coef_loss   | -0.617   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158595   |\n","---------------------------------\n","Eval num_timesteps=3808800, episode_reward=34.20 +/- 54.48\n","Episode length: 485.60 +/- 11.76\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 486      |\n","|    mean_reward     | 34.2     |\n","| time/              |          |\n","|    total_timesteps | 3808800  |\n","| train/             |          |\n","|    actor_loss      | -4.06    |\n","|    critic_loss     | 0.268    |\n","|    ent_coef        | 0.00404  |\n","|    ent_coef_loss   | 0.904    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6110     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12467    |\n","|    total_timesteps | 3810360  |\n","| train/             |          |\n","|    actor_loss      | -3.36    |\n","|    critic_loss     | 1.55     |\n","|    ent_coef        | 0.00406  |\n","|    ent_coef_loss   | -2.75    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158760   |\n","---------------------------------\n","Eval num_timesteps=3811200, episode_reward=106.55 +/- 1.41\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 107      |\n","| time/              |          |\n","|    total_timesteps | 3811200  |\n","| train/             |          |\n","|    actor_loss      | -2.48    |\n","|    critic_loss     | 1.46     |\n","|    ent_coef        | 0.00405  |\n","|    ent_coef_loss   | -1.37    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158795   |\n","---------------------------------\n","Eval num_timesteps=3813600, episode_reward=-83.61 +/- 19.40\n","Episode length: 285.80 +/- 130.80\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 286      |\n","|    mean_reward     | -83.6    |\n","| time/              |          |\n","|    total_timesteps | 3813600  |\n","| train/             |          |\n","|    actor_loss      | -2.42    |\n","|    critic_loss     | 6.57     |\n","|    ent_coef        | 0.00413  |\n","|    ent_coef_loss   | 2.6      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158895   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6120     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12487    |\n","|    total_timesteps | 3814896  |\n","| train/             |          |\n","|    actor_loss      | -3.41    |\n","|    critic_loss     | 0.771    |\n","|    ent_coef        | 0.00412  |\n","|    ent_coef_loss   | -1.52    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158949   |\n","---------------------------------\n","Eval num_timesteps=3816000, episode_reward=89.79 +/- 10.31\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 89.8     |\n","| time/              |          |\n","|    total_timesteps | 3816000  |\n","| train/             |          |\n","|    actor_loss      | -2.99    |\n","|    critic_loss     | 1.43     |\n","|    ent_coef        | 0.00411  |\n","|    ent_coef_loss   | 2.9      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158995   |\n","---------------------------------\n","Eval num_timesteps=3818400, episode_reward=106.33 +/- 1.39\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 106      |\n","| time/              |          |\n","|    total_timesteps | 3818400  |\n","| train/             |          |\n","|    actor_loss      | -3.59    |\n","|    critic_loss     | 0.732    |\n","|    ent_coef        | 0.00418  |\n","|    ent_coef_loss   | 1.14     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159095   |\n","---------------------------------\n","Eval num_timesteps=3820800, episode_reward=-26.95 +/- 102.79\n","Episode length: 282.80 +/- 177.34\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 283      |\n","|    mean_reward     | -27      |\n","| time/              |          |\n","|    total_timesteps | 3820800  |\n","| train/             |          |\n","|    actor_loss      | -2.74    |\n","|    critic_loss     | 3.58     |\n","|    ent_coef        | 0.0042   |\n","|    ent_coef_loss   | 1.07     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159195   |\n","---------------------------------\n","Eval num_timesteps=3823200, episode_reward=14.34 +/- 74.60\n","Episode length: 400.40 +/- 81.32\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 400      |\n","|    mean_reward     | 14.3     |\n","| time/              |          |\n","|    total_timesteps | 3823200  |\n","| train/             |          |\n","|    actor_loss      | -2.73    |\n","|    critic_loss     | 0.886    |\n","|    ent_coef        | 0.00423  |\n","|    ent_coef_loss   | 2.89     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159295   |\n","---------------------------------\n","Eval num_timesteps=3825600, episode_reward=107.80 +/- 2.01\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 108      |\n","| time/              |          |\n","|    total_timesteps | 3825600  |\n","| train/             |          |\n","|    actor_loss      | -3.64    |\n","|    critic_loss     | 1.8      |\n","|    ent_coef        | 0.00427  |\n","|    ent_coef_loss   | 0.0435   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159395   |\n","---------------------------------\n","Eval num_timesteps=3828000, episode_reward=116.81 +/- 3.61\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 117      |\n","| time/              |          |\n","|    total_timesteps | 3828000  |\n","| train/             |          |\n","|    actor_loss      | -3.59    |\n","|    critic_loss     | 2.42     |\n","|    ent_coef        | 0.00428  |\n","|    ent_coef_loss   | -2.27    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159495   |\n","---------------------------------\n","Eval num_timesteps=3830400, episode_reward=20.83 +/- 105.26\n","Episode length: 363.20 +/- 167.55\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 363      |\n","|    mean_reward     | 20.8     |\n","| time/              |          |\n","|    total_timesteps | 3830400  |\n","| train/             |          |\n","|    actor_loss      | -4.16    |\n","|    critic_loss     | 0.351    |\n","|    ent_coef        | 0.00419  |\n","|    ent_coef_loss   | -4.57    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159595   |\n","---------------------------------\n","Eval num_timesteps=3832800, episode_reward=100.12 +/- 1.57\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 100      |\n","| time/              |          |\n","|    total_timesteps | 3832800  |\n","| train/             |          |\n","|    actor_loss      | -4.31    |\n","|    critic_loss     | 39.4     |\n","|    ent_coef        | 0.00412  |\n","|    ent_coef_loss   | -1.86    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159695   |\n","---------------------------------\n","Eval num_timesteps=3835200, episode_reward=-15.17 +/- 97.31\n","Episode length: 272.60 +/- 185.67\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 273      |\n","|    mean_reward     | -15.2    |\n","| time/              |          |\n","|    total_timesteps | 3835200  |\n","| train/             |          |\n","|    actor_loss      | -3.68    |\n","|    critic_loss     | 0.828    |\n","|    ent_coef        | 0.00406  |\n","|    ent_coef_loss   | -0.706   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159795   |\n","---------------------------------\n","Eval num_timesteps=3837600, episode_reward=111.12 +/- 1.03\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 111      |\n","| time/              |          |\n","|    total_timesteps | 3837600  |\n","| train/             |          |\n","|    actor_loss      | -3.87    |\n","|    critic_loss     | 0.644    |\n","|    ent_coef        | 0.00398  |\n","|    ent_coef_loss   | -2.9     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159895   |\n","---------------------------------\n","Eval num_timesteps=3840000, episode_reward=118.90 +/- 1.21\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 119      |\n","| time/              |          |\n","|    total_timesteps | 3840000  |\n","| train/             |          |\n","|    actor_loss      | -4.03    |\n","|    critic_loss     | 0.576    |\n","|    ent_coef        | 0.00395  |\n","|    ent_coef_loss   | 2.21     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159995   |\n","---------------------------------\n","Eval num_timesteps=3842400, episode_reward=112.36 +/- 1.52\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 112      |\n","| time/              |          |\n","|    total_timesteps | 3842400  |\n","| train/             |          |\n","|    actor_loss      | -3.53    |\n","|    critic_loss     | 2.41     |\n","|    ent_coef        | 0.00398  |\n","|    ent_coef_loss   | 1.14     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160095   |\n","---------------------------------\n","Eval num_timesteps=3844800, episode_reward=82.33 +/- 2.56\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 82.3     |\n","| time/              |          |\n","|    total_timesteps | 3844800  |\n","| train/             |          |\n","|    actor_loss      | -3.89    |\n","|    critic_loss     | 0.42     |\n","|    ent_coef        | 0.00401  |\n","|    ent_coef_loss   | 1.02     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160195   |\n","---------------------------------\n","Eval num_timesteps=3847200, episode_reward=100.98 +/- 0.24\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 101      |\n","| time/              |          |\n","|    total_timesteps | 3847200  |\n","| train/             |          |\n","|    actor_loss      | -2.44    |\n","|    critic_loss     | 43.9     |\n","|    ent_coef        | 0.00404  |\n","|    ent_coef_loss   | 0.246    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160295   |\n","---------------------------------\n","Eval num_timesteps=3849600, episode_reward=21.24 +/- 105.35\n","Episode length: 352.00 +/- 181.26\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 352      |\n","|    mean_reward     | 21.2     |\n","| time/              |          |\n","|    total_timesteps | 3849600  |\n","| train/             |          |\n","|    actor_loss      | -4.4     |\n","|    critic_loss     | 2.09     |\n","|    ent_coef        | 0.004    |\n","|    ent_coef_loss   | 0.296    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160395   |\n","---------------------------------\n","Eval num_timesteps=3852000, episode_reward=101.47 +/- 4.10\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 101      |\n","| time/              |          |\n","|    total_timesteps | 3852000  |\n","| train/             |          |\n","|    actor_loss      | -2.72    |\n","|    critic_loss     | 0.469    |\n","|    ent_coef        | 0.00408  |\n","|    ent_coef_loss   | 0.0954   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160495   |\n","---------------------------------\n","Eval num_timesteps=3854400, episode_reward=103.89 +/- 0.86\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 104      |\n","| time/              |          |\n","|    total_timesteps | 3854400  |\n","| train/             |          |\n","|    actor_loss      | -3.4     |\n","|    critic_loss     | 0.928    |\n","|    ent_coef        | 0.00411  |\n","|    ent_coef_loss   | 3.47     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160595   |\n","---------------------------------\n","Eval num_timesteps=3856800, episode_reward=102.53 +/- 0.55\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 103      |\n","| time/              |          |\n","|    total_timesteps | 3856800  |\n","| train/             |          |\n","|    actor_loss      | -3.99    |\n","|    critic_loss     | 0.8      |\n","|    ent_coef        | 0.00408  |\n","|    ent_coef_loss   | -1.86    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160695   |\n","---------------------------------\n","Eval num_timesteps=3859200, episode_reward=117.78 +/- 0.60\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 118      |\n","| time/              |          |\n","|    total_timesteps | 3859200  |\n","| train/             |          |\n","|    actor_loss      | -3.91    |\n","|    critic_loss     | 27.7     |\n","|    ent_coef        | 0.00404  |\n","|    ent_coef_loss   | -1.24    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160795   |\n","---------------------------------\n","Eval num_timesteps=3861600, episode_reward=108.53 +/- 1.01\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 109      |\n","| time/              |          |\n","|    total_timesteps | 3861600  |\n","| train/             |          |\n","|    actor_loss      | -3.9     |\n","|    critic_loss     | 1.32     |\n","|    ent_coef        | 0.00402  |\n","|    ent_coef_loss   | -2.06    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160895   |\n","---------------------------------\n","Eval num_timesteps=3864000, episode_reward=109.30 +/- 1.00\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 109      |\n","| time/              |          |\n","|    total_timesteps | 3864000  |\n","| train/             |          |\n","|    actor_loss      | -2.78    |\n","|    critic_loss     | 1.93     |\n","|    ent_coef        | 0.004    |\n","|    ent_coef_loss   | -3.7     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160995   |\n","---------------------------------\n","Eval num_timesteps=3866400, episode_reward=110.41 +/- 2.39\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 110      |\n","| time/              |          |\n","|    total_timesteps | 3866400  |\n","| train/             |          |\n","|    actor_loss      | -3.6     |\n","|    critic_loss     | 1.39     |\n","|    ent_coef        | 0.00391  |\n","|    ent_coef_loss   | -0.316   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 161095   |\n","---------------------------------\n","Eval num_timesteps=3868800, episode_reward=113.14 +/- 3.22\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 113      |\n","| time/              |          |\n","|    total_timesteps | 3868800  |\n","| train/             |          |\n","|    actor_loss      | -3.29    |\n","|    critic_loss     | 1.02     |\n","|    ent_coef        | 0.0039   |\n","|    ent_coef_loss   | 1.3      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 161195   |\n","---------------------------------\n","Eval num_timesteps=3871200, episode_reward=-9.28 +/- 88.37\n","Episode length: 299.00 +/- 164.12\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 299      |\n","|    mean_reward     | -9.28    |\n","| time/              |          |\n","|    total_timesteps | 3871200  |\n","| train/             |          |\n","|    actor_loss      | -3.96    |\n","|    critic_loss     | 2.29     |\n","|    ent_coef        | 0.00382  |\n","|    ent_coef_loss   | -3.19    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 161295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6130     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12689    |\n","|    total_timesteps | 3873480  |\n","| train/             |          |\n","|    actor_loss      | -3.97    |\n","|    critic_loss     | 17.3     |\n","|    ent_coef        | 0.00378  |\n","|    ent_coef_loss   | 3.23     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 161390   |\n","---------------------------------\n","Eval num_timesteps=3873600, episode_reward=93.72 +/- 0.74\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 93.7     |\n","| time/              |          |\n","|    total_timesteps | 3873600  |\n","| train/             |          |\n","|    actor_loss      | -3.87    |\n","|    critic_loss     | 0.593    |\n","|    ent_coef        | 0.00379  |\n","|    ent_coef_loss   | 5.93     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 161395   |\n","---------------------------------\n","Eval num_timesteps=3876000, episode_reward=60.83 +/- 65.56\n","Episode length: 463.60 +/- 44.58\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 464      |\n","|    mean_reward     | 60.8     |\n","| time/              |          |\n","|    total_timesteps | 3876000  |\n","| train/             |          |\n","|    actor_loss      | -4.35    |\n","|    critic_loss     | 0.588    |\n","|    ent_coef        | 0.00379  |\n","|    ent_coef_loss   | -5.35    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 161495   |\n","---------------------------------\n","Eval num_timesteps=3878400, episode_reward=100.05 +/- 0.54\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 100      |\n","| time/              |          |\n","|    total_timesteps | 3878400  |\n","| train/             |          |\n","|    actor_loss      | -3.6     |\n","|    critic_loss     | 1.38     |\n","|    ent_coef        | 0.00365  |\n","|    ent_coef_loss   | 0.44     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 161595   |\n","---------------------------------\n","Eval num_timesteps=3880800, episode_reward=113.86 +/- 0.12\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 114      |\n","| time/              |          |\n","|    total_timesteps | 3880800  |\n","| train/             |          |\n","|    actor_loss      | -3.58    |\n","|    critic_loss     | 1.72     |\n","|    ent_coef        | 0.00355  |\n","|    ent_coef_loss   | -0.395   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 161695   |\n","---------------------------------\n","Eval num_timesteps=3883200, episode_reward=119.41 +/- 2.10\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 119      |\n","| time/              |          |\n","|    total_timesteps | 3883200  |\n","| train/             |          |\n","|    actor_loss      | -3.65    |\n","|    critic_loss     | 0.537    |\n","|    ent_coef        | 0.00353  |\n","|    ent_coef_loss   | -0.572   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 161795   |\n","---------------------------------\n","Eval num_timesteps=3885600, episode_reward=107.68 +/- 4.02\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 108      |\n","| time/              |          |\n","|    total_timesteps | 3885600  |\n","| train/             |          |\n","|    actor_loss      | -3.73    |\n","|    critic_loss     | 46.5     |\n","|    ent_coef        | 0.00347  |\n","|    ent_coef_loss   | -1.4     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 161895   |\n","---------------------------------\n","Eval num_timesteps=3888000, episode_reward=119.76 +/- 0.36\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 120      |\n","| time/              |          |\n","|    total_timesteps | 3888000  |\n","| train/             |          |\n","|    actor_loss      | -4.13    |\n","|    critic_loss     | 0.59     |\n","|    ent_coef        | 0.00344  |\n","|    ent_coef_loss   | -5.21    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 161995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6140     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12750    |\n","|    total_timesteps | 3890184  |\n","| train/             |          |\n","|    actor_loss      | -4.56    |\n","|    critic_loss     | 38.5     |\n","|    ent_coef        | 0.00343  |\n","|    ent_coef_loss   | -0.44    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162086   |\n","---------------------------------\n","Eval num_timesteps=3890400, episode_reward=107.18 +/- 3.85\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 107      |\n","| time/              |          |\n","|    total_timesteps | 3890400  |\n","| train/             |          |\n","|    actor_loss      | -3.54    |\n","|    critic_loss     | 0.778    |\n","|    ent_coef        | 0.00343  |\n","|    ent_coef_loss   | 1.8      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162095   |\n","---------------------------------\n","Eval num_timesteps=3892800, episode_reward=117.99 +/- 0.53\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 118      |\n","| time/              |          |\n","|    total_timesteps | 3892800  |\n","| train/             |          |\n","|    actor_loss      | -4.1     |\n","|    critic_loss     | 39.7     |\n","|    ent_coef        | 0.00343  |\n","|    ent_coef_loss   | 0.367    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162195   |\n","---------------------------------\n","Eval num_timesteps=3895200, episode_reward=55.62 +/- 62.02\n","Episode length: 466.00 +/- 41.64\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 466      |\n","|    mean_reward     | 55.6     |\n","| time/              |          |\n","|    total_timesteps | 3895200  |\n","| train/             |          |\n","|    actor_loss      | -4.07    |\n","|    critic_loss     | 1.86     |\n","|    ent_coef        | 0.00349  |\n","|    ent_coef_loss   | 1.85     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162295   |\n","---------------------------------\n","Eval num_timesteps=3897600, episode_reward=98.16 +/- 0.33\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 98.2     |\n","| time/              |          |\n","|    total_timesteps | 3897600  |\n","| train/             |          |\n","|    actor_loss      | -4.23    |\n","|    critic_loss     | 0.991    |\n","|    ent_coef        | 0.00359  |\n","|    ent_coef_loss   | 1.87     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162395   |\n","---------------------------------\n","Eval num_timesteps=3900000, episode_reward=108.87 +/- 1.64\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 109      |\n","| time/              |          |\n","|    total_timesteps | 3900000  |\n","| train/             |          |\n","|    actor_loss      | -4.14    |\n","|    critic_loss     | 43.1     |\n","|    ent_coef        | 0.00363  |\n","|    ent_coef_loss   | 3.96     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162495   |\n","---------------------------------\n","Eval num_timesteps=3902400, episode_reward=83.52 +/- 1.16\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 83.5     |\n","| time/              |          |\n","|    total_timesteps | 3902400  |\n","| train/             |          |\n","|    actor_loss      | -3.17    |\n","|    critic_loss     | 0.869    |\n","|    ent_coef        | 0.00376  |\n","|    ent_coef_loss   | 4.19     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162595   |\n","---------------------------------\n","Eval num_timesteps=3904800, episode_reward=86.51 +/- 4.63\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 86.5     |\n","| time/              |          |\n","|    total_timesteps | 3904800  |\n","| train/             |          |\n","|    actor_loss      | -3.73    |\n","|    critic_loss     | 0.949    |\n","|    ent_coef        | 0.00395  |\n","|    ent_coef_loss   | 2.19     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162695   |\n","---------------------------------\n","Eval num_timesteps=3907200, episode_reward=39.20 +/- 88.45\n","Episode length: 382.80 +/- 143.54\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 383      |\n","|    mean_reward     | 39.2     |\n","| time/              |          |\n","|    total_timesteps | 3907200  |\n","| train/             |          |\n","|    actor_loss      | -2.83    |\n","|    critic_loss     | 3.8      |\n","|    ent_coef        | 0.004    |\n","|    ent_coef_loss   | 0.824    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162795   |\n","---------------------------------\n","Eval num_timesteps=3909600, episode_reward=104.39 +/- 3.00\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 104      |\n","| time/              |          |\n","|    total_timesteps | 3909600  |\n","| train/             |          |\n","|    actor_loss      | -3.64    |\n","|    critic_loss     | 1.1      |\n","|    ent_coef        | 0.00402  |\n","|    ent_coef_loss   | 0.0735   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162895   |\n","---------------------------------\n","Eval num_timesteps=3912000, episode_reward=109.22 +/- 0.45\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 109      |\n","| time/              |          |\n","|    total_timesteps | 3912000  |\n","| train/             |          |\n","|    actor_loss      | -2.96    |\n","|    critic_loss     | 22.9     |\n","|    ent_coef        | 0.00401  |\n","|    ent_coef_loss   | -0.43    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162995   |\n","---------------------------------\n","Eval num_timesteps=3914400, episode_reward=102.29 +/- 0.36\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 102      |\n","| time/              |          |\n","|    total_timesteps | 3914400  |\n","| train/             |          |\n","|    actor_loss      | -3.13    |\n","|    critic_loss     | 1.21     |\n","|    ent_coef        | 0.00405  |\n","|    ent_coef_loss   | -1.67    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 163095   |\n","---------------------------------\n","Eval num_timesteps=3916800, episode_reward=112.84 +/- 1.90\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 113      |\n","| time/              |          |\n","|    total_timesteps | 3916800  |\n","| train/             |          |\n","|    actor_loss      | -2.95    |\n","|    critic_loss     | 1.62     |\n","|    ent_coef        | 0.00394  |\n","|    ent_coef_loss   | 1.76     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 163195   |\n","---------------------------------\n","Eval num_timesteps=3919200, episode_reward=-47.27 +/- 10.44\n","Episode length: 366.20 +/- 72.50\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 366      |\n","|    mean_reward     | -47.3    |\n","| time/              |          |\n","|    total_timesteps | 3919200  |\n","| train/             |          |\n","|    actor_loss      | -3.98    |\n","|    critic_loss     | 0.344    |\n","|    ent_coef        | 0.00393  |\n","|    ent_coef_loss   | 0.783    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 163295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6150     |\n","|    fps             | 304      |\n","|    time_elapsed    | 12855    |\n","|    total_timesteps | 3920280  |\n","| train/             |          |\n","|    actor_loss      | -3.5     |\n","|    critic_loss     | 1.65     |\n","|    ent_coef        | 0.00393  |\n","|    ent_coef_loss   | 4.87     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 163340   |\n","---------------------------------\n","Eval num_timesteps=3921600, episode_reward=28.70 +/- 56.45\n","Episode length: 490.40 +/- 7.84\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 490      |\n","|    mean_reward     | 28.7     |\n","| time/              |          |\n","|    total_timesteps | 3921600  |\n","| train/             |          |\n","|    actor_loss      | -4.35    |\n","|    critic_loss     | 0.618    |\n","|    ent_coef        | 0.00394  |\n","|    ent_coef_loss   | -1.59    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 163395   |\n","---------------------------------\n","Eval num_timesteps=3924000, episode_reward=22.91 +/- 80.90\n","Episode length: 369.20 +/- 106.80\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 369      |\n","|    mean_reward     | 22.9     |\n","| time/              |          |\n","|    total_timesteps | 3924000  |\n","| train/             |          |\n","|    actor_loss      | -4.12    |\n","|    critic_loss     | 0.507    |\n","|    ent_coef        | 0.00406  |\n","|    ent_coef_loss   | -5.03    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 163495   |\n","---------------------------------\n","Eval num_timesteps=3926400, episode_reward=48.22 +/- 87.20\n","Episode length: 392.80 +/- 131.29\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 393      |\n","|    mean_reward     | 48.2     |\n","| time/              |          |\n","|    total_timesteps | 3926400  |\n","| train/             |          |\n","|    actor_loss      | -2.98    |\n","|    critic_loss     | 1.39     |\n","|    ent_coef        | 0.00399  |\n","|    ent_coef_loss   | -1.01    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 163595   |\n","---------------------------------\n","Eval num_timesteps=3928800, episode_reward=-67.02 +/- 13.93\n","Episode length: 230.20 +/- 37.72\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 230      |\n","|    mean_reward     | -67      |\n","| time/              |          |\n","|    total_timesteps | 3928800  |\n","| train/             |          |\n","|    actor_loss      | -3.63    |\n","|    critic_loss     | 2.18     |\n","|    ent_coef        | 0.00392  |\n","|    ent_coef_loss   | 2.15     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 163695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6160     |\n","|    fps             | 304      |\n","|    time_elapsed    | 12883    |\n","|    total_timesteps | 3928896  |\n","| train/             |          |\n","|    actor_loss      | -3.92    |\n","|    critic_loss     | 0.925    |\n","|    ent_coef        | 0.00392  |\n","|    ent_coef_loss   | 4.39     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 163699   |\n","---------------------------------\n","Eval num_timesteps=3931200, episode_reward=97.30 +/- 1.37\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 97.3     |\n","| time/              |          |\n","|    total_timesteps | 3931200  |\n","| train/             |          |\n","|    actor_loss      | -4.33    |\n","|    critic_loss     | 0.542    |\n","|    ent_coef        | 0.00402  |\n","|    ent_coef_loss   | -4.44    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 163795   |\n","---------------------------------\n","Eval num_timesteps=3933600, episode_reward=20.28 +/- 71.13\n","Episode length: 411.80 +/- 72.01\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 412      |\n","|    mean_reward     | 20.3     |\n","| time/              |          |\n","|    total_timesteps | 3933600  |\n","| train/             |          |\n","|    actor_loss      | -4.22    |\n","|    critic_loss     | 0.405    |\n","|    ent_coef        | 0.00396  |\n","|    ent_coef_loss   | -3.75    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 163895   |\n","---------------------------------\n","Eval num_timesteps=3936000, episode_reward=103.20 +/- 1.82\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 103      |\n","| time/              |          |\n","|    total_timesteps | 3936000  |\n","| train/             |          |\n","|    actor_loss      | -3.77    |\n","|    critic_loss     | 1.66     |\n","|    ent_coef        | 0.00394  |\n","|    ent_coef_loss   | 2.09     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 163995   |\n","---------------------------------\n","Eval num_timesteps=3938400, episode_reward=94.07 +/- 1.59\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 94.1     |\n","| time/              |          |\n","|    total_timesteps | 3938400  |\n","| train/             |          |\n","|    actor_loss      | -3.42    |\n","|    critic_loss     | 0.458    |\n","|    ent_coef        | 0.00397  |\n","|    ent_coef_loss   | -3.12    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164095   |\n","---------------------------------\n","Eval num_timesteps=3940800, episode_reward=93.19 +/- 0.36\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 93.2     |\n","| time/              |          |\n","|    total_timesteps | 3940800  |\n","| train/             |          |\n","|    actor_loss      | -3.56    |\n","|    critic_loss     | 2.13     |\n","|    ent_coef        | 0.00397  |\n","|    ent_coef_loss   | -3.15    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6170     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12925    |\n","|    total_timesteps | 3942768  |\n","| train/             |          |\n","|    actor_loss      | -4.65    |\n","|    critic_loss     | 3.82     |\n","|    ent_coef        | 0.00395  |\n","|    ent_coef_loss   | -3.19    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164277   |\n","---------------------------------\n","Eval num_timesteps=3943200, episode_reward=85.47 +/- 14.12\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 85.5     |\n","| time/              |          |\n","|    total_timesteps | 3943200  |\n","| train/             |          |\n","|    actor_loss      | -2.83    |\n","|    critic_loss     | 1.64     |\n","|    ent_coef        | 0.00395  |\n","|    ent_coef_loss   | 1.93     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164295   |\n","---------------------------------\n","Eval num_timesteps=3945600, episode_reward=104.46 +/- 0.65\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 104      |\n","| time/              |          |\n","|    total_timesteps | 3945600  |\n","| train/             |          |\n","|    actor_loss      | -3.77    |\n","|    critic_loss     | 0.71     |\n","|    ent_coef        | 0.00396  |\n","|    ent_coef_loss   | -1.52    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164395   |\n","---------------------------------\n","Eval num_timesteps=3948000, episode_reward=108.69 +/- 3.16\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 109      |\n","| time/              |          |\n","|    total_timesteps | 3948000  |\n","| train/             |          |\n","|    actor_loss      | -4.03    |\n","|    critic_loss     | 0.53     |\n","|    ent_coef        | 0.0039   |\n","|    ent_coef_loss   | -3.92    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164495   |\n","---------------------------------\n","Eval num_timesteps=3950400, episode_reward=55.27 +/- 65.89\n","Episode length: 454.80 +/- 55.36\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 455      |\n","|    mean_reward     | 55.3     |\n","| time/              |          |\n","|    total_timesteps | 3950400  |\n","| train/             |          |\n","|    actor_loss      | -3.98    |\n","|    critic_loss     | 2.05     |\n","|    ent_coef        | 0.00382  |\n","|    ent_coef_loss   | -2.1     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164595   |\n","---------------------------------\n","Eval num_timesteps=3952800, episode_reward=92.15 +/- 4.62\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 92.1     |\n","| time/              |          |\n","|    total_timesteps | 3952800  |\n","| train/             |          |\n","|    actor_loss      | -3.1     |\n","|    critic_loss     | 1.13     |\n","|    ent_coef        | 0.00383  |\n","|    ent_coef_loss   | -4.24    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164695   |\n","---------------------------------\n","Eval num_timesteps=3955200, episode_reward=115.74 +/- 0.35\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 116      |\n","| time/              |          |\n","|    total_timesteps | 3955200  |\n","| train/             |          |\n","|    actor_loss      | -4.16    |\n","|    critic_loss     | 0.55     |\n","|    ent_coef        | 0.00376  |\n","|    ent_coef_loss   | -3.34    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164795   |\n","---------------------------------\n","Eval num_timesteps=3957600, episode_reward=112.41 +/- 1.36\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 112      |\n","| time/              |          |\n","|    total_timesteps | 3957600  |\n","| train/             |          |\n","|    actor_loss      | -4.05    |\n","|    critic_loss     | 1.04     |\n","|    ent_coef        | 0.0037   |\n","|    ent_coef_loss   | 0.0795   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164895   |\n","---------------------------------\n","Eval num_timesteps=3960000, episode_reward=116.17 +/- 4.43\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 116      |\n","| time/              |          |\n","|    total_timesteps | 3960000  |\n","| train/             |          |\n","|    actor_loss      | -3.76    |\n","|    critic_loss     | 0.594    |\n","|    ent_coef        | 0.00364  |\n","|    ent_coef_loss   | 3.14     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164995   |\n","---------------------------------\n","Eval num_timesteps=3962400, episode_reward=100.03 +/- 9.24\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 100      |\n","| time/              |          |\n","|    total_timesteps | 3962400  |\n","| train/             |          |\n","|    actor_loss      | -3.4     |\n","|    critic_loss     | 0.536    |\n","|    ent_coef        | 0.00372  |\n","|    ent_coef_loss   | 0.496    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 165095   |\n","---------------------------------\n","Eval num_timesteps=3964800, episode_reward=107.16 +/- 2.51\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 107      |\n","| time/              |          |\n","|    total_timesteps | 3964800  |\n","| train/             |          |\n","|    actor_loss      | -3.93    |\n","|    critic_loss     | 1.04     |\n","|    ent_coef        | 0.00376  |\n","|    ent_coef_loss   | -1.93    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 165195   |\n","---------------------------------\n","Eval num_timesteps=3967200, episode_reward=107.77 +/- 0.41\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 108      |\n","| time/              |          |\n","|    total_timesteps | 3967200  |\n","| train/             |          |\n","|    actor_loss      | -3.54    |\n","|    critic_loss     | 0.494    |\n","|    ent_coef        | 0.00376  |\n","|    ent_coef_loss   | 0.0161   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 165295   |\n","---------------------------------\n","Eval num_timesteps=3969600, episode_reward=108.15 +/- 4.17\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 108      |\n","| time/              |          |\n","|    total_timesteps | 3969600  |\n","| train/             |          |\n","|    actor_loss      | -3.59    |\n","|    critic_loss     | 6.83     |\n","|    ent_coef        | 0.00368  |\n","|    ent_coef_loss   | -0.305   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 165395   |\n","---------------------------------\n","Eval num_timesteps=3972000, episode_reward=114.49 +/- 2.52\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 114      |\n","| time/              |          |\n","|    total_timesteps | 3972000  |\n","| train/             |          |\n","|    actor_loss      | -4.15    |\n","|    critic_loss     | 43.2     |\n","|    ent_coef        | 0.00364  |\n","|    ent_coef_loss   | -2.41    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 165495   |\n","---------------------------------\n","Eval num_timesteps=3974400, episode_reward=116.36 +/- 0.81\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 116      |\n","| time/              |          |\n","|    total_timesteps | 3974400  |\n","| train/             |          |\n","|    actor_loss      | -3.53    |\n","|    critic_loss     | 1.07     |\n","|    ent_coef        | 0.00359  |\n","|    ent_coef_loss   | -2.96    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 165595   |\n","---------------------------------\n","Eval num_timesteps=3976800, episode_reward=124.88 +/- 0.78\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 125      |\n","| time/              |          |\n","|    total_timesteps | 3976800  |\n","| train/             |          |\n","|    actor_loss      | -2.6     |\n","|    critic_loss     | 1.05     |\n","|    ent_coef        | 0.00353  |\n","|    ent_coef_loss   | 3.23     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 165695   |\n","---------------------------------\n","New best mean reward!\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6180     |\n","|    fps             | 304      |\n","|    time_elapsed    | 13052    |\n","|    total_timesteps | 3978576  |\n","| train/             |          |\n","|    actor_loss      | -3.16    |\n","|    critic_loss     | 1.03     |\n","|    ent_coef        | 0.00354  |\n","|    ent_coef_loss   | 1.04     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 165769   |\n","---------------------------------\n","Eval num_timesteps=3979200, episode_reward=129.54 +/- 0.30\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 130      |\n","| time/              |          |\n","|    total_timesteps | 3979200  |\n","| train/             |          |\n","|    actor_loss      | -3.17    |\n","|    critic_loss     | 1.33     |\n","|    ent_coef        | 0.00356  |\n","|    ent_coef_loss   | 2.91     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 165795   |\n","---------------------------------\n","New best mean reward!\n","Eval num_timesteps=3981600, episode_reward=82.69 +/- 3.14\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 82.7     |\n","| time/              |          |\n","|    total_timesteps | 3981600  |\n","| train/             |          |\n","|    actor_loss      | -3.67    |\n","|    critic_loss     | 0.492    |\n","|    ent_coef        | 0.00358  |\n","|    ent_coef_loss   | 1.36     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 165895   |\n","---------------------------------\n","Eval num_timesteps=3984000, episode_reward=109.07 +/- 2.80\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 109      |\n","| time/              |          |\n","|    total_timesteps | 3984000  |\n","| train/             |          |\n","|    actor_loss      | -4.09    |\n","|    critic_loss     | 3.02     |\n","|    ent_coef        | 0.00349  |\n","|    ent_coef_loss   | -3.38    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 165995   |\n","---------------------------------\n","Eval num_timesteps=3986400, episode_reward=115.39 +/- 2.76\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 115      |\n","| time/              |          |\n","|    total_timesteps | 3986400  |\n","| train/             |          |\n","|    actor_loss      | -4       |\n","|    critic_loss     | 1.57     |\n","|    ent_coef        | 0.00343  |\n","|    ent_coef_loss   | -1.91    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 166095   |\n","---------------------------------\n","Eval num_timesteps=3988800, episode_reward=100.92 +/- 1.10\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 101      |\n","| time/              |          |\n","|    total_timesteps | 3988800  |\n","| train/             |          |\n","|    actor_loss      | -4.06    |\n","|    critic_loss     | 0.823    |\n","|    ent_coef        | 0.00336  |\n","|    ent_coef_loss   | -1.01    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 166195   |\n","---------------------------------\n","Eval num_timesteps=3991200, episode_reward=119.99 +/- 0.95\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 120      |\n","| time/              |          |\n","|    total_timesteps | 3991200  |\n","| train/             |          |\n","|    actor_loss      | -2.33    |\n","|    critic_loss     | 1.75     |\n","|    ent_coef        | 0.00333  |\n","|    ent_coef_loss   | 1.34     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 166295   |\n","---------------------------------\n","Eval num_timesteps=3993600, episode_reward=109.90 +/- 6.31\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 110      |\n","| time/              |          |\n","|    total_timesteps | 3993600  |\n","| train/             |          |\n","|    actor_loss      | -3.79    |\n","|    critic_loss     | 1.35     |\n","|    ent_coef        | 0.0033   |\n","|    ent_coef_loss   | 0.624    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 166395   |\n","---------------------------------\n","Eval num_timesteps=3996000, episode_reward=117.67 +/- 2.74\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 118      |\n","| time/              |          |\n","|    total_timesteps | 3996000  |\n","| train/             |          |\n","|    actor_loss      | -3.73    |\n","|    critic_loss     | 1.07     |\n","|    ent_coef        | 0.00325  |\n","|    ent_coef_loss   | -2.38    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 166495   |\n","---------------------------------\n","Eval num_timesteps=3998400, episode_reward=113.95 +/- 3.52\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 114      |\n","| time/              |          |\n","|    total_timesteps | 3998400  |\n","| train/             |          |\n","|    actor_loss      | -4.59    |\n","|    critic_loss     | 0.516    |\n","|    ent_coef        | 0.00326  |\n","|    ent_coef_loss   | 1.96     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 166595   |\n","---------------------------------\n"]}],"source":["from stable_baselines3.common.logger import configure\n","\n","tmp_path = \"multiwalker_sac_00_log_eval/\"\n","# set up logger\n","new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n","\n","model = SAC(\"MlpPolicy\", env, verbose=3) #,train_freq=1\n","\n","model.set_logger(new_logger)\n","model.learn(total_timesteps=4000000, log_interval=10,callback=eval_callback)\n","model.save(\"multiwalker_sac_00\")"]},{"cell_type":"code","source":["from stable_baselines3 import SAC\n","from pettingzoo.sisl import multiwalker_v9\n","import supersuit as ss\n","from stable_baselines3.common.monitor import Monitor"],"metadata":{"id":"-6BKh_FuTZzN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HTOVAZKHTdeV","executionInfo":{"status":"ok","timestamp":1697725712497,"user_tz":-120,"elapsed":334,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"67c4f770-06db-4aed-c296-ff714445355d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["'=2.13'\t\t\t\t     multiwalker_sac_log_eval\n"," DQN_new_pettingzoo_gym_cap.ipynb    multiwalker_td3_log_eval\n"," DQN_policies\t\t\t     policy_log_eval\n","'Entrenamientos antiguos sin logs'   PPO_policies\n"," Entrenamientos_log_no_eval\t     results_rllib\n"," MCR_TFM.ipynb\t\t\t     TFM_Multiwalker_DDPG_PPO_gym_cap.ipynb\n"," multi_car_racing\t\t     TFM_Multiwalker_SAC_gym_cap.ipynb\n"," multiwalker_ddpg_log_eval\t     TFM_Multiwalker_TD3_gym_cap.ipynb\n"," multiwalker_ddpg.zip\t\t     TFM_PPO_new_pettingzoo_gym_cap.ipynb\n"," multiwalker_ppo_log_eval\t     TFM_PPO_pettingzoo_gym_cap.ipynb\n"," multiwalker_ppo.zip\n"]}]},{"cell_type":"code","source":["env = multiwalker_v9.parallel_env(render_mode=\"rgb_array\",shared_reward=0)\n","\n","# #---------------------------------\n","# log_dir = \"./policy_log\"\n","# os.makedirs(log_dir, exist_ok=True)\n","# env = Monitor(env, log_dir)\n","# #---------------------------------\n","# env = ss.color_reduction_v0(env, mode='B')\n","# env = ss.black_death_v3(env)\n","# env = ss.resize_v1(env, x_size=84, y_size=84)\n","env = ss.frame_stack_v1(env, 3)\n","env = ss.pettingzoo_env_to_vec_env_v1(env)\n","env = ss.concat_vec_envs_v1(env, 8, num_cpus=4, base_class='stable_baselines3')"],"metadata":{"id":"U6TtjGBcTedF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3.common.callbacks import EvalCallback\n","eval_callback = EvalCallback(env, best_model_save_path=\"./multiwalker_sac2_00_log_eval/\",\n","                             log_path=\"./multiwalker_sac2_00_log_eval/\", eval_freq=100,\n","                             deterministic=False, render=False)\n"],"metadata":{"id":"1ah3u9HuTga_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3.common.logger import configure\n","\n","tmp_path = \"multiwalker_sac2_00_log_eval/\"\n","# set up logger\n","new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n","\n","model = SAC(\"MlpPolicy\", env, verbose=3,batch_size=512, learning_rate=0.0005) #,train_freq=1\n","\n","model.set_logger(new_logger)\n","model.learn(total_timesteps=4000000, log_interval=10,callback=eval_callback)\n","model.save(\"multiwalker_sac2_00\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"muMFxxDZTg_Z","outputId":"d6de0798-0b95-4cd6-9875-09e84d5cba80","executionInfo":{"status":"ok","timestamp":1699028154418,"user_tz":-60,"elapsed":13124395,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Logging to multiwalker_sac2_00_log_eval/\n","Using cuda device\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 10       |\n","|    fps             | 319      |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 1536     |\n","| train/             |          |\n","|    actor_loss      | -5.05    |\n","|    critic_loss     | 65.1     |\n","|    ent_coef        | 0.971    |\n","|    ent_coef_loss   | -0.195   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 59       |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 20       |\n","|    fps             | 362      |\n","|    time_elapsed    | 5        |\n","|    total_timesteps | 1848     |\n","| train/             |          |\n","|    actor_loss      | -4.39    |\n","|    critic_loss     | 90       |\n","|    ent_coef        | 0.965    |\n","|    ent_coef_loss   | -0.239   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 72       |\n","---------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n","|    actor_loss      | 4.16     |\n","|    critic_loss     | 1.21     |\n","|    ent_coef        | 0.00321  |\n","|    ent_coef_loss   | -0.0739  |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 139195   |\n","---------------------------------\n","Eval num_timesteps=3343200, episode_reward=-55.15 +/- 50.50\n","Episode length: 264.80 +/- 192.04\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 265      |\n","|    mean_reward     | -55.2    |\n","| time/              |          |\n","|    total_timesteps | 3343200  |\n","| train/             |          |\n","|    actor_loss      | 3        |\n","|    critic_loss     | 13.4     |\n","|    ent_coef        | 0.00314  |\n","|    ent_coef_loss   | 1.57     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 139295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5730     |\n","|    fps             | 306      |\n","|    time_elapsed    | 10917    |\n","|    total_timesteps | 3343488  |\n","| train/             |          |\n","|    actor_loss      | 2.98     |\n","|    critic_loss     | 0.666    |\n","|    ent_coef        | 0.00314  |\n","|    ent_coef_loss   | 0.182    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 139307   |\n","---------------------------------\n","Eval num_timesteps=3345600, episode_reward=0.71 +/- 1.14\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.708    |\n","| time/              |          |\n","|    total_timesteps | 3345600  |\n","| train/             |          |\n","|    actor_loss      | 2.62     |\n","|    critic_loss     | 0.386    |\n","|    ent_coef        | 0.00312  |\n","|    ent_coef_loss   | -0.414   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 139395   |\n","---------------------------------\n","Eval num_timesteps=3348000, episode_reward=6.20 +/- 4.55\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.2      |\n","| time/              |          |\n","|    total_timesteps | 3348000  |\n","| train/             |          |\n","|    actor_loss      | 3.22     |\n","|    critic_loss     | 43.2     |\n","|    ent_coef        | 0.00311  |\n","|    ent_coef_loss   | -0.0984  |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 139495   |\n","---------------------------------\n","Eval num_timesteps=3350400, episode_reward=-5.40 +/- 0.90\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -5.4     |\n","| time/              |          |\n","|    total_timesteps | 3350400  |\n","| train/             |          |\n","|    actor_loss      | 2.83     |\n","|    critic_loss     | 0.572    |\n","|    ent_coef        | 0.00307  |\n","|    ent_coef_loss   | -1.65    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 139595   |\n","---------------------------------\n","Eval num_timesteps=3352800, episode_reward=-0.33 +/- 1.75\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.334   |\n","| time/              |          |\n","|    total_timesteps | 3352800  |\n","| train/             |          |\n","|    actor_loss      | 3.06     |\n","|    critic_loss     | 15.4     |\n","|    ent_coef        | 0.00299  |\n","|    ent_coef_loss   | -1.4     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 139695   |\n","---------------------------------\n","Eval num_timesteps=3355200, episode_reward=-0.19 +/- 1.25\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.189   |\n","| time/              |          |\n","|    total_timesteps | 3355200  |\n","| train/             |          |\n","|    actor_loss      | 3.52     |\n","|    critic_loss     | 0.47     |\n","|    ent_coef        | 0.00304  |\n","|    ent_coef_loss   | 0.158    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 139795   |\n","---------------------------------\n","Eval num_timesteps=3357600, episode_reward=4.82 +/- 3.39\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.82     |\n","| time/              |          |\n","|    total_timesteps | 3357600  |\n","| train/             |          |\n","|    actor_loss      | 3.01     |\n","|    critic_loss     | 0.757    |\n","|    ent_coef        | 0.00296  |\n","|    ent_coef_loss   | -1.5     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 139895   |\n","---------------------------------\n","Eval num_timesteps=3360000, episode_reward=-56.70 +/- 48.17\n","Episode length: 387.20 +/- 92.10\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 387      |\n","|    mean_reward     | -56.7    |\n","| time/              |          |\n","|    total_timesteps | 3360000  |\n","| train/             |          |\n","|    actor_loss      | 3.07     |\n","|    critic_loss     | 0.746    |\n","|    ent_coef        | 0.00287  |\n","|    ent_coef_loss   | 2.53     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 139995   |\n","---------------------------------\n","Eval num_timesteps=3362400, episode_reward=3.70 +/- 0.73\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.7      |\n","| time/              |          |\n","|    total_timesteps | 3362400  |\n","| train/             |          |\n","|    actor_loss      | 3.21     |\n","|    critic_loss     | 1.72     |\n","|    ent_coef        | 0.00295  |\n","|    ent_coef_loss   | 1.22     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 140095   |\n","---------------------------------\n","Eval num_timesteps=3364800, episode_reward=10.28 +/- 8.94\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 10.3     |\n","| time/              |          |\n","|    total_timesteps | 3364800  |\n","| train/             |          |\n","|    actor_loss      | 2.76     |\n","|    critic_loss     | 0.439    |\n","|    ent_coef        | 0.00311  |\n","|    ent_coef_loss   | -1.18    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 140195   |\n","---------------------------------\n","Eval num_timesteps=3367200, episode_reward=0.69 +/- 1.87\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.689    |\n","| time/              |          |\n","|    total_timesteps | 3367200  |\n","| train/             |          |\n","|    actor_loss      | 3.19     |\n","|    critic_loss     | 23.8     |\n","|    ent_coef        | 0.00308  |\n","|    ent_coef_loss   | 0.219    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 140295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5740     |\n","|    fps             | 306      |\n","|    time_elapsed    | 11001    |\n","|    total_timesteps | 3369264  |\n","| train/             |          |\n","|    actor_loss      | 2.89     |\n","|    critic_loss     | 0.913    |\n","|    ent_coef        | 0.00308  |\n","|    ent_coef_loss   | -0.703   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 140381   |\n","---------------------------------\n","Eval num_timesteps=3369600, episode_reward=-54.08 +/- 48.05\n","Episode length: 305.60 +/- 158.73\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 306      |\n","|    mean_reward     | -54.1    |\n","| time/              |          |\n","|    total_timesteps | 3369600  |\n","| train/             |          |\n","|    actor_loss      | 2.72     |\n","|    critic_loss     | 0.974    |\n","|    ent_coef        | 0.00308  |\n","|    ent_coef_loss   | -0.693   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 140395   |\n","---------------------------------\n","Eval num_timesteps=3372000, episode_reward=-31.63 +/- 46.26\n","Episode length: 353.20 +/- 179.79\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 353      |\n","|    mean_reward     | -31.6    |\n","| time/              |          |\n","|    total_timesteps | 3372000  |\n","| train/             |          |\n","|    actor_loss      | 3.52     |\n","|    critic_loss     | 0.531    |\n","|    ent_coef        | 0.00301  |\n","|    ent_coef_loss   | 2.22     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 140495   |\n","---------------------------------\n","Eval num_timesteps=3374400, episode_reward=4.82 +/- 1.21\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.82     |\n","| time/              |          |\n","|    total_timesteps | 3374400  |\n","| train/             |          |\n","|    actor_loss      | 2.94     |\n","|    critic_loss     | 0.306    |\n","|    ent_coef        | 0.00321  |\n","|    ent_coef_loss   | 0.671    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 140595   |\n","---------------------------------\n","Eval num_timesteps=3376800, episode_reward=11.72 +/- 3.30\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 11.7     |\n","| time/              |          |\n","|    total_timesteps | 3376800  |\n","| train/             |          |\n","|    actor_loss      | 2.65     |\n","|    critic_loss     | 0.686    |\n","|    ent_coef        | 0.00322  |\n","|    ent_coef_loss   | -1.19    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 140695   |\n","---------------------------------\n","Eval num_timesteps=3379200, episode_reward=5.80 +/- 1.07\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.8      |\n","| time/              |          |\n","|    total_timesteps | 3379200  |\n","| train/             |          |\n","|    actor_loss      | 2.97     |\n","|    critic_loss     | 0.604    |\n","|    ent_coef        | 0.00324  |\n","|    ent_coef_loss   | 1.56     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 140795   |\n","---------------------------------\n","Eval num_timesteps=3381600, episode_reward=-86.30 +/- 9.32\n","Episode length: 418.80 +/- 5.88\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 419      |\n","|    mean_reward     | -86.3    |\n","| time/              |          |\n","|    total_timesteps | 3381600  |\n","| train/             |          |\n","|    actor_loss      | 2.92     |\n","|    critic_loss     | 18.8     |\n","|    ent_coef        | 0.00312  |\n","|    ent_coef_loss   | -1.57    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 140895   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5750     |\n","|    fps             | 306      |\n","|    time_elapsed    | 11048    |\n","|    total_timesteps | 3383376  |\n","| train/             |          |\n","|    actor_loss      | 3.29     |\n","|    critic_loss     | 15.4     |\n","|    ent_coef        | 0.00314  |\n","|    ent_coef_loss   | 5.88     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 140969   |\n","---------------------------------\n","Eval num_timesteps=3384000, episode_reward=6.76 +/- 2.16\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.76     |\n","| time/              |          |\n","|    total_timesteps | 3384000  |\n","| train/             |          |\n","|    actor_loss      | 3.41     |\n","|    critic_loss     | 0.632    |\n","|    ent_coef        | 0.00319  |\n","|    ent_coef_loss   | -0.476   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 140995   |\n","---------------------------------\n","Eval num_timesteps=3386400, episode_reward=4.13 +/- 0.13\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.13     |\n","| time/              |          |\n","|    total_timesteps | 3386400  |\n","| train/             |          |\n","|    actor_loss      | 2.53     |\n","|    critic_loss     | 0.452    |\n","|    ent_coef        | 0.0031   |\n","|    ent_coef_loss   | -1.26    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 141095   |\n","---------------------------------\n","Eval num_timesteps=3388800, episode_reward=-44.73 +/- 50.81\n","Episode length: 335.60 +/- 201.35\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 336      |\n","|    mean_reward     | -44.7    |\n","| time/              |          |\n","|    total_timesteps | 3388800  |\n","| train/             |          |\n","|    actor_loss      | 3.05     |\n","|    critic_loss     | 18.1     |\n","|    ent_coef        | 0.00298  |\n","|    ent_coef_loss   | -1.19    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 141195   |\n","---------------------------------\n","Eval num_timesteps=3391200, episode_reward=1.95 +/- 0.17\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.95     |\n","| time/              |          |\n","|    total_timesteps | 3391200  |\n","| train/             |          |\n","|    actor_loss      | 2.97     |\n","|    critic_loss     | 0.348    |\n","|    ent_coef        | 0.00293  |\n","|    ent_coef_loss   | 1.12     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 141295   |\n","---------------------------------\n","Eval num_timesteps=3393600, episode_reward=4.98 +/- 1.81\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.98     |\n","| time/              |          |\n","|    total_timesteps | 3393600  |\n","| train/             |          |\n","|    actor_loss      | 3.05     |\n","|    critic_loss     | 0.61     |\n","|    ent_coef        | 0.00294  |\n","|    ent_coef_loss   | -1.51    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 141395   |\n","---------------------------------\n","Eval num_timesteps=3396000, episode_reward=-2.09 +/- 2.05\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -2.09    |\n","| time/              |          |\n","|    total_timesteps | 3396000  |\n","| train/             |          |\n","|    actor_loss      | 2.69     |\n","|    critic_loss     | 0.438    |\n","|    ent_coef        | 0.00304  |\n","|    ent_coef_loss   | 5.69     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 141495   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5760     |\n","|    fps             | 306      |\n","|    time_elapsed    | 11101    |\n","|    total_timesteps | 3398112  |\n","| train/             |          |\n","|    actor_loss      | 3.52     |\n","|    critic_loss     | 2.24     |\n","|    ent_coef        | 0.00335  |\n","|    ent_coef_loss   | 3.93     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 141583   |\n","---------------------------------\n","Eval num_timesteps=3398400, episode_reward=-1.66 +/- 3.13\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.66    |\n","| time/              |          |\n","|    total_timesteps | 3398400  |\n","| train/             |          |\n","|    actor_loss      | 2.88     |\n","|    critic_loss     | 0.363    |\n","|    ent_coef        | 0.00337  |\n","|    ent_coef_loss   | 2.92     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 141595   |\n","---------------------------------\n","Eval num_timesteps=3400800, episode_reward=5.33 +/- 0.52\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.33     |\n","| time/              |          |\n","|    total_timesteps | 3400800  |\n","| train/             |          |\n","|    actor_loss      | 2.93     |\n","|    critic_loss     | 0.528    |\n","|    ent_coef        | 0.00344  |\n","|    ent_coef_loss   | -2.59    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 141695   |\n","---------------------------------\n","Eval num_timesteps=3403200, episode_reward=8.63 +/- 1.98\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 8.63     |\n","| time/              |          |\n","|    total_timesteps | 3403200  |\n","| train/             |          |\n","|    actor_loss      | 3.24     |\n","|    critic_loss     | 0.731    |\n","|    ent_coef        | 0.00348  |\n","|    ent_coef_loss   | 3.69     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 141795   |\n","---------------------------------\n","Eval num_timesteps=3405600, episode_reward=3.21 +/- 0.32\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.21     |\n","| time/              |          |\n","|    total_timesteps | 3405600  |\n","| train/             |          |\n","|    actor_loss      | 3.37     |\n","|    critic_loss     | 1.2      |\n","|    ent_coef        | 0.00349  |\n","|    ent_coef_loss   | -2.77    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 141895   |\n","---------------------------------\n","Eval num_timesteps=3408000, episode_reward=-54.62 +/- 52.00\n","Episode length: 281.60 +/- 178.32\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 282      |\n","|    mean_reward     | -54.6    |\n","| time/              |          |\n","|    total_timesteps | 3408000  |\n","| train/             |          |\n","|    actor_loss      | 3.25     |\n","|    critic_loss     | 0.583    |\n","|    ent_coef        | 0.00339  |\n","|    ent_coef_loss   | -2.06    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 141995   |\n","---------------------------------\n","Eval num_timesteps=3410400, episode_reward=-60.93 +/- 53.10\n","Episode length: 280.40 +/- 179.30\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 280      |\n","|    mean_reward     | -60.9    |\n","| time/              |          |\n","|    total_timesteps | 3410400  |\n","| train/             |          |\n","|    actor_loss      | 3.23     |\n","|    critic_loss     | 1.19     |\n","|    ent_coef        | 0.0034   |\n","|    ent_coef_loss   | 0.507    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 142095   |\n","---------------------------------\n","Eval num_timesteps=3412800, episode_reward=-61.13 +/- 53.06\n","Episode length: 257.00 +/- 198.41\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 257      |\n","|    mean_reward     | -61.1    |\n","| time/              |          |\n","|    total_timesteps | 3412800  |\n","| train/             |          |\n","|    actor_loss      | 3.24     |\n","|    critic_loss     | 0.81     |\n","|    ent_coef        | 0.00336  |\n","|    ent_coef_loss   | 1.04     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 142195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5770     |\n","|    fps             | 306      |\n","|    time_elapsed    | 11158    |\n","|    total_timesteps | 3415080  |\n","| train/             |          |\n","|    actor_loss      | 3.32     |\n","|    critic_loss     | 17.3     |\n","|    ent_coef        | 0.00333  |\n","|    ent_coef_loss   | -4.76    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 142290   |\n","---------------------------------\n","Eval num_timesteps=3415200, episode_reward=6.85 +/- 0.35\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.85     |\n","| time/              |          |\n","|    total_timesteps | 3415200  |\n","| train/             |          |\n","|    actor_loss      | 3.02     |\n","|    critic_loss     | 0.36     |\n","|    ent_coef        | 0.00332  |\n","|    ent_coef_loss   | -3.98    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 142295   |\n","---------------------------------\n","Eval num_timesteps=3417600, episode_reward=4.57 +/- 0.29\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.57     |\n","| time/              |          |\n","|    total_timesteps | 3417600  |\n","| train/             |          |\n","|    actor_loss      | 3.14     |\n","|    critic_loss     | 18.7     |\n","|    ent_coef        | 0.00318  |\n","|    ent_coef_loss   | 0.174    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 142395   |\n","---------------------------------\n","Eval num_timesteps=3420000, episode_reward=5.74 +/- 0.19\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.74     |\n","| time/              |          |\n","|    total_timesteps | 3420000  |\n","| train/             |          |\n","|    actor_loss      | 3.69     |\n","|    critic_loss     | 0.655    |\n","|    ent_coef        | 0.00311  |\n","|    ent_coef_loss   | 0.223    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 142495   |\n","---------------------------------\n","Eval num_timesteps=3422400, episode_reward=-57.67 +/- 56.61\n","Episode length: 276.20 +/- 182.73\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 276      |\n","|    mean_reward     | -57.7    |\n","| time/              |          |\n","|    total_timesteps | 3422400  |\n","| train/             |          |\n","|    actor_loss      | 3.06     |\n","|    critic_loss     | 0.471    |\n","|    ent_coef        | 0.00312  |\n","|    ent_coef_loss   | -1.17    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 142595   |\n","---------------------------------\n","Eval num_timesteps=3424800, episode_reward=-65.22 +/- 56.98\n","Episode length: 260.60 +/- 195.47\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 261      |\n","|    mean_reward     | -65.2    |\n","| time/              |          |\n","|    total_timesteps | 3424800  |\n","| train/             |          |\n","|    actor_loss      | 3.26     |\n","|    critic_loss     | 11.3     |\n","|    ent_coef        | 0.00318  |\n","|    ent_coef_loss   | 3.14     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 142695   |\n","---------------------------------\n","Eval num_timesteps=3427200, episode_reward=-36.97 +/- 45.31\n","Episode length: 375.60 +/- 152.36\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 376      |\n","|    mean_reward     | -37      |\n","| time/              |          |\n","|    total_timesteps | 3427200  |\n","| train/             |          |\n","|    actor_loss      | 3.11     |\n","|    critic_loss     | 0.735    |\n","|    ent_coef        | 0.00322  |\n","|    ent_coef_loss   | -1.69    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 142795   |\n","---------------------------------\n","Eval num_timesteps=3429600, episode_reward=-41.77 +/- 52.20\n","Episode length: 377.20 +/- 150.40\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 377      |\n","|    mean_reward     | -41.8    |\n","| time/              |          |\n","|    total_timesteps | 3429600  |\n","| train/             |          |\n","|    actor_loss      | 3.18     |\n","|    critic_loss     | 0.575    |\n","|    ent_coef        | 0.00309  |\n","|    ent_coef_loss   | 1.24     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 142895   |\n","---------------------------------\n","Eval num_timesteps=3432000, episode_reward=-105.94 +/- 7.78\n","Episode length: 124.20 +/- 30.86\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 124      |\n","|    mean_reward     | -106     |\n","| time/              |          |\n","|    total_timesteps | 3432000  |\n","| train/             |          |\n","|    actor_loss      | 3.08     |\n","|    critic_loss     | 1.38     |\n","|    ent_coef        | 0.00311  |\n","|    ent_coef_loss   | 2.33     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 142995   |\n","---------------------------------\n","Eval num_timesteps=3434400, episode_reward=10.94 +/- 4.93\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 10.9     |\n","| time/              |          |\n","|    total_timesteps | 3434400  |\n","| train/             |          |\n","|    actor_loss      | 3.46     |\n","|    critic_loss     | 18.8     |\n","|    ent_coef        | 0.00309  |\n","|    ent_coef_loss   | 0.577    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 143095   |\n","---------------------------------\n","Eval num_timesteps=3436800, episode_reward=0.20 +/- 1.38\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.199    |\n","| time/              |          |\n","|    total_timesteps | 3436800  |\n","| train/             |          |\n","|    actor_loss      | 2.88     |\n","|    critic_loss     | 0.306    |\n","|    ent_coef        | 0.00298  |\n","|    ent_coef_loss   | -0.584   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 143195   |\n","---------------------------------\n","Eval num_timesteps=3439200, episode_reward=7.69 +/- 2.23\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.69     |\n","| time/              |          |\n","|    total_timesteps | 3439200  |\n","| train/             |          |\n","|    actor_loss      | 3.17     |\n","|    critic_loss     | 0.641    |\n","|    ent_coef        | 0.0028   |\n","|    ent_coef_loss   | -0.259   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 143295   |\n","---------------------------------\n","Eval num_timesteps=3441600, episode_reward=-43.56 +/- 54.01\n","Episode length: 356.00 +/- 176.36\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 356      |\n","|    mean_reward     | -43.6    |\n","| time/              |          |\n","|    total_timesteps | 3441600  |\n","| train/             |          |\n","|    actor_loss      | 3.17     |\n","|    critic_loss     | 0.483    |\n","|    ent_coef        | 0.00284  |\n","|    ent_coef_loss   | 11.2     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 143395   |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    episodes        | 5780    |\n","|    fps             | 305     |\n","|    time_elapsed    | 11252   |\n","|    total_timesteps | 3441600 |\n","--------------------------------\n","Eval num_timesteps=3444000, episode_reward=7.46 +/- 5.19\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.46     |\n","| time/              |          |\n","|    total_timesteps | 3444000  |\n","| train/             |          |\n","|    actor_loss      | 3.06     |\n","|    critic_loss     | 2.95     |\n","|    ent_coef        | 0.00303  |\n","|    ent_coef_loss   | -1.91    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 143495   |\n","---------------------------------\n","Eval num_timesteps=3446400, episode_reward=-37.44 +/- 45.56\n","Episode length: 355.20 +/- 177.34\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 355      |\n","|    mean_reward     | -37.4    |\n","| time/              |          |\n","|    total_timesteps | 3446400  |\n","| train/             |          |\n","|    actor_loss      | 2.96     |\n","|    critic_loss     | 1.11     |\n","|    ent_coef        | 0.00301  |\n","|    ent_coef_loss   | 5.22     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 143595   |\n","---------------------------------\n","Eval num_timesteps=3448800, episode_reward=-1.82 +/- 1.08\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.82    |\n","| time/              |          |\n","|    total_timesteps | 3448800  |\n","| train/             |          |\n","|    actor_loss      | 3.02     |\n","|    critic_loss     | 0.396    |\n","|    ent_coef        | 0.00309  |\n","|    ent_coef_loss   | 1.12     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 143695   |\n","---------------------------------\n","Eval num_timesteps=3451200, episode_reward=7.59 +/- 0.38\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.59     |\n","| time/              |          |\n","|    total_timesteps | 3451200  |\n","| train/             |          |\n","|    actor_loss      | 3.17     |\n","|    critic_loss     | 2.97     |\n","|    ent_coef        | 0.00309  |\n","|    ent_coef_loss   | -0.687   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 143795   |\n","---------------------------------\n","Eval num_timesteps=3453600, episode_reward=-36.63 +/- 59.12\n","Episode length: 346.80 +/- 187.63\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 347      |\n","|    mean_reward     | -36.6    |\n","| time/              |          |\n","|    total_timesteps | 3453600  |\n","| train/             |          |\n","|    actor_loss      | 3.14     |\n","|    critic_loss     | 1.08     |\n","|    ent_coef        | 0.00314  |\n","|    ent_coef_loss   | 1.93     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 143895   |\n","---------------------------------\n","Eval num_timesteps=3456000, episode_reward=-1.45 +/- 7.48\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.45    |\n","| time/              |          |\n","|    total_timesteps | 3456000  |\n","| train/             |          |\n","|    actor_loss      | 3.05     |\n","|    critic_loss     | 0.848    |\n","|    ent_coef        | 0.00325  |\n","|    ent_coef_loss   | 3.17     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 143995   |\n","---------------------------------\n","Eval num_timesteps=3458400, episode_reward=7.65 +/- 0.56\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.65     |\n","| time/              |          |\n","|    total_timesteps | 3458400  |\n","| train/             |          |\n","|    actor_loss      | 3.23     |\n","|    critic_loss     | 18.5     |\n","|    ent_coef        | 0.00342  |\n","|    ent_coef_loss   | 0.983    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 144095   |\n","---------------------------------\n","Eval num_timesteps=3460800, episode_reward=3.99 +/- 0.59\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.99     |\n","| time/              |          |\n","|    total_timesteps | 3460800  |\n","| train/             |          |\n","|    actor_loss      | 3.01     |\n","|    critic_loss     | 0.76     |\n","|    ent_coef        | 0.00346  |\n","|    ent_coef_loss   | -2.08    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 144195   |\n","---------------------------------\n","Eval num_timesteps=3463200, episode_reward=4.96 +/- 1.38\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.96     |\n","| time/              |          |\n","|    total_timesteps | 3463200  |\n","| train/             |          |\n","|    actor_loss      | 3.14     |\n","|    critic_loss     | 0.69     |\n","|    ent_coef        | 0.00342  |\n","|    ent_coef_loss   | -1.68    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 144295   |\n","---------------------------------\n","Eval num_timesteps=3465600, episode_reward=6.84 +/- 3.39\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.84     |\n","| time/              |          |\n","|    total_timesteps | 3465600  |\n","| train/             |          |\n","|    actor_loss      | 2.94     |\n","|    critic_loss     | 0.319    |\n","|    ent_coef        | 0.0033   |\n","|    ent_coef_loss   | -0.368   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 144395   |\n","---------------------------------\n","Eval num_timesteps=3468000, episode_reward=7.29 +/- 1.76\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.29     |\n","| time/              |          |\n","|    total_timesteps | 3468000  |\n","| train/             |          |\n","|    actor_loss      | 2.93     |\n","|    critic_loss     | 23       |\n","|    ent_coef        | 0.00328  |\n","|    ent_coef_loss   | -0.474   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 144495   |\n","---------------------------------\n","Eval num_timesteps=3470400, episode_reward=-36.01 +/- 57.78\n","Episode length: 328.00 +/- 210.66\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 328      |\n","|    mean_reward     | -36      |\n","| time/              |          |\n","|    total_timesteps | 3470400  |\n","| train/             |          |\n","|    actor_loss      | 2.74     |\n","|    critic_loss     | 0.386    |\n","|    ent_coef        | 0.00325  |\n","|    ent_coef_loss   | -0.917   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 144595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5790     |\n","|    fps             | 305      |\n","|    time_elapsed    | 11356    |\n","|    total_timesteps | 3471216  |\n","| train/             |          |\n","|    actor_loss      | 3.23     |\n","|    critic_loss     | 0.524    |\n","|    ent_coef        | 0.00323  |\n","|    ent_coef_loss   | -2.3     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 144629   |\n","---------------------------------\n","Eval num_timesteps=3472800, episode_reward=6.13 +/- 0.37\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.13     |\n","| time/              |          |\n","|    total_timesteps | 3472800  |\n","| train/             |          |\n","|    actor_loss      | 3.54     |\n","|    critic_loss     | 0.546    |\n","|    ent_coef        | 0.00318  |\n","|    ent_coef_loss   | -2.15    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 144695   |\n","---------------------------------\n","Eval num_timesteps=3475200, episode_reward=-58.30 +/- 52.07\n","Episode length: 489.20 +/- 8.82\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 489      |\n","|    mean_reward     | -58.3    |\n","| time/              |          |\n","|    total_timesteps | 3475200  |\n","| train/             |          |\n","|    actor_loss      | 3.47     |\n","|    critic_loss     | 0.644    |\n","|    ent_coef        | 0.00321  |\n","|    ent_coef_loss   | 0.563    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 144795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5800     |\n","|    fps             | 305      |\n","|    time_elapsed    | 11374    |\n","|    total_timesteps | 3477192  |\n","| train/             |          |\n","|    actor_loss      | 2.98     |\n","|    critic_loss     | 0.442    |\n","|    ent_coef        | 0.00322  |\n","|    ent_coef_loss   | 2.67     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 144878   |\n","---------------------------------\n","Eval num_timesteps=3477600, episode_reward=16.68 +/- 0.61\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 16.7     |\n","| time/              |          |\n","|    total_timesteps | 3477600  |\n","| train/             |          |\n","|    actor_loss      | 3.07     |\n","|    critic_loss     | 8.51     |\n","|    ent_coef        | 0.00326  |\n","|    ent_coef_loss   | 4.29     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 144895   |\n","---------------------------------\n","Eval num_timesteps=3480000, episode_reward=-96.14 +/- 5.90\n","Episode length: 225.60 +/- 87.69\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 226      |\n","|    mean_reward     | -96.1    |\n","| time/              |          |\n","|    total_timesteps | 3480000  |\n","| train/             |          |\n","|    actor_loss      | 3.18     |\n","|    critic_loss     | 0.466    |\n","|    ent_coef        | 0.00326  |\n","|    ent_coef_loss   | -0.443   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 144995   |\n","---------------------------------\n","Eval num_timesteps=3482400, episode_reward=6.18 +/- 8.29\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.18     |\n","| time/              |          |\n","|    total_timesteps | 3482400  |\n","| train/             |          |\n","|    actor_loss      | 3.48     |\n","|    critic_loss     | 0.82     |\n","|    ent_coef        | 0.00325  |\n","|    ent_coef_loss   | 4.37     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 145095   |\n","---------------------------------\n","Eval num_timesteps=3484800, episode_reward=4.58 +/- 3.32\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.58     |\n","| time/              |          |\n","|    total_timesteps | 3484800  |\n","| train/             |          |\n","|    actor_loss      | 2.87     |\n","|    critic_loss     | 0.274    |\n","|    ent_coef        | 0.00322  |\n","|    ent_coef_loss   | -2.52    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 145195   |\n","---------------------------------\n","Eval num_timesteps=3487200, episode_reward=8.37 +/- 0.79\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 8.37     |\n","| time/              |          |\n","|    total_timesteps | 3487200  |\n","| train/             |          |\n","|    actor_loss      | 3.27     |\n","|    critic_loss     | 11.7     |\n","|    ent_coef        | 0.00313  |\n","|    ent_coef_loss   | -2.97    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 145295   |\n","---------------------------------\n","Eval num_timesteps=3489600, episode_reward=-3.12 +/- 1.81\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -3.12    |\n","| time/              |          |\n","|    total_timesteps | 3489600  |\n","| train/             |          |\n","|    actor_loss      | 3.37     |\n","|    critic_loss     | 0.489    |\n","|    ent_coef        | 0.00303  |\n","|    ent_coef_loss   | 1.19     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 145395   |\n","---------------------------------\n","Eval num_timesteps=3492000, episode_reward=2.81 +/- 0.72\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.81     |\n","| time/              |          |\n","|    total_timesteps | 3492000  |\n","| train/             |          |\n","|    actor_loss      | 3.15     |\n","|    critic_loss     | 1        |\n","|    ent_coef        | 0.00296  |\n","|    ent_coef_loss   | -0.147   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 145495   |\n","---------------------------------\n","Eval num_timesteps=3494400, episode_reward=7.72 +/- 3.00\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.72     |\n","| time/              |          |\n","|    total_timesteps | 3494400  |\n","| train/             |          |\n","|    actor_loss      | 3.15     |\n","|    critic_loss     | 18.4     |\n","|    ent_coef        | 0.00313  |\n","|    ent_coef_loss   | 2.66     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 145595   |\n","---------------------------------\n","Eval num_timesteps=3496800, episode_reward=7.16 +/- 0.70\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.16     |\n","| time/              |          |\n","|    total_timesteps | 3496800  |\n","| train/             |          |\n","|    actor_loss      | 3.79     |\n","|    critic_loss     | 18.1     |\n","|    ent_coef        | 0.00327  |\n","|    ent_coef_loss   | -0.587   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 145695   |\n","---------------------------------\n","Eval num_timesteps=3499200, episode_reward=-0.73 +/- 5.73\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.727   |\n","| time/              |          |\n","|    total_timesteps | 3499200  |\n","| train/             |          |\n","|    actor_loss      | 3.09     |\n","|    critic_loss     | 0.531    |\n","|    ent_coef        | 0.00322  |\n","|    ent_coef_loss   | -2.9     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 145795   |\n","---------------------------------\n","Eval num_timesteps=3501600, episode_reward=6.07 +/- 3.92\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.07     |\n","| time/              |          |\n","|    total_timesteps | 3501600  |\n","| train/             |          |\n","|    actor_loss      | 2.84     |\n","|    critic_loss     | 0.679    |\n","|    ent_coef        | 0.00309  |\n","|    ent_coef_loss   | -2.22    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 145895   |\n","---------------------------------\n","Eval num_timesteps=3504000, episode_reward=-9.18 +/- 5.91\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -9.18    |\n","| time/              |          |\n","|    total_timesteps | 3504000  |\n","| train/             |          |\n","|    actor_loss      | 3.52     |\n","|    critic_loss     | 0.465    |\n","|    ent_coef        | 0.003    |\n","|    ent_coef_loss   | 1.96     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 145995   |\n","---------------------------------\n","Eval num_timesteps=3506400, episode_reward=-52.47 +/- 50.22\n","Episode length: 270.20 +/- 187.63\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 270      |\n","|    mean_reward     | -52.5    |\n","| time/              |          |\n","|    total_timesteps | 3506400  |\n","| train/             |          |\n","|    actor_loss      | 2.88     |\n","|    critic_loss     | 0.586    |\n","|    ent_coef        | 0.00321  |\n","|    ent_coef_loss   | 5.71     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 146095   |\n","---------------------------------\n","Eval num_timesteps=3508800, episode_reward=-41.64 +/- 50.02\n","Episode length: 373.60 +/- 154.81\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 374      |\n","|    mean_reward     | -41.6    |\n","| time/              |          |\n","|    total_timesteps | 3508800  |\n","| train/             |          |\n","|    actor_loss      | 3.11     |\n","|    critic_loss     | 0.832    |\n","|    ent_coef        | 0.00324  |\n","|    ent_coef_loss   | 2.65     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 146195   |\n","---------------------------------\n","Eval num_timesteps=3511200, episode_reward=-3.10 +/- 4.35\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -3.1     |\n","| time/              |          |\n","|    total_timesteps | 3511200  |\n","| train/             |          |\n","|    actor_loss      | 3.17     |\n","|    critic_loss     | 0.541    |\n","|    ent_coef        | 0.00339  |\n","|    ent_coef_loss   | 1.7      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 146295   |\n","---------------------------------\n","Eval num_timesteps=3513600, episode_reward=-39.91 +/- 53.74\n","Episode length: 337.60 +/- 198.90\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 338      |\n","|    mean_reward     | -39.9    |\n","| time/              |          |\n","|    total_timesteps | 3513600  |\n","| train/             |          |\n","|    actor_loss      | 3.01     |\n","|    critic_loss     | 0.355    |\n","|    ent_coef        | 0.00368  |\n","|    ent_coef_loss   | 2.24     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 146395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5810     |\n","|    fps             | 305      |\n","|    time_elapsed    | 11509    |\n","|    total_timesteps | 3515856  |\n","| train/             |          |\n","|    actor_loss      | 3.44     |\n","|    critic_loss     | 1.43     |\n","|    ent_coef        | 0.00365  |\n","|    ent_coef_loss   | 1.37     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 146489   |\n","---------------------------------\n","Eval num_timesteps=3516000, episode_reward=5.92 +/- 4.44\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.92     |\n","| time/              |          |\n","|    total_timesteps | 3516000  |\n","| train/             |          |\n","|    actor_loss      | 2.89     |\n","|    critic_loss     | 0.502    |\n","|    ent_coef        | 0.00365  |\n","|    ent_coef_loss   | 1.01     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 146495   |\n","---------------------------------\n","Eval num_timesteps=3518400, episode_reward=-6.63 +/- 2.61\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -6.63    |\n","| time/              |          |\n","|    total_timesteps | 3518400  |\n","| train/             |          |\n","|    actor_loss      | 2.9      |\n","|    critic_loss     | 0.77     |\n","|    ent_coef        | 0.00349  |\n","|    ent_coef_loss   | -4.68    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 146595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5820     |\n","|    fps             | 305      |\n","|    time_elapsed    | 11524    |\n","|    total_timesteps | 3520728  |\n","| train/             |          |\n","|    actor_loss      | 2.73     |\n","|    critic_loss     | 0.507    |\n","|    ent_coef        | 0.00331  |\n","|    ent_coef_loss   | -2.7     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 146692   |\n","---------------------------------\n","Eval num_timesteps=3520800, episode_reward=7.44 +/- 0.39\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.44     |\n","| time/              |          |\n","|    total_timesteps | 3520800  |\n","| train/             |          |\n","|    actor_loss      | 2.79     |\n","|    critic_loss     | 0.374    |\n","|    ent_coef        | 0.0033   |\n","|    ent_coef_loss   | -2.35    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 146695   |\n","---------------------------------\n","Eval num_timesteps=3523200, episode_reward=11.38 +/- 3.65\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 11.4     |\n","| time/              |          |\n","|    total_timesteps | 3523200  |\n","| train/             |          |\n","|    actor_loss      | 2.82     |\n","|    critic_loss     | 0.353    |\n","|    ent_coef        | 0.00334  |\n","|    ent_coef_loss   | 3.2      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 146795   |\n","---------------------------------\n","Eval num_timesteps=3525600, episode_reward=4.49 +/- 1.46\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.49     |\n","| time/              |          |\n","|    total_timesteps | 3525600  |\n","| train/             |          |\n","|    actor_loss      | 3.11     |\n","|    critic_loss     | 0.546    |\n","|    ent_coef        | 0.00335  |\n","|    ent_coef_loss   | -0.429   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 146895   |\n","---------------------------------\n","Eval num_timesteps=3528000, episode_reward=5.47 +/- 0.17\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.47     |\n","| time/              |          |\n","|    total_timesteps | 3528000  |\n","| train/             |          |\n","|    actor_loss      | 3.05     |\n","|    critic_loss     | 0.38     |\n","|    ent_coef        | 0.00333  |\n","|    ent_coef_loss   | 3.3      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 146995   |\n","---------------------------------\n","Eval num_timesteps=3530400, episode_reward=6.48 +/- 0.25\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.48     |\n","| time/              |          |\n","|    total_timesteps | 3530400  |\n","| train/             |          |\n","|    actor_loss      | 2.89     |\n","|    critic_loss     | 17.4     |\n","|    ent_coef        | 0.00323  |\n","|    ent_coef_loss   | -4.88    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 147095   |\n","---------------------------------\n","Eval num_timesteps=3532800, episode_reward=-57.52 +/- 52.32\n","Episode length: 347.60 +/- 124.43\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 348      |\n","|    mean_reward     | -57.5    |\n","| time/              |          |\n","|    total_timesteps | 3532800  |\n","| train/             |          |\n","|    actor_loss      | 2.88     |\n","|    critic_loss     | 0.54     |\n","|    ent_coef        | 0.00322  |\n","|    ent_coef_loss   | 1.86     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 147195   |\n","---------------------------------\n","Eval num_timesteps=3535200, episode_reward=8.42 +/- 0.45\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 8.42     |\n","| time/              |          |\n","|    total_timesteps | 3535200  |\n","| train/             |          |\n","|    actor_loss      | 3.53     |\n","|    critic_loss     | 11.7     |\n","|    ent_coef        | 0.00338  |\n","|    ent_coef_loss   | -1.2     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 147295   |\n","---------------------------------\n","Eval num_timesteps=3537600, episode_reward=9.57 +/- 1.27\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 9.57     |\n","| time/              |          |\n","|    total_timesteps | 3537600  |\n","| train/             |          |\n","|    actor_loss      | 3.25     |\n","|    critic_loss     | 1.04     |\n","|    ent_coef        | 0.00337  |\n","|    ent_coef_loss   | -0.0585  |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 147395   |\n","---------------------------------\n","Eval num_timesteps=3540000, episode_reward=-89.68 +/- 12.23\n","Episode length: 360.00 +/- 29.39\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 360      |\n","|    mean_reward     | -89.7    |\n","| time/              |          |\n","|    total_timesteps | 3540000  |\n","| train/             |          |\n","|    actor_loss      | 2.59     |\n","|    critic_loss     | 2.93     |\n","|    ent_coef        | 0.0034   |\n","|    ent_coef_loss   | 4.1      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 147495   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5830     |\n","|    fps             | 305      |\n","|    time_elapsed    | 11597    |\n","|    total_timesteps | 3540600  |\n","| train/             |          |\n","|    actor_loss      | 3.37     |\n","|    critic_loss     | 2        |\n","|    ent_coef        | 0.00344  |\n","|    ent_coef_loss   | -0.324   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 147520   |\n","---------------------------------\n","Eval num_timesteps=3542400, episode_reward=-52.30 +/- 52.66\n","Episode length: 259.40 +/- 196.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 259      |\n","|    mean_reward     | -52.3    |\n","| time/              |          |\n","|    total_timesteps | 3542400  |\n","| train/             |          |\n","|    actor_loss      | 3.22     |\n","|    critic_loss     | 21.2     |\n","|    ent_coef        | 0.00333  |\n","|    ent_coef_loss   | -2.78    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 147595   |\n","---------------------------------\n","Eval num_timesteps=3544800, episode_reward=14.28 +/- 7.02\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 14.3     |\n","| time/              |          |\n","|    total_timesteps | 3544800  |\n","| train/             |          |\n","|    actor_loss      | 2.68     |\n","|    critic_loss     | 0.604    |\n","|    ent_coef        | 0.00325  |\n","|    ent_coef_loss   | -0.691   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 147695   |\n","---------------------------------\n","Eval num_timesteps=3547200, episode_reward=2.06 +/- 0.19\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.06     |\n","| time/              |          |\n","|    total_timesteps | 3547200  |\n","| train/             |          |\n","|    actor_loss      | 3.42     |\n","|    critic_loss     | 0.469    |\n","|    ent_coef        | 0.00326  |\n","|    ent_coef_loss   | 0.455    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 147795   |\n","---------------------------------\n","Eval num_timesteps=3549600, episode_reward=9.48 +/- 2.86\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 9.48     |\n","| time/              |          |\n","|    total_timesteps | 3549600  |\n","| train/             |          |\n","|    actor_loss      | 2.3      |\n","|    critic_loss     | 0.644    |\n","|    ent_coef        | 0.00319  |\n","|    ent_coef_loss   | 1.75     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 147895   |\n","---------------------------------\n","Eval num_timesteps=3552000, episode_reward=-56.36 +/- 50.57\n","Episode length: 328.40 +/- 140.11\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 328      |\n","|    mean_reward     | -56.4    |\n","| time/              |          |\n","|    total_timesteps | 3552000  |\n","| train/             |          |\n","|    actor_loss      | 3.06     |\n","|    critic_loss     | 0.432    |\n","|    ent_coef        | 0.00337  |\n","|    ent_coef_loss   | -3.62    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 147995   |\n","---------------------------------\n","Eval num_timesteps=3554400, episode_reward=8.53 +/- 1.01\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 8.53     |\n","| time/              |          |\n","|    total_timesteps | 3554400  |\n","| train/             |          |\n","|    actor_loss      | 2.81     |\n","|    critic_loss     | 0.562    |\n","|    ent_coef        | 0.00333  |\n","|    ent_coef_loss   | -2.28    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 148095   |\n","---------------------------------\n","Eval num_timesteps=3556800, episode_reward=-34.32 +/- 50.03\n","Episode length: 359.20 +/- 172.44\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 359      |\n","|    mean_reward     | -34.3    |\n","| time/              |          |\n","|    total_timesteps | 3556800  |\n","| train/             |          |\n","|    actor_loss      | 3.34     |\n","|    critic_loss     | 0.371    |\n","|    ent_coef        | 0.00335  |\n","|    ent_coef_loss   | -1.97    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 148195   |\n","---------------------------------\n","Eval num_timesteps=3559200, episode_reward=5.45 +/- 2.59\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.45     |\n","| time/              |          |\n","|    total_timesteps | 3559200  |\n","| train/             |          |\n","|    actor_loss      | 2.86     |\n","|    critic_loss     | 0.644    |\n","|    ent_coef        | 0.00331  |\n","|    ent_coef_loss   | 2.13     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 148295   |\n","---------------------------------\n","Eval num_timesteps=3561600, episode_reward=7.83 +/- 1.92\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.83     |\n","| time/              |          |\n","|    total_timesteps | 3561600  |\n","| train/             |          |\n","|    actor_loss      | 2.89     |\n","|    critic_loss     | 0.42     |\n","|    ent_coef        | 0.00327  |\n","|    ent_coef_loss   | -1       |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 148395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5840     |\n","|    fps             | 305      |\n","|    time_elapsed    | 11674    |\n","|    total_timesteps | 3562128  |\n","| train/             |          |\n","|    actor_loss      | 2.73     |\n","|    critic_loss     | 0.559    |\n","|    ent_coef        | 0.00325  |\n","|    ent_coef_loss   | -1.34    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 148417   |\n","---------------------------------\n","Eval num_timesteps=3564000, episode_reward=5.22 +/- 1.80\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.22     |\n","| time/              |          |\n","|    total_timesteps | 3564000  |\n","| train/             |          |\n","|    actor_loss      | 2.89     |\n","|    critic_loss     | 0.429    |\n","|    ent_coef        | 0.00329  |\n","|    ent_coef_loss   | 2.09     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 148495   |\n","---------------------------------\n","Eval num_timesteps=3566400, episode_reward=-33.38 +/- 50.22\n","Episode length: 344.40 +/- 190.57\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 344      |\n","|    mean_reward     | -33.4    |\n","| time/              |          |\n","|    total_timesteps | 3566400  |\n","| train/             |          |\n","|    actor_loss      | 2.62     |\n","|    critic_loss     | 0.78     |\n","|    ent_coef        | 0.00323  |\n","|    ent_coef_loss   | -2.21    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 148595   |\n","---------------------------------\n","Eval num_timesteps=3568800, episode_reward=2.64 +/- 2.70\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.64     |\n","| time/              |          |\n","|    total_timesteps | 3568800  |\n","| train/             |          |\n","|    actor_loss      | 3.34     |\n","|    critic_loss     | 7.91     |\n","|    ent_coef        | 0.00328  |\n","|    ent_coef_loss   | 0.579    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 148695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5850     |\n","|    fps             | 305      |\n","|    time_elapsed    | 11700    |\n","|    total_timesteps | 3570912  |\n","| train/             |          |\n","|    actor_loss      | 2.98     |\n","|    critic_loss     | 0.599    |\n","|    ent_coef        | 0.00323  |\n","|    ent_coef_loss   | -0.823   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 148783   |\n","---------------------------------\n","Eval num_timesteps=3571200, episode_reward=9.12 +/- 0.58\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 9.12     |\n","| time/              |          |\n","|    total_timesteps | 3571200  |\n","| train/             |          |\n","|    actor_loss      | 2.71     |\n","|    critic_loss     | 0.258    |\n","|    ent_coef        | 0.00321  |\n","|    ent_coef_loss   | -0.175   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 148795   |\n","---------------------------------\n","Eval num_timesteps=3573600, episode_reward=-28.30 +/- 47.49\n","Episode length: 370.00 +/- 159.22\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 370      |\n","|    mean_reward     | -28.3    |\n","| time/              |          |\n","|    total_timesteps | 3573600  |\n","| train/             |          |\n","|    actor_loss      | 2.79     |\n","|    critic_loss     | 0.701    |\n","|    ent_coef        | 0.00323  |\n","|    ent_coef_loss   | -3.64    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 148895   |\n","---------------------------------\n","Eval num_timesteps=3576000, episode_reward=1.30 +/- 0.45\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.3      |\n","| time/              |          |\n","|    total_timesteps | 3576000  |\n","| train/             |          |\n","|    actor_loss      | 2.84     |\n","|    critic_loss     | 14.9     |\n","|    ent_coef        | 0.00325  |\n","|    ent_coef_loss   | 0.813    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 148995   |\n","---------------------------------\n","Eval num_timesteps=3578400, episode_reward=5.30 +/- 0.98\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.3      |\n","| time/              |          |\n","|    total_timesteps | 3578400  |\n","| train/             |          |\n","|    actor_loss      | 2.59     |\n","|    critic_loss     | 3.62     |\n","|    ent_coef        | 0.00334  |\n","|    ent_coef_loss   | 2.85     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 149095   |\n","---------------------------------\n","Eval num_timesteps=3580800, episode_reward=-82.84 +/- 9.08\n","Episode length: 270.60 +/- 93.57\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 271      |\n","|    mean_reward     | -82.8    |\n","| time/              |          |\n","|    total_timesteps | 3580800  |\n","| train/             |          |\n","|    actor_loss      | 2.99     |\n","|    critic_loss     | 0.455    |\n","|    ent_coef        | 0.00343  |\n","|    ent_coef_loss   | 0.876    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 149195   |\n","---------------------------------\n","Eval num_timesteps=3583200, episode_reward=4.92 +/- 1.14\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.92     |\n","| time/              |          |\n","|    total_timesteps | 3583200  |\n","| train/             |          |\n","|    actor_loss      | 2.95     |\n","|    critic_loss     | 2.3      |\n","|    ent_coef        | 0.00332  |\n","|    ent_coef_loss   | -3.04    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 149295   |\n","---------------------------------\n","Eval num_timesteps=3585600, episode_reward=8.57 +/- 5.58\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 8.57     |\n","| time/              |          |\n","|    total_timesteps | 3585600  |\n","| train/             |          |\n","|    actor_loss      | 2.84     |\n","|    critic_loss     | 1.86     |\n","|    ent_coef        | 0.00316  |\n","|    ent_coef_loss   | -2.83    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 149395   |\n","---------------------------------\n","Eval num_timesteps=3588000, episode_reward=12.72 +/- 3.38\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 12.7     |\n","| time/              |          |\n","|    total_timesteps | 3588000  |\n","| train/             |          |\n","|    actor_loss      | 3.25     |\n","|    critic_loss     | 18.9     |\n","|    ent_coef        | 0.003    |\n","|    ent_coef_loss   | 0.0297   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 149495   |\n","---------------------------------\n","Eval num_timesteps=3590400, episode_reward=-34.55 +/- 46.40\n","Episode length: 356.40 +/- 175.87\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 356      |\n","|    mean_reward     | -34.5    |\n","| time/              |          |\n","|    total_timesteps | 3590400  |\n","| train/             |          |\n","|    actor_loss      | 2.88     |\n","|    critic_loss     | 0.335    |\n","|    ent_coef        | 0.00301  |\n","|    ent_coef_loss   | -2.77    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 149595   |\n","---------------------------------\n","Eval num_timesteps=3592800, episode_reward=2.78 +/- 1.85\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.78     |\n","| time/              |          |\n","|    total_timesteps | 3592800  |\n","| train/             |          |\n","|    actor_loss      | 2.89     |\n","|    critic_loss     | 0.678    |\n","|    ent_coef        | 0.00298  |\n","|    ent_coef_loss   | 1.34     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 149695   |\n","---------------------------------\n","Eval num_timesteps=3595200, episode_reward=9.95 +/- 2.96\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 9.95     |\n","| time/              |          |\n","|    total_timesteps | 3595200  |\n","| train/             |          |\n","|    actor_loss      | 2.78     |\n","|    critic_loss     | 0.471    |\n","|    ent_coef        | 0.00306  |\n","|    ent_coef_loss   | -0.438   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 149795   |\n","---------------------------------\n","Eval num_timesteps=3597600, episode_reward=-96.58 +/- 1.65\n","Episode length: 141.20 +/- 55.36\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 141      |\n","|    mean_reward     | -96.6    |\n","| time/              |          |\n","|    total_timesteps | 3597600  |\n","| train/             |          |\n","|    actor_loss      | 2.73     |\n","|    critic_loss     | 0.659    |\n","|    ent_coef        | 0.00304  |\n","|    ent_coef_loss   | 0.102    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 149895   |\n","---------------------------------\n","Eval num_timesteps=3600000, episode_reward=-95.38 +/- 4.98\n","Episode length: 155.80 +/- 62.22\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 156      |\n","|    mean_reward     | -95.4    |\n","| time/              |          |\n","|    total_timesteps | 3600000  |\n","| train/             |          |\n","|    actor_loss      | 2.64     |\n","|    critic_loss     | 0.662    |\n","|    ent_coef        | 0.0031   |\n","|    ent_coef_loss   | 1.68     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 149995   |\n","---------------------------------\n","Eval num_timesteps=3602400, episode_reward=-93.50 +/- 6.70\n","Episode length: 154.60 +/- 20.09\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 155      |\n","|    mean_reward     | -93.5    |\n","| time/              |          |\n","|    total_timesteps | 3602400  |\n","| train/             |          |\n","|    actor_loss      | 3.36     |\n","|    critic_loss     | 0.768    |\n","|    ent_coef        | 0.00309  |\n","|    ent_coef_loss   | -1.34    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 150095   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5860     |\n","|    fps             | 305      |\n","|    time_elapsed    | 11804    |\n","|    total_timesteps | 3604704  |\n","| train/             |          |\n","|    actor_loss      | 3.08     |\n","|    critic_loss     | 0.473    |\n","|    ent_coef        | 0.00299  |\n","|    ent_coef_loss   | 0.366    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 150191   |\n","---------------------------------\n","Eval num_timesteps=3604800, episode_reward=3.55 +/- 0.64\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.55     |\n","| time/              |          |\n","|    total_timesteps | 3604800  |\n","| train/             |          |\n","|    actor_loss      | 3.2      |\n","|    critic_loss     | 16.3     |\n","|    ent_coef        | 0.00299  |\n","|    ent_coef_loss   | 0.435    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 150195   |\n","---------------------------------\n","Eval num_timesteps=3607200, episode_reward=-50.60 +/- 49.45\n","Episode length: 387.80 +/- 91.61\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 388      |\n","|    mean_reward     | -50.6    |\n","| time/              |          |\n","|    total_timesteps | 3607200  |\n","| train/             |          |\n","|    actor_loss      | 3.24     |\n","|    critic_loss     | 0.58     |\n","|    ent_coef        | 0.00306  |\n","|    ent_coef_loss   | -1.76    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 150295   |\n","---------------------------------\n","Eval num_timesteps=3609600, episode_reward=7.41 +/- 5.66\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.41     |\n","| time/              |          |\n","|    total_timesteps | 3609600  |\n","| train/             |          |\n","|    actor_loss      | 2.81     |\n","|    critic_loss     | 16.8     |\n","|    ent_coef        | 0.00327  |\n","|    ent_coef_loss   | -2.16    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 150395   |\n","---------------------------------\n","Eval num_timesteps=3612000, episode_reward=4.61 +/- 0.41\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.61     |\n","| time/              |          |\n","|    total_timesteps | 3612000  |\n","| train/             |          |\n","|    actor_loss      | 2.83     |\n","|    critic_loss     | 0.387    |\n","|    ent_coef        | 0.00327  |\n","|    ent_coef_loss   | -1.77    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 150495   |\n","---------------------------------\n","Eval num_timesteps=3614400, episode_reward=4.68 +/- 2.20\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.68     |\n","| time/              |          |\n","|    total_timesteps | 3614400  |\n","| train/             |          |\n","|    actor_loss      | 3.1      |\n","|    critic_loss     | 10.9     |\n","|    ent_coef        | 0.00318  |\n","|    ent_coef_loss   | -0.192   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 150595   |\n","---------------------------------\n","Eval num_timesteps=3616800, episode_reward=3.15 +/- 1.63\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.15     |\n","| time/              |          |\n","|    total_timesteps | 3616800  |\n","| train/             |          |\n","|    actor_loss      | 2.75     |\n","|    critic_loss     | 0.266    |\n","|    ent_coef        | 0.00311  |\n","|    ent_coef_loss   | -3.14    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 150695   |\n","---------------------------------\n","Eval num_timesteps=3619200, episode_reward=5.46 +/- 0.44\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.46     |\n","| time/              |          |\n","|    total_timesteps | 3619200  |\n","| train/             |          |\n","|    actor_loss      | 3.37     |\n","|    critic_loss     | 1.15     |\n","|    ent_coef        | 0.00295  |\n","|    ent_coef_loss   | -1.63    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 150795   |\n","---------------------------------\n","Eval num_timesteps=3621600, episode_reward=7.89 +/- 2.83\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.89     |\n","| time/              |          |\n","|    total_timesteps | 3621600  |\n","| train/             |          |\n","|    actor_loss      | 3.02     |\n","|    critic_loss     | 0.566    |\n","|    ent_coef        | 0.00289  |\n","|    ent_coef_loss   | -2.67    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 150895   |\n","---------------------------------\n","Eval num_timesteps=3624000, episode_reward=7.94 +/- 0.89\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.94     |\n","| time/              |          |\n","|    total_timesteps | 3624000  |\n","| train/             |          |\n","|    actor_loss      | 2.9      |\n","|    critic_loss     | 0.341    |\n","|    ent_coef        | 0.00286  |\n","|    ent_coef_loss   | 0.678    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 150995   |\n","---------------------------------\n","Eval num_timesteps=3626400, episode_reward=11.38 +/- 2.09\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 11.4     |\n","| time/              |          |\n","|    total_timesteps | 3626400  |\n","| train/             |          |\n","|    actor_loss      | 2.82     |\n","|    critic_loss     | 0.753    |\n","|    ent_coef        | 0.00295  |\n","|    ent_coef_loss   | -1.45    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 151095   |\n","---------------------------------\n","Eval num_timesteps=3628800, episode_reward=-34.58 +/- 54.29\n","Episode length: 362.00 +/- 169.01\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 362      |\n","|    mean_reward     | -34.6    |\n","| time/              |          |\n","|    total_timesteps | 3628800  |\n","| train/             |          |\n","|    actor_loss      | 2.92     |\n","|    critic_loss     | 0.378    |\n","|    ent_coef        | 0.00292  |\n","|    ent_coef_loss   | 1.08     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 151195   |\n","---------------------------------\n","Eval num_timesteps=3631200, episode_reward=-36.57 +/- 47.74\n","Episode length: 338.80 +/- 197.43\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 339      |\n","|    mean_reward     | -36.6    |\n","| time/              |          |\n","|    total_timesteps | 3631200  |\n","| train/             |          |\n","|    actor_loss      | 3.06     |\n","|    critic_loss     | 0.507    |\n","|    ent_coef        | 0.00296  |\n","|    ent_coef_loss   | 1.9      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 151295   |\n","---------------------------------\n","Eval num_timesteps=3633600, episode_reward=-41.01 +/- 51.83\n","Episode length: 337.60 +/- 198.90\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 338      |\n","|    mean_reward     | -41      |\n","| time/              |          |\n","|    total_timesteps | 3633600  |\n","| train/             |          |\n","|    actor_loss      | 2.95     |\n","|    critic_loss     | 0.835    |\n","|    ent_coef        | 0.00286  |\n","|    ent_coef_loss   | -0.411   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 151395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5870     |\n","|    fps             | 305      |\n","|    time_elapsed    | 11909    |\n","|    total_timesteps | 3633792  |\n","| train/             |          |\n","|    actor_loss      | 3.01     |\n","|    critic_loss     | 2.44     |\n","|    ent_coef        | 0.00286  |\n","|    ent_coef_loss   | 1.98     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 151403   |\n","---------------------------------\n","Eval num_timesteps=3636000, episode_reward=1.75 +/- 0.35\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.75     |\n","| time/              |          |\n","|    total_timesteps | 3636000  |\n","| train/             |          |\n","|    actor_loss      | 3.1      |\n","|    critic_loss     | 1.91     |\n","|    ent_coef        | 0.00284  |\n","|    ent_coef_loss   | -0.606   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 151495   |\n","---------------------------------\n","Eval num_timesteps=3638400, episode_reward=-33.48 +/- 42.95\n","Episode length: 369.60 +/- 159.71\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 370      |\n","|    mean_reward     | -33.5    |\n","| time/              |          |\n","|    total_timesteps | 3638400  |\n","| train/             |          |\n","|    actor_loss      | 3.13     |\n","|    critic_loss     | 0.565    |\n","|    ent_coef        | 0.00281  |\n","|    ent_coef_loss   | 1.01     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 151595   |\n","---------------------------------\n","Eval num_timesteps=3640800, episode_reward=-62.15 +/- 55.95\n","Episode length: 313.40 +/- 152.36\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 313      |\n","|    mean_reward     | -62.1    |\n","| time/              |          |\n","|    total_timesteps | 3640800  |\n","| train/             |          |\n","|    actor_loss      | 2.71     |\n","|    critic_loss     | 6.42     |\n","|    ent_coef        | 0.00274  |\n","|    ent_coef_loss   | 0.887    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 151695   |\n","---------------------------------\n","Eval num_timesteps=3643200, episode_reward=0.50 +/- 0.81\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.502    |\n","| time/              |          |\n","|    total_timesteps | 3643200  |\n","| train/             |          |\n","|    actor_loss      | 3.01     |\n","|    critic_loss     | 0.417    |\n","|    ent_coef        | 0.00279  |\n","|    ent_coef_loss   | -0.0243  |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 151795   |\n","---------------------------------\n","Eval num_timesteps=3645600, episode_reward=-57.40 +/- 49.15\n","Episode length: 286.40 +/- 174.40\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 286      |\n","|    mean_reward     | -57.4    |\n","| time/              |          |\n","|    total_timesteps | 3645600  |\n","| train/             |          |\n","|    actor_loss      | 3.25     |\n","|    critic_loss     | 1.18     |\n","|    ent_coef        | 0.00283  |\n","|    ent_coef_loss   | 1.13     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 151895   |\n","---------------------------------\n","Eval num_timesteps=3648000, episode_reward=-101.21 +/- 6.55\n","Episode length: 159.60 +/- 2.94\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 160      |\n","|    mean_reward     | -101     |\n","| time/              |          |\n","|    total_timesteps | 3648000  |\n","| train/             |          |\n","|    actor_loss      | 2.88     |\n","|    critic_loss     | 0.479    |\n","|    ent_coef        | 0.00289  |\n","|    ent_coef_loss   | 0.667    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 151995   |\n","---------------------------------\n","Eval num_timesteps=3650400, episode_reward=-0.86 +/- 9.47\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.857   |\n","| time/              |          |\n","|    total_timesteps | 3650400  |\n","| train/             |          |\n","|    actor_loss      | 3        |\n","|    critic_loss     | 15.2     |\n","|    ent_coef        | 0.00292  |\n","|    ent_coef_loss   | 0.299    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 152095   |\n","---------------------------------\n","Eval num_timesteps=3652800, episode_reward=3.41 +/- 1.56\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.41     |\n","| time/              |          |\n","|    total_timesteps | 3652800  |\n","| train/             |          |\n","|    actor_loss      | 2.52     |\n","|    critic_loss     | 10.4     |\n","|    ent_coef        | 0.0029   |\n","|    ent_coef_loss   | 3.93     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 152195   |\n","---------------------------------\n","Eval num_timesteps=3655200, episode_reward=6.01 +/- 0.21\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.01     |\n","| time/              |          |\n","|    total_timesteps | 3655200  |\n","| train/             |          |\n","|    actor_loss      | 2.85     |\n","|    critic_loss     | 1.17     |\n","|    ent_coef        | 0.00297  |\n","|    ent_coef_loss   | -1.74    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 152295   |\n","---------------------------------\n","Eval num_timesteps=3657600, episode_reward=-56.93 +/- 54.07\n","Episode length: 455.00 +/- 36.74\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 455      |\n","|    mean_reward     | -56.9    |\n","| time/              |          |\n","|    total_timesteps | 3657600  |\n","| train/             |          |\n","|    actor_loss      | 3.21     |\n","|    critic_loss     | 0.419    |\n","|    ent_coef        | 0.00297  |\n","|    ent_coef_loss   | 0.284    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 152395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5880     |\n","|    fps             | 305      |\n","|    time_elapsed    | 11990    |\n","|    total_timesteps | 3658872  |\n","| train/             |          |\n","|    actor_loss      | 2.93     |\n","|    critic_loss     | 0.404    |\n","|    ent_coef        | 0.00297  |\n","|    ent_coef_loss   | 0.332    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 152448   |\n","---------------------------------\n","Eval num_timesteps=3660000, episode_reward=-56.74 +/- 49.69\n","Episode length: 362.60 +/- 112.19\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 363      |\n","|    mean_reward     | -56.7    |\n","| time/              |          |\n","|    total_timesteps | 3660000  |\n","| train/             |          |\n","|    actor_loss      | 2.86     |\n","|    critic_loss     | 0.401    |\n","|    ent_coef        | 0.00296  |\n","|    ent_coef_loss   | 6.11     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 152495   |\n","---------------------------------\n","Eval num_timesteps=3662400, episode_reward=-1.50 +/- 6.09\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.5     |\n","| time/              |          |\n","|    total_timesteps | 3662400  |\n","| train/             |          |\n","|    actor_loss      | 3.02     |\n","|    critic_loss     | 0.333    |\n","|    ent_coef        | 0.00298  |\n","|    ent_coef_loss   | -0.503   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 152595   |\n","---------------------------------\n","Eval num_timesteps=3664800, episode_reward=5.72 +/- 0.17\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.72     |\n","| time/              |          |\n","|    total_timesteps | 3664800  |\n","| train/             |          |\n","|    actor_loss      | 2.69     |\n","|    critic_loss     | 0.414    |\n","|    ent_coef        | 0.00295  |\n","|    ent_coef_loss   | -2.74    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 152695   |\n","---------------------------------\n","Eval num_timesteps=3667200, episode_reward=1.32 +/- 1.98\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.32     |\n","| time/              |          |\n","|    total_timesteps | 3667200  |\n","| train/             |          |\n","|    actor_loss      | 3.06     |\n","|    critic_loss     | 10.3     |\n","|    ent_coef        | 0.00288  |\n","|    ent_coef_loss   | 0.594    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 152795   |\n","---------------------------------\n","Eval num_timesteps=3669600, episode_reward=6.42 +/- 0.51\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.42     |\n","| time/              |          |\n","|    total_timesteps | 3669600  |\n","| train/             |          |\n","|    actor_loss      | 2.8      |\n","|    critic_loss     | 2.27     |\n","|    ent_coef        | 0.00288  |\n","|    ent_coef_loss   | -2.23    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 152895   |\n","---------------------------------\n","Eval num_timesteps=3672000, episode_reward=-54.53 +/- 47.74\n","Episode length: 341.00 +/- 129.82\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 341      |\n","|    mean_reward     | -54.5    |\n","| time/              |          |\n","|    total_timesteps | 3672000  |\n","| train/             |          |\n","|    actor_loss      | 2.94     |\n","|    critic_loss     | 0.989    |\n","|    ent_coef        | 0.00287  |\n","|    ent_coef_loss   | 2.49     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 152995   |\n","---------------------------------\n","Eval num_timesteps=3674400, episode_reward=-59.45 +/- 50.38\n","Episode length: 306.80 +/- 157.75\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 307      |\n","|    mean_reward     | -59.4    |\n","| time/              |          |\n","|    total_timesteps | 3674400  |\n","| train/             |          |\n","|    actor_loss      | 2.93     |\n","|    critic_loss     | 13.3     |\n","|    ent_coef        | 0.00288  |\n","|    ent_coef_loss   | -0.23    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 153095   |\n","---------------------------------\n","Eval num_timesteps=3676800, episode_reward=-42.30 +/- 52.41\n","Episode length: 361.20 +/- 169.99\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 361      |\n","|    mean_reward     | -42.3    |\n","| time/              |          |\n","|    total_timesteps | 3676800  |\n","| train/             |          |\n","|    actor_loss      | 2.99     |\n","|    critic_loss     | 1.11     |\n","|    ent_coef        | 0.00283  |\n","|    ent_coef_loss   | -2.99    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 153195   |\n","---------------------------------\n","Eval num_timesteps=3679200, episode_reward=-107.81 +/- 4.87\n","Episode length: 102.20 +/- 1.47\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 102      |\n","|    mean_reward     | -108     |\n","| time/              |          |\n","|    total_timesteps | 3679200  |\n","| train/             |          |\n","|    actor_loss      | 3.01     |\n","|    critic_loss     | 0.678    |\n","|    ent_coef        | 0.00278  |\n","|    ent_coef_loss   | 3.18     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 153295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5890     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12061    |\n","|    total_timesteps | 3679608  |\n","| train/             |          |\n","|    actor_loss      | 2.89     |\n","|    critic_loss     | 7.8      |\n","|    ent_coef        | 0.0028   |\n","|    ent_coef_loss   | -1.66    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 153312   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5900     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12062    |\n","|    total_timesteps | 3680136  |\n","| train/             |          |\n","|    actor_loss      | 3.2      |\n","|    critic_loss     | 0.756    |\n","|    ent_coef        | 0.00278  |\n","|    ent_coef_loss   | -2.26    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 153334   |\n","---------------------------------\n","Eval num_timesteps=3681600, episode_reward=-35.56 +/- 47.21\n","Episode length: 350.00 +/- 183.71\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 350      |\n","|    mean_reward     | -35.6    |\n","| time/              |          |\n","|    total_timesteps | 3681600  |\n","| train/             |          |\n","|    actor_loss      | 2.84     |\n","|    critic_loss     | 0.363    |\n","|    ent_coef        | 0.00271  |\n","|    ent_coef_loss   | -1.08    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 153395   |\n","---------------------------------\n","Eval num_timesteps=3684000, episode_reward=-93.78 +/- 9.02\n","Episode length: 156.20 +/- 3.43\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 156      |\n","|    mean_reward     | -93.8    |\n","| time/              |          |\n","|    total_timesteps | 3684000  |\n","| train/             |          |\n","|    actor_loss      | 3.02     |\n","|    critic_loss     | 8.64     |\n","|    ent_coef        | 0.00271  |\n","|    ent_coef_loss   | -1.57    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 153495   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5910     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12073    |\n","|    total_timesteps | 3685944  |\n","| train/             |          |\n","|    actor_loss      | 2.66     |\n","|    critic_loss     | 1.76     |\n","|    ent_coef        | 0.00271  |\n","|    ent_coef_loss   | 0.623    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 153576   |\n","---------------------------------\n","Eval num_timesteps=3686400, episode_reward=-59.27 +/- 52.57\n","Episode length: 236.60 +/- 215.07\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 237      |\n","|    mean_reward     | -59.3    |\n","| time/              |          |\n","|    total_timesteps | 3686400  |\n","| train/             |          |\n","|    actor_loss      | 3.06     |\n","|    critic_loss     | 0.47     |\n","|    ent_coef        | 0.00272  |\n","|    ent_coef_loss   | 2.08     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 153595   |\n","---------------------------------\n","Eval num_timesteps=3688800, episode_reward=4.14 +/- 1.12\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.14     |\n","| time/              |          |\n","|    total_timesteps | 3688800  |\n","| train/             |          |\n","|    actor_loss      | 2.82     |\n","|    critic_loss     | 0.324    |\n","|    ent_coef        | 0.00274  |\n","|    ent_coef_loss   | -0.0991  |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 153695   |\n","---------------------------------\n","Eval num_timesteps=3691200, episode_reward=8.25 +/- 3.10\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 8.25     |\n","| time/              |          |\n","|    total_timesteps | 3691200  |\n","| train/             |          |\n","|    actor_loss      | 2.67     |\n","|    critic_loss     | 17.8     |\n","|    ent_coef        | 0.00289  |\n","|    ent_coef_loss   | 0.179    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 153795   |\n","---------------------------------\n","Eval num_timesteps=3693600, episode_reward=-56.76 +/- 52.69\n","Episode length: 296.00 +/- 166.57\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 296      |\n","|    mean_reward     | -56.8    |\n","| time/              |          |\n","|    total_timesteps | 3693600  |\n","| train/             |          |\n","|    actor_loss      | 3.13     |\n","|    critic_loss     | 0.536    |\n","|    ent_coef        | 0.00296  |\n","|    ent_coef_loss   | 0.732    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 153895   |\n","---------------------------------\n","Eval num_timesteps=3696000, episode_reward=-32.14 +/- 54.99\n","Episode length: 326.00 +/- 213.11\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 326      |\n","|    mean_reward     | -32.1    |\n","| time/              |          |\n","|    total_timesteps | 3696000  |\n","| train/             |          |\n","|    actor_loss      | 2.36     |\n","|    critic_loss     | 0.466    |\n","|    ent_coef        | 0.00295  |\n","|    ent_coef_loss   | -1.54    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 153995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5920     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12114    |\n","|    total_timesteps | 3697560  |\n","| train/             |          |\n","|    actor_loss      | 3.05     |\n","|    critic_loss     | 0.52     |\n","|    ent_coef        | 0.00304  |\n","|    ent_coef_loss   | 2.69     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 154060   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5930     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12115    |\n","|    total_timesteps | 3697800  |\n","| train/             |          |\n","|    actor_loss      | 2.86     |\n","|    critic_loss     | 18.7     |\n","|    ent_coef        | 0.00305  |\n","|    ent_coef_loss   | 0.133    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 154070   |\n","---------------------------------\n","Eval num_timesteps=3698400, episode_reward=-36.36 +/- 49.64\n","Episode length: 355.60 +/- 176.85\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 356      |\n","|    mean_reward     | -36.4    |\n","| time/              |          |\n","|    total_timesteps | 3698400  |\n","| train/             |          |\n","|    actor_loss      | 2.4      |\n","|    critic_loss     | 0.364    |\n","|    ent_coef        | 0.00305  |\n","|    ent_coef_loss   | -0.465   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 154095   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5940     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12126    |\n","|    total_timesteps | 3700272  |\n","| train/             |          |\n","|    actor_loss      | 2.62     |\n","|    critic_loss     | 0.38     |\n","|    ent_coef        | 0.00311  |\n","|    ent_coef_loss   | 1.14     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 154173   |\n","---------------------------------\n","Eval num_timesteps=3700800, episode_reward=-91.75 +/- 6.67\n","Episode length: 123.60 +/- 19.11\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 124      |\n","|    mean_reward     | -91.7    |\n","| time/              |          |\n","|    total_timesteps | 3700800  |\n","| train/             |          |\n","|    actor_loss      | 3.44     |\n","|    critic_loss     | 0.909    |\n","|    ent_coef        | 0.00312  |\n","|    ent_coef_loss   | -0.269   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 154195   |\n","---------------------------------\n","Eval num_timesteps=3703200, episode_reward=5.24 +/- 5.69\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.24     |\n","| time/              |          |\n","|    total_timesteps | 3703200  |\n","| train/             |          |\n","|    actor_loss      | 2.72     |\n","|    critic_loss     | 0.488    |\n","|    ent_coef        | 0.00313  |\n","|    ent_coef_loss   | -3.44    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 154295   |\n","---------------------------------\n","Eval num_timesteps=3705600, episode_reward=0.11 +/- 0.31\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.108    |\n","| time/              |          |\n","|    total_timesteps | 3705600  |\n","| train/             |          |\n","|    actor_loss      | 2.37     |\n","|    critic_loss     | 11       |\n","|    ent_coef        | 0.00299  |\n","|    ent_coef_loss   | 4.39     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 154395   |\n","---------------------------------\n","Eval num_timesteps=3708000, episode_reward=-40.02 +/- 51.39\n","Episode length: 399.20 +/- 123.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 399      |\n","|    mean_reward     | -40      |\n","| time/              |          |\n","|    total_timesteps | 3708000  |\n","| train/             |          |\n","|    actor_loss      | 2.59     |\n","|    critic_loss     | 1.03     |\n","|    ent_coef        | 0.0031   |\n","|    ent_coef_loss   | 0.962    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 154495   |\n","---------------------------------\n","Eval num_timesteps=3710400, episode_reward=-0.25 +/- 4.15\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.251   |\n","| time/              |          |\n","|    total_timesteps | 3710400  |\n","| train/             |          |\n","|    actor_loss      | 2.66     |\n","|    critic_loss     | 0.336    |\n","|    ent_coef        | 0.00318  |\n","|    ent_coef_loss   | -0.629   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 154595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5950     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12162    |\n","|    total_timesteps | 3712080  |\n","| train/             |          |\n","|    actor_loss      | 3.04     |\n","|    critic_loss     | 9.82     |\n","|    ent_coef        | 0.00312  |\n","|    ent_coef_loss   | 0.384    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 154665   |\n","---------------------------------\n","Eval num_timesteps=3712800, episode_reward=14.12 +/- 0.97\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 14.1     |\n","| time/              |          |\n","|    total_timesteps | 3712800  |\n","| train/             |          |\n","|    actor_loss      | 2.78     |\n","|    critic_loss     | 0.553    |\n","|    ent_coef        | 0.0031   |\n","|    ent_coef_loss   | -1.37    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 154695   |\n","---------------------------------\n","Eval num_timesteps=3715200, episode_reward=-97.99 +/- 8.53\n","Episode length: 135.60 +/- 32.33\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 136      |\n","|    mean_reward     | -98      |\n","| time/              |          |\n","|    total_timesteps | 3715200  |\n","| train/             |          |\n","|    actor_loss      | 3.11     |\n","|    critic_loss     | 0.46     |\n","|    ent_coef        | 0.003    |\n","|    ent_coef_loss   | -0.556   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 154795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5960     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12173    |\n","|    total_timesteps | 3716424  |\n","| train/             |          |\n","|    actor_loss      | 2.66     |\n","|    critic_loss     | 0.32     |\n","|    ent_coef        | 0.00294  |\n","|    ent_coef_loss   | -3.15    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 154846   |\n","---------------------------------\n","Eval num_timesteps=3717600, episode_reward=-39.59 +/- 53.09\n","Episode length: 335.60 +/- 201.35\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 336      |\n","|    mean_reward     | -39.6    |\n","| time/              |          |\n","|    total_timesteps | 3717600  |\n","| train/             |          |\n","|    actor_loss      | 3.27     |\n","|    critic_loss     | 0.443    |\n","|    ent_coef        | 0.00286  |\n","|    ent_coef_loss   | -3.39    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 154895   |\n","---------------------------------\n","Eval num_timesteps=3720000, episode_reward=1.66 +/- 4.62\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.66     |\n","| time/              |          |\n","|    total_timesteps | 3720000  |\n","| train/             |          |\n","|    actor_loss      | 3.04     |\n","|    critic_loss     | 0.386    |\n","|    ent_coef        | 0.00289  |\n","|    ent_coef_loss   | 1.14     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 154995   |\n","---------------------------------\n","Eval num_timesteps=3722400, episode_reward=5.56 +/- 1.81\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.56     |\n","| time/              |          |\n","|    total_timesteps | 3722400  |\n","| train/             |          |\n","|    actor_loss      | 3.18     |\n","|    critic_loss     | 0.906    |\n","|    ent_coef        | 0.00285  |\n","|    ent_coef_loss   | 3.67     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 155095   |\n","---------------------------------\n","Eval num_timesteps=3724800, episode_reward=7.01 +/- 3.78\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.01     |\n","| time/              |          |\n","|    total_timesteps | 3724800  |\n","| train/             |          |\n","|    actor_loss      | 3.09     |\n","|    critic_loss     | 0.6      |\n","|    ent_coef        | 0.00287  |\n","|    ent_coef_loss   | 1.75     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 155195   |\n","---------------------------------\n","Eval num_timesteps=3727200, episode_reward=3.22 +/- 0.48\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.22     |\n","| time/              |          |\n","|    total_timesteps | 3727200  |\n","| train/             |          |\n","|    actor_loss      | 2.88     |\n","|    critic_loss     | 0.472    |\n","|    ent_coef        | 0.00294  |\n","|    ent_coef_loss   | 0.584    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 155295   |\n","---------------------------------\n","Eval num_timesteps=3729600, episode_reward=6.48 +/- 1.48\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.48     |\n","| time/              |          |\n","|    total_timesteps | 3729600  |\n","| train/             |          |\n","|    actor_loss      | 2.99     |\n","|    critic_loss     | 0.855    |\n","|    ent_coef        | 0.00295  |\n","|    ent_coef_loss   | -1.31    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 155395   |\n","---------------------------------\n","Eval num_timesteps=3732000, episode_reward=-37.83 +/- 49.61\n","Episode length: 353.60 +/- 179.30\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 354      |\n","|    mean_reward     | -37.8    |\n","| time/              |          |\n","|    total_timesteps | 3732000  |\n","| train/             |          |\n","|    actor_loss      | 2.58     |\n","|    critic_loss     | 0.301    |\n","|    ent_coef        | 0.00298  |\n","|    ent_coef_loss   | -2.15    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 155495   |\n","---------------------------------\n","Eval num_timesteps=3734400, episode_reward=-61.26 +/- 52.43\n","Episode length: 398.00 +/- 83.28\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 398      |\n","|    mean_reward     | -61.3    |\n","| time/              |          |\n","|    total_timesteps | 3734400  |\n","| train/             |          |\n","|    actor_loss      | 2.85     |\n","|    critic_loss     | 0.456    |\n","|    ent_coef        | 0.00298  |\n","|    ent_coef_loss   | 0.464    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 155595   |\n","---------------------------------\n","Eval num_timesteps=3736800, episode_reward=-93.92 +/- 5.57\n","Episode length: 220.80 +/- 54.87\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 221      |\n","|    mean_reward     | -93.9    |\n","| time/              |          |\n","|    total_timesteps | 3736800  |\n","| train/             |          |\n","|    actor_loss      | 2.98     |\n","|    critic_loss     | 4.53     |\n","|    ent_coef        | 0.00302  |\n","|    ent_coef_loss   | 5.25     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 155695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5970     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12246    |\n","|    total_timesteps | 3737328  |\n","| train/             |          |\n","|    actor_loss      | 2.63     |\n","|    critic_loss     | 0.394    |\n","|    ent_coef        | 0.00311  |\n","|    ent_coef_loss   | 4.19     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 155717   |\n","---------------------------------\n","Eval num_timesteps=3739200, episode_reward=-43.89 +/- 58.97\n","Episode length: 391.20 +/- 133.25\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 391      |\n","|    mean_reward     | -43.9    |\n","| time/              |          |\n","|    total_timesteps | 3739200  |\n","| train/             |          |\n","|    actor_loss      | 3.06     |\n","|    critic_loss     | 0.837    |\n","|    ent_coef        | 0.00329  |\n","|    ent_coef_loss   | 4.6      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 155795   |\n","---------------------------------\n","Eval num_timesteps=3741600, episode_reward=3.83 +/- 2.04\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.83     |\n","| time/              |          |\n","|    total_timesteps | 3741600  |\n","| train/             |          |\n","|    actor_loss      | 2.69     |\n","|    critic_loss     | 0.385    |\n","|    ent_coef        | 0.00325  |\n","|    ent_coef_loss   | -1.85    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 155895   |\n","---------------------------------\n","Eval num_timesteps=3744000, episode_reward=10.24 +/- 1.38\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 10.2     |\n","| time/              |          |\n","|    total_timesteps | 3744000  |\n","| train/             |          |\n","|    actor_loss      | 2.9      |\n","|    critic_loss     | 0.441    |\n","|    ent_coef        | 0.00315  |\n","|    ent_coef_loss   | -0.0378  |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 155995   |\n","---------------------------------\n","Eval num_timesteps=3746400, episode_reward=-1.38 +/- 1.49\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.38    |\n","| time/              |          |\n","|    total_timesteps | 3746400  |\n","| train/             |          |\n","|    actor_loss      | 2.67     |\n","|    critic_loss     | 0.4      |\n","|    ent_coef        | 0.0031   |\n","|    ent_coef_loss   | -2.01    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 156095   |\n","---------------------------------\n","Eval num_timesteps=3748800, episode_reward=-0.84 +/- 0.56\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.842   |\n","| time/              |          |\n","|    total_timesteps | 3748800  |\n","| train/             |          |\n","|    actor_loss      | 3.1      |\n","|    critic_loss     | 0.301    |\n","|    ent_coef        | 0.00307  |\n","|    ent_coef_loss   | -0.941   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 156195   |\n","---------------------------------\n","Eval num_timesteps=3751200, episode_reward=8.47 +/- 5.51\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 8.47     |\n","| time/              |          |\n","|    total_timesteps | 3751200  |\n","| train/             |          |\n","|    actor_loss      | 3.27     |\n","|    critic_loss     | 1.66     |\n","|    ent_coef        | 0.00304  |\n","|    ent_coef_loss   | 0.157    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 156295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5980     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12295    |\n","|    total_timesteps | 3752952  |\n","| train/             |          |\n","|    actor_loss      | 2.94     |\n","|    critic_loss     | 0.319    |\n","|    ent_coef        | 0.003    |\n","|    ent_coef_loss   | -0.458   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 156368   |\n","---------------------------------\n","Eval num_timesteps=3753600, episode_reward=-39.65 +/- 52.58\n","Episode length: 491.60 +/- 10.29\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 492      |\n","|    mean_reward     | -39.6    |\n","| time/              |          |\n","|    total_timesteps | 3753600  |\n","| train/             |          |\n","|    actor_loss      | 3.3      |\n","|    critic_loss     | 0.499    |\n","|    ent_coef        | 0.00301  |\n","|    ent_coef_loss   | 1.38     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 156395   |\n","---------------------------------\n","Eval num_timesteps=3756000, episode_reward=-42.14 +/- 53.94\n","Episode length: 360.80 +/- 170.48\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 361      |\n","|    mean_reward     | -42.1    |\n","| time/              |          |\n","|    total_timesteps | 3756000  |\n","| train/             |          |\n","|    actor_loss      | 3.32     |\n","|    critic_loss     | 0.528    |\n","|    ent_coef        | 0.00312  |\n","|    ent_coef_loss   | 1.92     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 156495   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5990     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12312    |\n","|    total_timesteps | 3756240  |\n","| train/             |          |\n","|    actor_loss      | 2.91     |\n","|    critic_loss     | 0.721    |\n","|    ent_coef        | 0.00312  |\n","|    ent_coef_loss   | -1.65    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 156505   |\n","---------------------------------\n","Eval num_timesteps=3758400, episode_reward=-107.15 +/- 11.28\n","Episode length: 157.00 +/- 19.60\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 157      |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 3758400  |\n","| train/             |          |\n","|    actor_loss      | 3.36     |\n","|    critic_loss     | 1.72     |\n","|    ent_coef        | 0.00312  |\n","|    ent_coef_loss   | 0.278    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 156595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6000     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12315    |\n","|    total_timesteps | 3758712  |\n","| train/             |          |\n","|    actor_loss      | 3.09     |\n","|    critic_loss     | 1.14     |\n","|    ent_coef        | 0.00312  |\n","|    ent_coef_loss   | -0.172   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 156608   |\n","---------------------------------\n","Eval num_timesteps=3760800, episode_reward=-93.59 +/- 5.62\n","Episode length: 158.80 +/- 32.82\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 159      |\n","|    mean_reward     | -93.6    |\n","| time/              |          |\n","|    total_timesteps | 3760800  |\n","| train/             |          |\n","|    actor_loss      | 3.05     |\n","|    critic_loss     | 4.95     |\n","|    ent_coef        | 0.00313  |\n","|    ent_coef_loss   | 0.186    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 156695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6010     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12322    |\n","|    total_timesteps | 3763080  |\n","| train/             |          |\n","|    actor_loss      | 3.11     |\n","|    critic_loss     | 17       |\n","|    ent_coef        | 0.00307  |\n","|    ent_coef_loss   | -0.526   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 156790   |\n","---------------------------------\n","Eval num_timesteps=3763200, episode_reward=-44.06 +/- 52.49\n","Episode length: 344.00 +/- 191.06\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 344      |\n","|    mean_reward     | -44.1    |\n","| time/              |          |\n","|    total_timesteps | 3763200  |\n","| train/             |          |\n","|    actor_loss      | 3.3      |\n","|    critic_loss     | 0.362    |\n","|    ent_coef        | 0.00307  |\n","|    ent_coef_loss   | 0.566    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 156795   |\n","---------------------------------\n","Eval num_timesteps=3765600, episode_reward=0.03 +/- 0.28\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.031    |\n","| time/              |          |\n","|    total_timesteps | 3765600  |\n","| train/             |          |\n","|    actor_loss      | 3.34     |\n","|    critic_loss     | 1.13     |\n","|    ent_coef        | 0.0034   |\n","|    ent_coef_loss   | 5.58     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 156895   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6020     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12339    |\n","|    total_timesteps | 3767784  |\n","| train/             |          |\n","|    actor_loss      | 2.95     |\n","|    critic_loss     | 0.811    |\n","|    ent_coef        | 0.0035   |\n","|    ent_coef_loss   | -2.4     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 156986   |\n","---------------------------------\n","Eval num_timesteps=3768000, episode_reward=7.24 +/- 1.97\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.24     |\n","| time/              |          |\n","|    total_timesteps | 3768000  |\n","| train/             |          |\n","|    actor_loss      | 3.1      |\n","|    critic_loss     | 0.439    |\n","|    ent_coef        | 0.00349  |\n","|    ent_coef_loss   | -2.15    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 156995   |\n","---------------------------------\n","Eval num_timesteps=3770400, episode_reward=5.28 +/- 0.49\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.28     |\n","| time/              |          |\n","|    total_timesteps | 3770400  |\n","| train/             |          |\n","|    actor_loss      | 3.16     |\n","|    critic_loss     | 20.9     |\n","|    ent_coef        | 0.0034   |\n","|    ent_coef_loss   | -1.63    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 157095   |\n","---------------------------------\n","Eval num_timesteps=3772800, episode_reward=-94.15 +/- 6.65\n","Episode length: 208.60 +/- 109.74\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 209      |\n","|    mean_reward     | -94.2    |\n","| time/              |          |\n","|    total_timesteps | 3772800  |\n","| train/             |          |\n","|    actor_loss      | 3.89     |\n","|    critic_loss     | 0.642    |\n","|    ent_coef        | 0.0034   |\n","|    ent_coef_loss   | 3.7      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 157195   |\n","---------------------------------\n","Eval num_timesteps=3775200, episode_reward=6.18 +/- 1.19\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.18     |\n","| time/              |          |\n","|    total_timesteps | 3775200  |\n","| train/             |          |\n","|    actor_loss      | 3        |\n","|    critic_loss     | 0.542    |\n","|    ent_coef        | 0.0034   |\n","|    ent_coef_loss   | -3.7     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 157295   |\n","---------------------------------\n","Eval num_timesteps=3777600, episode_reward=8.75 +/- 0.53\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 8.75     |\n","| time/              |          |\n","|    total_timesteps | 3777600  |\n","| train/             |          |\n","|    actor_loss      | 2.65     |\n","|    critic_loss     | 0.322    |\n","|    ent_coef        | 0.00335  |\n","|    ent_coef_loss   | -2.17    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 157395   |\n","---------------------------------\n","Eval num_timesteps=3780000, episode_reward=-61.15 +/- 49.71\n","Episode length: 295.40 +/- 167.06\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 295      |\n","|    mean_reward     | -61.2    |\n","| time/              |          |\n","|    total_timesteps | 3780000  |\n","| train/             |          |\n","|    actor_loss      | 2.91     |\n","|    critic_loss     | 0.658    |\n","|    ent_coef        | 0.00325  |\n","|    ent_coef_loss   | -4.06    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 157495   |\n","---------------------------------\n","Eval num_timesteps=3782400, episode_reward=-101.86 +/- 8.20\n","Episode length: 121.00 +/- 29.39\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 121      |\n","|    mean_reward     | -102     |\n","| time/              |          |\n","|    total_timesteps | 3782400  |\n","| train/             |          |\n","|    actor_loss      | 3.07     |\n","|    critic_loss     | 1.47     |\n","|    ent_coef        | 0.00328  |\n","|    ent_coef_loss   | 3.37     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 157595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6030     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12392    |\n","|    total_timesteps | 3782616  |\n","| train/             |          |\n","|    actor_loss      | 3.22     |\n","|    critic_loss     | 0.803    |\n","|    ent_coef        | 0.0033   |\n","|    ent_coef_loss   | 0.257    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 157604   |\n","---------------------------------\n","Eval num_timesteps=3784800, episode_reward=7.81 +/- 1.29\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.81     |\n","| time/              |          |\n","|    total_timesteps | 3784800  |\n","| train/             |          |\n","|    actor_loss      | 2.62     |\n","|    critic_loss     | 0.339    |\n","|    ent_coef        | 0.00341  |\n","|    ent_coef_loss   | 0.0599   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 157695   |\n","---------------------------------\n","Eval num_timesteps=3787200, episode_reward=-94.01 +/- 2.99\n","Episode length: 114.60 +/- 0.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 115      |\n","|    mean_reward     | -94      |\n","| time/              |          |\n","|    total_timesteps | 3787200  |\n","| train/             |          |\n","|    actor_loss      | 2.9      |\n","|    critic_loss     | 0.822    |\n","|    ent_coef        | 0.00346  |\n","|    ent_coef_loss   | 2.08     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 157795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6040     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12404    |\n","|    total_timesteps | 3789504  |\n","| train/             |          |\n","|    actor_loss      | 3.2      |\n","|    critic_loss     | 0.598    |\n","|    ent_coef        | 0.00374  |\n","|    ent_coef_loss   | 1.75     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 157891   |\n","---------------------------------\n","Eval num_timesteps=3789600, episode_reward=0.45 +/- 0.54\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.453    |\n","| time/              |          |\n","|    total_timesteps | 3789600  |\n","| train/             |          |\n","|    actor_loss      | 4.05     |\n","|    critic_loss     | 1.74     |\n","|    ent_coef        | 0.00374  |\n","|    ent_coef_loss   | 3.03     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 157895   |\n","---------------------------------\n","Eval num_timesteps=3792000, episode_reward=4.56 +/- 3.47\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.56     |\n","| time/              |          |\n","|    total_timesteps | 3792000  |\n","| train/             |          |\n","|    actor_loss      | 3.21     |\n","|    critic_loss     | 20.9     |\n","|    ent_coef        | 0.00367  |\n","|    ent_coef_loss   | -1.26    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 157995   |\n","---------------------------------\n","Eval num_timesteps=3794400, episode_reward=3.55 +/- 1.13\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.55     |\n","| time/              |          |\n","|    total_timesteps | 3794400  |\n","| train/             |          |\n","|    actor_loss      | 3.32     |\n","|    critic_loss     | 0.686    |\n","|    ent_coef        | 0.00351  |\n","|    ent_coef_loss   | -3.46    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 158095   |\n","---------------------------------\n","Eval num_timesteps=3796800, episode_reward=-0.96 +/- 1.23\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.963   |\n","| time/              |          |\n","|    total_timesteps | 3796800  |\n","| train/             |          |\n","|    actor_loss      | 3.54     |\n","|    critic_loss     | 0.909    |\n","|    ent_coef        | 0.00332  |\n","|    ent_coef_loss   | -1.24    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 158195   |\n","---------------------------------\n","Eval num_timesteps=3799200, episode_reward=1.06 +/- 1.80\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.06     |\n","| time/              |          |\n","|    total_timesteps | 3799200  |\n","| train/             |          |\n","|    actor_loss      | 3.59     |\n","|    critic_loss     | 0.789    |\n","|    ent_coef        | 0.00332  |\n","|    ent_coef_loss   | -1.54    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 158295   |\n","---------------------------------\n","Eval num_timesteps=3801600, episode_reward=4.21 +/- 5.43\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.21     |\n","| time/              |          |\n","|    total_timesteps | 3801600  |\n","| train/             |          |\n","|    actor_loss      | 2.85     |\n","|    critic_loss     | 0.473    |\n","|    ent_coef        | 0.0033   |\n","|    ent_coef_loss   | -1.64    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 158395   |\n","---------------------------------\n","Eval num_timesteps=3804000, episode_reward=0.44 +/- 1.84\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.444    |\n","| time/              |          |\n","|    total_timesteps | 3804000  |\n","| train/             |          |\n","|    actor_loss      | 3.16     |\n","|    critic_loss     | 0.624    |\n","|    ent_coef        | 0.00329  |\n","|    ent_coef_loss   | 0.564    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 158495   |\n","---------------------------------\n","Eval num_timesteps=3806400, episode_reward=0.52 +/- 1.69\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.519    |\n","| time/              |          |\n","|    total_timesteps | 3806400  |\n","| train/             |          |\n","|    actor_loss      | 4.05     |\n","|    critic_loss     | 3.13     |\n","|    ent_coef        | 0.00332  |\n","|    ent_coef_loss   | 2.05     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 158595   |\n","---------------------------------\n","Eval num_timesteps=3808800, episode_reward=1.99 +/- 0.82\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.99     |\n","| time/              |          |\n","|    total_timesteps | 3808800  |\n","| train/             |          |\n","|    actor_loss      | 2.91     |\n","|    critic_loss     | 3.87     |\n","|    ent_coef        | 0.00334  |\n","|    ent_coef_loss   | -3.16    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 158695   |\n","---------------------------------\n","Eval num_timesteps=3811200, episode_reward=-40.45 +/- 50.16\n","Episode length: 466.40 +/- 41.15\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 466      |\n","|    mean_reward     | -40.5    |\n","| time/              |          |\n","|    total_timesteps | 3811200  |\n","| train/             |          |\n","|    actor_loss      | 3.31     |\n","|    critic_loss     | 0.391    |\n","|    ent_coef        | 0.00321  |\n","|    ent_coef_loss   | 1.63     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 158795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6050     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12485    |\n","|    total_timesteps | 3812208  |\n","| train/             |          |\n","|    actor_loss      | 3.36     |\n","|    critic_loss     | 16.6     |\n","|    ent_coef        | 0.00323  |\n","|    ent_coef_loss   | -0.924   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 158837   |\n","---------------------------------\n","Eval num_timesteps=3813600, episode_reward=3.83 +/- 1.34\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.83     |\n","| time/              |          |\n","|    total_timesteps | 3813600  |\n","| train/             |          |\n","|    actor_loss      | 3.04     |\n","|    critic_loss     | 0.463    |\n","|    ent_coef        | 0.00321  |\n","|    ent_coef_loss   | -0.535   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 158895   |\n","---------------------------------\n","Eval num_timesteps=3816000, episode_reward=4.88 +/- 0.48\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.88     |\n","| time/              |          |\n","|    total_timesteps | 3816000  |\n","| train/             |          |\n","|    actor_loss      | 3.75     |\n","|    critic_loss     | 1.86     |\n","|    ent_coef        | 0.00323  |\n","|    ent_coef_loss   | -1.05    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 158995   |\n","---------------------------------\n","Eval num_timesteps=3818400, episode_reward=2.25 +/- 1.99\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.25     |\n","| time/              |          |\n","|    total_timesteps | 3818400  |\n","| train/             |          |\n","|    actor_loss      | 3.28     |\n","|    critic_loss     | 0.836    |\n","|    ent_coef        | 0.00323  |\n","|    ent_coef_loss   | 0.662    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 159095   |\n","---------------------------------\n","Eval num_timesteps=3820800, episode_reward=-99.06 +/- 3.49\n","Episode length: 200.60 +/- 54.38\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 201      |\n","|    mean_reward     | -99.1    |\n","| time/              |          |\n","|    total_timesteps | 3820800  |\n","| train/             |          |\n","|    actor_loss      | 3.17     |\n","|    critic_loss     | 0.397    |\n","|    ent_coef        | 0.00316  |\n","|    ent_coef_loss   | -3.24    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 159195   |\n","---------------------------------\n","Eval num_timesteps=3823200, episode_reward=6.87 +/- 3.31\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.87     |\n","| time/              |          |\n","|    total_timesteps | 3823200  |\n","| train/             |          |\n","|    actor_loss      | 3.83     |\n","|    critic_loss     | 0.662    |\n","|    ent_coef        | 0.00312  |\n","|    ent_coef_loss   | -1.09    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 159295   |\n","---------------------------------\n","Eval num_timesteps=3825600, episode_reward=9.80 +/- 1.28\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 9.8      |\n","| time/              |          |\n","|    total_timesteps | 3825600  |\n","| train/             |          |\n","|    actor_loss      | 3.18     |\n","|    critic_loss     | 0.369    |\n","|    ent_coef        | 0.00297  |\n","|    ent_coef_loss   | -0.616   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 159395   |\n","---------------------------------\n","Eval num_timesteps=3828000, episode_reward=-45.17 +/- 44.53\n","Episode length: 430.80 +/- 84.75\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 431      |\n","|    mean_reward     | -45.2    |\n","| time/              |          |\n","|    total_timesteps | 3828000  |\n","| train/             |          |\n","|    actor_loss      | 2.84     |\n","|    critic_loss     | 0.33     |\n","|    ent_coef        | 0.00295  |\n","|    ent_coef_loss   | -1.37    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 159495   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6060     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12540    |\n","|    total_timesteps | 3828168  |\n","| train/             |          |\n","|    actor_loss      | 3.45     |\n","|    critic_loss     | 1.89     |\n","|    ent_coef        | 0.00295  |\n","|    ent_coef_loss   | 0.361    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 159502   |\n","---------------------------------\n","Eval num_timesteps=3830400, episode_reward=6.58 +/- 2.98\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.58     |\n","| time/              |          |\n","|    total_timesteps | 3830400  |\n","| train/             |          |\n","|    actor_loss      | 3.6      |\n","|    critic_loss     | 1.42     |\n","|    ent_coef        | 0.00297  |\n","|    ent_coef_loss   | 1.25     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 159595   |\n","---------------------------------\n","Eval num_timesteps=3832800, episode_reward=-38.64 +/- 46.68\n","Episode length: 336.80 +/- 199.88\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 337      |\n","|    mean_reward     | -38.6    |\n","| time/              |          |\n","|    total_timesteps | 3832800  |\n","| train/             |          |\n","|    actor_loss      | 3.63     |\n","|    critic_loss     | 2.6      |\n","|    ent_coef        | 0.00311  |\n","|    ent_coef_loss   | 2.42     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 159695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6070     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12559    |\n","|    total_timesteps | 3835152  |\n","| train/             |          |\n","|    actor_loss      | 2.96     |\n","|    critic_loss     | 0.771    |\n","|    ent_coef        | 0.00309  |\n","|    ent_coef_loss   | -1.88    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 159793   |\n","---------------------------------\n","Eval num_timesteps=3835200, episode_reward=-98.80 +/- 7.23\n","Episode length: 379.40 +/- 16.66\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 379      |\n","|    mean_reward     | -98.8    |\n","| time/              |          |\n","|    total_timesteps | 3835200  |\n","| train/             |          |\n","|    actor_loss      | 3.39     |\n","|    critic_loss     | 0.442    |\n","|    ent_coef        | 0.00309  |\n","|    ent_coef_loss   | 0.151    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 159795   |\n","---------------------------------\n","Eval num_timesteps=3837600, episode_reward=-0.93 +/- 3.33\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.927   |\n","| time/              |          |\n","|    total_timesteps | 3837600  |\n","| train/             |          |\n","|    actor_loss      | 3.45     |\n","|    critic_loss     | 0.681    |\n","|    ent_coef        | 0.00305  |\n","|    ent_coef_loss   | 1.97     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 159895   |\n","---------------------------------\n","Eval num_timesteps=3840000, episode_reward=-0.95 +/- 2.87\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.955   |\n","| time/              |          |\n","|    total_timesteps | 3840000  |\n","| train/             |          |\n","|    actor_loss      | 3.61     |\n","|    critic_loss     | 0.669    |\n","|    ent_coef        | 0.00311  |\n","|    ent_coef_loss   | 0.0903   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 159995   |\n","---------------------------------\n","Eval num_timesteps=3842400, episode_reward=0.89 +/- 0.81\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.885    |\n","| time/              |          |\n","|    total_timesteps | 3842400  |\n","| train/             |          |\n","|    actor_loss      | 3.46     |\n","|    critic_loss     | 0.591    |\n","|    ent_coef        | 0.00312  |\n","|    ent_coef_loss   | -1.3     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 160095   |\n","---------------------------------\n","Eval num_timesteps=3844800, episode_reward=0.36 +/- 1.91\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.363    |\n","| time/              |          |\n","|    total_timesteps | 3844800  |\n","| train/             |          |\n","|    actor_loss      | 3.43     |\n","|    critic_loss     | 1.03     |\n","|    ent_coef        | 0.00308  |\n","|    ent_coef_loss   | -0.814   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 160195   |\n","---------------------------------\n","Eval num_timesteps=3847200, episode_reward=2.72 +/- 1.16\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.72     |\n","| time/              |          |\n","|    total_timesteps | 3847200  |\n","| train/             |          |\n","|    actor_loss      | 3.5      |\n","|    critic_loss     | 18.5     |\n","|    ent_coef        | 0.00298  |\n","|    ent_coef_loss   | -1.31    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 160295   |\n","---------------------------------\n","Eval num_timesteps=3849600, episode_reward=4.76 +/- 1.60\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.76     |\n","| time/              |          |\n","|    total_timesteps | 3849600  |\n","| train/             |          |\n","|    actor_loss      | 3.49     |\n","|    critic_loss     | 2.65     |\n","|    ent_coef        | 0.00304  |\n","|    ent_coef_loss   | -0.4     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 160395   |\n","---------------------------------\n","Eval num_timesteps=3852000, episode_reward=-2.03 +/- 1.75\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -2.03    |\n","| time/              |          |\n","|    total_timesteps | 3852000  |\n","| train/             |          |\n","|    actor_loss      | 3.09     |\n","|    critic_loss     | 0.559    |\n","|    ent_coef        | 0.00316  |\n","|    ent_coef_loss   | 2.53     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 160495   |\n","---------------------------------\n","Eval num_timesteps=3854400, episode_reward=13.86 +/- 0.46\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 13.9     |\n","| time/              |          |\n","|    total_timesteps | 3854400  |\n","| train/             |          |\n","|    actor_loss      | 3.49     |\n","|    critic_loss     | 1.1      |\n","|    ent_coef        | 0.00327  |\n","|    ent_coef_loss   | 2.39     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 160595   |\n","---------------------------------\n","Eval num_timesteps=3856800, episode_reward=2.48 +/- 0.69\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.48     |\n","| time/              |          |\n","|    total_timesteps | 3856800  |\n","| train/             |          |\n","|    actor_loss      | 4.04     |\n","|    critic_loss     | 2.34     |\n","|    ent_coef        | 0.00337  |\n","|    ent_coef_loss   | -1.42    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 160695   |\n","---------------------------------\n","Eval num_timesteps=3859200, episode_reward=11.46 +/- 3.23\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 11.5     |\n","| time/              |          |\n","|    total_timesteps | 3859200  |\n","| train/             |          |\n","|    actor_loss      | 3.75     |\n","|    critic_loss     | 23.2     |\n","|    ent_coef        | 0.00329  |\n","|    ent_coef_loss   | 2.37     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 160795   |\n","---------------------------------\n","Eval num_timesteps=3861600, episode_reward=-92.10 +/- 3.62\n","Episode length: 153.20 +/- 16.17\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 153      |\n","|    mean_reward     | -92.1    |\n","| time/              |          |\n","|    total_timesteps | 3861600  |\n","| train/             |          |\n","|    actor_loss      | 3.58     |\n","|    critic_loss     | 1.13     |\n","|    ent_coef        | 0.00331  |\n","|    ent_coef_loss   | 1.68     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 160895   |\n","---------------------------------\n","Eval num_timesteps=3864000, episode_reward=-40.98 +/- 48.04\n","Episode length: 343.60 +/- 191.55\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 344      |\n","|    mean_reward     | -41      |\n","| time/              |          |\n","|    total_timesteps | 3864000  |\n","| train/             |          |\n","|    actor_loss      | 3.63     |\n","|    critic_loss     | 0.395    |\n","|    ent_coef        | 0.00332  |\n","|    ent_coef_loss   | -1.05    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 160995   |\n","---------------------------------\n","Eval num_timesteps=3866400, episode_reward=1.75 +/- 1.52\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.75     |\n","| time/              |          |\n","|    total_timesteps | 3866400  |\n","| train/             |          |\n","|    actor_loss      | 3.13     |\n","|    critic_loss     | 0.408    |\n","|    ent_coef        | 0.00329  |\n","|    ent_coef_loss   | -1.15    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 161095   |\n","---------------------------------\n","Eval num_timesteps=3868800, episode_reward=0.13 +/- 0.24\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.13     |\n","| time/              |          |\n","|    total_timesteps | 3868800  |\n","| train/             |          |\n","|    actor_loss      | 3.28     |\n","|    critic_loss     | 0.701    |\n","|    ent_coef        | 0.0034   |\n","|    ent_coef_loss   | 3.57     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 161195   |\n","---------------------------------\n","Eval num_timesteps=3871200, episode_reward=1.47 +/- 0.71\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.47     |\n","| time/              |          |\n","|    total_timesteps | 3871200  |\n","| train/             |          |\n","|    actor_loss      | 3.35     |\n","|    critic_loss     | 0.797    |\n","|    ent_coef        | 0.00341  |\n","|    ent_coef_loss   | 3.22     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 161295   |\n","---------------------------------\n","Eval num_timesteps=3873600, episode_reward=3.66 +/- 1.43\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.66     |\n","| time/              |          |\n","|    total_timesteps | 3873600  |\n","| train/             |          |\n","|    actor_loss      | 3.78     |\n","|    critic_loss     | 0.635    |\n","|    ent_coef        | 0.00337  |\n","|    ent_coef_loss   | 1.31     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 161395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6080     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12696    |\n","|    total_timesteps | 3875952  |\n","| train/             |          |\n","|    actor_loss      | 3.55     |\n","|    critic_loss     | 0.749    |\n","|    ent_coef        | 0.00342  |\n","|    ent_coef_loss   | -0.516   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 161493   |\n","---------------------------------\n","Eval num_timesteps=3876000, episode_reward=4.82 +/- 0.39\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.82     |\n","| time/              |          |\n","|    total_timesteps | 3876000  |\n","| train/             |          |\n","|    actor_loss      | 3.78     |\n","|    critic_loss     | 16.3     |\n","|    ent_coef        | 0.00342  |\n","|    ent_coef_loss   | -1.43    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 161495   |\n","---------------------------------\n","Eval num_timesteps=3878400, episode_reward=-37.05 +/- 50.88\n","Episode length: 340.40 +/- 195.47\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 340      |\n","|    mean_reward     | -37      |\n","| time/              |          |\n","|    total_timesteps | 3878400  |\n","| train/             |          |\n","|    actor_loss      | 3.67     |\n","|    critic_loss     | 0.636    |\n","|    ent_coef        | 0.00349  |\n","|    ent_coef_loss   | 0.259    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 161595   |\n","---------------------------------\n","Eval num_timesteps=3880800, episode_reward=-0.92 +/- 0.58\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.922   |\n","| time/              |          |\n","|    total_timesteps | 3880800  |\n","| train/             |          |\n","|    actor_loss      | 3.74     |\n","|    critic_loss     | 0.72     |\n","|    ent_coef        | 0.00377  |\n","|    ent_coef_loss   | 2.53     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 161695   |\n","---------------------------------\n","Eval num_timesteps=3883200, episode_reward=2.22 +/- 2.81\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.22     |\n","| time/              |          |\n","|    total_timesteps | 3883200  |\n","| train/             |          |\n","|    actor_loss      | 3.82     |\n","|    critic_loss     | 0.465    |\n","|    ent_coef        | 0.00379  |\n","|    ent_coef_loss   | -1.23    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 161795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6090     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12727    |\n","|    total_timesteps | 3885360  |\n","| train/             |          |\n","|    actor_loss      | 3.42     |\n","|    critic_loss     | 0.804    |\n","|    ent_coef        | 0.00366  |\n","|    ent_coef_loss   | 0.584    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 161885   |\n","---------------------------------\n","Eval num_timesteps=3885600, episode_reward=-38.93 +/- 52.45\n","Episode length: 374.40 +/- 153.83\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 374      |\n","|    mean_reward     | -38.9    |\n","| time/              |          |\n","|    total_timesteps | 3885600  |\n","| train/             |          |\n","|    actor_loss      | 3.56     |\n","|    critic_loss     | 0.893    |\n","|    ent_coef        | 0.00366  |\n","|    ent_coef_loss   | 1.04     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 161895   |\n","---------------------------------\n","Eval num_timesteps=3888000, episode_reward=5.72 +/- 0.48\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.72     |\n","| time/              |          |\n","|    total_timesteps | 3888000  |\n","| train/             |          |\n","|    actor_loss      | 3.68     |\n","|    critic_loss     | 0.672    |\n","|    ent_coef        | 0.00378  |\n","|    ent_coef_loss   | -1.85    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 161995   |\n","---------------------------------\n","Eval num_timesteps=3890400, episode_reward=3.70 +/- 2.94\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.7      |\n","| time/              |          |\n","|    total_timesteps | 3890400  |\n","| train/             |          |\n","|    actor_loss      | 3.83     |\n","|    critic_loss     | 0.638    |\n","|    ent_coef        | 0.00374  |\n","|    ent_coef_loss   | -0.42    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 162095   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6100     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12752    |\n","|    total_timesteps | 3892344  |\n","| train/             |          |\n","|    actor_loss      | 3.5      |\n","|    critic_loss     | 16.4     |\n","|    ent_coef        | 0.00372  |\n","|    ent_coef_loss   | 1.38     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 162176   |\n","---------------------------------\n","Eval num_timesteps=3892800, episode_reward=4.63 +/- 2.12\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.63     |\n","| time/              |          |\n","|    total_timesteps | 3892800  |\n","| train/             |          |\n","|    actor_loss      | 3.72     |\n","|    critic_loss     | 1.74     |\n","|    ent_coef        | 0.00374  |\n","|    ent_coef_loss   | 0.0855   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 162195   |\n","---------------------------------\n","Eval num_timesteps=3895200, episode_reward=4.98 +/- 4.01\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.98     |\n","| time/              |          |\n","|    total_timesteps | 3895200  |\n","| train/             |          |\n","|    actor_loss      | 3.93     |\n","|    critic_loss     | 0.773    |\n","|    ent_coef        | 0.00362  |\n","|    ent_coef_loss   | -0.965   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 162295   |\n","---------------------------------\n","Eval num_timesteps=3897600, episode_reward=4.21 +/- 3.75\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.21     |\n","| time/              |          |\n","|    total_timesteps | 3897600  |\n","| train/             |          |\n","|    actor_loss      | 3.3      |\n","|    critic_loss     | 3.07     |\n","|    ent_coef        | 0.0035   |\n","|    ent_coef_loss   | -1.29    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 162395   |\n","---------------------------------\n","Eval num_timesteps=3900000, episode_reward=-2.12 +/- 1.22\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -2.12    |\n","| time/              |          |\n","|    total_timesteps | 3900000  |\n","| train/             |          |\n","|    actor_loss      | 3.48     |\n","|    critic_loss     | 0.57     |\n","|    ent_coef        | 0.00346  |\n","|    ent_coef_loss   | -1.78    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 162495   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6110     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12786    |\n","|    total_timesteps | 3901992  |\n","| train/             |          |\n","|    actor_loss      | 3.5      |\n","|    critic_loss     | 0.649    |\n","|    ent_coef        | 0.00336  |\n","|    ent_coef_loss   | -3.09    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 162578   |\n","---------------------------------\n","Eval num_timesteps=3902400, episode_reward=-51.96 +/- 42.38\n","Episode length: 393.20 +/- 87.20\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 393      |\n","|    mean_reward     | -52      |\n","| time/              |          |\n","|    total_timesteps | 3902400  |\n","| train/             |          |\n","|    actor_loss      | 3.87     |\n","|    critic_loss     | 23.1     |\n","|    ent_coef        | 0.00333  |\n","|    ent_coef_loss   | -1.15    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 162595   |\n","---------------------------------\n","Eval num_timesteps=3904800, episode_reward=-37.30 +/- 44.60\n","Episode length: 365.60 +/- 164.61\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 366      |\n","|    mean_reward     | -37.3    |\n","| time/              |          |\n","|    total_timesteps | 3904800  |\n","| train/             |          |\n","|    actor_loss      | 3.84     |\n","|    critic_loss     | 0.5      |\n","|    ent_coef        | 0.0033   |\n","|    ent_coef_loss   | 0.599    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 162695   |\n","---------------------------------\n","Eval num_timesteps=3907200, episode_reward=-34.65 +/- 51.03\n","Episode length: 356.00 +/- 176.36\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 356      |\n","|    mean_reward     | -34.7    |\n","| time/              |          |\n","|    total_timesteps | 3907200  |\n","| train/             |          |\n","|    actor_loss      | 4.17     |\n","|    critic_loss     | 20.4     |\n","|    ent_coef        | 0.00331  |\n","|    ent_coef_loss   | -1.2     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 162795   |\n","---------------------------------\n","Eval num_timesteps=3909600, episode_reward=1.78 +/- 1.10\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.78     |\n","| time/              |          |\n","|    total_timesteps | 3909600  |\n","| train/             |          |\n","|    actor_loss      | 4.29     |\n","|    critic_loss     | 5.48     |\n","|    ent_coef        | 0.00349  |\n","|    ent_coef_loss   | 2.49     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 162895   |\n","---------------------------------\n","Eval num_timesteps=3912000, episode_reward=0.60 +/- 1.65\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.597    |\n","| time/              |          |\n","|    total_timesteps | 3912000  |\n","| train/             |          |\n","|    actor_loss      | 4.14     |\n","|    critic_loss     | 1.46     |\n","|    ent_coef        | 0.00359  |\n","|    ent_coef_loss   | -0.448   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 162995   |\n","---------------------------------\n","Eval num_timesteps=3914400, episode_reward=-36.47 +/- 45.66\n","Episode length: 376.40 +/- 151.38\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 376      |\n","|    mean_reward     | -36.5    |\n","| time/              |          |\n","|    total_timesteps | 3914400  |\n","| train/             |          |\n","|    actor_loss      | 3.48     |\n","|    critic_loss     | 0.333    |\n","|    ent_coef        | 0.00353  |\n","|    ent_coef_loss   | -1.98    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 163095   |\n","---------------------------------\n","Eval num_timesteps=3916800, episode_reward=2.14 +/- 0.68\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.14     |\n","| time/              |          |\n","|    total_timesteps | 3916800  |\n","| train/             |          |\n","|    actor_loss      | 4.1      |\n","|    critic_loss     | 0.542    |\n","|    ent_coef        | 0.0034   |\n","|    ent_coef_loss   | -3.95    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 163195   |\n","---------------------------------\n","Eval num_timesteps=3919200, episode_reward=-92.41 +/- 2.98\n","Episode length: 226.80 +/- 135.70\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 227      |\n","|    mean_reward     | -92.4    |\n","| time/              |          |\n","|    total_timesteps | 3919200  |\n","| train/             |          |\n","|    actor_loss      | 3.99     |\n","|    critic_loss     | 19.4     |\n","|    ent_coef        | 0.0032   |\n","|    ent_coef_loss   | -1.87    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 163295   |\n","---------------------------------\n","Eval num_timesteps=3921600, episode_reward=7.06 +/- 2.59\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.06     |\n","| time/              |          |\n","|    total_timesteps | 3921600  |\n","| train/             |          |\n","|    actor_loss      | 3.84     |\n","|    critic_loss     | 0.853    |\n","|    ent_coef        | 0.00326  |\n","|    ent_coef_loss   | 4.91     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 163395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6120     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12861    |\n","|    total_timesteps | 3923832  |\n","| train/             |          |\n","|    actor_loss      | 4.17     |\n","|    critic_loss     | 29.6     |\n","|    ent_coef        | 0.00333  |\n","|    ent_coef_loss   | -0.111   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 163488   |\n","---------------------------------\n","Eval num_timesteps=3924000, episode_reward=7.36 +/- 1.83\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.36     |\n","| time/              |          |\n","|    total_timesteps | 3924000  |\n","| train/             |          |\n","|    actor_loss      | 3.63     |\n","|    critic_loss     | 2.15     |\n","|    ent_coef        | 0.00333  |\n","|    ent_coef_loss   | 0.388    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 163495   |\n","---------------------------------\n","Eval num_timesteps=3926400, episode_reward=4.97 +/- 0.55\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.97     |\n","| time/              |          |\n","|    total_timesteps | 3926400  |\n","| train/             |          |\n","|    actor_loss      | 3.72     |\n","|    critic_loss     | 11.5     |\n","|    ent_coef        | 0.00354  |\n","|    ent_coef_loss   | -2.33    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 163595   |\n","---------------------------------\n","Eval num_timesteps=3928800, episode_reward=-100.99 +/- 5.58\n","Episode length: 155.60 +/- 27.43\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 156      |\n","|    mean_reward     | -101     |\n","| time/              |          |\n","|    total_timesteps | 3928800  |\n","| train/             |          |\n","|    actor_loss      | 4.01     |\n","|    critic_loss     | 0.921    |\n","|    ent_coef        | 0.00351  |\n","|    ent_coef_loss   | 1.09     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 163695   |\n","---------------------------------\n","Eval num_timesteps=3931200, episode_reward=1.67 +/- 0.29\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.67     |\n","| time/              |          |\n","|    total_timesteps | 3931200  |\n","| train/             |          |\n","|    actor_loss      | 3.83     |\n","|    critic_loss     | 1.1      |\n","|    ent_coef        | 0.00346  |\n","|    ent_coef_loss   | 0.323    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 163795   |\n","---------------------------------\n","Eval num_timesteps=3933600, episode_reward=-59.71 +/- 54.53\n","Episode length: 411.20 +/- 72.50\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 411      |\n","|    mean_reward     | -59.7    |\n","| time/              |          |\n","|    total_timesteps | 3933600  |\n","| train/             |          |\n","|    actor_loss      | 3.88     |\n","|    critic_loss     | 0.703    |\n","|    ent_coef        | 0.00354  |\n","|    ent_coef_loss   | -1.99    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 163895   |\n","---------------------------------\n","Eval num_timesteps=3936000, episode_reward=5.34 +/- 0.88\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.34     |\n","| time/              |          |\n","|    total_timesteps | 3936000  |\n","| train/             |          |\n","|    actor_loss      | 4.14     |\n","|    critic_loss     | 21       |\n","|    ent_coef        | 0.00349  |\n","|    ent_coef_loss   | 1.78     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 163995   |\n","---------------------------------\n","Eval num_timesteps=3938400, episode_reward=6.23 +/- 3.55\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.23     |\n","| time/              |          |\n","|    total_timesteps | 3938400  |\n","| train/             |          |\n","|    actor_loss      | 4.19     |\n","|    critic_loss     | 18.7     |\n","|    ent_coef        | 0.00355  |\n","|    ent_coef_loss   | -0.64    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 164095   |\n","---------------------------------\n","Eval num_timesteps=3940800, episode_reward=-98.69 +/- 6.68\n","Episode length: 225.80 +/- 74.95\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 226      |\n","|    mean_reward     | -98.7    |\n","| time/              |          |\n","|    total_timesteps | 3940800  |\n","| train/             |          |\n","|    actor_loss      | 4        |\n","|    critic_loss     | 0.92     |\n","|    ent_coef        | 0.00349  |\n","|    ent_coef_loss   | -0.697   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 164195   |\n","---------------------------------\n","Eval num_timesteps=3943200, episode_reward=-41.84 +/- 49.70\n","Episode length: 333.20 +/- 204.29\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 333      |\n","|    mean_reward     | -41.8    |\n","| time/              |          |\n","|    total_timesteps | 3943200  |\n","| train/             |          |\n","|    actor_loss      | 3.7      |\n","|    critic_loss     | 0.503    |\n","|    ent_coef        | 0.00347  |\n","|    ent_coef_loss   | 0.484    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 164295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6130     |\n","|    fps             | 305      |\n","|    time_elapsed    | 12927    |\n","|    total_timesteps | 3945192  |\n","| train/             |          |\n","|    actor_loss      | 3.85     |\n","|    critic_loss     | 0.397    |\n","|    ent_coef        | 0.00337  |\n","|    ent_coef_loss   | 0.099    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 164378   |\n","---------------------------------\n","Eval num_timesteps=3945600, episode_reward=1.85 +/- 0.31\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.85     |\n","| time/              |          |\n","|    total_timesteps | 3945600  |\n","| train/             |          |\n","|    actor_loss      | 3.87     |\n","|    critic_loss     | 18       |\n","|    ent_coef        | 0.00336  |\n","|    ent_coef_loss   | 0.584    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 164395   |\n","---------------------------------\n","Eval num_timesteps=3948000, episode_reward=0.02 +/- 1.49\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.0217   |\n","| time/              |          |\n","|    total_timesteps | 3948000  |\n","| train/             |          |\n","|    actor_loss      | 4.3      |\n","|    critic_loss     | 17.8     |\n","|    ent_coef        | 0.00342  |\n","|    ent_coef_loss   | 4.18     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 164495   |\n","---------------------------------\n","Eval num_timesteps=3950400, episode_reward=1.75 +/- 0.41\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.75     |\n","| time/              |          |\n","|    total_timesteps | 3950400  |\n","| train/             |          |\n","|    actor_loss      | 4.2      |\n","|    critic_loss     | 1.16     |\n","|    ent_coef        | 0.00343  |\n","|    ent_coef_loss   | 0.85     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 164595   |\n","---------------------------------\n","Eval num_timesteps=3952800, episode_reward=4.35 +/- 0.14\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.35     |\n","| time/              |          |\n","|    total_timesteps | 3952800  |\n","| train/             |          |\n","|    actor_loss      | 4.28     |\n","|    critic_loss     | 4.29     |\n","|    ent_coef        | 0.00339  |\n","|    ent_coef_loss   | 1.8      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 164695   |\n","---------------------------------\n","Eval num_timesteps=3955200, episode_reward=4.42 +/- 0.46\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.42     |\n","| time/              |          |\n","|    total_timesteps | 3955200  |\n","| train/             |          |\n","|    actor_loss      | 3.59     |\n","|    critic_loss     | 10.4     |\n","|    ent_coef        | 0.00336  |\n","|    ent_coef_loss   | -0.748   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 164795   |\n","---------------------------------\n","Eval num_timesteps=3957600, episode_reward=8.79 +/- 3.36\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 8.79     |\n","| time/              |          |\n","|    total_timesteps | 3957600  |\n","| train/             |          |\n","|    actor_loss      | 4.67     |\n","|    critic_loss     | 2.27     |\n","|    ent_coef        | 0.00328  |\n","|    ent_coef_loss   | 1.22     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 164895   |\n","---------------------------------\n","Eval num_timesteps=3960000, episode_reward=-95.75 +/- 6.52\n","Episode length: 381.60 +/- 29.88\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 382      |\n","|    mean_reward     | -95.8    |\n","| time/              |          |\n","|    total_timesteps | 3960000  |\n","| train/             |          |\n","|    actor_loss      | 4.27     |\n","|    critic_loss     | 0.397    |\n","|    ent_coef        | 0.0032   |\n","|    ent_coef_loss   | -2.74    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 164995   |\n","---------------------------------\n","Eval num_timesteps=3962400, episode_reward=3.38 +/- 1.16\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.38     |\n","| time/              |          |\n","|    total_timesteps | 3962400  |\n","| train/             |          |\n","|    actor_loss      | 3.98     |\n","|    critic_loss     | 0.632    |\n","|    ent_coef        | 0.00327  |\n","|    ent_coef_loss   | 0.342    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 165095   |\n","---------------------------------\n","Eval num_timesteps=3964800, episode_reward=5.43 +/- 6.32\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.43     |\n","| time/              |          |\n","|    total_timesteps | 3964800  |\n","| train/             |          |\n","|    actor_loss      | 3.52     |\n","|    critic_loss     | 0.721    |\n","|    ent_coef        | 0.00333  |\n","|    ent_coef_loss   | -0.384   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 165195   |\n","---------------------------------\n","Eval num_timesteps=3967200, episode_reward=-38.53 +/- 48.38\n","Episode length: 333.20 +/- 204.29\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 333      |\n","|    mean_reward     | -38.5    |\n","| time/              |          |\n","|    total_timesteps | 3967200  |\n","| train/             |          |\n","|    actor_loss      | 3.84     |\n","|    critic_loss     | 0.989    |\n","|    ent_coef        | 0.00332  |\n","|    ent_coef_loss   | -0.882   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 165295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6140     |\n","|    fps             | 305      |\n","|    time_elapsed    | 13009    |\n","|    total_timesteps | 3969192  |\n","| train/             |          |\n","|    actor_loss      | 5        |\n","|    critic_loss     | 18.9     |\n","|    ent_coef        | 0.00325  |\n","|    ent_coef_loss   | 1.32     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 165378   |\n","---------------------------------\n","Eval num_timesteps=3969600, episode_reward=1.76 +/- 0.06\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.76     |\n","| time/              |          |\n","|    total_timesteps | 3969600  |\n","| train/             |          |\n","|    actor_loss      | 3.6      |\n","|    critic_loss     | 0.446    |\n","|    ent_coef        | 0.00326  |\n","|    ent_coef_loss   | 1.08     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 165395   |\n","---------------------------------\n","Eval num_timesteps=3972000, episode_reward=3.82 +/- 4.36\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.82     |\n","| time/              |          |\n","|    total_timesteps | 3972000  |\n","| train/             |          |\n","|    actor_loss      | 4.04     |\n","|    critic_loss     | 1.04     |\n","|    ent_coef        | 0.00318  |\n","|    ent_coef_loss   | -0.966   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 165495   |\n","---------------------------------\n","Eval num_timesteps=3974400, episode_reward=0.75 +/- 0.53\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.747    |\n","| time/              |          |\n","|    total_timesteps | 3974400  |\n","| train/             |          |\n","|    actor_loss      | 3.88     |\n","|    critic_loss     | 10.7     |\n","|    ent_coef        | 0.00325  |\n","|    ent_coef_loss   | -1.95    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 165595   |\n","---------------------------------\n","Eval num_timesteps=3976800, episode_reward=2.54 +/- 0.51\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.54     |\n","| time/              |          |\n","|    total_timesteps | 3976800  |\n","| train/             |          |\n","|    actor_loss      | 4.03     |\n","|    critic_loss     | 0.789    |\n","|    ent_coef        | 0.00331  |\n","|    ent_coef_loss   | 0.0495   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 165695   |\n","---------------------------------\n","Eval num_timesteps=3979200, episode_reward=0.29 +/- 1.27\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.292    |\n","| time/              |          |\n","|    total_timesteps | 3979200  |\n","| train/             |          |\n","|    actor_loss      | 4.08     |\n","|    critic_loss     | 1.18     |\n","|    ent_coef        | 0.00327  |\n","|    ent_coef_loss   | 2.63     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 165795   |\n","---------------------------------\n","Eval num_timesteps=3981600, episode_reward=-31.90 +/- 49.64\n","Episode length: 374.40 +/- 153.83\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 374      |\n","|    mean_reward     | -31.9    |\n","| time/              |          |\n","|    total_timesteps | 3981600  |\n","| train/             |          |\n","|    actor_loss      | 4.1      |\n","|    critic_loss     | 1.44     |\n","|    ent_coef        | 0.00338  |\n","|    ent_coef_loss   | 1.79     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 165895   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6150     |\n","|    fps             | 305      |\n","|    time_elapsed    | 13058    |\n","|    total_timesteps | 3983928  |\n","| train/             |          |\n","|    actor_loss      | 4.45     |\n","|    critic_loss     | 3.36     |\n","|    ent_coef        | 0.00363  |\n","|    ent_coef_loss   | 2.27     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 165992   |\n","---------------------------------\n","Eval num_timesteps=3984000, episode_reward=2.85 +/- 0.74\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.85     |\n","| time/              |          |\n","|    total_timesteps | 3984000  |\n","| train/             |          |\n","|    actor_loss      | 4.03     |\n","|    critic_loss     | 2.16     |\n","|    ent_coef        | 0.00364  |\n","|    ent_coef_loss   | 3.05     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 165995   |\n","---------------------------------\n","Eval num_timesteps=3986400, episode_reward=-0.65 +/- 0.66\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.649   |\n","| time/              |          |\n","|    total_timesteps | 3986400  |\n","| train/             |          |\n","|    actor_loss      | 3.48     |\n","|    critic_loss     | 0.635    |\n","|    ent_coef        | 0.00384  |\n","|    ent_coef_loss   | -0.0998  |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 166095   |\n","---------------------------------\n","Eval num_timesteps=3988800, episode_reward=-93.34 +/- 3.43\n","Episode length: 312.00 +/- 56.34\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 312      |\n","|    mean_reward     | -93.3    |\n","| time/              |          |\n","|    total_timesteps | 3988800  |\n","| train/             |          |\n","|    actor_loss      | 4.12     |\n","|    critic_loss     | 1.66     |\n","|    ent_coef        | 0.00391  |\n","|    ent_coef_loss   | 0.117    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 166195   |\n","---------------------------------\n","Eval num_timesteps=3991200, episode_reward=9.82 +/- 7.27\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 9.82     |\n","| time/              |          |\n","|    total_timesteps | 3991200  |\n","| train/             |          |\n","|    actor_loss      | 3.81     |\n","|    critic_loss     | 1.99     |\n","|    ent_coef        | 0.00389  |\n","|    ent_coef_loss   | -1.32    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 166295   |\n","---------------------------------\n","Eval num_timesteps=3993600, episode_reward=-55.92 +/- 48.75\n","Episode length: 262.40 +/- 194.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 262      |\n","|    mean_reward     | -55.9    |\n","| time/              |          |\n","|    total_timesteps | 3993600  |\n","| train/             |          |\n","|    actor_loss      | 4.01     |\n","|    critic_loss     | 0.445    |\n","|    ent_coef        | 0.00385  |\n","|    ent_coef_loss   | -0.588   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 166395   |\n","---------------------------------\n","Eval num_timesteps=3996000, episode_reward=2.76 +/- 1.73\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.76     |\n","| time/              |          |\n","|    total_timesteps | 3996000  |\n","| train/             |          |\n","|    actor_loss      | 4.64     |\n","|    critic_loss     | 24.8     |\n","|    ent_coef        | 0.00396  |\n","|    ent_coef_loss   | 2.94     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 166495   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6160     |\n","|    fps             | 304      |\n","|    time_elapsed    | 13107    |\n","|    total_timesteps | 3997800  |\n","| train/             |          |\n","|    actor_loss      | 4.26     |\n","|    critic_loss     | 1.23     |\n","|    ent_coef        | 0.00398  |\n","|    ent_coef_loss   | 0.227    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 166570   |\n","---------------------------------\n","Eval num_timesteps=3998400, episode_reward=-39.92 +/- 46.47\n","Episode length: 348.80 +/- 185.18\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 349      |\n","|    mean_reward     | -39.9    |\n","| time/              |          |\n","|    total_timesteps | 3998400  |\n","| train/             |          |\n","|    actor_loss      | 4.11     |\n","|    critic_loss     | 2.26     |\n","|    ent_coef        | 0.00396  |\n","|    ent_coef_loss   | 0.065    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 166595   |\n","---------------------------------\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"OooX9WwN9AGO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3 import SAC\n","from pettingzoo.sisl import multiwalker_v9\n","import supersuit as ss\n","from stable_baselines3.common.monitor import Monitor"],"metadata":{"id":"5ypcPewx9AxR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697887381542,"user_tz":-120,"elapsed":59,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"bcbc999c-9da8-47cf-d68d-f554b75a1956","id":"mqyCzx7d9AxS"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["'=2.13'\t\t\t\t     multiwalker_sac2_log_eval\n"," DQN_new_pettingzoo_gym_cap.ipynb    multiwalker_sac_log_eval\n"," DQN_policies\t\t\t     multiwalker_td3_log_eval\n","'Entrenamientos antiguos sin logs'   policy_log_eval\n"," Entrenamientos_log_no_eval\t     PPO_policies\n"," MCR_TFM.ipynb\t\t\t     results_rllib\n"," multi_car_racing\t\t     TFM_Multiwalker_DDPG_PPO_gym_cap.ipynb\n"," multiwalker_ddpg_log_eval\t     TFM_Multiwalker_SAC_gym_cap.ipynb\n"," multiwalker_ddpg.zip\t\t     TFM_Multiwalker_TD3_gym_cap.ipynb\n"," multiwalker_ppo_log_eval\t     TFM_PPO_new_pettingzoo_gym_cap.ipynb\n"," multiwalker_ppo.zip\t\t     TFM_PPO_pettingzoo_gym_cap.ipynb\n"]}]},{"cell_type":"code","source":["env = multiwalker_v9.parallel_env(render_mode=\"rgb_array\",shared_reward=0)\n","\n","env = ss.frame_stack_v1(env, 3)\n","env = ss.pettingzoo_env_to_vec_env_v1(env)\n","env = ss.concat_vec_envs_v1(env, 8, num_cpus=4, base_class='stable_baselines3')"],"metadata":{"id":"oBQ_pARy9AxT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3.common.callbacks import EvalCallback\n","eval_callback = EvalCallback(env, best_model_save_path=\"./multiwalker_sac3_00_log_eval/\",\n","                             log_path=\"./multiwalker_sac3_00_log_eval/\", eval_freq=200,\n","                             deterministic=False, render=False)\n"],"metadata":{"id":"iN3G9S1T9AxU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3.common.logger import configure\n","\n","tmp_path = \"multiwalker_sac3_00_log_eval/\"\n","# set up logger\n","new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n","\n","model = SAC(\"MlpPolicy\", env, verbose=3,batch_size=512, learning_rate=0.001,gamma=0.9,tau=0.01) #,train_freq=1\n","\n","model.set_logger(new_logger)\n","model.learn(total_timesteps=4000000, log_interval=10,callback=eval_callback)\n","model.save(\"multiwalker_sac3_00\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"944d8e45-83d1-4496-d0f9-db0cd5e4123d","id":"eyrgIrJl9AxV","executionInfo":{"status":"ok","timestamp":1699034991715,"user_tz":-60,"elapsed":5901679,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Logging to multiwalker_sac3_00_log_eval/\n","Using cuda device\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 10       |\n","|    fps             | 255      |\n","|    time_elapsed    | 5        |\n","|    total_timesteps | 1392     |\n","| train/             |          |\n","|    actor_loss      | -5.12    |\n","|    critic_loss     | 65.6     |\n","|    ent_coef        | 0.949    |\n","|    ent_coef_loss   | -0.349   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 53       |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 20       |\n","|    fps             | 308      |\n","|    time_elapsed    | 5        |\n","|    total_timesteps | 1800     |\n","| train/             |          |\n","|    actor_loss      | -4.71    |\n","|    critic_loss     | 233      |\n","|    ent_coef        | 0.933    |\n","|    ent_coef_loss   | -0.464   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 70       |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 30       |\n","|    fps             | 397      |\n","|    time_elapsed    | 6        |\n","|    total_timesteps | 2616     |\n","| train/             |          |\n","|    actor_loss      | -4.78    |\n","|    critic_loss     | 123      |\n","|    ent_coef        | 0.902    |\n","|    ent_coef_loss   | -0.689   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 104      |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40       |\n","|    fps             | 445      |\n","|    time_elapsed    | 7        |\n","|    total_timesteps | 3144     |\n","| train/             |          |\n","|    actor_loss      | -5       |\n","|    critic_loss     | 124      |\n","|    ent_coef        | 0.883    |\n","|    ent_coef_loss   | -0.833   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 126      |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 50       |\n","|    fps             | 483      |\n","|    time_elapsed    | 7        |\n","|    total_timesteps | 3624     |\n","| train/             |          |\n","|    actor_loss      | -4.22    |\n","|    critic_loss     | 244      |\n","|    ent_coef        | 0.865    |\n","|    ent_coef_loss   | -0.969   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 146      |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 60       |\n","|    fps             | 536      |\n","|    time_elapsed    | 8        |\n","|    total_timesteps | 4416     |\n","| train/             |          |\n","|    actor_loss      | -5.43    |\n","|    critic_loss     | 97.3     |\n","|    ent_coef        | 0.837    |\n","|    ent_coef_loss   | -1.18    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 179      |\n","---------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42320    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5452     |\n","|    total_timesteps | 3713904  |\n","| train/             |          |\n","|    actor_loss      | 13.9     |\n","|    critic_loss     | 22.1     |\n","|    ent_coef        | 0.0121   |\n","|    ent_coef_loss   | 4.12     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154741   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42330    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5453     |\n","|    total_timesteps | 3714552  |\n","| train/             |          |\n","|    actor_loss      | 13.6     |\n","|    critic_loss     | 19.9     |\n","|    ent_coef        | 0.0124   |\n","|    ent_coef_loss   | 0.478    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154768   |\n","---------------------------------\n","Eval num_timesteps=3715200, episode_reward=-115.63 +/- 2.80\n","Episode length: 82.00 +/- 2.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 82       |\n","|    mean_reward     | -116     |\n","| time/              |          |\n","|    total_timesteps | 3715200  |\n","| train/             |          |\n","|    actor_loss      | 16.3     |\n","|    critic_loss     | 17.6     |\n","|    ent_coef        | 0.0124   |\n","|    ent_coef_loss   | 0.0979   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42340    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5458     |\n","|    total_timesteps | 3716736  |\n","| train/             |          |\n","|    actor_loss      | 14.2     |\n","|    critic_loss     | 21.2     |\n","|    ent_coef        | 0.012    |\n","|    ent_coef_loss   | -2.49    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154859   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42350    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5459     |\n","|    total_timesteps | 3717336  |\n","| train/             |          |\n","|    actor_loss      | 13.3     |\n","|    critic_loss     | 13.2     |\n","|    ent_coef        | 0.0116   |\n","|    ent_coef_loss   | -2.36    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154884   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42360    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5461     |\n","|    total_timesteps | 3719016  |\n","| train/             |          |\n","|    actor_loss      | 13.1     |\n","|    critic_loss     | 50.7     |\n","|    ent_coef        | 0.0116   |\n","|    ent_coef_loss   | -0.792   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154954   |\n","---------------------------------\n","Eval num_timesteps=3720000, episode_reward=-112.28 +/- 11.71\n","Episode length: 100.40 +/- 16.66\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 100      |\n","|    mean_reward     | -112     |\n","| time/              |          |\n","|    total_timesteps | 3720000  |\n","| train/             |          |\n","|    actor_loss      | 13.3     |\n","|    critic_loss     | 8.67     |\n","|    ent_coef        | 0.0117   |\n","|    ent_coef_loss   | 0.504    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42370    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5464     |\n","|    total_timesteps | 3720168  |\n","| train/             |          |\n","|    actor_loss      | 13       |\n","|    critic_loss     | 4.95     |\n","|    ent_coef        | 0.0118   |\n","|    ent_coef_loss   | 0.0477   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155002   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42380    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5466     |\n","|    total_timesteps | 3721512  |\n","| train/             |          |\n","|    actor_loss      | 15       |\n","|    critic_loss     | 44.3     |\n","|    ent_coef        | 0.0114   |\n","|    ent_coef_loss   | -0.0917  |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155058   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42390    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5466     |\n","|    total_timesteps | 3721992  |\n","| train/             |          |\n","|    actor_loss      | 13.7     |\n","|    critic_loss     | 5.17     |\n","|    ent_coef        | 0.0113   |\n","|    ent_coef_loss   | -0.246   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155078   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42400    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5468     |\n","|    total_timesteps | 3724008  |\n","| train/             |          |\n","|    actor_loss      | 13.8     |\n","|    critic_loss     | 12.8     |\n","|    ent_coef        | 0.0111   |\n","|    ent_coef_loss   | -2.58    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155162   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42410    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5468     |\n","|    total_timesteps | 3724368  |\n","| train/             |          |\n","|    actor_loss      | 13.4     |\n","|    critic_loss     | 20.6     |\n","|    ent_coef        | 0.0109   |\n","|    ent_coef_loss   | -2.69    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155177   |\n","---------------------------------\n","Eval num_timesteps=3724800, episode_reward=-102.24 +/- 5.58\n","Episode length: 159.60 +/- 59.28\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 160      |\n","|    mean_reward     | -102     |\n","| time/              |          |\n","|    total_timesteps | 3724800  |\n","| train/             |          |\n","|    actor_loss      | 14.1     |\n","|    critic_loss     | 9.95     |\n","|    ent_coef        | 0.0107   |\n","|    ent_coef_loss   | -2.47    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42420    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5471     |\n","|    total_timesteps | 3725328  |\n","| train/             |          |\n","|    actor_loss      | 13.4     |\n","|    critic_loss     | 21       |\n","|    ent_coef        | 0.0104   |\n","|    ent_coef_loss   | -2.16    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155217   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42430    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5472     |\n","|    total_timesteps | 3726552  |\n","| train/             |          |\n","|    actor_loss      | 14.8     |\n","|    critic_loss     | 21.6     |\n","|    ent_coef        | 0.01     |\n","|    ent_coef_loss   | -1.74    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155268   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42440    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5473     |\n","|    total_timesteps | 3727200  |\n","| train/             |          |\n","|    actor_loss      | 14.7     |\n","|    critic_loss     | 27.1     |\n","|    ent_coef        | 0.00999  |\n","|    ent_coef_loss   | 1.56     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42450    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5475     |\n","|    total_timesteps | 3728592  |\n","| train/             |          |\n","|    actor_loss      | 13.6     |\n","|    critic_loss     | 40.2     |\n","|    ent_coef        | 0.0102   |\n","|    ent_coef_loss   | 0.252    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155353   |\n","---------------------------------\n","Eval num_timesteps=3729600, episode_reward=-111.49 +/- 11.01\n","Episode length: 87.60 +/- 10.29\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 87.6     |\n","|    mean_reward     | -111     |\n","| time/              |          |\n","|    total_timesteps | 3729600  |\n","| train/             |          |\n","|    actor_loss      | 14.9     |\n","|    critic_loss     | 45.4     |\n","|    ent_coef        | 0.01     |\n","|    ent_coef_loss   | 3.7      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42460    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5478     |\n","|    total_timesteps | 3729648  |\n","| train/             |          |\n","|    actor_loss      | 15.9     |\n","|    critic_loss     | 30.2     |\n","|    ent_coef        | 0.01     |\n","|    ent_coef_loss   | 1.15     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155397   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42470    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5480     |\n","|    total_timesteps | 3731232  |\n","| train/             |          |\n","|    actor_loss      | 13.9     |\n","|    critic_loss     | 26.1     |\n","|    ent_coef        | 0.0101   |\n","|    ent_coef_loss   | 1.34     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155463   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42480    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5481     |\n","|    total_timesteps | 3731784  |\n","| train/             |          |\n","|    actor_loss      | 14.3     |\n","|    critic_loss     | 24.3     |\n","|    ent_coef        | 0.0102   |\n","|    ent_coef_loss   | -1.66    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155486   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42490    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5482     |\n","|    total_timesteps | 3732840  |\n","| train/             |          |\n","|    actor_loss      | 12.6     |\n","|    critic_loss     | 23.4     |\n","|    ent_coef        | 0.0101   |\n","|    ent_coef_loss   | -0.515   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155530   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42500    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5484     |\n","|    total_timesteps | 3733656  |\n","| train/             |          |\n","|    actor_loss      | 13.4     |\n","|    critic_loss     | 56.5     |\n","|    ent_coef        | 0.0102   |\n","|    ent_coef_loss   | 0.055    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155564   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42510    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5484     |\n","|    total_timesteps | 3734376  |\n","| train/             |          |\n","|    actor_loss      | 12.7     |\n","|    critic_loss     | 50.9     |\n","|    ent_coef        | 0.0103   |\n","|    ent_coef_loss   | 0.293    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155594   |\n","---------------------------------\n","Eval num_timesteps=3734400, episode_reward=-106.23 +/- 8.86\n","Episode length: 78.80 +/- 3.43\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 78.8     |\n","|    mean_reward     | -106     |\n","| time/              |          |\n","|    total_timesteps | 3734400  |\n","| train/             |          |\n","|    actor_loss      | 14.6     |\n","|    critic_loss     | 24       |\n","|    ent_coef        | 0.0103   |\n","|    ent_coef_loss   | 0.0724   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42520    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5486     |\n","|    total_timesteps | 3735672  |\n","| train/             |          |\n","|    actor_loss      | 13.4     |\n","|    critic_loss     | 18.8     |\n","|    ent_coef        | 0.0105   |\n","|    ent_coef_loss   | 1.34     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155648   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42530    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5487     |\n","|    total_timesteps | 3736392  |\n","| train/             |          |\n","|    actor_loss      | 13.4     |\n","|    critic_loss     | 23.1     |\n","|    ent_coef        | 0.0105   |\n","|    ent_coef_loss   | 0.122    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155678   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42540    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5488     |\n","|    total_timesteps | 3737064  |\n","| train/             |          |\n","|    actor_loss      | 15.9     |\n","|    critic_loss     | 41.6     |\n","|    ent_coef        | 0.0107   |\n","|    ent_coef_loss   | 3.64     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155706   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42550    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5489     |\n","|    total_timesteps | 3738360  |\n","| train/             |          |\n","|    actor_loss      | 15.4     |\n","|    critic_loss     | 40.2     |\n","|    ent_coef        | 0.0106   |\n","|    ent_coef_loss   | -0.481   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155760   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42560    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5489     |\n","|    total_timesteps | 3738576  |\n","| train/             |          |\n","|    actor_loss      | 13.1     |\n","|    critic_loss     | 15.3     |\n","|    ent_coef        | 0.0105   |\n","|    ent_coef_loss   | -1.74    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155769   |\n","---------------------------------\n","Eval num_timesteps=3739200, episode_reward=-113.66 +/- 11.35\n","Episode length: 77.40 +/- 0.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 77.4     |\n","|    mean_reward     | -114     |\n","| time/              |          |\n","|    total_timesteps | 3739200  |\n","| train/             |          |\n","|    actor_loss      | 13.6     |\n","|    critic_loss     | 30.4     |\n","|    ent_coef        | 0.0103   |\n","|    ent_coef_loss   | -1.76    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42570    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5490     |\n","|    total_timesteps | 3739368  |\n","| train/             |          |\n","|    actor_loss      | 13.3     |\n","|    critic_loss     | 4.59     |\n","|    ent_coef        | 0.0102   |\n","|    ent_coef_loss   | -0.618   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155802   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42580    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5492     |\n","|    total_timesteps | 3740976  |\n","| train/             |          |\n","|    actor_loss      | 13.2     |\n","|    critic_loss     | 7.5      |\n","|    ent_coef        | 0.0101   |\n","|    ent_coef_loss   | 0.114    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155869   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42590    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5492     |\n","|    total_timesteps | 3741312  |\n","| train/             |          |\n","|    actor_loss      | 14.6     |\n","|    critic_loss     | 14.7     |\n","|    ent_coef        | 0.0101   |\n","|    ent_coef_loss   | 2.15     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155883   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42600    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5493     |\n","|    total_timesteps | 3742728  |\n","| train/             |          |\n","|    actor_loss      | 13       |\n","|    critic_loss     | 6.22     |\n","|    ent_coef        | 0.0105   |\n","|    ent_coef_loss   | 0.0923   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155942   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42610    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5494     |\n","|    total_timesteps | 3743376  |\n","| train/             |          |\n","|    actor_loss      | 13.6     |\n","|    critic_loss     | 21.5     |\n","|    ent_coef        | 0.0105   |\n","|    ent_coef_loss   | 0.367    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155969   |\n","---------------------------------\n","Eval num_timesteps=3744000, episode_reward=-105.86 +/- 7.53\n","Episode length: 84.40 +/- 5.39\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 84.4     |\n","|    mean_reward     | -106     |\n","| time/              |          |\n","|    total_timesteps | 3744000  |\n","| train/             |          |\n","|    actor_loss      | 13.7     |\n","|    critic_loss     | 26.6     |\n","|    ent_coef        | 0.0105   |\n","|    ent_coef_loss   | -1.69    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42620    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5497     |\n","|    total_timesteps | 3744576  |\n","| train/             |          |\n","|    actor_loss      | 13.5     |\n","|    critic_loss     | 30.2     |\n","|    ent_coef        | 0.0105   |\n","|    ent_coef_loss   | 2.92     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156019   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42630    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5498     |\n","|    total_timesteps | 3745272  |\n","| train/             |          |\n","|    actor_loss      | 12.9     |\n","|    critic_loss     | 21       |\n","|    ent_coef        | 0.0109   |\n","|    ent_coef_loss   | -1.81    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156048   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42640    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5500     |\n","|    total_timesteps | 3746376  |\n","| train/             |          |\n","|    actor_loss      | 14.7     |\n","|    critic_loss     | 43.7     |\n","|    ent_coef        | 0.0109   |\n","|    ent_coef_loss   | 0.529    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156094   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42650    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5502     |\n","|    total_timesteps | 3747336  |\n","| train/             |          |\n","|    actor_loss      | 14.9     |\n","|    critic_loss     | 36.4     |\n","|    ent_coef        | 0.0111   |\n","|    ent_coef_loss   | 0.588    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156134   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42660    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5503     |\n","|    total_timesteps | 3747960  |\n","| train/             |          |\n","|    actor_loss      | 14.5     |\n","|    critic_loss     | 12.5     |\n","|    ent_coef        | 0.0112   |\n","|    ent_coef_loss   | 0.889    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156160   |\n","---------------------------------\n","Eval num_timesteps=3748800, episode_reward=-110.86 +/- 8.36\n","Episode length: 57.20 +/- 8.82\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 57.2     |\n","|    mean_reward     | -111     |\n","| time/              |          |\n","|    total_timesteps | 3748800  |\n","| train/             |          |\n","|    actor_loss      | 14.6     |\n","|    critic_loss     | 10.3     |\n","|    ent_coef        | 0.0111   |\n","|    ent_coef_loss   | 0.415    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42670    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5505     |\n","|    total_timesteps | 3748944  |\n","| train/             |          |\n","|    actor_loss      | 13       |\n","|    critic_loss     | 25.7     |\n","|    ent_coef        | 0.0111   |\n","|    ent_coef_loss   | 0.203    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156201   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42680    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5506     |\n","|    total_timesteps | 3749688  |\n","| train/             |          |\n","|    actor_loss      | 13.1     |\n","|    critic_loss     | 18.9     |\n","|    ent_coef        | 0.0109   |\n","|    ent_coef_loss   | -1.68    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156232   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42690    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5507     |\n","|    total_timesteps | 3750600  |\n","| train/             |          |\n","|    actor_loss      | 13.8     |\n","|    critic_loss     | 12.5     |\n","|    ent_coef        | 0.0106   |\n","|    ent_coef_loss   | -1.31    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156270   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42700    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5507     |\n","|    total_timesteps | 3751104  |\n","| train/             |          |\n","|    actor_loss      | 13.3     |\n","|    critic_loss     | 38       |\n","|    ent_coef        | 0.0104   |\n","|    ent_coef_loss   | -2.76    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156291   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42710    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5508     |\n","|    total_timesteps | 3752352  |\n","| train/             |          |\n","|    actor_loss      | 13.2     |\n","|    critic_loss     | 22.1     |\n","|    ent_coef        | 0.00989  |\n","|    ent_coef_loss   | -1.19    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156343   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42720    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5509     |\n","|    total_timesteps | 3752952  |\n","| train/             |          |\n","|    actor_loss      | 14.1     |\n","|    critic_loss     | 21       |\n","|    ent_coef        | 0.00981  |\n","|    ent_coef_loss   | 1.57     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156368   |\n","---------------------------------\n","Eval num_timesteps=3753600, episode_reward=-112.30 +/- 11.74\n","Episode length: 67.00 +/- 2.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 67       |\n","|    mean_reward     | -112     |\n","| time/              |          |\n","|    total_timesteps | 3753600  |\n","| train/             |          |\n","|    actor_loss      | 13.4     |\n","|    critic_loss     | 26.4     |\n","|    ent_coef        | 0.0097   |\n","|    ent_coef_loss   | -1.02    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42730    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5510     |\n","|    total_timesteps | 3753720  |\n","| train/             |          |\n","|    actor_loss      | 13.8     |\n","|    critic_loss     | 41.8     |\n","|    ent_coef        | 0.00967  |\n","|    ent_coef_loss   | 2.38     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156400   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42740    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5510     |\n","|    total_timesteps | 3754032  |\n","| train/             |          |\n","|    actor_loss      | 14.2     |\n","|    critic_loss     | 7.4      |\n","|    ent_coef        | 0.00968  |\n","|    ent_coef_loss   | 2.05     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156413   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42750    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5511     |\n","|    total_timesteps | 3754752  |\n","| train/             |          |\n","|    actor_loss      | 13.5     |\n","|    critic_loss     | 42.9     |\n","|    ent_coef        | 0.00984  |\n","|    ent_coef_loss   | -0.0102  |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156443   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42760    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5512     |\n","|    total_timesteps | 3755784  |\n","| train/             |          |\n","|    actor_loss      | 14.7     |\n","|    critic_loss     | 30.5     |\n","|    ent_coef        | 0.0101   |\n","|    ent_coef_loss   | 0.899    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156486   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42770    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5513     |\n","|    total_timesteps | 3757320  |\n","| train/             |          |\n","|    actor_loss      | 14.7     |\n","|    critic_loss     | 30.2     |\n","|    ent_coef        | 0.0107   |\n","|    ent_coef_loss   | 0.304    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156550   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42780    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5514     |\n","|    total_timesteps | 3757872  |\n","| train/             |          |\n","|    actor_loss      | 14.2     |\n","|    critic_loss     | 9.51     |\n","|    ent_coef        | 0.0105   |\n","|    ent_coef_loss   | -3.36    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156573   |\n","---------------------------------\n","Eval num_timesteps=3758400, episode_reward=-114.42 +/- 11.40\n","Episode length: 101.20 +/- 1.47\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 101      |\n","|    mean_reward     | -114     |\n","| time/              |          |\n","|    total_timesteps | 3758400  |\n","| train/             |          |\n","|    actor_loss      | 15.5     |\n","|    critic_loss     | 15.1     |\n","|    ent_coef        | 0.0103   |\n","|    ent_coef_loss   | 1.82     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42790    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5516     |\n","|    total_timesteps | 3758448  |\n","| train/             |          |\n","|    actor_loss      | 12.4     |\n","|    critic_loss     | 35.7     |\n","|    ent_coef        | 0.0102   |\n","|    ent_coef_loss   | -1.15    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156597   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42800    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5519     |\n","|    total_timesteps | 3760584  |\n","| train/             |          |\n","|    actor_loss      | 14.4     |\n","|    critic_loss     | 13.5     |\n","|    ent_coef        | 0.0103   |\n","|    ent_coef_loss   | 0.0884   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156686   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42810    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5521     |\n","|    total_timesteps | 3762096  |\n","| train/             |          |\n","|    actor_loss      | 13.2     |\n","|    critic_loss     | 42.7     |\n","|    ent_coef        | 0.0105   |\n","|    ent_coef_loss   | 0.128    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156749   |\n","---------------------------------\n","Eval num_timesteps=3763200, episode_reward=-114.53 +/- 11.24\n","Episode length: 113.00 +/- 9.80\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 113      |\n","|    mean_reward     | -115     |\n","| time/              |          |\n","|    total_timesteps | 3763200  |\n","| train/             |          |\n","|    actor_loss      | 14       |\n","|    critic_loss     | 7.51     |\n","|    ent_coef        | 0.0104   |\n","|    ent_coef_loss   | 2.67     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42820    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5528     |\n","|    total_timesteps | 3764616  |\n","| train/             |          |\n","|    actor_loss      | 14.1     |\n","|    critic_loss     | 40.3     |\n","|    ent_coef        | 0.0106   |\n","|    ent_coef_loss   | 3.67     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156854   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42830    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5530     |\n","|    total_timesteps | 3765984  |\n","| train/             |          |\n","|    actor_loss      | 14.7     |\n","|    critic_loss     | 21       |\n","|    ent_coef        | 0.011    |\n","|    ent_coef_loss   | 3.2      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156911   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42840    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5532     |\n","|    total_timesteps | 3767664  |\n","| train/             |          |\n","|    actor_loss      | 14.2     |\n","|    critic_loss     | 24.9     |\n","|    ent_coef        | 0.0113   |\n","|    ent_coef_loss   | 1.45     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156981   |\n","---------------------------------\n","Eval num_timesteps=3768000, episode_reward=-112.30 +/- 11.67\n","Episode length: 71.20 +/- 11.27\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 71.2     |\n","|    mean_reward     | -112     |\n","| time/              |          |\n","|    total_timesteps | 3768000  |\n","| train/             |          |\n","|    actor_loss      | 16.3     |\n","|    critic_loss     | 12.9     |\n","|    ent_coef        | 0.0112   |\n","|    ent_coef_loss   | 0.266    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42850    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5535     |\n","|    total_timesteps | 3768672  |\n","| train/             |          |\n","|    actor_loss      | 13.2     |\n","|    critic_loss     | 21.3     |\n","|    ent_coef        | 0.0113   |\n","|    ent_coef_loss   | -3.25    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157023   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42860    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5535     |\n","|    total_timesteps | 3768864  |\n","| train/             |          |\n","|    actor_loss      | 14.4     |\n","|    critic_loss     | 46.9     |\n","|    ent_coef        | 0.0112   |\n","|    ent_coef_loss   | -0.413   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157031   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42870    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5537     |\n","|    total_timesteps | 3770280  |\n","| train/             |          |\n","|    actor_loss      | 13.5     |\n","|    critic_loss     | 12.3     |\n","|    ent_coef        | 0.0112   |\n","|    ent_coef_loss   | 2.28     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157090   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42880    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5537     |\n","|    total_timesteps | 3770664  |\n","| train/             |          |\n","|    actor_loss      | 13.1     |\n","|    critic_loss     | 11.8     |\n","|    ent_coef        | 0.0113   |\n","|    ent_coef_loss   | -1.09    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157106   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42890    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5538     |\n","|    total_timesteps | 3771312  |\n","| train/             |          |\n","|    actor_loss      | 14.7     |\n","|    critic_loss     | 5.33     |\n","|    ent_coef        | 0.0113   |\n","|    ent_coef_loss   | 0.0498   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157133   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42900    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5539     |\n","|    total_timesteps | 3772608  |\n","| train/             |          |\n","|    actor_loss      | 14       |\n","|    critic_loss     | 12.4     |\n","|    ent_coef        | 0.0114   |\n","|    ent_coef_loss   | -1.3     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157187   |\n","---------------------------------\n","Eval num_timesteps=3772800, episode_reward=-109.80 +/- 9.25\n","Episode length: 74.60 +/- 1.96\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 74.6     |\n","|    mean_reward     | -110     |\n","| time/              |          |\n","|    total_timesteps | 3772800  |\n","| train/             |          |\n","|    actor_loss      | 13.4     |\n","|    critic_loss     | 5.86     |\n","|    ent_coef        | 0.0114   |\n","|    ent_coef_loss   | -0.903   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42910    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5541     |\n","|    total_timesteps | 3772896  |\n","| train/             |          |\n","|    actor_loss      | 12.8     |\n","|    critic_loss     | 16.8     |\n","|    ent_coef        | 0.0114   |\n","|    ent_coef_loss   | -1.55    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157199   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42920    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5542     |\n","|    total_timesteps | 3773808  |\n","| train/             |          |\n","|    actor_loss      | 12.3     |\n","|    critic_loss     | 16.3     |\n","|    ent_coef        | 0.0112   |\n","|    ent_coef_loss   | 0.393    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157237   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42930    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5543     |\n","|    total_timesteps | 3774480  |\n","| train/             |          |\n","|    actor_loss      | 14.3     |\n","|    critic_loss     | 30.1     |\n","|    ent_coef        | 0.0114   |\n","|    ent_coef_loss   | 3.21     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157265   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42940    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5544     |\n","|    total_timesteps | 3775080  |\n","| train/             |          |\n","|    actor_loss      | 13.4     |\n","|    critic_loss     | 57       |\n","|    ent_coef        | 0.0117   |\n","|    ent_coef_loss   | -2.33    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157290   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42950    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5546     |\n","|    total_timesteps | 3776280  |\n","| train/             |          |\n","|    actor_loss      | 14.3     |\n","|    critic_loss     | 22.8     |\n","|    ent_coef        | 0.0116   |\n","|    ent_coef_loss   | -1.03    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157340   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42960    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5547     |\n","|    total_timesteps | 3777168  |\n","| train/             |          |\n","|    actor_loss      | 12.3     |\n","|    critic_loss     | 5.01     |\n","|    ent_coef        | 0.0111   |\n","|    ent_coef_loss   | 0.517    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157377   |\n","---------------------------------\n","Eval num_timesteps=3777600, episode_reward=-114.93 +/- 10.33\n","Episode length: 134.80 +/- 52.91\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 135      |\n","|    mean_reward     | -115     |\n","| time/              |          |\n","|    total_timesteps | 3777600  |\n","| train/             |          |\n","|    actor_loss      | 14.7     |\n","|    critic_loss     | 60       |\n","|    ent_coef        | 0.0112   |\n","|    ent_coef_loss   | 1.04     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42970    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5551     |\n","|    total_timesteps | 3778536  |\n","| train/             |          |\n","|    actor_loss      | 12.1     |\n","|    critic_loss     | 38.8     |\n","|    ent_coef        | 0.011    |\n","|    ent_coef_loss   | -1.24    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157434   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42980    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5552     |\n","|    total_timesteps | 3779640  |\n","| train/             |          |\n","|    actor_loss      | 14.4     |\n","|    critic_loss     | 12.2     |\n","|    ent_coef        | 0.0107   |\n","|    ent_coef_loss   | -0.176   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157480   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 42990    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5553     |\n","|    total_timesteps | 3780600  |\n","| train/             |          |\n","|    actor_loss      | 13.7     |\n","|    critic_loss     | 16.8     |\n","|    ent_coef        | 0.0105   |\n","|    ent_coef_loss   | -0.63    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157520   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43000    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5554     |\n","|    total_timesteps | 3781680  |\n","| train/             |          |\n","|    actor_loss      | 13.4     |\n","|    critic_loss     | 13.5     |\n","|    ent_coef        | 0.0104   |\n","|    ent_coef_loss   | -0.00849 |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157565   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43010    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5554     |\n","|    total_timesteps | 3782352  |\n","| train/             |          |\n","|    actor_loss      | 16.2     |\n","|    critic_loss     | 8.2      |\n","|    ent_coef        | 0.0103   |\n","|    ent_coef_loss   | 1.54     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157593   |\n","---------------------------------\n","Eval num_timesteps=3782400, episode_reward=-107.89 +/- 10.72\n","Episode length: 62.80 +/- 11.27\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 62.8     |\n","|    mean_reward     | -108     |\n","| time/              |          |\n","|    total_timesteps | 3782400  |\n","| train/             |          |\n","|    actor_loss      | 14.1     |\n","|    critic_loss     | 26.2     |\n","|    ent_coef        | 0.0103   |\n","|    ent_coef_loss   | 1.12     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43020    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5555     |\n","|    total_timesteps | 3782592  |\n","| train/             |          |\n","|    actor_loss      | 15.5     |\n","|    critic_loss     | 14.3     |\n","|    ent_coef        | 0.0103   |\n","|    ent_coef_loss   | 1.15     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157603   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43030    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5557     |\n","|    total_timesteps | 3783984  |\n","| train/             |          |\n","|    actor_loss      | 15.3     |\n","|    critic_loss     | 36.4     |\n","|    ent_coef        | 0.0104   |\n","|    ent_coef_loss   | 2.09     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157661   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43040    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5558     |\n","|    total_timesteps | 3785664  |\n","| train/             |          |\n","|    actor_loss      | 12.2     |\n","|    critic_loss     | 26.4     |\n","|    ent_coef        | 0.0107   |\n","|    ent_coef_loss   | -1.87    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157731   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43050    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5559     |\n","|    total_timesteps | 3786216  |\n","| train/             |          |\n","|    actor_loss      | 12.7     |\n","|    critic_loss     | 4.8      |\n","|    ent_coef        | 0.0109   |\n","|    ent_coef_loss   | 1.02     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157754   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43060    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5559     |\n","|    total_timesteps | 3786984  |\n","| train/             |          |\n","|    actor_loss      | 14.7     |\n","|    critic_loss     | 52.3     |\n","|    ent_coef        | 0.0111   |\n","|    ent_coef_loss   | -1.49    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157786   |\n","---------------------------------\n","Eval num_timesteps=3787200, episode_reward=-109.53 +/- 11.52\n","Episode length: 88.20 +/- 3.92\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 88.2     |\n","|    mean_reward     | -110     |\n","| time/              |          |\n","|    total_timesteps | 3787200  |\n","| train/             |          |\n","|    actor_loss      | 14.8     |\n","|    critic_loss     | 52.7     |\n","|    ent_coef        | 0.0111   |\n","|    ent_coef_loss   | 1.18     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43070    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5563     |\n","|    total_timesteps | 3788424  |\n","| train/             |          |\n","|    actor_loss      | 13.7     |\n","|    critic_loss     | 19.6     |\n","|    ent_coef        | 0.0114   |\n","|    ent_coef_loss   | 0.697    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157846   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43080    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5564     |\n","|    total_timesteps | 3789216  |\n","| train/             |          |\n","|    actor_loss      | 14.6     |\n","|    critic_loss     | 11.2     |\n","|    ent_coef        | 0.0114   |\n","|    ent_coef_loss   | 3.06     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157879   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43090    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5565     |\n","|    total_timesteps | 3790032  |\n","| train/             |          |\n","|    actor_loss      | 13.4     |\n","|    critic_loss     | 5.07     |\n","|    ent_coef        | 0.0115   |\n","|    ent_coef_loss   | 0.132    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157913   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43100    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5566     |\n","|    total_timesteps | 3790872  |\n","| train/             |          |\n","|    actor_loss      | 12.2     |\n","|    critic_loss     | 24.5     |\n","|    ent_coef        | 0.0112   |\n","|    ent_coef_loss   | -3.63    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157948   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43110    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5567     |\n","|    total_timesteps | 3791280  |\n","| train/             |          |\n","|    actor_loss      | 14.8     |\n","|    critic_loss     | 18.5     |\n","|    ent_coef        | 0.0109   |\n","|    ent_coef_loss   | -1.02    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157965   |\n","---------------------------------\n","Eval num_timesteps=3792000, episode_reward=-115.90 +/- 9.74\n","Episode length: 78.60 +/- 1.96\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 78.6     |\n","|    mean_reward     | -116     |\n","| time/              |          |\n","|    total_timesteps | 3792000  |\n","| train/             |          |\n","|    actor_loss      | 14.6     |\n","|    critic_loss     | 33.5     |\n","|    ent_coef        | 0.0107   |\n","|    ent_coef_loss   | 2.09     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43120    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5571     |\n","|    total_timesteps | 3793416  |\n","| train/             |          |\n","|    actor_loss      | 15.6     |\n","|    critic_loss     | 19.4     |\n","|    ent_coef        | 0.0107   |\n","|    ent_coef_loss   | 0.0127   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158054   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43130    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5572     |\n","|    total_timesteps | 3793896  |\n","| train/             |          |\n","|    actor_loss      | 14.5     |\n","|    critic_loss     | 12.9     |\n","|    ent_coef        | 0.0107   |\n","|    ent_coef_loss   | -0.963   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158074   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43140    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5572     |\n","|    total_timesteps | 3794424  |\n","| train/             |          |\n","|    actor_loss      | 15.5     |\n","|    critic_loss     | 7.53     |\n","|    ent_coef        | 0.0106   |\n","|    ent_coef_loss   | 0.459    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158096   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43150    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5574     |\n","|    total_timesteps | 3796032  |\n","| train/             |          |\n","|    actor_loss      | 13.9     |\n","|    critic_loss     | 28.3     |\n","|    ent_coef        | 0.0101   |\n","|    ent_coef_loss   | -2.44    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158163   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43160    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5574     |\n","|    total_timesteps | 3796176  |\n","| train/             |          |\n","|    actor_loss      | 14.1     |\n","|    critic_loss     | 43.3     |\n","|    ent_coef        | 0.00999  |\n","|    ent_coef_loss   | -1.45    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158169   |\n","---------------------------------\n","Eval num_timesteps=3796800, episode_reward=-99.96 +/- 5.75\n","Episode length: 105.80 +/- 0.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 106      |\n","|    mean_reward     | -100     |\n","| time/              |          |\n","|    total_timesteps | 3796800  |\n","| train/             |          |\n","|    actor_loss      | 14.9     |\n","|    critic_loss     | 20.5     |\n","|    ent_coef        | 0.00987  |\n","|    ent_coef_loss   | 0.0267   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43170    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5576     |\n","|    total_timesteps | 3797112  |\n","| train/             |          |\n","|    actor_loss      | 14.3     |\n","|    critic_loss     | 11.3     |\n","|    ent_coef        | 0.00984  |\n","|    ent_coef_loss   | -0.0406  |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158208   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43180    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5577     |\n","|    total_timesteps | 3798696  |\n","| train/             |          |\n","|    actor_loss      | 13.9     |\n","|    critic_loss     | 14.1     |\n","|    ent_coef        | 0.00973  |\n","|    ent_coef_loss   | -0.341   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158274   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43190    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5577     |\n","|    total_timesteps | 3799056  |\n","| train/             |          |\n","|    actor_loss      | 14.5     |\n","|    critic_loss     | 9.92     |\n","|    ent_coef        | 0.00969  |\n","|    ent_coef_loss   | 0.775    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158289   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43200    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5578     |\n","|    total_timesteps | 3800208  |\n","| train/             |          |\n","|    actor_loss      | 14.6     |\n","|    critic_loss     | 32.5     |\n","|    ent_coef        | 0.00943  |\n","|    ent_coef_loss   | 0.164    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158337   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43210    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5579     |\n","|    total_timesteps | 3801072  |\n","| train/             |          |\n","|    actor_loss      | 15.8     |\n","|    critic_loss     | 44.4     |\n","|    ent_coef        | 0.00953  |\n","|    ent_coef_loss   | 4.09     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158373   |\n","---------------------------------\n","Eval num_timesteps=3801600, episode_reward=-107.22 +/- 9.04\n","Episode length: 100.00 +/- 2.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 100      |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 3801600  |\n","| train/             |          |\n","|    actor_loss      | 14.2     |\n","|    critic_loss     | 26.1     |\n","|    ent_coef        | 0.00966  |\n","|    ent_coef_loss   | -1.59    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43220    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5581     |\n","|    total_timesteps | 3801888  |\n","| train/             |          |\n","|    actor_loss      | 14.5     |\n","|    critic_loss     | 43.8     |\n","|    ent_coef        | 0.00961  |\n","|    ent_coef_loss   | -0.595   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158407   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43230    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5583     |\n","|    total_timesteps | 3803208  |\n","| train/             |          |\n","|    actor_loss      | 13       |\n","|    critic_loss     | 32.6     |\n","|    ent_coef        | 0.00967  |\n","|    ent_coef_loss   | 1.41     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158462   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43240    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5584     |\n","|    total_timesteps | 3803520  |\n","| train/             |          |\n","|    actor_loss      | 13.6     |\n","|    critic_loss     | 34.2     |\n","|    ent_coef        | 0.00972  |\n","|    ent_coef_loss   | 1.14     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158475   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43250    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5585     |\n","|    total_timesteps | 3804696  |\n","| train/             |          |\n","|    actor_loss      | 13.3     |\n","|    critic_loss     | 6.07     |\n","|    ent_coef        | 0.00996  |\n","|    ent_coef_loss   | 0.0896   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158524   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43260    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5586     |\n","|    total_timesteps | 3805392  |\n","| train/             |          |\n","|    actor_loss      | 16       |\n","|    critic_loss     | 12.3     |\n","|    ent_coef        | 0.0101   |\n","|    ent_coef_loss   | 2.53     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158553   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43270    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5588     |\n","|    total_timesteps | 3806280  |\n","| train/             |          |\n","|    actor_loss      | 16.3     |\n","|    critic_loss     | 24.5     |\n","|    ent_coef        | 0.0102   |\n","|    ent_coef_loss   | 0.303    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158590   |\n","---------------------------------\n","Eval num_timesteps=3806400, episode_reward=-111.52 +/- 11.29\n","Episode length: 72.00 +/- 7.35\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 72       |\n","|    mean_reward     | -112     |\n","| time/              |          |\n","|    total_timesteps | 3806400  |\n","| train/             |          |\n","|    actor_loss      | 12.9     |\n","|    critic_loss     | 11.8     |\n","|    ent_coef        | 0.0102   |\n","|    ent_coef_loss   | -1.62    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43280    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5591     |\n","|    total_timesteps | 3807792  |\n","| train/             |          |\n","|    actor_loss      | 16.1     |\n","|    critic_loss     | 23.3     |\n","|    ent_coef        | 0.0103   |\n","|    ent_coef_loss   | 2.82     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158653   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43290    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5592     |\n","|    total_timesteps | 3808032  |\n","| train/             |          |\n","|    actor_loss      | 12.3     |\n","|    critic_loss     | 12.4     |\n","|    ent_coef        | 0.0104   |\n","|    ent_coef_loss   | 0.00376  |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158663   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43300    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5593     |\n","|    total_timesteps | 3809136  |\n","| train/             |          |\n","|    actor_loss      | 14.1     |\n","|    critic_loss     | 29.9     |\n","|    ent_coef        | 0.0106   |\n","|    ent_coef_loss   | -0.779   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158709   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43310    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5593     |\n","|    total_timesteps | 3809952  |\n","| train/             |          |\n","|    actor_loss      | 13.3     |\n","|    critic_loss     | 29.4     |\n","|    ent_coef        | 0.0105   |\n","|    ent_coef_loss   | -1.21    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158743   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43320    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5594     |\n","|    total_timesteps | 3810384  |\n","| train/             |          |\n","|    actor_loss      | 14.2     |\n","|    critic_loss     | 24.7     |\n","|    ent_coef        | 0.0105   |\n","|    ent_coef_loss   | 0.122    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158761   |\n","---------------------------------\n","Eval num_timesteps=3811200, episode_reward=-114.50 +/- 11.71\n","Episode length: 70.80 +/- 1.47\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 70.8     |\n","|    mean_reward     | -115     |\n","| time/              |          |\n","|    total_timesteps | 3811200  |\n","| train/             |          |\n","|    actor_loss      | 13.3     |\n","|    critic_loss     | 24.4     |\n","|    ent_coef        | 0.0103   |\n","|    ent_coef_loss   | 0.228    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43330    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5595     |\n","|    total_timesteps | 3811296  |\n","| train/             |          |\n","|    actor_loss      | 15.3     |\n","|    critic_loss     | 19.2     |\n","|    ent_coef        | 0.0103   |\n","|    ent_coef_loss   | 1.74     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158799   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43340    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5596     |\n","|    total_timesteps | 3812424  |\n","| train/             |          |\n","|    actor_loss      | 14.1     |\n","|    critic_loss     | 10.4     |\n","|    ent_coef        | 0.0105   |\n","|    ent_coef_loss   | 0.857    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158846   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43350    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5597     |\n","|    total_timesteps | 3812808  |\n","| train/             |          |\n","|    actor_loss      | 13.1     |\n","|    critic_loss     | 22.6     |\n","|    ent_coef        | 0.0106   |\n","|    ent_coef_loss   | -0.0393  |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158862   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43360    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5598     |\n","|    total_timesteps | 3813864  |\n","| train/             |          |\n","|    actor_loss      | 13.1     |\n","|    critic_loss     | 35.9     |\n","|    ent_coef        | 0.0106   |\n","|    ent_coef_loss   | -0.937   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158906   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43370    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5598     |\n","|    total_timesteps | 3814584  |\n","| train/             |          |\n","|    actor_loss      | 13.5     |\n","|    critic_loss     | 7.93     |\n","|    ent_coef        | 0.0107   |\n","|    ent_coef_loss   | 0.889    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158936   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43380    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5599     |\n","|    total_timesteps | 3815256  |\n","| train/             |          |\n","|    actor_loss      | 14.7     |\n","|    critic_loss     | 24       |\n","|    ent_coef        | 0.0109   |\n","|    ent_coef_loss   | 0.836    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158964   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43390    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5600     |\n","|    total_timesteps | 3815904  |\n","| train/             |          |\n","|    actor_loss      | 14.1     |\n","|    critic_loss     | 51.6     |\n","|    ent_coef        | 0.0109   |\n","|    ent_coef_loss   | 1.1      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158991   |\n","---------------------------------\n","Eval num_timesteps=3816000, episode_reward=-112.24 +/- 9.75\n","Episode length: 101.20 +/- 3.92\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 101      |\n","|    mean_reward     | -112     |\n","| time/              |          |\n","|    total_timesteps | 3816000  |\n","| train/             |          |\n","|    actor_loss      | 12.8     |\n","|    critic_loss     | 15.1     |\n","|    ent_coef        | 0.0109   |\n","|    ent_coef_loss   | -0.00441 |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43400    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5602     |\n","|    total_timesteps | 3816912  |\n","| train/             |          |\n","|    actor_loss      | 13.7     |\n","|    critic_loss     | 15       |\n","|    ent_coef        | 0.011    |\n","|    ent_coef_loss   | -0.343   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159033   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43410    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5603     |\n","|    total_timesteps | 3817704  |\n","| train/             |          |\n","|    actor_loss      | 14.1     |\n","|    critic_loss     | 36.2     |\n","|    ent_coef        | 0.0109   |\n","|    ent_coef_loss   | -1.31    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159066   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43420    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5604     |\n","|    total_timesteps | 3818304  |\n","| train/             |          |\n","|    actor_loss      | 13.4     |\n","|    critic_loss     | 17.7     |\n","|    ent_coef        | 0.011    |\n","|    ent_coef_loss   | 2.16     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159091   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43430    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5606     |\n","|    total_timesteps | 3819480  |\n","| train/             |          |\n","|    actor_loss      | 13.2     |\n","|    critic_loss     | 23.7     |\n","|    ent_coef        | 0.0114   |\n","|    ent_coef_loss   | -0.753   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159140   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43440    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5606     |\n","|    total_timesteps | 3819696  |\n","| train/             |          |\n","|    actor_loss      | 14.4     |\n","|    critic_loss     | 24.9     |\n","|    ent_coef        | 0.0114   |\n","|    ent_coef_loss   | -0.0564  |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159149   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43450    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5607     |\n","|    total_timesteps | 3820680  |\n","| train/             |          |\n","|    actor_loss      | 13.7     |\n","|    critic_loss     | 43.5     |\n","|    ent_coef        | 0.0114   |\n","|    ent_coef_loss   | 0.648    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159190   |\n","---------------------------------\n","Eval num_timesteps=3820800, episode_reward=-110.76 +/- 10.33\n","Episode length: 69.60 +/- 5.39\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 69.6     |\n","|    mean_reward     | -111     |\n","| time/              |          |\n","|    total_timesteps | 3820800  |\n","| train/             |          |\n","|    actor_loss      | 13.3     |\n","|    critic_loss     | 34.4     |\n","|    ent_coef        | 0.0114   |\n","|    ent_coef_loss   | 3.02     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43460    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5611     |\n","|    total_timesteps | 3822216  |\n","| train/             |          |\n","|    actor_loss      | 14.3     |\n","|    critic_loss     | 16.5     |\n","|    ent_coef        | 0.0118   |\n","|    ent_coef_loss   | 1.06     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159254   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43470    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5612     |\n","|    total_timesteps | 3822528  |\n","| train/             |          |\n","|    actor_loss      | 13.9     |\n","|    critic_loss     | 24.1     |\n","|    ent_coef        | 0.0118   |\n","|    ent_coef_loss   | -1.77    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159267   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43480    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5613     |\n","|    total_timesteps | 3823608  |\n","| train/             |          |\n","|    actor_loss      | 13.5     |\n","|    critic_loss     | 9.92     |\n","|    ent_coef        | 0.0118   |\n","|    ent_coef_loss   | 1.4      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159312   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43490    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5614     |\n","|    total_timesteps | 3824856  |\n","| train/             |          |\n","|    actor_loss      | 13.2     |\n","|    critic_loss     | 39.7     |\n","|    ent_coef        | 0.0117   |\n","|    ent_coef_loss   | -2.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159364   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43500    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5615     |\n","|    total_timesteps | 3825408  |\n","| train/             |          |\n","|    actor_loss      | 14.1     |\n","|    critic_loss     | 15.3     |\n","|    ent_coef        | 0.0116   |\n","|    ent_coef_loss   | 2.65     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159387   |\n","---------------------------------\n","Eval num_timesteps=3825600, episode_reward=-112.68 +/- 11.20\n","Episode length: 75.60 +/- 4.41\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 75.6     |\n","|    mean_reward     | -113     |\n","| time/              |          |\n","|    total_timesteps | 3825600  |\n","| train/             |          |\n","|    actor_loss      | 12.6     |\n","|    critic_loss     | 25.3     |\n","|    ent_coef        | 0.0117   |\n","|    ent_coef_loss   | 1.35     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43510    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5617     |\n","|    total_timesteps | 3826944  |\n","| train/             |          |\n","|    actor_loss      | 13.8     |\n","|    critic_loss     | 12.8     |\n","|    ent_coef        | 0.0119   |\n","|    ent_coef_loss   | -3.23    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159451   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43520    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5617     |\n","|    total_timesteps | 3827520  |\n","| train/             |          |\n","|    actor_loss      | 15.5     |\n","|    critic_loss     | 9.22     |\n","|    ent_coef        | 0.0117   |\n","|    ent_coef_loss   | 0.594    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159475   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43530    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5618     |\n","|    total_timesteps | 3828528  |\n","| train/             |          |\n","|    actor_loss      | 13       |\n","|    critic_loss     | 4.31     |\n","|    ent_coef        | 0.0113   |\n","|    ent_coef_loss   | -1.11    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159517   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43540    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5619     |\n","|    total_timesteps | 3828936  |\n","| train/             |          |\n","|    actor_loss      | 13.7     |\n","|    critic_loss     | 18.7     |\n","|    ent_coef        | 0.0112   |\n","|    ent_coef_loss   | -0.671   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159534   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43550    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5620     |\n","|    total_timesteps | 3829920  |\n","| train/             |          |\n","|    actor_loss      | 14.6     |\n","|    critic_loss     | 34.1     |\n","|    ent_coef        | 0.0111   |\n","|    ent_coef_loss   | -0.00711 |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159575   |\n","---------------------------------\n","Eval num_timesteps=3830400, episode_reward=-111.41 +/- 10.19\n","Episode length: 77.20 +/- 8.33\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 77.2     |\n","|    mean_reward     | -111     |\n","| time/              |          |\n","|    total_timesteps | 3830400  |\n","| train/             |          |\n","|    actor_loss      | 13.6     |\n","|    critic_loss     | 18.4     |\n","|    ent_coef        | 0.011    |\n","|    ent_coef_loss   | 0.792    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43560    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5621     |\n","|    total_timesteps | 3830664  |\n","| train/             |          |\n","|    actor_loss      | 12.6     |\n","|    critic_loss     | 31.7     |\n","|    ent_coef        | 0.011    |\n","|    ent_coef_loss   | -3.76    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159606   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43570    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5622     |\n","|    total_timesteps | 3832008  |\n","| train/             |          |\n","|    actor_loss      | 13.2     |\n","|    critic_loss     | 11.9     |\n","|    ent_coef        | 0.0109   |\n","|    ent_coef_loss   | -1.46    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159662   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43580    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5623     |\n","|    total_timesteps | 3832608  |\n","| train/             |          |\n","|    actor_loss      | 12.9     |\n","|    critic_loss     | 18.4     |\n","|    ent_coef        | 0.0109   |\n","|    ent_coef_loss   | -1.66    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159687   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43590    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5625     |\n","|    total_timesteps | 3833616  |\n","| train/             |          |\n","|    actor_loss      | 11.2     |\n","|    critic_loss     | 4.41     |\n","|    ent_coef        | 0.0108   |\n","|    ent_coef_loss   | -1.91    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159729   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43600    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5626     |\n","|    total_timesteps | 3834312  |\n","| train/             |          |\n","|    actor_loss      | 15.4     |\n","|    critic_loss     | 53.1     |\n","|    ent_coef        | 0.0109   |\n","|    ent_coef_loss   | -1.79    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159758   |\n","---------------------------------\n","Eval num_timesteps=3835200, episode_reward=-104.10 +/- 6.11\n","Episode length: 141.40 +/- 58.30\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 141      |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 3835200  |\n","| train/             |          |\n","|    actor_loss      | 12       |\n","|    critic_loss     | 27.1     |\n","|    ent_coef        | 0.0107   |\n","|    ent_coef_loss   | -1.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43610    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5631     |\n","|    total_timesteps | 3835392  |\n","| train/             |          |\n","|    actor_loss      | 13.8     |\n","|    critic_loss     | 22.5     |\n","|    ent_coef        | 0.0107   |\n","|    ent_coef_loss   | -0.744   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159803   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43620    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5632     |\n","|    total_timesteps | 3836472  |\n","| train/             |          |\n","|    actor_loss      | 13.3     |\n","|    critic_loss     | 34.7     |\n","|    ent_coef        | 0.0105   |\n","|    ent_coef_loss   | -1.22    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159848   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43630    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5634     |\n","|    total_timesteps | 3837816  |\n","| train/             |          |\n","|    actor_loss      | 13.9     |\n","|    critic_loss     | 32.5     |\n","|    ent_coef        | 0.0102   |\n","|    ent_coef_loss   | 0.809    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159904   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43640    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5634     |\n","|    total_timesteps | 3838368  |\n","| train/             |          |\n","|    actor_loss      | 13.5     |\n","|    critic_loss     | 4.49     |\n","|    ent_coef        | 0.0102   |\n","|    ent_coef_loss   | -2.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159927   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43650    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5635     |\n","|    total_timesteps | 3838656  |\n","| train/             |          |\n","|    actor_loss      | 13.3     |\n","|    critic_loss     | 13.6     |\n","|    ent_coef        | 0.0101   |\n","|    ent_coef_loss   | 0.544    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159939   |\n","---------------------------------\n","Eval num_timesteps=3840000, episode_reward=-111.11 +/- 9.98\n","Episode length: 62.00 +/- 4.90\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 62       |\n","|    mean_reward     | -111     |\n","| time/              |          |\n","|    total_timesteps | 3840000  |\n","| train/             |          |\n","|    actor_loss      | 14.8     |\n","|    critic_loss     | 5.18     |\n","|    ent_coef        | 0.0103   |\n","|    ent_coef_loss   | 2.58     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43660    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5637     |\n","|    total_timesteps | 3840264  |\n","| train/             |          |\n","|    actor_loss      | 12.4     |\n","|    critic_loss     | 3.02     |\n","|    ent_coef        | 0.0104   |\n","|    ent_coef_loss   | 0.709    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160006   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43670    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5638     |\n","|    total_timesteps | 3841704  |\n","| train/             |          |\n","|    actor_loss      | 14.1     |\n","|    critic_loss     | 17.1     |\n","|    ent_coef        | 0.0107   |\n","|    ent_coef_loss   | 1.41     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160066   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43680    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5638     |\n","|    total_timesteps | 3842016  |\n","| train/             |          |\n","|    actor_loss      | 14.7     |\n","|    critic_loss     | 19.1     |\n","|    ent_coef        | 0.0109   |\n","|    ent_coef_loss   | 1.29     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160079   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43690    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5640     |\n","|    total_timesteps | 3843600  |\n","| train/             |          |\n","|    actor_loss      | 14.6     |\n","|    critic_loss     | 15.7     |\n","|    ent_coef        | 0.0105   |\n","|    ent_coef_loss   | 2.05     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160145   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43700    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5640     |\n","|    total_timesteps | 3843816  |\n","| train/             |          |\n","|    actor_loss      | 15.5     |\n","|    critic_loss     | 19.9     |\n","|    ent_coef        | 0.0106   |\n","|    ent_coef_loss   | 1.55     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160154   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43710    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5640     |\n","|    total_timesteps | 3844176  |\n","| train/             |          |\n","|    actor_loss      | 13.8     |\n","|    critic_loss     | 29.4     |\n","|    ent_coef        | 0.0107   |\n","|    ent_coef_loss   | 2.32     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160169   |\n","---------------------------------\n","Eval num_timesteps=3844800, episode_reward=-116.92 +/- 5.09\n","Episode length: 92.20 +/- 0.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 92.2     |\n","|    mean_reward     | -117     |\n","| time/              |          |\n","|    total_timesteps | 3844800  |\n","| train/             |          |\n","|    actor_loss      | 13.5     |\n","|    critic_loss     | 10.2     |\n","|    ent_coef        | 0.0109   |\n","|    ent_coef_loss   | -0.985   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43720    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5643     |\n","|    total_timesteps | 3846384  |\n","| train/             |          |\n","|    actor_loss      | 13.4     |\n","|    critic_loss     | 4.06     |\n","|    ent_coef        | 0.0108   |\n","|    ent_coef_loss   | -1       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160261   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43730    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5643     |\n","|    total_timesteps | 3846456  |\n","| train/             |          |\n","|    actor_loss      | 11.7     |\n","|    critic_loss     | 7.4      |\n","|    ent_coef        | 0.0108   |\n","|    ent_coef_loss   | -2.63    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160264   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43740    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5646     |\n","|    total_timesteps | 3847968  |\n","| train/             |          |\n","|    actor_loss      | 14.8     |\n","|    critic_loss     | 13.3     |\n","|    ent_coef        | 0.0103   |\n","|    ent_coef_loss   | 0.12     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160327   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43750    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5646     |\n","|    total_timesteps | 3848520  |\n","| train/             |          |\n","|    actor_loss      | 13.8     |\n","|    critic_loss     | 11.6     |\n","|    ent_coef        | 0.0103   |\n","|    ent_coef_loss   | -0.168   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160350   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43760    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5647     |\n","|    total_timesteps | 3848736  |\n","| train/             |          |\n","|    actor_loss      | 13.2     |\n","|    critic_loss     | 11.8     |\n","|    ent_coef        | 0.0103   |\n","|    ent_coef_loss   | 0.0491   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160359   |\n","---------------------------------\n","Eval num_timesteps=3849600, episode_reward=-106.43 +/- 9.99\n","Episode length: 78.00 +/- 7.35\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 78       |\n","|    mean_reward     | -106     |\n","| time/              |          |\n","|    total_timesteps | 3849600  |\n","| train/             |          |\n","|    actor_loss      | 13.8     |\n","|    critic_loss     | 25       |\n","|    ent_coef        | 0.0107   |\n","|    ent_coef_loss   | 3.57     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43770    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5650     |\n","|    total_timesteps | 3849840  |\n","| train/             |          |\n","|    actor_loss      | 12.8     |\n","|    critic_loss     | 25.5     |\n","|    ent_coef        | 0.0108   |\n","|    ent_coef_loss   | 3.84     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160405   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43780    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5653     |\n","|    total_timesteps | 3851496  |\n","| train/             |          |\n","|    actor_loss      | 13.1     |\n","|    critic_loss     | 21.8     |\n","|    ent_coef        | 0.0114   |\n","|    ent_coef_loss   | 1.57     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160474   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43790    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5654     |\n","|    total_timesteps | 3852072  |\n","| train/             |          |\n","|    actor_loss      | 13.6     |\n","|    critic_loss     | 11       |\n","|    ent_coef        | 0.0116   |\n","|    ent_coef_loss   | -0.765   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160498   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43800    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5654     |\n","|    total_timesteps | 3852960  |\n","| train/             |          |\n","|    actor_loss      | 13.4     |\n","|    critic_loss     | 47.2     |\n","|    ent_coef        | 0.0116   |\n","|    ent_coef_loss   | -2.13    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160535   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43810    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5656     |\n","|    total_timesteps | 3854112  |\n","| train/             |          |\n","|    actor_loss      | 13.9     |\n","|    critic_loss     | 14.2     |\n","|    ent_coef        | 0.0111   |\n","|    ent_coef_loss   | -0.505   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160583   |\n","---------------------------------\n","Eval num_timesteps=3854400, episode_reward=-113.60 +/- 4.94\n","Episode length: 92.40 +/- 5.39\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 92.4     |\n","|    mean_reward     | -114     |\n","| time/              |          |\n","|    total_timesteps | 3854400  |\n","| train/             |          |\n","|    actor_loss      | 14.6     |\n","|    critic_loss     | 62.8     |\n","|    ent_coef        | 0.0111   |\n","|    ent_coef_loss   | -0.66    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43820    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5658     |\n","|    total_timesteps | 3855864  |\n","| train/             |          |\n","|    actor_loss      | 14.6     |\n","|    critic_loss     | 25.3     |\n","|    ent_coef        | 0.0107   |\n","|    ent_coef_loss   | -0.498   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160656   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43830    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5659     |\n","|    total_timesteps | 3856416  |\n","| train/             |          |\n","|    actor_loss      | 14.1     |\n","|    critic_loss     | 21.7     |\n","|    ent_coef        | 0.0107   |\n","|    ent_coef_loss   | -0.928   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160679   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43840    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5660     |\n","|    total_timesteps | 3858168  |\n","| train/             |          |\n","|    actor_loss      | 14       |\n","|    critic_loss     | 30.6     |\n","|    ent_coef        | 0.0103   |\n","|    ent_coef_loss   | -0.895   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160752   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43850    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5661     |\n","|    total_timesteps | 3858792  |\n","| train/             |          |\n","|    actor_loss      | 12.6     |\n","|    critic_loss     | 34       |\n","|    ent_coef        | 0.0101   |\n","|    ent_coef_loss   | -2.47    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160778   |\n","---------------------------------\n","Eval num_timesteps=3859200, episode_reward=-111.90 +/- 10.20\n","Episode length: 83.00 +/- 9.80\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 83       |\n","|    mean_reward     | -112     |\n","| time/              |          |\n","|    total_timesteps | 3859200  |\n","| train/             |          |\n","|    actor_loss      | 14.4     |\n","|    critic_loss     | 7.33     |\n","|    ent_coef        | 0.00989  |\n","|    ent_coef_loss   | -1.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43860    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5663     |\n","|    total_timesteps | 3860400  |\n","| train/             |          |\n","|    actor_loss      | 12.8     |\n","|    critic_loss     | 15       |\n","|    ent_coef        | 0.00957  |\n","|    ent_coef_loss   | -1       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160845   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43870    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5664     |\n","|    total_timesteps | 3860832  |\n","| train/             |          |\n","|    actor_loss      | 13       |\n","|    critic_loss     | 26.4     |\n","|    ent_coef        | 0.00951  |\n","|    ent_coef_loss   | -0.189   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160863   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43880    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5666     |\n","|    total_timesteps | 3862272  |\n","| train/             |          |\n","|    actor_loss      | 13.2     |\n","|    critic_loss     | 29.7     |\n","|    ent_coef        | 0.00994  |\n","|    ent_coef_loss   | -0.359   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160923   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43890    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5666     |\n","|    total_timesteps | 3862464  |\n","| train/             |          |\n","|    actor_loss      | 14       |\n","|    critic_loss     | 29.3     |\n","|    ent_coef        | 0.00997  |\n","|    ent_coef_loss   | 3.38     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160931   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43900    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5667     |\n","|    total_timesteps | 3863472  |\n","| train/             |          |\n","|    actor_loss      | 12.2     |\n","|    critic_loss     | 42.5     |\n","|    ent_coef        | 0.0106   |\n","|    ent_coef_loss   | 0.153    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160973   |\n","---------------------------------\n","Eval num_timesteps=3864000, episode_reward=-0.51 +/- 0.24\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.506   |\n","| time/              |          |\n","|    total_timesteps | 3864000  |\n","| train/             |          |\n","|    actor_loss      | 14.5     |\n","|    critic_loss     | 42.9     |\n","|    ent_coef        | 0.0108   |\n","|    ent_coef_loss   | 4.24     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160995   |\n","---------------------------------\n","New best mean reward!\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43910    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5678     |\n","|    total_timesteps | 3865968  |\n","| train/             |          |\n","|    actor_loss      | 12.6     |\n","|    critic_loss     | 9.99     |\n","|    ent_coef        | 0.0114   |\n","|    ent_coef_loss   | -1.25    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161077   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43920    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5678     |\n","|    total_timesteps | 3866592  |\n","| train/             |          |\n","|    actor_loss      | 12.8     |\n","|    critic_loss     | 19.9     |\n","|    ent_coef        | 0.0113   |\n","|    ent_coef_loss   | -1.99    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161103   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43930    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5680     |\n","|    total_timesteps | 3868416  |\n","| train/             |          |\n","|    actor_loss      | 14.5     |\n","|    critic_loss     | 14.8     |\n","|    ent_coef        | 0.0107   |\n","|    ent_coef_loss   | -1.61    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161179   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43940    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5680     |\n","|    total_timesteps | 3868752  |\n","| train/             |          |\n","|    actor_loss      | 14.3     |\n","|    critic_loss     | 9.44     |\n","|    ent_coef        | 0.0107   |\n","|    ent_coef_loss   | -1.13    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161193   |\n","---------------------------------\n","Eval num_timesteps=3868800, episode_reward=-112.37 +/- 11.62\n","Episode length: 98.60 +/- 4.41\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 98.6     |\n","|    mean_reward     | -112     |\n","| time/              |          |\n","|    total_timesteps | 3868800  |\n","| train/             |          |\n","|    actor_loss      | 12.8     |\n","|    critic_loss     | 33.6     |\n","|    ent_coef        | 0.0106   |\n","|    ent_coef_loss   | -1.29    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43950    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5682     |\n","|    total_timesteps | 3869664  |\n","| train/             |          |\n","|    actor_loss      | 14.9     |\n","|    critic_loss     | 21.9     |\n","|    ent_coef        | 0.0106   |\n","|    ent_coef_loss   | -0.455   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161231   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43960    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5683     |\n","|    total_timesteps | 3870912  |\n","| train/             |          |\n","|    actor_loss      | 15.2     |\n","|    critic_loss     | 19.9     |\n","|    ent_coef        | 0.0104   |\n","|    ent_coef_loss   | -1.12    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161283   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43970    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5684     |\n","|    total_timesteps | 3872280  |\n","| train/             |          |\n","|    actor_loss      | 14.6     |\n","|    critic_loss     | 17.3     |\n","|    ent_coef        | 0.0101   |\n","|    ent_coef_loss   | 1.12     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161340   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43980    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5685     |\n","|    total_timesteps | 3872736  |\n","| train/             |          |\n","|    actor_loss      | 14.7     |\n","|    critic_loss     | 34       |\n","|    ent_coef        | 0.0101   |\n","|    ent_coef_loss   | -0.0633  |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161359   |\n","---------------------------------\n","Eval num_timesteps=3873600, episode_reward=-107.09 +/- 6.96\n","Episode length: 53.00 +/- 2.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 53       |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 3873600  |\n","| train/             |          |\n","|    actor_loss      | 15       |\n","|    critic_loss     | 11.9     |\n","|    ent_coef        | 0.00994  |\n","|    ent_coef_loss   | 3.17     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 43990    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5687     |\n","|    total_timesteps | 3873864  |\n","| train/             |          |\n","|    actor_loss      | 13.8     |\n","|    critic_loss     | 14.5     |\n","|    ent_coef        | 0.00994  |\n","|    ent_coef_loss   | 0.296    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161406   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44000    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5688     |\n","|    total_timesteps | 3874560  |\n","| train/             |          |\n","|    actor_loss      | 14.4     |\n","|    critic_loss     | 57       |\n","|    ent_coef        | 0.0101   |\n","|    ent_coef_loss   | 2.86     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161435   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44010    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5691     |\n","|    total_timesteps | 3876024  |\n","| train/             |          |\n","|    actor_loss      | 12.3     |\n","|    critic_loss     | 20.3     |\n","|    ent_coef        | 0.0104   |\n","|    ent_coef_loss   | -0.736   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161496   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44020    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5693     |\n","|    total_timesteps | 3877464  |\n","| train/             |          |\n","|    actor_loss      | 15.1     |\n","|    critic_loss     | 33.6     |\n","|    ent_coef        | 0.0104   |\n","|    ent_coef_loss   | 0.844    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161556   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44030    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5694     |\n","|    total_timesteps | 3878256  |\n","| train/             |          |\n","|    actor_loss      | 13.1     |\n","|    critic_loss     | 28.9     |\n","|    ent_coef        | 0.0108   |\n","|    ent_coef_loss   | -0.758   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161589   |\n","---------------------------------\n","Eval num_timesteps=3878400, episode_reward=-114.59 +/- 9.78\n","Episode length: 175.00 +/- 56.34\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 175      |\n","|    mean_reward     | -115     |\n","| time/              |          |\n","|    total_timesteps | 3878400  |\n","| train/             |          |\n","|    actor_loss      | 12.9     |\n","|    critic_loss     | 24.7     |\n","|    ent_coef        | 0.0109   |\n","|    ent_coef_loss   | 0.543    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44040    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5698     |\n","|    total_timesteps | 3879456  |\n","| train/             |          |\n","|    actor_loss      | 14.3     |\n","|    critic_loss     | 13.4     |\n","|    ent_coef        | 0.0112   |\n","|    ent_coef_loss   | 1.06     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161639   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44050    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5700     |\n","|    total_timesteps | 3881136  |\n","| train/             |          |\n","|    actor_loss      | 13.6     |\n","|    critic_loss     | 17.5     |\n","|    ent_coef        | 0.0115   |\n","|    ent_coef_loss   | 1.09     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161709   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44060    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5700     |\n","|    total_timesteps | 3881976  |\n","| train/             |          |\n","|    actor_loss      | 14.9     |\n","|    critic_loss     | 6.91     |\n","|    ent_coef        | 0.0116   |\n","|    ent_coef_loss   | -1.06    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161744   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44070    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5701     |\n","|    total_timesteps | 3883032  |\n","| train/             |          |\n","|    actor_loss      | 13.9     |\n","|    critic_loss     | 45.3     |\n","|    ent_coef        | 0.0113   |\n","|    ent_coef_loss   | -2.11    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161788   |\n","---------------------------------\n","Eval num_timesteps=3883200, episode_reward=-111.49 +/- 5.70\n","Episode length: 98.40 +/- 5.39\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 98.4     |\n","|    mean_reward     | -111     |\n","| time/              |          |\n","|    total_timesteps | 3883200  |\n","| train/             |          |\n","|    actor_loss      | 14.4     |\n","|    critic_loss     | 4.27     |\n","|    ent_coef        | 0.0112   |\n","|    ent_coef_loss   | -0.0397  |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44080    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5705     |\n","|    total_timesteps | 3885192  |\n","| train/             |          |\n","|    actor_loss      | 15.7     |\n","|    critic_loss     | 16       |\n","|    ent_coef        | 0.0108   |\n","|    ent_coef_loss   | 2.2      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161878   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44090    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5705     |\n","|    total_timesteps | 3885984  |\n","| train/             |          |\n","|    actor_loss      | 14.4     |\n","|    critic_loss     | 28.5     |\n","|    ent_coef        | 0.0109   |\n","|    ent_coef_loss   | -1.42    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161911   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44100    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5706     |\n","|    total_timesteps | 3886776  |\n","| train/             |          |\n","|    actor_loss      | 13.7     |\n","|    critic_loss     | 5.03     |\n","|    ent_coef        | 0.0107   |\n","|    ent_coef_loss   | -0.404   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161944   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44110    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5708     |\n","|    total_timesteps | 3887592  |\n","| train/             |          |\n","|    actor_loss      | 13       |\n","|    critic_loss     | 30.9     |\n","|    ent_coef        | 0.0107   |\n","|    ent_coef_loss   | -2.76    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161978   |\n","---------------------------------\n","Eval num_timesteps=3888000, episode_reward=-112.70 +/- 10.20\n","Episode length: 108.80 +/- 6.37\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 109      |\n","|    mean_reward     | -113     |\n","| time/              |          |\n","|    total_timesteps | 3888000  |\n","| train/             |          |\n","|    actor_loss      | 13.9     |\n","|    critic_loss     | 11.4     |\n","|    ent_coef        | 0.0106   |\n","|    ent_coef_loss   | -1.04    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44120    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5712     |\n","|    total_timesteps | 3889320  |\n","| train/             |          |\n","|    actor_loss      | 12.8     |\n","|    critic_loss     | 20.8     |\n","|    ent_coef        | 0.0105   |\n","|    ent_coef_loss   | -1.53    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162050   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44130    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5713     |\n","|    total_timesteps | 3889872  |\n","| train/             |          |\n","|    actor_loss      | 13.6     |\n","|    critic_loss     | 33.4     |\n","|    ent_coef        | 0.0104   |\n","|    ent_coef_loss   | -2.43    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162073   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44140    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5716     |\n","|    total_timesteps | 3891216  |\n","| train/             |          |\n","|    actor_loss      | 13.4     |\n","|    critic_loss     | 19.3     |\n","|    ent_coef        | 0.0103   |\n","|    ent_coef_loss   | 0.282    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162129   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44150    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5717     |\n","|    total_timesteps | 3892080  |\n","| train/             |          |\n","|    actor_loss      | 15.1     |\n","|    critic_loss     | 6.2      |\n","|    ent_coef        | 0.0103   |\n","|    ent_coef_loss   | -0.122   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162165   |\n","---------------------------------\n","Eval num_timesteps=3892800, episode_reward=-103.52 +/- 10.19\n","Episode length: 95.60 +/- 17.64\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 95.6     |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 3892800  |\n","| train/             |          |\n","|    actor_loss      | 15.5     |\n","|    critic_loss     | 28.4     |\n","|    ent_coef        | 0.0103   |\n","|    ent_coef_loss   | 4.3      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44160    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5720     |\n","|    total_timesteps | 3893928  |\n","| train/             |          |\n","|    actor_loss      | 13.4     |\n","|    critic_loss     | 24.3     |\n","|    ent_coef        | 0.0105   |\n","|    ent_coef_loss   | -0.393   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162242   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44170    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5721     |\n","|    total_timesteps | 3895032  |\n","| train/             |          |\n","|    actor_loss      | 12.8     |\n","|    critic_loss     | 27.9     |\n","|    ent_coef        | 0.0103   |\n","|    ent_coef_loss   | -2.52    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162288   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44180    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5721     |\n","|    total_timesteps | 3895704  |\n","| train/             |          |\n","|    actor_loss      | 12.7     |\n","|    critic_loss     | 13.9     |\n","|    ent_coef        | 0.0102   |\n","|    ent_coef_loss   | -2.17    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162316   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44190    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5722     |\n","|    total_timesteps | 3896736  |\n","| train/             |          |\n","|    actor_loss      | 14.1     |\n","|    critic_loss     | 33.5     |\n","|    ent_coef        | 0.00989  |\n","|    ent_coef_loss   | 0.179    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162359   |\n","---------------------------------\n","Eval num_timesteps=3897600, episode_reward=-116.45 +/- 8.74\n","Episode length: 109.40 +/- 5.39\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 109      |\n","|    mean_reward     | -116     |\n","| time/              |          |\n","|    total_timesteps | 3897600  |\n","| train/             |          |\n","|    actor_loss      | 14.9     |\n","|    critic_loss     | 36.3     |\n","|    ent_coef        | 0.00982  |\n","|    ent_coef_loss   | 0.502    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44200    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5724     |\n","|    total_timesteps | 3897744  |\n","| train/             |          |\n","|    actor_loss      | 14       |\n","|    critic_loss     | 52.1     |\n","|    ent_coef        | 0.00981  |\n","|    ent_coef_loss   | 1.73     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162401   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44210    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5726     |\n","|    total_timesteps | 3899184  |\n","| train/             |          |\n","|    actor_loss      | 14.6     |\n","|    critic_loss     | 18.9     |\n","|    ent_coef        | 0.0097   |\n","|    ent_coef_loss   | -0.749   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162461   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44220    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5728     |\n","|    total_timesteps | 3901152  |\n","| train/             |          |\n","|    actor_loss      | 14.1     |\n","|    critic_loss     | 30.5     |\n","|    ent_coef        | 0.0102   |\n","|    ent_coef_loss   | 2.96     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162543   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44230    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5729     |\n","|    total_timesteps | 3901968  |\n","| train/             |          |\n","|    actor_loss      | 14.3     |\n","|    critic_loss     | 49.5     |\n","|    ent_coef        | 0.0108   |\n","|    ent_coef_loss   | 1.04     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162577   |\n","---------------------------------\n","Eval num_timesteps=3902400, episode_reward=-114.47 +/- 10.69\n","Episode length: 107.80 +/- 0.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 108      |\n","|    mean_reward     | -114     |\n","| time/              |          |\n","|    total_timesteps | 3902400  |\n","| train/             |          |\n","|    actor_loss      | 13.5     |\n","|    critic_loss     | 20.5     |\n","|    ent_coef        | 0.011    |\n","|    ent_coef_loss   | -0.677   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44240    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5732     |\n","|    total_timesteps | 3902472  |\n","| train/             |          |\n","|    actor_loss      | 14.6     |\n","|    critic_loss     | 9.13     |\n","|    ent_coef        | 0.011    |\n","|    ent_coef_loss   | -0.365   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162598   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44250    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5735     |\n","|    total_timesteps | 3904248  |\n","| train/             |          |\n","|    actor_loss      | 14       |\n","|    critic_loss     | 19.8     |\n","|    ent_coef        | 0.0116   |\n","|    ent_coef_loss   | -0.0628  |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44260    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5735     |\n","|    total_timesteps | 3904824  |\n","| train/             |          |\n","|    actor_loss      | 14       |\n","|    critic_loss     | 8.49     |\n","|    ent_coef        | 0.0115   |\n","|    ent_coef_loss   | 2.51     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162696   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44270    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5737     |\n","|    total_timesteps | 3905664  |\n","| train/             |          |\n","|    actor_loss      | 13.5     |\n","|    critic_loss     | 38.7     |\n","|    ent_coef        | 0.0115   |\n","|    ent_coef_loss   | -4.17    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162731   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44280    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5738     |\n","|    total_timesteps | 3906792  |\n","| train/             |          |\n","|    actor_loss      | 15.1     |\n","|    critic_loss     | 33.2     |\n","|    ent_coef        | 0.011    |\n","|    ent_coef_loss   | -1.45    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162778   |\n","---------------------------------\n","Eval num_timesteps=3907200, episode_reward=-111.07 +/- 10.46\n","Episode length: 96.00 +/- 9.80\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 96       |\n","|    mean_reward     | -111     |\n","| time/              |          |\n","|    total_timesteps | 3907200  |\n","| train/             |          |\n","|    actor_loss      | 12.9     |\n","|    critic_loss     | 3.43     |\n","|    ent_coef        | 0.0108   |\n","|    ent_coef_loss   | -2.45    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44290    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5741     |\n","|    total_timesteps | 3908520  |\n","| train/             |          |\n","|    actor_loss      | 15.1     |\n","|    critic_loss     | 34.1     |\n","|    ent_coef        | 0.0102   |\n","|    ent_coef_loss   | 0.985    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162850   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44300    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5741     |\n","|    total_timesteps | 3909384  |\n","| train/             |          |\n","|    actor_loss      | 13.4     |\n","|    critic_loss     | 26       |\n","|    ent_coef        | 0.0104   |\n","|    ent_coef_loss   | -1.7     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162886   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44310    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5742     |\n","|    total_timesteps | 3910224  |\n","| train/             |          |\n","|    actor_loss      | 13.9     |\n","|    critic_loss     | 42.4     |\n","|    ent_coef        | 0.0102   |\n","|    ent_coef_loss   | -0.386   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162921   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44320    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5743     |\n","|    total_timesteps | 3911544  |\n","| train/             |          |\n","|    actor_loss      | 13.3     |\n","|    critic_loss     | 42.1     |\n","|    ent_coef        | 0.0101   |\n","|    ent_coef_loss   | 0.519    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162976   |\n","---------------------------------\n","Eval num_timesteps=3912000, episode_reward=-114.03 +/- 10.21\n","Episode length: 91.60 +/- 1.96\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 91.6     |\n","|    mean_reward     | -114     |\n","| time/              |          |\n","|    total_timesteps | 3912000  |\n","| train/             |          |\n","|    actor_loss      | 13       |\n","|    critic_loss     | 52       |\n","|    ent_coef        | 0.0101   |\n","|    ent_coef_loss   | -1.83    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44330    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5745     |\n","|    total_timesteps | 3912600  |\n","| train/             |          |\n","|    actor_loss      | 14.2     |\n","|    critic_loss     | 32.7     |\n","|    ent_coef        | 0.00993  |\n","|    ent_coef_loss   | -1.46    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163020   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44340    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5747     |\n","|    total_timesteps | 3913992  |\n","| train/             |          |\n","|    actor_loss      | 13.5     |\n","|    critic_loss     | 12       |\n","|    ent_coef        | 0.0102   |\n","|    ent_coef_loss   | 0.859    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163078   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44350    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5748     |\n","|    total_timesteps | 3914832  |\n","| train/             |          |\n","|    actor_loss      | 14.7     |\n","|    critic_loss     | 28.5     |\n","|    ent_coef        | 0.0102   |\n","|    ent_coef_loss   | 0.232    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163113   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44360    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5750     |\n","|    total_timesteps | 3916200  |\n","| train/             |          |\n","|    actor_loss      | 14.4     |\n","|    critic_loss     | 24.2     |\n","|    ent_coef        | 0.0106   |\n","|    ent_coef_loss   | -1.07    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163170   |\n","---------------------------------\n","Eval num_timesteps=3916800, episode_reward=-108.87 +/- 6.49\n","Episode length: 83.80 +/- 10.78\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 83.8     |\n","|    mean_reward     | -109     |\n","| time/              |          |\n","|    total_timesteps | 3916800  |\n","| train/             |          |\n","|    actor_loss      | 15.2     |\n","|    critic_loss     | 19.8     |\n","|    ent_coef        | 0.0106   |\n","|    ent_coef_loss   | 0.033    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44370    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5755     |\n","|    total_timesteps | 3918240  |\n","| train/             |          |\n","|    actor_loss      | 14.2     |\n","|    critic_loss     | 18.1     |\n","|    ent_coef        | 0.0103   |\n","|    ent_coef_loss   | -0.573   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163255   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44380    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5755     |\n","|    total_timesteps | 3918456  |\n","| train/             |          |\n","|    actor_loss      | 13.3     |\n","|    critic_loss     | 40.1     |\n","|    ent_coef        | 0.0102   |\n","|    ent_coef_loss   | -1.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163264   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44390    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5756     |\n","|    total_timesteps | 3918984  |\n","| train/             |          |\n","|    actor_loss      | 13.1     |\n","|    critic_loss     | 20.7     |\n","|    ent_coef        | 0.0101   |\n","|    ent_coef_loss   | -1.68    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163286   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44400    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5758     |\n","|    total_timesteps | 3920376  |\n","| train/             |          |\n","|    actor_loss      | 13       |\n","|    critic_loss     | 6.22     |\n","|    ent_coef        | 0.00985  |\n","|    ent_coef_loss   | -0.256   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163344   |\n","---------------------------------\n","Eval num_timesteps=3921600, episode_reward=-116.17 +/- 9.93\n","Episode length: 74.80 +/- 0.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 74.8     |\n","|    mean_reward     | -116     |\n","| time/              |          |\n","|    total_timesteps | 3921600  |\n","| train/             |          |\n","|    actor_loss      | 13.1     |\n","|    critic_loss     | 5.59     |\n","|    ent_coef        | 0.00974  |\n","|    ent_coef_loss   | -1.15    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44410    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5760     |\n","|    total_timesteps | 3921960  |\n","| train/             |          |\n","|    actor_loss      | 13.6     |\n","|    critic_loss     | 22.5     |\n","|    ent_coef        | 0.00965  |\n","|    ent_coef_loss   | -1.85    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163410   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44420    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5760     |\n","|    total_timesteps | 3922032  |\n","| train/             |          |\n","|    actor_loss      | 14.5     |\n","|    critic_loss     | 26.7     |\n","|    ent_coef        | 0.00963  |\n","|    ent_coef_loss   | -1.31    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163413   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44430    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5761     |\n","|    total_timesteps | 3923424  |\n","| train/             |          |\n","|    actor_loss      | 12.9     |\n","|    critic_loss     | 47.3     |\n","|    ent_coef        | 0.0102   |\n","|    ent_coef_loss   | 0.959    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163471   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44440    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5762     |\n","|    total_timesteps | 3924024  |\n","| train/             |          |\n","|    actor_loss      | 12.1     |\n","|    critic_loss     | 8.43     |\n","|    ent_coef        | 0.0103   |\n","|    ent_coef_loss   | -3.78    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163496   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44450    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5763     |\n","|    total_timesteps | 3925776  |\n","| train/             |          |\n","|    actor_loss      | 14.4     |\n","|    critic_loss     | 6.29     |\n","|    ent_coef        | 0.0105   |\n","|    ent_coef_loss   | 0.677    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163569   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44460    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5764     |\n","|    total_timesteps | 3926064  |\n","| train/             |          |\n","|    actor_loss      | 13.5     |\n","|    critic_loss     | 17.1     |\n","|    ent_coef        | 0.0106   |\n","|    ent_coef_loss   | -0.634   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163581   |\n","---------------------------------\n","Eval num_timesteps=3926400, episode_reward=-109.76 +/- 7.33\n","Episode length: 78.20 +/- 13.72\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 78.2     |\n","|    mean_reward     | -110     |\n","| time/              |          |\n","|    total_timesteps | 3926400  |\n","| train/             |          |\n","|    actor_loss      | 15.1     |\n","|    critic_loss     | 12.3     |\n","|    ent_coef        | 0.0105   |\n","|    ent_coef_loss   | -1.9     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44470    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5765     |\n","|    total_timesteps | 3926784  |\n","| train/             |          |\n","|    actor_loss      | 14       |\n","|    critic_loss     | 24.4     |\n","|    ent_coef        | 0.0104   |\n","|    ent_coef_loss   | -2.9     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163611   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44480    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5767     |\n","|    total_timesteps | 3928152  |\n","| train/             |          |\n","|    actor_loss      | 14.5     |\n","|    critic_loss     | 28.5     |\n","|    ent_coef        | 0.0101   |\n","|    ent_coef_loss   | -0.54    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163668   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44490    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5767     |\n","|    total_timesteps | 3928608  |\n","| train/             |          |\n","|    actor_loss      | 14       |\n","|    critic_loss     | 27.6     |\n","|    ent_coef        | 0.01     |\n","|    ent_coef_loss   | 0.612    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163687   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44500    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5769     |\n","|    total_timesteps | 3929856  |\n","| train/             |          |\n","|    actor_loss      | 13.8     |\n","|    critic_loss     | 11.4     |\n","|    ent_coef        | 0.01     |\n","|    ent_coef_loss   | 2.61     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163739   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44510    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5770     |\n","|    total_timesteps | 3930720  |\n","| train/             |          |\n","|    actor_loss      | 12.9     |\n","|    critic_loss     | 40.3     |\n","|    ent_coef        | 0.0104   |\n","|    ent_coef_loss   | -0.951   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163775   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44520    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5770     |\n","|    total_timesteps | 3930984  |\n","| train/             |          |\n","|    actor_loss      | 14.9     |\n","|    critic_loss     | 8.37     |\n","|    ent_coef        | 0.0105   |\n","|    ent_coef_loss   | -0.343   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163786   |\n","---------------------------------\n","Eval num_timesteps=3931200, episode_reward=-108.48 +/- 8.24\n","Episode length: 126.20 +/- 3.92\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 126      |\n","|    mean_reward     | -108     |\n","| time/              |          |\n","|    total_timesteps | 3931200  |\n","| train/             |          |\n","|    actor_loss      | 12.1     |\n","|    critic_loss     | 3.65     |\n","|    ent_coef        | 0.0105   |\n","|    ent_coef_loss   | -2.04    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44530    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5774     |\n","|    total_timesteps | 3931992  |\n","| train/             |          |\n","|    actor_loss      | 13.9     |\n","|    critic_loss     | 71.3     |\n","|    ent_coef        | 0.0104   |\n","|    ent_coef_loss   | 0.603    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163828   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44540    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5777     |\n","|    total_timesteps | 3933672  |\n","| train/             |          |\n","|    actor_loss      | 13.6     |\n","|    critic_loss     | 43       |\n","|    ent_coef        | 0.0104   |\n","|    ent_coef_loss   | -1.63    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163898   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44550    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5778     |\n","|    total_timesteps | 3934704  |\n","| train/             |          |\n","|    actor_loss      | 15.5     |\n","|    critic_loss     | 47.5     |\n","|    ent_coef        | 0.0101   |\n","|    ent_coef_loss   | -1.99    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163941   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44560    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5780     |\n","|    total_timesteps | 3935832  |\n","| train/             |          |\n","|    actor_loss      | 12.7     |\n","|    critic_loss     | 10.5     |\n","|    ent_coef        | 0.00974  |\n","|    ent_coef_loss   | -0.675   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163988   |\n","---------------------------------\n","Eval num_timesteps=3936000, episode_reward=-112.09 +/- 13.17\n","Episode length: 97.20 +/- 13.72\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 97.2     |\n","|    mean_reward     | -112     |\n","| time/              |          |\n","|    total_timesteps | 3936000  |\n","| train/             |          |\n","|    actor_loss      | 12.8     |\n","|    critic_loss     | 26.3     |\n","|    ent_coef        | 0.00973  |\n","|    ent_coef_loss   | -2.95    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44570    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5782     |\n","|    total_timesteps | 3937128  |\n","| train/             |          |\n","|    actor_loss      | 13.3     |\n","|    critic_loss     | 2.95     |\n","|    ent_coef        | 0.00991  |\n","|    ent_coef_loss   | -0.722   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164042   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44580    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5782     |\n","|    total_timesteps | 3937392  |\n","| train/             |          |\n","|    actor_loss      | 14.7     |\n","|    critic_loss     | 57.3     |\n","|    ent_coef        | 0.00991  |\n","|    ent_coef_loss   | 1.74     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164053   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44590    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5785     |\n","|    total_timesteps | 3939456  |\n","| train/             |          |\n","|    actor_loss      | 13.1     |\n","|    critic_loss     | 34.3     |\n","|    ent_coef        | 0.0096   |\n","|    ent_coef_loss   | 0.902    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164139   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44600    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5786     |\n","|    total_timesteps | 3939984  |\n","| train/             |          |\n","|    actor_loss      | 14       |\n","|    critic_loss     | 9.14     |\n","|    ent_coef        | 0.00971  |\n","|    ent_coef_loss   | 0.324    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164161   |\n","---------------------------------\n","Eval num_timesteps=3940800, episode_reward=-43.68 +/- 50.94\n","Episode length: 339.20 +/- 196.94\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 339      |\n","|    mean_reward     | -43.7    |\n","| time/              |          |\n","|    total_timesteps | 3940800  |\n","| train/             |          |\n","|    actor_loss      | 15.1     |\n","|    critic_loss     | 30       |\n","|    ent_coef        | 0.00974  |\n","|    ent_coef_loss   | 2.15     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44610    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5797     |\n","|    total_timesteps | 3941112  |\n","| train/             |          |\n","|    actor_loss      | 15.4     |\n","|    critic_loss     | 19.8     |\n","|    ent_coef        | 0.00972  |\n","|    ent_coef_loss   | 2.57     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164208   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44620    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5801     |\n","|    total_timesteps | 3943272  |\n","| train/             |          |\n","|    actor_loss      | 13.4     |\n","|    critic_loss     | 26.7     |\n","|    ent_coef        | 0.00984  |\n","|    ent_coef_loss   | 1.86     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164298   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44630    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5801     |\n","|    total_timesteps | 3943608  |\n","| train/             |          |\n","|    actor_loss      | 14.6     |\n","|    critic_loss     | 49       |\n","|    ent_coef        | 0.00998  |\n","|    ent_coef_loss   | 3.2      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164312   |\n","---------------------------------\n","Eval num_timesteps=3945600, episode_reward=-114.46 +/- 10.21\n","Episode length: 97.00 +/- 7.35\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 97       |\n","|    mean_reward     | -114     |\n","| time/              |          |\n","|    total_timesteps | 3945600  |\n","| train/             |          |\n","|    actor_loss      | 13.9     |\n","|    critic_loss     | 37.3     |\n","|    ent_coef        | 0.0113   |\n","|    ent_coef_loss   | 1.39     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44640    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5805     |\n","|    total_timesteps | 3946008  |\n","| train/             |          |\n","|    actor_loss      | 12.2     |\n","|    critic_loss     | 10.6     |\n","|    ent_coef        | 0.0113   |\n","|    ent_coef_loss   | -2.03    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164412   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44650    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5807     |\n","|    total_timesteps | 3947304  |\n","| train/             |          |\n","|    actor_loss      | 13.7     |\n","|    critic_loss     | 21.1     |\n","|    ent_coef        | 0.0111   |\n","|    ent_coef_loss   | -0.78    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164466   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44660    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5807     |\n","|    total_timesteps | 3947832  |\n","| train/             |          |\n","|    actor_loss      | 12       |\n","|    critic_loss     | 36.6     |\n","|    ent_coef        | 0.0112   |\n","|    ent_coef_loss   | -0.701   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164488   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44670    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5808     |\n","|    total_timesteps | 3948552  |\n","| train/             |          |\n","|    actor_loss      | 13.8     |\n","|    critic_loss     | 37.6     |\n","|    ent_coef        | 0.0111   |\n","|    ent_coef_loss   | -0.253   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164518   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44680    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5809     |\n","|    total_timesteps | 3949920  |\n","| train/             |          |\n","|    actor_loss      | 14.2     |\n","|    critic_loss     | 13.3     |\n","|    ent_coef        | 0.0108   |\n","|    ent_coef_loss   | -3.24    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164575   |\n","---------------------------------\n","Eval num_timesteps=3950400, episode_reward=-113.71 +/- 11.02\n","Episode length: 74.60 +/- 9.31\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 74.6     |\n","|    mean_reward     | -114     |\n","| time/              |          |\n","|    total_timesteps | 3950400  |\n","| train/             |          |\n","|    actor_loss      | 12.8     |\n","|    critic_loss     | 24.2     |\n","|    ent_coef        | 0.0105   |\n","|    ent_coef_loss   | -0.93    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44690    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5811     |\n","|    total_timesteps | 3950544  |\n","| train/             |          |\n","|    actor_loss      | 14.5     |\n","|    critic_loss     | 28.9     |\n","|    ent_coef        | 0.0104   |\n","|    ent_coef_loss   | 0.648    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164601   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44700    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5812     |\n","|    total_timesteps | 3951984  |\n","| train/             |          |\n","|    actor_loss      | 12.8     |\n","|    critic_loss     | 3.45     |\n","|    ent_coef        | 0.01     |\n","|    ent_coef_loss   | -3.46    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164661   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44710    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5813     |\n","|    total_timesteps | 3952416  |\n","| train/             |          |\n","|    actor_loss      | 15.8     |\n","|    critic_loss     | 33.3     |\n","|    ent_coef        | 0.0098   |\n","|    ent_coef_loss   | -1.35    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164679   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44720    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5813     |\n","|    total_timesteps | 3952920  |\n","| train/             |          |\n","|    actor_loss      | 14.6     |\n","|    critic_loss     | 38       |\n","|    ent_coef        | 0.0096   |\n","|    ent_coef_loss   | 0.271    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164700   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44730    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5815     |\n","|    total_timesteps | 3954528  |\n","| train/             |          |\n","|    actor_loss      | 15.3     |\n","|    critic_loss     | 9.8      |\n","|    ent_coef        | 0.00944  |\n","|    ent_coef_loss   | 1.34     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164767   |\n","---------------------------------\n","Eval num_timesteps=3955200, episode_reward=-106.96 +/- 9.87\n","Episode length: 71.00 +/- 9.80\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 71       |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 3955200  |\n","| train/             |          |\n","|    actor_loss      | 14.7     |\n","|    critic_loss     | 15.7     |\n","|    ent_coef        | 0.00958  |\n","|    ent_coef_loss   | 0.986    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44740    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5818     |\n","|    total_timesteps | 3955248  |\n","| train/             |          |\n","|    actor_loss      | 14.5     |\n","|    critic_loss     | 22.8     |\n","|    ent_coef        | 0.00958  |\n","|    ent_coef_loss   | 0.925    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164797   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44750    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5820     |\n","|    total_timesteps | 3956448  |\n","| train/             |          |\n","|    actor_loss      | 14       |\n","|    critic_loss     | 6.76     |\n","|    ent_coef        | 0.00954  |\n","|    ent_coef_loss   | 1.62     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164847   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44760    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5820     |\n","|    total_timesteps | 3956640  |\n","| train/             |          |\n","|    actor_loss      | 14.2     |\n","|    critic_loss     | 11.2     |\n","|    ent_coef        | 0.00959  |\n","|    ent_coef_loss   | 0.667    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164855   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44770    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5823     |\n","|    total_timesteps | 3958152  |\n","| train/             |          |\n","|    actor_loss      | 12.9     |\n","|    critic_loss     | 4.96     |\n","|    ent_coef        | 0.0101   |\n","|    ent_coef_loss   | -2.92    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164918   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44780    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5824     |\n","|    total_timesteps | 3959448  |\n","| train/             |          |\n","|    actor_loss      | 11.5     |\n","|    critic_loss     | 6.08     |\n","|    ent_coef        | 0.0101   |\n","|    ent_coef_loss   | -0.997   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164972   |\n","---------------------------------\n","Eval num_timesteps=3960000, episode_reward=-112.12 +/- 8.76\n","Episode length: 91.60 +/- 12.74\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 91.6     |\n","|    mean_reward     | -112     |\n","| time/              |          |\n","|    total_timesteps | 3960000  |\n","| train/             |          |\n","|    actor_loss      | 13.8     |\n","|    critic_loss     | 10.5     |\n","|    ent_coef        | 0.01     |\n","|    ent_coef_loss   | 0.511    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44790    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5827     |\n","|    total_timesteps | 3960960  |\n","| train/             |          |\n","|    actor_loss      | 14.1     |\n","|    critic_loss     | 3.91     |\n","|    ent_coef        | 0.00978  |\n","|    ent_coef_loss   | -0.928   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165035   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44800    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5828     |\n","|    total_timesteps | 3961512  |\n","| train/             |          |\n","|    actor_loss      | 13.3     |\n","|    critic_loss     | 14       |\n","|    ent_coef        | 0.00975  |\n","|    ent_coef_loss   | -0.205   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165058   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44810    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5829     |\n","|    total_timesteps | 3962880  |\n","| train/             |          |\n","|    actor_loss      | 14.1     |\n","|    critic_loss     | 16.1     |\n","|    ent_coef        | 0.00986  |\n","|    ent_coef_loss   | 2.84     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165115   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44820    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5829     |\n","|    total_timesteps | 3963312  |\n","| train/             |          |\n","|    actor_loss      | 12.8     |\n","|    critic_loss     | 46.9     |\n","|    ent_coef        | 0.0101   |\n","|    ent_coef_loss   | 2.57     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165133   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44830    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5830     |\n","|    total_timesteps | 3964152  |\n","| train/             |          |\n","|    actor_loss      | 13       |\n","|    critic_loss     | 4.61     |\n","|    ent_coef        | 0.0103   |\n","|    ent_coef_loss   | 0.944    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165168   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44840    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5831     |\n","|    total_timesteps | 3964656  |\n","| train/             |          |\n","|    actor_loss      | 13.9     |\n","|    critic_loss     | 36.5     |\n","|    ent_coef        | 0.0103   |\n","|    ent_coef_loss   | 0.252    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165189   |\n","---------------------------------\n","Eval num_timesteps=3964800, episode_reward=-109.62 +/- 11.13\n","Episode length: 76.40 +/- 1.96\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 76.4     |\n","|    mean_reward     | -110     |\n","| time/              |          |\n","|    total_timesteps | 3964800  |\n","| train/             |          |\n","|    actor_loss      | 14.7     |\n","|    critic_loss     | 23.4     |\n","|    ent_coef        | 0.0103   |\n","|    ent_coef_loss   | 0.869    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44850    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5832     |\n","|    total_timesteps | 3964944  |\n","| train/             |          |\n","|    actor_loss      | 14.4     |\n","|    critic_loss     | 31.5     |\n","|    ent_coef        | 0.0103   |\n","|    ent_coef_loss   | 1.44     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165201   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44860    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5833     |\n","|    total_timesteps | 3966432  |\n","| train/             |          |\n","|    actor_loss      | 15.6     |\n","|    critic_loss     | 55.9     |\n","|    ent_coef        | 0.0105   |\n","|    ent_coef_loss   | -0.0431  |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165263   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44870    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5833     |\n","|    total_timesteps | 3966768  |\n","| train/             |          |\n","|    actor_loss      | 13       |\n","|    critic_loss     | 58       |\n","|    ent_coef        | 0.0105   |\n","|    ent_coef_loss   | 0.577    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165277   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44880    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5834     |\n","|    total_timesteps | 3967440  |\n","| train/             |          |\n","|    actor_loss      | 14       |\n","|    critic_loss     | 20.3     |\n","|    ent_coef        | 0.0105   |\n","|    ent_coef_loss   | 0.885    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165305   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44890    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5836     |\n","|    total_timesteps | 3968592  |\n","| train/             |          |\n","|    actor_loss      | 15.6     |\n","|    critic_loss     | 46.9     |\n","|    ent_coef        | 0.0107   |\n","|    ent_coef_loss   | -1.88    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165353   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44900    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5837     |\n","|    total_timesteps | 3969264  |\n","| train/             |          |\n","|    actor_loss      | 13.9     |\n","|    critic_loss     | 12.2     |\n","|    ent_coef        | 0.0105   |\n","|    ent_coef_loss   | 2.12     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165381   |\n","---------------------------------\n","Eval num_timesteps=3969600, episode_reward=-104.48 +/- 4.80\n","Episode length: 92.40 +/- 6.86\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 92.4     |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 3969600  |\n","| train/             |          |\n","|    actor_loss      | 13.9     |\n","|    critic_loss     | 28.1     |\n","|    ent_coef        | 0.0105   |\n","|    ent_coef_loss   | -1.06    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44910    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5841     |\n","|    total_timesteps | 3970704  |\n","| train/             |          |\n","|    actor_loss      | 14.3     |\n","|    critic_loss     | 38       |\n","|    ent_coef        | 0.0102   |\n","|    ent_coef_loss   | 0.369    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165441   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44920    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5842     |\n","|    total_timesteps | 3971328  |\n","| train/             |          |\n","|    actor_loss      | 14.8     |\n","|    critic_loss     | 36.4     |\n","|    ent_coef        | 0.01     |\n","|    ent_coef_loss   | -1.11    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165467   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44930    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5843     |\n","|    total_timesteps | 3972144  |\n","| train/             |          |\n","|    actor_loss      | 15.5     |\n","|    critic_loss     | 33       |\n","|    ent_coef        | 0.00965  |\n","|    ent_coef_loss   | 2.4      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165501   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44940    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5845     |\n","|    total_timesteps | 3973272  |\n","| train/             |          |\n","|    actor_loss      | 13.3     |\n","|    critic_loss     | 34.7     |\n","|    ent_coef        | 0.01     |\n","|    ent_coef_loss   | 0.22     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165548   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44950    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5846     |\n","|    total_timesteps | 3974040  |\n","| train/             |          |\n","|    actor_loss      | 13.8     |\n","|    critic_loss     | 18.5     |\n","|    ent_coef        | 0.0102   |\n","|    ent_coef_loss   | 0.196    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165580   |\n","---------------------------------\n","Eval num_timesteps=3974400, episode_reward=-107.59 +/- 7.08\n","Episode length: 75.20 +/- 1.47\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 75.2     |\n","|    mean_reward     | -108     |\n","| time/              |          |\n","|    total_timesteps | 3974400  |\n","| train/             |          |\n","|    actor_loss      | 14.2     |\n","|    critic_loss     | 16.2     |\n","|    ent_coef        | 0.0102   |\n","|    ent_coef_loss   | 1.38     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44960    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5847     |\n","|    total_timesteps | 3974520  |\n","| train/             |          |\n","|    actor_loss      | 14.4     |\n","|    critic_loss     | 11.7     |\n","|    ent_coef        | 0.0102   |\n","|    ent_coef_loss   | 1.68     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165600   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44970    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5848     |\n","|    total_timesteps | 3975000  |\n","| train/             |          |\n","|    actor_loss      | 13.5     |\n","|    critic_loss     | 27.6     |\n","|    ent_coef        | 0.0103   |\n","|    ent_coef_loss   | 0.927    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165620   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44980    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5849     |\n","|    total_timesteps | 3976584  |\n","| train/             |          |\n","|    actor_loss      | 13.6     |\n","|    critic_loss     | 12.5     |\n","|    ent_coef        | 0.0108   |\n","|    ent_coef_loss   | -0.388   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165686   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 44990    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5850     |\n","|    total_timesteps | 3976992  |\n","| train/             |          |\n","|    actor_loss      | 14.1     |\n","|    critic_loss     | 16.6     |\n","|    ent_coef        | 0.0109   |\n","|    ent_coef_loss   | 1.13     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165703   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 45000    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5851     |\n","|    total_timesteps | 3978144  |\n","| train/             |          |\n","|    actor_loss      | 12.5     |\n","|    critic_loss     | 11.4     |\n","|    ent_coef        | 0.0109   |\n","|    ent_coef_loss   | -1.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165751   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 45010    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5851     |\n","|    total_timesteps | 3978768  |\n","| train/             |          |\n","|    actor_loss      | 13.8     |\n","|    critic_loss     | 14.9     |\n","|    ent_coef        | 0.011    |\n","|    ent_coef_loss   | -1.5     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165777   |\n","---------------------------------\n","Eval num_timesteps=3979200, episode_reward=-111.50 +/- 9.84\n","Episode length: 87.20 +/- 30.86\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 87.2     |\n","|    mean_reward     | -111     |\n","| time/              |          |\n","|    total_timesteps | 3979200  |\n","| train/             |          |\n","|    actor_loss      | 14.2     |\n","|    critic_loss     | 27.7     |\n","|    ent_coef        | 0.0111   |\n","|    ent_coef_loss   | 2.34     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 45020    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5854     |\n","|    total_timesteps | 3980448  |\n","| train/             |          |\n","|    actor_loss      | 13.8     |\n","|    critic_loss     | 19.5     |\n","|    ent_coef        | 0.0109   |\n","|    ent_coef_loss   | 0.959    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165847   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 45030    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5855     |\n","|    total_timesteps | 3981000  |\n","| train/             |          |\n","|    actor_loss      | 13.8     |\n","|    critic_loss     | 29.8     |\n","|    ent_coef        | 0.011    |\n","|    ent_coef_loss   | 0.325    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165870   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 45040    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5856     |\n","|    total_timesteps | 3982176  |\n","| train/             |          |\n","|    actor_loss      | 12.7     |\n","|    critic_loss     | 8.3      |\n","|    ent_coef        | 0.0107   |\n","|    ent_coef_loss   | -2.95    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165919   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 45050    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5857     |\n","|    total_timesteps | 3982944  |\n","| train/             |          |\n","|    actor_loss      | 15.3     |\n","|    critic_loss     | 27.1     |\n","|    ent_coef        | 0.0103   |\n","|    ent_coef_loss   | -2.16    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165951   |\n","---------------------------------\n","Eval num_timesteps=3984000, episode_reward=-0.62 +/- 0.15\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.623   |\n","| time/              |          |\n","|    total_timesteps | 3984000  |\n","| train/             |          |\n","|    actor_loss      | 14.2     |\n","|    critic_loss     | 19.3     |\n","|    ent_coef        | 0.00993  |\n","|    ent_coef_loss   | 0.354    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165995   |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    episodes        | 45060   |\n","|    fps             | 678     |\n","|    time_elapsed    | 5868    |\n","|    total_timesteps | 3984000 |\n","--------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 45070    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5870     |\n","|    total_timesteps | 3986112  |\n","| train/             |          |\n","|    actor_loss      | 14.3     |\n","|    critic_loss     | 11.2     |\n","|    ent_coef        | 0.00959  |\n","|    ent_coef_loss   | 2.81     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166083   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 45080    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5871     |\n","|    total_timesteps | 3987504  |\n","| train/             |          |\n","|    actor_loss      | 14       |\n","|    critic_loss     | 4.68     |\n","|    ent_coef        | 0.0096   |\n","|    ent_coef_loss   | 0.321    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166141   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 45090    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5872     |\n","|    total_timesteps | 3988416  |\n","| train/             |          |\n","|    actor_loss      | 12.3     |\n","|    critic_loss     | 13.3     |\n","|    ent_coef        | 0.00979  |\n","|    ent_coef_loss   | 2.7      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166179   |\n","---------------------------------\n","Eval num_timesteps=3988800, episode_reward=-111.92 +/- 7.35\n","Episode length: 98.40 +/- 7.84\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 98.4     |\n","|    mean_reward     | -112     |\n","| time/              |          |\n","|    total_timesteps | 3988800  |\n","| train/             |          |\n","|    actor_loss      | 10.6     |\n","|    critic_loss     | 7.23     |\n","|    ent_coef        | 0.00999  |\n","|    ent_coef_loss   | -4.49    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 45100    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5875     |\n","|    total_timesteps | 3990288  |\n","| train/             |          |\n","|    actor_loss      | 13.4     |\n","|    critic_loss     | 32.1     |\n","|    ent_coef        | 0.00992  |\n","|    ent_coef_loss   | 1.37     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166257   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 45110    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5875     |\n","|    total_timesteps | 3990912  |\n","| train/             |          |\n","|    actor_loss      | 14.3     |\n","|    critic_loss     | 10.7     |\n","|    ent_coef        | 0.00997  |\n","|    ent_coef_loss   | 1.48     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166283   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 45120    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5876     |\n","|    total_timesteps | 3991440  |\n","| train/             |          |\n","|    actor_loss      | 13.7     |\n","|    critic_loss     | 42.4     |\n","|    ent_coef        | 0.01     |\n","|    ent_coef_loss   | 1.39     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166305   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 45130    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5877     |\n","|    total_timesteps | 3992808  |\n","| train/             |          |\n","|    actor_loss      | 15.2     |\n","|    critic_loss     | 27.5     |\n","|    ent_coef        | 0.0101   |\n","|    ent_coef_loss   | 0.457    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166362   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 45140    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5878     |\n","|    total_timesteps | 3993528  |\n","| train/             |          |\n","|    actor_loss      | 13.1     |\n","|    critic_loss     | 17.1     |\n","|    ent_coef        | 0.01     |\n","|    ent_coef_loss   | -0.0916  |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166392   |\n","---------------------------------\n","Eval num_timesteps=3993600, episode_reward=-110.45 +/- 9.56\n","Episode length: 91.20 +/- 10.78\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 91.2     |\n","|    mean_reward     | -110     |\n","| time/              |          |\n","|    total_timesteps | 3993600  |\n","| train/             |          |\n","|    actor_loss      | 13.2     |\n","|    critic_loss     | 16.6     |\n","|    ent_coef        | 0.01     |\n","|    ent_coef_loss   | 0.338    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 45150    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5882     |\n","|    total_timesteps | 3994584  |\n","| train/             |          |\n","|    actor_loss      | 12.8     |\n","|    critic_loss     | 41.4     |\n","|    ent_coef        | 0.0101   |\n","|    ent_coef_loss   | -2.98    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166436   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 45160    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5883     |\n","|    total_timesteps | 3995664  |\n","| train/             |          |\n","|    actor_loss      | 14       |\n","|    critic_loss     | 27.3     |\n","|    ent_coef        | 0.00996  |\n","|    ent_coef_loss   | 1.21     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166481   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 45170    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5885     |\n","|    total_timesteps | 3996576  |\n","| train/             |          |\n","|    actor_loss      | 14.2     |\n","|    critic_loss     | 18.3     |\n","|    ent_coef        | 0.0102   |\n","|    ent_coef_loss   | 4.41     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166519   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 45180    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5886     |\n","|    total_timesteps | 3997104  |\n","| train/             |          |\n","|    actor_loss      | 14       |\n","|    critic_loss     | 34.8     |\n","|    ent_coef        | 0.0106   |\n","|    ent_coef_loss   | 0.677    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166541   |\n","---------------------------------\n","Eval num_timesteps=3998400, episode_reward=-110.18 +/- 5.53\n","Episode length: 78.20 +/- 8.33\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 78.2     |\n","|    mean_reward     | -110     |\n","| time/              |          |\n","|    total_timesteps | 3998400  |\n","| train/             |          |\n","|    actor_loss      | 14.9     |\n","|    critic_loss     | 38.2     |\n","|    ent_coef        | 0.0111   |\n","|    ent_coef_loss   | 0.941    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 45190    |\n","|    fps             | 678      |\n","|    time_elapsed    | 5889     |\n","|    total_timesteps | 3998640  |\n","| train/             |          |\n","|    actor_loss      | 11.8     |\n","|    critic_loss     | 19.7     |\n","|    ent_coef        | 0.0111   |\n","|    ent_coef_loss   | -0.638   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166605   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 45200    |\n","|    fps             | 678      |\n","|    time_elapsed    | 5890     |\n","|    total_timesteps | 3999768  |\n","| train/             |          |\n","|    actor_loss      | 13       |\n","|    critic_loss     | 19.3     |\n","|    ent_coef        | 0.0116   |\n","|    ent_coef_loss   | 0.00918  |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166652   |\n","---------------------------------\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"s_YTQjcU9ANN"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"mount_file_id":"1tNH0xWOkEmzP-ic_xuZPO9HHOk_62LLx","authorship_tag":"ABX9TyMsdtHLEiwuWNqH0KjTYxHz"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}