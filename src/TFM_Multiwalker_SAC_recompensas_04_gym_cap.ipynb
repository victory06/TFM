{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"d8JAmEUyj9De"},"outputs":[],"source":["# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n","mount='/content/gdrive'\n","drive_root = mount + \"/My Drive/TFM\"\n","\n","try:\n","  from google.colab import drive\n","  IN_COLAB=True\n","except:\n","  IN_COLAB=False"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19422,"status":"ok","timestamp":1699027447823,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-60},"id":"DrEo9QnxkAne","outputId":"e1d79824-64d4-45ac-9f41-0679cf3576fa"},"outputs":[{"output_type":"stream","name":"stdout","text":["We're running Colab\n","Colab: mounting Google drive on  /content/gdrive\n","Mounted at /content/gdrive\n","\n","Colab: making sure  /content/gdrive/My Drive/TFM  exists.\n","\n","Colab: Changing directory to  /content/gdrive/My Drive/TFM\n","/content/gdrive/My Drive/TFM\n","Archivos en el directorio: \n","['TFM_Multiwalker_SAC_recompensas_00_gym_cap.ipynb', 'Entrenamientos antiguos sin logs', '.ipynb_checkpoints', 'Entrenamientos_log_no_eval', 'PPO_policies', 'DQN_new_pettingzoo_gym_cap.ipynb', 'multi_car_racing', 'policy_log_eval', 'DQN_policies', 'results_rllib', 'MCR_TFM.ipynb', 'multiwalker_ddpg_log_eval', 'multiwalker_sac_log_eval', 'multiwalker_ddpg.zip', 'multiwalker_ppo_log_eval', 'multiwalker_ppo.zip', 'multiwalker_td3_log_eval', 'multiwalker_sac2_log_eval', 'multiwalker_td3_2_log_eval', 'multiwalker_sac3_log_eval', 'multiwalker_sac3.zip', 'multiwalker_ppo_2_log_eval', 'multiwalker_ddpg2_log_eval', 'multiwalker_ppo_2.zip', 'TFM_Multiwalker_DDPG_PPO_gym_cap.ipynb', 'multiwalker_td3_3_log_eval', 'TFM_Multiwalker_TD3_gym_cap.ipynb', 'TFM_Multiwalker_SAC_gym_cap.ipynb', 'TFM_PPO_KAZ_gym_cap.ipynb', 'TFM_PPO_new_KAZ_gym_cap.ipynb', 'TFM_Multiwalker_DDPG_recompensas_gym_cap.ipynb', 'multiwalker_ddpg2_5_log_eval', 'multiwalker_ppo_rew_08_log_eval', 'multiwalker_ppo_rew_08.zip', 'multiwalker_ppo_08_2_log_eval', 'multiwalker_ddpg2_6_log_eval', 'multiwalker_ppo_08_2.zip', 'multiwalker_sac_08_log_eval', 'TFM_Multiwalker_DDPG_gym_cap.ipynb', 'multiwalker_sac2_08_log_eval', 'multiwalker_ppo_rew_04_log_eval', 'multiwalker_ppo_rew_04.zip', 'multiwalker_ppo_04_2_log_eval', 'multiwalker_ppo_04_2.zip', 'multiwalker_ppo_rew_0_log_eval', 'multiwalker_sac3_08_log_eval', 'multiwalker_ppo_rew_0.zip', 'multiwalker_ppo_0_2_log_eval', 'multiwalker_sac3_08.zip', 'TFM_Multiwalker_SAC_recompensas_gym_cap.ipynb', 'multiwalker_ppo_0_2.zip', 'TFM_Multiwalker_PPO_recompensas_gym_cap.ipynb', 'multiwalker_sac_00_log_eval', 'multiwalker_sac_04_log_eval', 'multiwalker_sac_00.zip', '=2.13', 'multiwalker_sac2_00_log_eval', 'multiwalker_sac2_04_log_eval', 'multiwalker_sac2_04.zip', 'TFM_Multiwalker_SAC_recompensas_04_gym_cap.ipynb']\n"]}],"source":["# Switch to the directory on the Google Drive that you want to use\n","import os\n","if IN_COLAB:\n","  print(\"We're running Colab\")\n","\n","  if IN_COLAB:\n","    # Mount the Google Drive at mount\n","    print(\"Colab: mounting Google drive on \", mount)\n","\n","    drive.mount(mount)\n","\n","    # Create drive_root if it doesn't exist\n","    create_drive_root = True\n","    if create_drive_root:\n","      print(\"\\nColab: making sure \", drive_root, \" exists.\")\n","      os.makedirs(drive_root, exist_ok=True)\n","\n","    # Change to the directory\n","    print(\"\\nColab: Changing directory to \", drive_root)\n","    %cd $drive_root\n","# Verify we're in the correct working directory\n","%pwd\n","print(\"Archivos en el directorio: \")\n","print(os.listdir())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xAZDg478kEbs","outputId":"cf3cca28-d2b2-4642-c9d8-402f7ebc587d","executionInfo":{"status":"ok","timestamp":1699027706091,"user_tz":-60,"elapsed":258276,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gym==0.17.3\n","  Downloading gym-0.17.3.tar.gz (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym==0.17.3) (1.11.3)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.17.3) (1.23.5)\n","Collecting pyglet<=1.5.0,>=1.4.0 (from gym==0.17.3)\n","  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cloudpickle<1.7.0,>=1.2.0 (from gym==0.17.3)\n","  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3) (0.18.3)\n","Building wheels for collected packages: gym\n","  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym: filename=gym-0.17.3-py3-none-any.whl size=1654617 sha256=d90f45fd8e5c08c33f3c68aab61fc1b560fe0d88bb5c1146134fd522f2710eb1\n","  Stored in directory: /root/.cache/pip/wheels/af/4b/74/fcfc8238472c34d7f96508a63c962ff3ac9485a9a4137afd4e\n","Successfully built gym\n","Installing collected packages: pyglet, cloudpickle, gym\n","  Attempting uninstall: cloudpickle\n","    Found existing installation: cloudpickle 2.2.1\n","    Uninstalling cloudpickle-2.2.1:\n","      Successfully uninstalled cloudpickle-2.2.1\n","  Attempting uninstall: gym\n","    Found existing installation: gym 0.25.2\n","    Uninstalling gym-0.25.2:\n","      Successfully uninstalled gym-0.25.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","bigframes 0.10.0 requires cloudpickle>=2.0.0, but you have cloudpickle 1.6.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed cloudpickle-1.6.0 gym-0.17.3 pyglet-1.5.0\n","Collecting git+https://github.com/Kojoley/atari-py.git\n","  Cloning https://github.com/Kojoley/atari-py.git to /tmp/pip-req-build-_t3m3ik9\n","  Running command git clone --filter=blob:none --quiet https://github.com/Kojoley/atari-py.git /tmp/pip-req-build-_t3m3ik9\n","  Resolved https://github.com/Kojoley/atari-py.git to commit 86a1e05c0a95e9e6233c3a413521fdb34ca8a089\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from atari-py==1.2.2) (1.23.5)\n","Building wheels for collected packages: atari-py\n","  Building wheel for atari-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for atari-py: filename=atari_py-1.2.2-cp310-cp310-linux_x86_64.whl size=4738553 sha256=97275765897ced84037008f4cbf773af87d8e707bb3f8f32a95a030570ded2d5\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-jxadlxcs/wheels/6c/9b/f1/8a80a63a233d6f4eb2e2ee8c7357f4875f21c3ffc7f2613f9f\n","Successfully built atari-py\n","Installing collected packages: atari-py\n","Successfully installed atari-py-1.2.2\n","Collecting keras-rl2==1.0.5\n","  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from keras-rl2==1.0.5) (2.14.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (23.5.26)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (16.0.6)\n","Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.2.0)\n","Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.23.5)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (23.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (4.5.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.34.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.59.0)\n","Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.14.1)\n","Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.14.0)\n","Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.14.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2==1.0.5) (0.41.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.2.2)\n","Installing collected packages: keras-rl2\n","Successfully installed keras-rl2-1.0.5\n","Collecting tensorflow==2.8\n","  Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl (497.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.6.3)\n","Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (23.5.26)\n","Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.9.0)\n","Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.8)\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (16.0.6)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.23.5)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.3.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (4.5.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.14.1)\n","Collecting tensorboard<2.9,>=2.8 (from tensorflow==2.8)\n","  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tf-estimator-nightly==2.8.0.dev2021122109 (from tensorflow==2.8)\n","  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting keras<2.9,>=2.8.0rc0 (from tensorflow==2.8)\n","  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.34.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.59.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.8) (0.41.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.17.3)\n","Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.31.0)\n","Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n","  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n","  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.2.2)\n","Installing collected packages: tf-estimator-nightly, tensorboard-plugin-wit, keras, tensorboard-data-server, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.14.0\n","    Uninstalling keras-2.14.0:\n","      Successfully uninstalled keras-2.14.0\n","  Attempting uninstall: tensorboard-data-server\n","    Found existing installation: tensorboard-data-server 0.7.2\n","    Uninstalling tensorboard-data-server-0.7.2:\n","      Successfully uninstalled tensorboard-data-server-0.7.2\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 1.0.0\n","    Uninstalling google-auth-oauthlib-1.0.0:\n","      Successfully uninstalled google-auth-oauthlib-1.0.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.14.1\n","    Uninstalling tensorboard-2.14.1:\n","      Successfully uninstalled tensorboard-2.14.1\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.14.0\n","    Uninstalling tensorflow-2.14.0:\n","      Successfully uninstalled tensorflow-2.14.0\n","Successfully installed google-auth-oauthlib-0.4.6 keras-2.8.0 keras-preprocessing-1.1.2 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tf-estimator-nightly-2.8.0.dev2021122109\n","Collecting gym-cap\n","  Downloading gym_cap-0.1.4.12-py3-none-any.whl (32 kB)\n","Requirement already satisfied: gym>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from gym-cap) (0.17.3)\n","Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from gym-cap) (1.23.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym>=0.16.0->gym-cap) (1.11.3)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.16.0->gym-cap) (1.5.0)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.16.0->gym-cap) (1.6.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.16.0->gym-cap) (0.18.3)\n","Installing collected packages: gym-cap\n","Successfully installed gym-cap-0.1.4.12\n","Collecting pettingzoo[butterfly]\n","  Downloading pettingzoo-1.24.1-py3-none-any.whl (840 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.8/840.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[butterfly]) (1.23.5)\n","Collecting gymnasium>=0.28.0 (from pettingzoo[butterfly])\n","  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pygame==2.3.0 (from pettingzoo[butterfly])\n","  Downloading pygame-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pymunk==6.2.0 (from pettingzoo[butterfly])\n","  Downloading pymunk-6.2.0.zip (7.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: cffi>1.14.0 in /usr/local/lib/python3.10/dist-packages (from pymunk==6.2.0->pettingzoo[butterfly]) (1.16.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[butterfly]) (1.6.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[butterfly]) (4.5.0)\n","Collecting farama-notifications>=0.0.1 (from gymnasium>=0.28.0->pettingzoo[butterfly])\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>1.14.0->pymunk==6.2.0->pettingzoo[butterfly]) (2.21)\n","Building wheels for collected packages: pymunk\n","  Building wheel for pymunk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pymunk: filename=pymunk-6.2.0-cp310-cp310-linux_x86_64.whl size=801640 sha256=faea06a7c617e532595a64059069aa98cb57f190cba0d36cb9a02672a565d9a4\n","  Stored in directory: /root/.cache/pip/wheels/2e/81/a2/f941a9ff417bb4020c37ae218fb7117d12d3fc019ea493d66f\n","Successfully built pymunk\n","Installing collected packages: farama-notifications, pygame, gymnasium, pymunk, pettingzoo\n","  Attempting uninstall: pygame\n","    Found existing installation: pygame 2.5.2\n","    Uninstalling pygame-2.5.2:\n","      Successfully uninstalled pygame-2.5.2\n","Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1 pettingzoo-1.24.1 pygame-2.3.0 pymunk-6.2.0\n"]}],"source":["if IN_COLAB:\n","  %pip install gym==0.17.3\n","  %pip install git+https://github.com/Kojoley/atari-py.git\n","  %pip install keras-rl2==1.0.5\n","  %pip install tensorflow==2.8\n","  %pip install gym-cap\n","  %pip install pettingzoo[butterfly]\n","else:\n","  %pip install gym==0.17.3\n","  %pip install git+https://github.com/Kojoley/atari-py.git\n","  %pip install pyglet==1.5.0\n","  %pip install h5py==3.1.0\n","  %pip install Pillow==9.5.0\n","  %pip install keras-rl2==1.0.5\n","  %pip install Keras==2.2.4\n","  %pip install tensorflow==2.5.3\n","  %pip install torch==2.0.1\n","  %pip install agents==1.4.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G8cw-IX3laE9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699027706540,"user_tz":-60,"elapsed":499,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"53fb2380-e0b6-4279-dcc5-008e21f47cf6"},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'multi_car_racing' already exists and is not an empty directory.\n"]}],"source":["!git clone https://github.com/igilitschenski/multi_car_racing.git\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IqbMo3gK7vBG"},"outputs":[],"source":["!cd multi_car_racing\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jekec6f98b3A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699027712882,"user_tz":-60,"elapsed":6347,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"c4a201d3-8489-41ef-ef18-d52c43a09a37"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting swig\n","  Downloading swig-4.1.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: swig\n","Successfully installed swig-4.1.1\n"]}],"source":["!pip install swig"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KKxRPBFx85k6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699027779872,"user_tz":-60,"elapsed":66998,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"e310535e-6a53-4208-a9b2-e44d3d627c4f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting box2d\n","  Downloading Box2D-2.3.2.tar.gz (427 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/427.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/427.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.9/427.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: box2d\n","  Building wheel for box2d (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d: filename=Box2D-2.3.2-cp310-cp310-linux_x86_64.whl size=2391312 sha256=2312e8cd972247b736f6cdbc797bcabf327264c96d5f8409f6ace2b746049bc4\n","  Stored in directory: /root/.cache/pip/wheels/eb/cb/be/e663f3ce9aba6580611c0febaf7cd3cf7603f87047de2a52f9\n","Successfully built box2d\n","Installing collected packages: box2d\n","Successfully installed box2d-2.3.2\n"]}],"source":["!pip install box2d"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ijp5V0i09MRF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699027842903,"user_tz":-60,"elapsed":63039,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"3dd2b32b-5b7d-4fb4-feed-eb97d2b5a7ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting box2d-py\n","  Downloading box2d-py-2.3.8.tar.gz (374 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/374.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m276.5/374.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.5/374.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: box2d-py\n","  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-py: filename=box2d_py-2.3.8-cp310-cp310-linux_x86_64.whl size=2373128 sha256=bcf04daa05777b0fdf9a0a808e936f4fe0f854e685d3a30cb93f014287df7296\n","  Stored in directory: /root/.cache/pip/wheels/47/01/d2/6a780da77ccb98b1d2facdd520a8d10838a03b590f6f8d50c0\n","Successfully built box2d-py\n","Installing collected packages: box2d-py\n","Successfully installed box2d-py-2.3.8\n"]}],"source":["!pip install box2d-py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BwjugqI99g0I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699027870914,"user_tz":-60,"elapsed":28057,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"626add0c-67e1-49fe-c446-85931ba9139b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Obtaining file:///content/gdrive/MyDrive/TFM/multi_car_racing\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: box2d-py~=2.3.5 in /usr/local/lib/python3.10/dist-packages (from gym-multi-car-racing==0.0.1) (2.3.8)\n","Collecting shapely~=1.7.0 (from gym-multi-car-racing==0.0.1)\n","  Downloading Shapely-1.7.1.tar.gz (383 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.2/383.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from gym-multi-car-racing==0.0.1) (1.23.5)\n","Requirement already satisfied: gym~=0.17.2 in /usr/local/lib/python3.10/dist-packages (from gym-multi-car-racing==0.0.1) (0.17.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym~=0.17.2->gym-multi-car-racing==0.0.1) (1.11.3)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from gym~=0.17.2->gym-multi-car-racing==0.0.1) (1.5.0)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym~=0.17.2->gym-multi-car-racing==0.0.1) (1.6.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym~=0.17.2->gym-multi-car-racing==0.0.1) (0.18.3)\n","Building wheels for collected packages: shapely\n","  Building wheel for shapely (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for shapely: filename=Shapely-1.7.1-cp310-cp310-linux_x86_64.whl size=997156 sha256=049efd96f5f24d4e4261b7b8211227341f7b76304eb22952ce439913c32d7dab\n","  Stored in directory: /root/.cache/pip/wheels/2e/fa/97/c85f587c35afcaf4a81c481741d36592518d1e50445572f0d4\n","Successfully built shapely\n","Installing collected packages: shapely, gym-multi-car-racing\n","  Attempting uninstall: shapely\n","    Found existing installation: shapely 2.0.2\n","    Uninstalling shapely-2.0.2:\n","      Successfully uninstalled shapely-2.0.2\n","  Running setup.py develop for gym-multi-car-racing\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires fastapi, which is not installed.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed gym-multi-car-racing-0.0.1 shapely-1.7.1\n"]}],"source":["!pip install -e multi_car_racing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IrAvXzCW-Z3e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699027882061,"user_tz":-60,"elapsed":11194,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"986af4f5-c59f-446a-d087-d1419da1de5a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  freeglut3 libegl-dev libgl-dev libgl1-mesa-dev libgles-dev libgles1\n","  libglu1-mesa libglu1-mesa-dev libglvnd-core-dev libglvnd-dev libglx-dev\n","  libice-dev libopengl-dev libsm-dev libxt-dev\n","Suggested packages:\n","  libice-doc libsm-doc libxt-doc\n","The following NEW packages will be installed:\n","  freeglut3 freeglut3-dev libegl-dev libgl-dev libgl1-mesa-dev libgles-dev\n","  libgles1 libglu1-mesa libglu1-mesa-dev libglvnd-core-dev libglvnd-dev\n","  libglx-dev libice-dev libopengl-dev libsm-dev libxt-dev\n","0 upgraded, 16 newly installed, 0 to remove and 19 not upgraded.\n","Need to get 1,261 kB of archives.\n","After this operation, 6,753 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3 amd64 2.8.1-6 [74.0 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglx-dev amd64 1.4.0-1 [14.1 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgl-dev amd64 1.4.0-1 [101 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-core-dev amd64 1.4.0-1 [12.7 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libegl-dev amd64 1.4.0-1 [18.0 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles1 amd64 1.4.0-1 [11.5 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles-dev amd64 1.4.0-1 [49.4 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libopengl-dev amd64 1.4.0-1 [3,400 B]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-dev amd64 1.4.0-1 [3,162 B]\n","Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgl1-mesa-dev amd64 23.0.4-0ubuntu1~22.04.1 [6,510 B]\n","Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n","Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa-dev amd64 9.0.2-1 [231 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 libice-dev amd64 2:1.0.10-1build2 [51.4 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsm-dev amd64 2:1.2.3-1build2 [18.1 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3-dev amd64 2.8.1-6 [126 kB]\n","Fetched 1,261 kB in 2s (701 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 16.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package freeglut3:amd64.\n","(Reading database ... 120874 files and directories currently installed.)\n","Preparing to unpack .../00-freeglut3_2.8.1-6_amd64.deb ...\n","Unpacking freeglut3:amd64 (2.8.1-6) ...\n","Selecting previously unselected package libglx-dev:amd64.\n","Preparing to unpack .../01-libglx-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglx-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgl-dev:amd64.\n","Preparing to unpack .../02-libgl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libgl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libglvnd-core-dev:amd64.\n","Preparing to unpack .../03-libglvnd-core-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglvnd-core-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libegl-dev:amd64.\n","Preparing to unpack .../04-libegl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libegl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgles1:amd64.\n","Preparing to unpack .../05-libgles1_1.4.0-1_amd64.deb ...\n","Unpacking libgles1:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgles-dev:amd64.\n","Preparing to unpack .../06-libgles-dev_1.4.0-1_amd64.deb ...\n","Unpacking libgles-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libopengl-dev:amd64.\n","Preparing to unpack .../07-libopengl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libopengl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libglvnd-dev:amd64.\n","Preparing to unpack .../08-libglvnd-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglvnd-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgl1-mesa-dev:amd64.\n","Preparing to unpack .../09-libgl1-mesa-dev_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n","Unpacking libgl1-mesa-dev:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Selecting previously unselected package libglu1-mesa:amd64.\n","Preparing to unpack .../10-libglu1-mesa_9.0.2-1_amd64.deb ...\n","Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n","Selecting previously unselected package libglu1-mesa-dev:amd64.\n","Preparing to unpack .../11-libglu1-mesa-dev_9.0.2-1_amd64.deb ...\n","Unpacking libglu1-mesa-dev:amd64 (9.0.2-1) ...\n","Selecting previously unselected package libice-dev:amd64.\n","Preparing to unpack .../12-libice-dev_2%3a1.0.10-1build2_amd64.deb ...\n","Unpacking libice-dev:amd64 (2:1.0.10-1build2) ...\n","Selecting previously unselected package libsm-dev:amd64.\n","Preparing to unpack .../13-libsm-dev_2%3a1.2.3-1build2_amd64.deb ...\n","Unpacking libsm-dev:amd64 (2:1.2.3-1build2) ...\n","Selecting previously unselected package libxt-dev:amd64.\n","Preparing to unpack .../14-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n","Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n","Selecting previously unselected package freeglut3-dev:amd64.\n","Preparing to unpack .../15-freeglut3-dev_2.8.1-6_amd64.deb ...\n","Unpacking freeglut3-dev:amd64 (2.8.1-6) ...\n","Setting up freeglut3:amd64 (2.8.1-6) ...\n","Setting up libglvnd-core-dev:amd64 (1.4.0-1) ...\n","Setting up libice-dev:amd64 (2:1.0.10-1build2) ...\n","Setting up libsm-dev:amd64 (2:1.2.3-1build2) ...\n","Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n","Setting up libgles1:amd64 (1.4.0-1) ...\n","Setting up libglx-dev:amd64 (1.4.0-1) ...\n","Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n","Setting up libopengl-dev:amd64 (1.4.0-1) ...\n","Setting up libgl-dev:amd64 (1.4.0-1) ...\n","Setting up libegl-dev:amd64 (1.4.0-1) ...\n","Setting up libglu1-mesa-dev:amd64 (9.0.2-1) ...\n","Setting up libgles-dev:amd64 (1.4.0-1) ...\n","Setting up libglvnd-dev:amd64 (1.4.0-1) ...\n","Setting up libgl1-mesa-dev:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Setting up freeglut3-dev:amd64 (2.8.1-6) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n"]}],"source":["!sudo apt-get install freeglut3-dev"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cgGdQ6n9EERW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699027904329,"user_tz":-60,"elapsed":22275,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"177dea6b-8f88-44a1-bc18-e8de0c725110"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n","  xserver-common\n","The following NEW packages will be installed:\n","  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n","  xserver-common xvfb\n","0 upgraded, 9 newly installed, 0 to remove and 19 not upgraded.\n","Need to get 7,814 kB of archives.\n","After this operation, 11.9 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.2 [28.1 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.2 [864 kB]\n","Fetched 7,814 kB in 2s (4,457 kB/s)\n","Selecting previously unselected package libfontenc1:amd64.\n","(Reading database ... 121332 files and directories currently installed.)\n","Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n","Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n","Selecting previously unselected package libxfont2:amd64.\n","Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n","Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n","Selecting previously unselected package libxkbfile1:amd64.\n","Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n","Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n","Selecting previously unselected package x11-xkb-utils.\n","Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n","Unpacking x11-xkb-utils (7.7+5build4) ...\n","Selecting previously unselected package xfonts-encodings.\n","Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n","Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n","Selecting previously unselected package xfonts-utils.\n","Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n","Unpacking xfonts-utils (1:7.7+6build2) ...\n","Selecting previously unselected package xfonts-base.\n","Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n","Unpacking xfonts-base (1:1.0.5) ...\n","Selecting previously unselected package xserver-common.\n","Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.2_all.deb ...\n","Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n","Selecting previously unselected package xvfb.\n","Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.2_amd64.deb ...\n","Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.2) ...\n","Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n","Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n","Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n","Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n","Setting up x11-xkb-utils (7.7+5build4) ...\n","Setting up xfonts-utils (1:7.7+6build2) ...\n","Setting up xfonts-base (1:1.0.5) ...\n","Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n","Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.2) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","Collecting pyvirtualdisplay\n","  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n","Installing collected packages: pyvirtualdisplay\n","Successfully installed pyvirtualdisplay-3.0\n","Collecting piglet\n","  Downloading piglet-1.0.0-py2.py3-none-any.whl (2.2 kB)\n","Collecting piglet-templates (from piglet)\n","  Downloading piglet_templates-1.3.0-py3-none-any.whl (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.5/67.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from piglet-templates->piglet) (3.1.1)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from piglet-templates->piglet) (23.1.0)\n","Requirement already satisfied: astunparse in /usr/local/lib/python3.10/dist-packages (from piglet-templates->piglet) (1.6.3)\n","Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from piglet-templates->piglet) (2.1.3)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse->piglet-templates->piglet) (0.41.2)\n","Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from astunparse->piglet-templates->piglet) (1.16.0)\n","Installing collected packages: piglet-templates, piglet\n","Successfully installed piglet-1.0.0 piglet-templates-1.3.0\n"]}],"source":["!apt install xvfb -y\n","!pip install pyvirtualdisplay\n","!pip install piglet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5OaWkBSmhm6R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699027920939,"user_tz":-60,"elapsed":16641,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"f4b0f0b5-3b2a-45b8-8a7a-90e0e8372064"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting supersuit\n","  Downloading SuperSuit-3.9.0-py3-none-any.whl (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from supersuit) (1.23.5)\n","Requirement already satisfied: gymnasium>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from supersuit) (0.29.1)\n","Collecting tinyscaler>=1.2.6 (from supersuit)\n","  Downloading tinyscaler-1.2.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (517 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.1/517.1 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (1.6.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (0.0.4)\n","Installing collected packages: tinyscaler, supersuit\n","Successfully installed supersuit-3.9.0 tinyscaler-1.2.7\n","Collecting stable_baselines3\n","  Downloading stable_baselines3-2.1.0-py3-none-any.whl (178 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (0.29.1)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.23.5)\n","Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.1.0+cu118)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.6.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.5.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (3.7.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (0.0.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.12.4)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2.1.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.1.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (4.43.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (23.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable_baselines3) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable_baselines3) (1.3.0)\n","Installing collected packages: stable_baselines3\n","Successfully installed stable_baselines3-2.1.0\n"]}],"source":["!pip install supersuit\n","!pip install stable_baselines3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"thmOvcHdjKHw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699027927163,"user_tz":-60,"elapsed":6271,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"55512ab9-dd92-4fa9-ac53-f91944141ce5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting shimmy>=0.2.1\n","  Downloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from shimmy>=0.2.1) (1.23.5)\n","Requirement already satisfied: gymnasium>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from shimmy>=0.2.1) (0.29.1)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy>=0.2.1) (1.6.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy>=0.2.1) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy>=0.2.1) (0.0.4)\n","Installing collected packages: shimmy\n","Successfully installed shimmy-1.3.0\n"]}],"source":["!pip install 'shimmy>=0.2.1'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k0iVvep_spQz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699027949321,"user_tz":-60,"elapsed":22202,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"4fa67c17-4a03-4cb9-9f1e-14433045ae2f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tf-agents-nightly\n","  Downloading tf_agents_nightly-0.19.0.dev20231010-py3-none-any.whl (1.4 MB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.4 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.4.0)\n","Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.6.0)\n","Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (0.5.0)\n","Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (0.17.3)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.23.5)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (9.4.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.16.0)\n","Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (3.20.3)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.14.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (4.5.0)\n","Collecting pygame==2.1.3 (from tf-agents-nightly)\n","  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tfp-nightly (from tf-agents-nightly)\n","  Downloading tfp_nightly-0.23.0.dev20231103-py2.py3-none-any.whl (6.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents-nightly) (1.11.3)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents-nightly) (1.5.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tfp-nightly->tf-agents-nightly) (4.4.2)\n","Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tfp-nightly->tf-agents-nightly) (0.5.4)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tfp-nightly->tf-agents-nightly) (0.1.8)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<=0.23.0,>=0.17.0->tf-agents-nightly) (0.18.3)\n","Installing collected packages: tfp-nightly, pygame, tf-agents-nightly\n","  Attempting uninstall: pygame\n","    Found existing installation: pygame 2.3.0\n","    Uninstalling pygame-2.3.0:\n","      Successfully uninstalled pygame-2.3.0\n","Successfully installed pygame-2.1.3 tf-agents-nightly-0.19.0.dev20231010 tfp-nightly-0.23.0.dev20231103\n"]}],"source":["!pip install tf-agents-nightly"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UlXxViz9tdvH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699027963441,"user_tz":-60,"elapsed":14168,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"01a5092c-dc9f-4c6b-daa7-e53c325a6d44"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: tensorflow 2.8.0\n","Uninstalling tensorflow-2.8.0:\n","  Would remove:\n","    /usr/local/bin/estimator_ckpt_converter\n","    /usr/local/bin/import_pb_to_tensorboard\n","    /usr/local/bin/saved_model_cli\n","    /usr/local/bin/tensorboard\n","    /usr/local/bin/tf_upgrade_v2\n","    /usr/local/bin/tflite_convert\n","    /usr/local/bin/toco\n","    /usr/local/bin/toco_from_protos\n","    /usr/local/lib/python3.10/dist-packages/tensorflow-2.8.0.dist-info/*\n","    /usr/local/lib/python3.10/dist-packages/tensorflow/*\n","Proceed (Y/n)? Y\n","  Successfully uninstalled tensorflow-2.8.0\n"]}],"source":["!pip uninstall tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dsWlVQ6MtKLj"},"outputs":[],"source":["!pip install tensorflow>=2.13"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wE5AiVtFtZDc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699028015360,"user_tz":-60,"elapsed":5491,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"c21f9589-869d-4fc5-ecf3-0d46b7d47951"},"outputs":[{"output_type":"stream","name":"stdout","text":["Name: tensorflow\n","Version: 2.14.0\n","Summary: TensorFlow is an open source machine learning framework for everyone.\n","Home-page: https://www.tensorflow.org/\n","Author: Google Inc.\n","Author-email: packages@tensorflow.org\n","License: Apache 2.0\n","Location: /usr/local/lib/python3.10/dist-packages\n","Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, libclang, ml-dtypes, numpy, opt-einsum, packaging, protobuf, setuptools, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n","Required-by: dopamine-rl, keras-rl2\n"]}],"source":["!pip show tensorflow"]},{"cell_type":"code","source":["!pip install pettingzoo[sisl]"],"metadata":{"id":"PZa1qybXZKSX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699028090095,"user_tz":-60,"elapsed":74742,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"fb70667a-45e3-4a24-926a-183fa0c76f16"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pettingzoo[sisl] in /usr/local/lib/python3.10/dist-packages (1.24.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[sisl]) (1.23.5)\n","Requirement already satisfied: gymnasium>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[sisl]) (0.29.1)\n","Collecting pygame==2.3.0 (from pettingzoo[sisl])\n","  Using cached pygame-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\n","Requirement already satisfied: pymunk==6.2.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[sisl]) (6.2.0)\n","Collecting box2d-py==2.3.5 (from pettingzoo[sisl])\n","  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[sisl]) (1.11.3)\n","Requirement already satisfied: cffi>1.14.0 in /usr/local/lib/python3.10/dist-packages (from pymunk==6.2.0->pettingzoo[sisl]) (1.16.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[sisl]) (1.6.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[sisl]) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[sisl]) (0.0.4)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>1.14.0->pymunk==6.2.0->pettingzoo[sisl]) (2.21)\n","Building wheels for collected packages: box2d-py\n","  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2373077 sha256=019b5247071294d9e0568e9881f97aa3b50155692263312fa0a90f920ae09dcd\n","  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n","Successfully built box2d-py\n","Installing collected packages: box2d-py, pygame\n","  Attempting uninstall: box2d-py\n","    Found existing installation: box2d-py 2.3.8\n","    Uninstalling box2d-py-2.3.8:\n","      Successfully uninstalled box2d-py-2.3.8\n","  Attempting uninstall: pygame\n","    Found existing installation: pygame 2.1.3\n","    Uninstalling pygame-2.1.3:\n","      Successfully uninstalled pygame-2.1.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tf-agents-nightly 0.19.0.dev20231010 requires pygame==2.1.3, but you have pygame 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed box2d-py-2.3.5 pygame-2.3.0\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JpA_YhKzCeC2"},"outputs":[],"source":["############################# Código para entrenar Multiwalker ######################"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tnz3fJDA33w8"},"outputs":[],"source":["# from stable_baselines3.dqn import MlpPolicy,CnnPolicy\n","from stable_baselines3 import SAC\n","from pettingzoo.sisl import multiwalker_v9\n","import supersuit as ss\n","from stable_baselines3.common.monitor import Monitor"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":218,"status":"ok","timestamp":1697644598142,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"6-kdhb3CI5VC","outputId":"4db87993-1f4b-464a-f0f0-20ed31aa0c34"},"outputs":[{"output_type":"stream","name":"stdout","text":["drive  gdrive  sample_data\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":395,"status":"ok","timestamp":1697644599460,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"04CbnRTvI9L2","outputId":"347477dc-2a63-4315-a08f-182fdf724421"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/TFM\n"]}],"source":["cd /content/drive/MyDrive/TFM"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":305,"status":"ok","timestamp":1697644601324,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"bUEH8254I_kb","outputId":"406e2b97-e693-46da-9e49-9d56c991f4e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["'=2.13'\t\t\t\t     multiwalker_sac_log_eval\n"," DQN_new_pettingzoo_gym_cap.ipynb    policy_log_eval\n"," DQN_policies\t\t\t     PPO_policies\n","'Entrenamientos antiguos sin logs'   results_rllib\n"," Entrenamientos_log_no_eval\t     TFM_Multiwalker_DDPG_gym_cap.ipynb\n"," MCR_TFM.ipynb\t\t\t     TFM_Multiwalker_SAC_gym_cap.ipynb\n"," multi_car_racing\t\t     TFM_PPO_new_pettingzoo_gym_cap.ipynb\n"," multiwalker_ddpg_log_eval\t     TFM_PPO_pettingzoo_gym_cap.ipynb\n"," multiwalker_ddpg.zip\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2HubK-2G3_vH"},"outputs":[],"source":["env = multiwalker_v9.parallel_env(render_mode=\"rgb_array\",shared_reward=0.4)\n","\n","# #---------------------------------\n","# log_dir = \"./policy_log\"\n","# os.makedirs(log_dir, exist_ok=True)\n","# env = Monitor(env, log_dir)\n","# #---------------------------------\n","# env = ss.color_reduction_v0(env, mode='B')\n","# env = ss.black_death_v3(env)\n","# env = ss.resize_v1(env, x_size=84, y_size=84)\n","env = ss.frame_stack_v1(env, 3)\n","env = ss.pettingzoo_env_to_vec_env_v1(env)\n","env = ss.concat_vec_envs_v1(env, 8, num_cpus=4, base_class='stable_baselines3')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"foI4bTFGbQo1"},"outputs":[],"source":["from stable_baselines3.common.callbacks import EvalCallback\n","eval_callback = EvalCallback(env, best_model_save_path=\"./multiwalker_sac_04_log_eval/\",\n","                             log_path=\"./multiwalker_sac_04_log_eval/\", eval_freq=100,\n","                             deterministic=False, render=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ib9VPIVgec47","outputId":"3a1e6011-c1c7-4fef-f8f4-833589737699"},"outputs":[{"output_type":"stream","name":"stdout","text":["Logging to multiwalker_sac_04_log_eval/\n","Using cuda device\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 10       |\n","|    fps             | 181      |\n","|    time_elapsed    | 8        |\n","|    total_timesteps | 1584     |\n","| train/             |          |\n","|    actor_loss      | -5.35    |\n","|    critic_loss     | 50.2     |\n","|    ent_coef        | 0.982    |\n","|    ent_coef_loss   | -0.121   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 61       |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 20       |\n","|    fps             | 213      |\n","|    time_elapsed    | 9        |\n","|    total_timesteps | 1992     |\n","| train/             |          |\n","|    actor_loss      | -4.8     |\n","|    critic_loss     | 27.5     |\n","|    ent_coef        | 0.977    |\n","|    ent_coef_loss   | -0.156   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 78       |\n","---------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n","|    critic_loss     | 0.192    |\n","|    ent_coef        | 0.00313  |\n","|    ent_coef_loss   | -2.09    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 137095   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6060     |\n","|    fps             | 279      |\n","|    time_elapsed    | 11795    |\n","|    total_timesteps | 3292536  |\n","| train/             |          |\n","|    actor_loss      | 2.79     |\n","|    critic_loss     | 0.227    |\n","|    ent_coef        | 0.00316  |\n","|    ent_coef_loss   | 0.227    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 137184   |\n","---------------------------------\n","Eval num_timesteps=3292800, episode_reward=-77.05 +/- 3.29\n","Episode length: 195.80 +/- 57.81\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 196      |\n","|    mean_reward     | -77.1    |\n","| time/              |          |\n","|    total_timesteps | 3292800  |\n","| train/             |          |\n","|    actor_loss      | 2.91     |\n","|    critic_loss     | 0.464    |\n","|    ent_coef        | 0.00316  |\n","|    ent_coef_loss   | 0.0413   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 137195   |\n","---------------------------------\n","Eval num_timesteps=3295200, episode_reward=-11.24 +/- 4.90\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -11.2    |\n","| time/              |          |\n","|    total_timesteps | 3295200  |\n","| train/             |          |\n","|    actor_loss      | 2.64     |\n","|    critic_loss     | 0.142    |\n","|    ent_coef        | 0.00315  |\n","|    ent_coef_loss   | -1.53    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 137295   |\n","---------------------------------\n","Eval num_timesteps=3297600, episode_reward=-80.38 +/- 2.54\n","Episode length: 159.00 +/- 41.64\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 159      |\n","|    mean_reward     | -80.4    |\n","| time/              |          |\n","|    total_timesteps | 3297600  |\n","| train/             |          |\n","|    actor_loss      | 3.02     |\n","|    critic_loss     | 15.9     |\n","|    ent_coef        | 0.00314  |\n","|    ent_coef_loss   | 1.52     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 137395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6070     |\n","|    fps             | 279      |\n","|    time_elapsed    | 11815    |\n","|    total_timesteps | 3298368  |\n","| train/             |          |\n","|    actor_loss      | 3.11     |\n","|    critic_loss     | 0.708    |\n","|    ent_coef        | 0.00315  |\n","|    ent_coef_loss   | -1.71    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 137427   |\n","---------------------------------\n","Eval num_timesteps=3300000, episode_reward=-52.15 +/- 33.68\n","Episode length: 338.60 +/- 131.78\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 339      |\n","|    mean_reward     | -52.2    |\n","| time/              |          |\n","|    total_timesteps | 3300000  |\n","| train/             |          |\n","|    actor_loss      | 2.51     |\n","|    critic_loss     | 21.7     |\n","|    ent_coef        | 0.00316  |\n","|    ent_coef_loss   | -3.18    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 137495   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6080     |\n","|    fps             | 279      |\n","|    time_elapsed    | 11825    |\n","|    total_timesteps | 3302040  |\n","| train/             |          |\n","|    actor_loss      | 3.31     |\n","|    critic_loss     | 21.1     |\n","|    ent_coef        | 0.00315  |\n","|    ent_coef_loss   | -0.768   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 137580   |\n","---------------------------------\n","Eval num_timesteps=3302400, episode_reward=-80.98 +/- 2.68\n","Episode length: 276.40 +/- 10.29\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 276      |\n","|    mean_reward     | -81      |\n","| time/              |          |\n","|    total_timesteps | 3302400  |\n","| train/             |          |\n","|    actor_loss      | 3.71     |\n","|    critic_loss     | 0.528    |\n","|    ent_coef        | 0.00314  |\n","|    ent_coef_loss   | -0.497   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 137595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6090     |\n","|    fps             | 279      |\n","|    time_elapsed    | 11830    |\n","|    total_timesteps | 3304560  |\n","| train/             |          |\n","|    actor_loss      | 2.89     |\n","|    critic_loss     | 0.204    |\n","|    ent_coef        | 0.00311  |\n","|    ent_coef_loss   | -2.89    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 137685   |\n","---------------------------------\n","Eval num_timesteps=3304800, episode_reward=-74.81 +/- 1.44\n","Episode length: 253.40 +/- 26.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 253      |\n","|    mean_reward     | -74.8    |\n","| time/              |          |\n","|    total_timesteps | 3304800  |\n","| train/             |          |\n","|    actor_loss      | 3.08     |\n","|    critic_loss     | 0.339    |\n","|    ent_coef        | 0.0031   |\n","|    ent_coef_loss   | -0.987   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 137695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6100     |\n","|    fps             | 279      |\n","|    time_elapsed    | 11836    |\n","|    total_timesteps | 3306336  |\n","| train/             |          |\n","|    actor_loss      | 3.13     |\n","|    critic_loss     | 0.952    |\n","|    ent_coef        | 0.00307  |\n","|    ent_coef_loss   | 0.686    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 137759   |\n","---------------------------------\n","Eval num_timesteps=3307200, episode_reward=-80.45 +/- 1.60\n","Episode length: 141.80 +/- 37.72\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 142      |\n","|    mean_reward     | -80.5    |\n","| time/              |          |\n","|    total_timesteps | 3307200  |\n","| train/             |          |\n","|    actor_loss      | 2.92     |\n","|    critic_loss     | 1.51     |\n","|    ent_coef        | 0.00306  |\n","|    ent_coef_loss   | -0.387   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 137795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6110     |\n","|    fps             | 279      |\n","|    time_elapsed    | 11841    |\n","|    total_timesteps | 3307704  |\n","| train/             |          |\n","|    actor_loss      | 2.73     |\n","|    critic_loss     | 0.611    |\n","|    ent_coef        | 0.00305  |\n","|    ent_coef_loss   | -4.16    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 137816   |\n","---------------------------------\n","Eval num_timesteps=3309600, episode_reward=-49.22 +/- 34.68\n","Episode length: 365.00 +/- 110.23\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 365      |\n","|    mean_reward     | -49.2    |\n","| time/              |          |\n","|    total_timesteps | 3309600  |\n","| train/             |          |\n","|    actor_loss      | 3.86     |\n","|    critic_loss     | 18.9     |\n","|    ent_coef        | 0.00303  |\n","|    ent_coef_loss   | 1.34     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 137895   |\n","---------------------------------\n","Eval num_timesteps=3312000, episode_reward=-79.03 +/- 2.42\n","Episode length: 140.00 +/- 12.25\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 140      |\n","|    mean_reward     | -79      |\n","| time/              |          |\n","|    total_timesteps | 3312000  |\n","| train/             |          |\n","|    actor_loss      | 3.98     |\n","|    critic_loss     | 0.674    |\n","|    ent_coef        | 0.00304  |\n","|    ent_coef_loss   | 3.5      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 137995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6120     |\n","|    fps             | 279      |\n","|    time_elapsed    | 11851    |\n","|    total_timesteps | 3312216  |\n","| train/             |          |\n","|    actor_loss      | 3.04     |\n","|    critic_loss     | 0.534    |\n","|    ent_coef        | 0.00305  |\n","|    ent_coef_loss   | -0.148   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 138004   |\n","---------------------------------\n","Eval num_timesteps=3314400, episode_reward=-37.43 +/- 45.32\n","Episode length: 369.80 +/- 106.31\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 370      |\n","|    mean_reward     | -37.4    |\n","| time/              |          |\n","|    total_timesteps | 3314400  |\n","| train/             |          |\n","|    actor_loss      | 3.21     |\n","|    critic_loss     | 0.357    |\n","|    ent_coef        | 0.00308  |\n","|    ent_coef_loss   | 1.47     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 138095   |\n","---------------------------------\n","Eval num_timesteps=3316800, episode_reward=-74.66 +/- 2.01\n","Episode length: 134.40 +/- 21.56\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 134      |\n","|    mean_reward     | -74.7    |\n","| time/              |          |\n","|    total_timesteps | 3316800  |\n","| train/             |          |\n","|    actor_loss      | 2.91     |\n","|    critic_loss     | 0.303    |\n","|    ent_coef        | 0.00315  |\n","|    ent_coef_loss   | 1.64     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 138195   |\n","---------------------------------\n","Eval num_timesteps=3319200, episode_reward=-78.92 +/- 6.26\n","Episode length: 325.60 +/- 61.73\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 326      |\n","|    mean_reward     | -78.9    |\n","| time/              |          |\n","|    total_timesteps | 3319200  |\n","| train/             |          |\n","|    actor_loss      | 2.51     |\n","|    critic_loss     | 0.229    |\n","|    ent_coef        | 0.0032   |\n","|    ent_coef_loss   | -3.59    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 138295   |\n","---------------------------------\n","Eval num_timesteps=3321600, episode_reward=-35.73 +/- 32.51\n","Episode length: 417.20 +/- 67.61\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 417      |\n","|    mean_reward     | -35.7    |\n","| time/              |          |\n","|    total_timesteps | 3321600  |\n","| train/             |          |\n","|    actor_loss      | 2.99     |\n","|    critic_loss     | 1.17     |\n","|    ent_coef        | 0.00323  |\n","|    ent_coef_loss   | -3.11    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 138395   |\n","---------------------------------\n","Eval num_timesteps=3324000, episode_reward=-77.37 +/- 1.49\n","Episode length: 86.40 +/- 29.88\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 86.4     |\n","|    mean_reward     | -77.4    |\n","| time/              |          |\n","|    total_timesteps | 3324000  |\n","| train/             |          |\n","|    actor_loss      | 3.17     |\n","|    critic_loss     | 0.32     |\n","|    ent_coef        | 0.00334  |\n","|    ent_coef_loss   | 3.5      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 138495   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6130     |\n","|    fps             | 279      |\n","|    time_elapsed    | 11888    |\n","|    total_timesteps | 3324888  |\n","| train/             |          |\n","|    actor_loss      | 3.47     |\n","|    critic_loss     | 19.9     |\n","|    ent_coef        | 0.00339  |\n","|    ent_coef_loss   | 1.78     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 138532   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6140     |\n","|    fps             | 279      |\n","|    time_elapsed    | 11889    |\n","|    total_timesteps | 3326328  |\n","| train/             |          |\n","|    actor_loss      | 3.3      |\n","|    critic_loss     | 0.551    |\n","|    ent_coef        | 0.00343  |\n","|    ent_coef_loss   | 1.15     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 138592   |\n","---------------------------------\n","Eval num_timesteps=3326400, episode_reward=-77.98 +/- 3.72\n","Episode length: 120.00 +/- 51.44\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 120      |\n","|    mean_reward     | -78      |\n","| time/              |          |\n","|    total_timesteps | 3326400  |\n","| train/             |          |\n","|    actor_loss      | 3.52     |\n","|    critic_loss     | 17.5     |\n","|    ent_coef        | 0.00343  |\n","|    ent_coef_loss   | 2.45     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 138595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6150     |\n","|    fps             | 279      |\n","|    time_elapsed    | 11893    |\n","|    total_timesteps | 3328104  |\n","| train/             |          |\n","|    actor_loss      | 3.32     |\n","|    critic_loss     | 0.476    |\n","|    ent_coef        | 0.00351  |\n","|    ent_coef_loss   | 2.02     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 138666   |\n","---------------------------------\n","Eval num_timesteps=3328800, episode_reward=-68.04 +/- 2.40\n","Episode length: 292.80 +/- 47.52\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 293      |\n","|    mean_reward     | -68      |\n","| time/              |          |\n","|    total_timesteps | 3328800  |\n","| train/             |          |\n","|    actor_loss      | 3.09     |\n","|    critic_loss     | 19.2     |\n","|    ent_coef        | 0.00352  |\n","|    ent_coef_loss   | -0.799   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 138695   |\n","---------------------------------\n","Eval num_timesteps=3331200, episode_reward=-39.77 +/- 32.28\n","Episode length: 367.40 +/- 108.27\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 367      |\n","|    mean_reward     | -39.8    |\n","| time/              |          |\n","|    total_timesteps | 3331200  |\n","| train/             |          |\n","|    actor_loss      | 4.41     |\n","|    critic_loss     | 2.74     |\n","|    ent_coef        | 0.00355  |\n","|    ent_coef_loss   | 2.93     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 138795   |\n","---------------------------------\n","Eval num_timesteps=3333600, episode_reward=-64.96 +/- 6.95\n","Episode length: 374.40 +/- 107.29\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 374      |\n","|    mean_reward     | -65      |\n","| time/              |          |\n","|    total_timesteps | 3333600  |\n","| train/             |          |\n","|    actor_loss      | 4        |\n","|    critic_loss     | 0.536    |\n","|    ent_coef        | 0.00357  |\n","|    ent_coef_loss   | 2.19     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 138895   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6160     |\n","|    fps             | 279      |\n","|    time_elapsed    | 11918    |\n","|    total_timesteps | 3334512  |\n","| train/             |          |\n","|    actor_loss      | 3.7      |\n","|    critic_loss     | 0.692    |\n","|    ent_coef        | 0.00358  |\n","|    ent_coef_loss   | 0.805    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 138933   |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    episodes        | 6170    |\n","|    fps             | 279     |\n","|    time_elapsed    | 11918   |\n","|    total_timesteps | 3334512 |\n","--------------------------------\n","Eval num_timesteps=3336000, episode_reward=1.66 +/- 1.26\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.66     |\n","| time/              |          |\n","|    total_timesteps | 3336000  |\n","| train/             |          |\n","|    actor_loss      | 3.23     |\n","|    critic_loss     | 0.224    |\n","|    ent_coef        | 0.00357  |\n","|    ent_coef_loss   | 0.488    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 138995   |\n","---------------------------------\n","Eval num_timesteps=3338400, episode_reward=-41.98 +/- 39.50\n","Episode length: 296.00 +/- 166.57\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 296      |\n","|    mean_reward     | -42      |\n","| time/              |          |\n","|    total_timesteps | 3338400  |\n","| train/             |          |\n","|    actor_loss      | 3.51     |\n","|    critic_loss     | 0.51     |\n","|    ent_coef        | 0.00366  |\n","|    ent_coef_loss   | 1.31     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 139095   |\n","---------------------------------\n","Eval num_timesteps=3340800, episode_reward=-17.27 +/- 39.79\n","Episode length: 436.80 +/- 77.40\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 437      |\n","|    mean_reward     | -17.3    |\n","| time/              |          |\n","|    total_timesteps | 3340800  |\n","| train/             |          |\n","|    actor_loss      | 3.04     |\n","|    critic_loss     | 0.373    |\n","|    ent_coef        | 0.00367  |\n","|    ent_coef_loss   | -0.0316  |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 139195   |\n","---------------------------------\n","Eval num_timesteps=3343200, episode_reward=-2.26 +/- 4.81\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -2.26    |\n","| time/              |          |\n","|    total_timesteps | 3343200  |\n","| train/             |          |\n","|    actor_loss      | 3.06     |\n","|    critic_loss     | 0.193    |\n","|    ent_coef        | 0.00374  |\n","|    ent_coef_loss   | -2.03    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 139295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6180     |\n","|    fps             | 279      |\n","|    time_elapsed    | 11959    |\n","|    total_timesteps | 3345312  |\n","| train/             |          |\n","|    actor_loss      | 3.87     |\n","|    critic_loss     | 0.301    |\n","|    ent_coef        | 0.00372  |\n","|    ent_coef_loss   | 1.84     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 139383   |\n","---------------------------------\n","Eval num_timesteps=3345600, episode_reward=10.04 +/- 9.37\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 10       |\n","| time/              |          |\n","|    total_timesteps | 3345600  |\n","| train/             |          |\n","|    actor_loss      | 3.47     |\n","|    critic_loss     | 0.68     |\n","|    ent_coef        | 0.00373  |\n","|    ent_coef_loss   | 0.515    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 139395   |\n","---------------------------------\n","Eval num_timesteps=3348000, episode_reward=-29.38 +/- 42.61\n","Episode length: 392.40 +/- 131.78\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 392      |\n","|    mean_reward     | -29.4    |\n","| time/              |          |\n","|    total_timesteps | 3348000  |\n","| train/             |          |\n","|    actor_loss      | 3.51     |\n","|    critic_loss     | 0.777    |\n","|    ent_coef        | 0.00369  |\n","|    ent_coef_loss   | -1.4     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 139495   |\n","---------------------------------\n","Eval num_timesteps=3350400, episode_reward=-11.00 +/- 40.62\n","Episode length: 472.00 +/- 34.29\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 472      |\n","|    mean_reward     | -11      |\n","| time/              |          |\n","|    total_timesteps | 3350400  |\n","| train/             |          |\n","|    actor_loss      | 4.57     |\n","|    critic_loss     | 0.97     |\n","|    ent_coef        | 0.00364  |\n","|    ent_coef_loss   | 2.44     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 139595   |\n","---------------------------------\n","Eval num_timesteps=3352800, episode_reward=-69.96 +/- 7.64\n","Episode length: 308.00 +/- 129.82\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 308      |\n","|    mean_reward     | -70      |\n","| time/              |          |\n","|    total_timesteps | 3352800  |\n","| train/             |          |\n","|    actor_loss      | 2.97     |\n","|    critic_loss     | 0.6      |\n","|    ent_coef        | 0.00361  |\n","|    ent_coef_loss   | -2.02    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 139695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6190     |\n","|    fps             | 279      |\n","|    time_elapsed    | 11996    |\n","|    total_timesteps | 3354864  |\n","| train/             |          |\n","|    actor_loss      | 3.86     |\n","|    critic_loss     | 40.9     |\n","|    ent_coef        | 0.00354  |\n","|    ent_coef_loss   | 2.72     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 139781   |\n","---------------------------------\n","Eval num_timesteps=3355200, episode_reward=-37.40 +/- 40.18\n","Episode length: 348.00 +/- 186.16\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 348      |\n","|    mean_reward     | -37.4    |\n","| time/              |          |\n","|    total_timesteps | 3355200  |\n","| train/             |          |\n","|    actor_loss      | 4.18     |\n","|    critic_loss     | 0.545    |\n","|    ent_coef        | 0.00355  |\n","|    ent_coef_loss   | 0.363    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 139795   |\n","---------------------------------\n","Eval num_timesteps=3357600, episode_reward=-1.99 +/- 0.77\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.99    |\n","| time/              |          |\n","|    total_timesteps | 3357600  |\n","| train/             |          |\n","|    actor_loss      | 5.44     |\n","|    critic_loss     | 25.4     |\n","|    ent_coef        | 0.00346  |\n","|    ent_coef_loss   | 5.13     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 139895   |\n","---------------------------------\n","Eval num_timesteps=3360000, episode_reward=-80.60 +/- 1.62\n","Episode length: 70.80 +/- 1.47\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 70.8     |\n","|    mean_reward     | -80.6    |\n","| time/              |          |\n","|    total_timesteps | 3360000  |\n","| train/             |          |\n","|    actor_loss      | 3.6      |\n","|    critic_loss     | 0.475    |\n","|    ent_coef        | 0.00347  |\n","|    ent_coef_loss   | 2.68     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 139995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6200     |\n","|    fps             | 279      |\n","|    time_elapsed    | 12015    |\n","|    total_timesteps | 3360480  |\n","| train/             |          |\n","|    actor_loss      | 3.82     |\n","|    critic_loss     | 9.09     |\n","|    ent_coef        | 0.00348  |\n","|    ent_coef_loss   | -0.601   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140015   |\n","---------------------------------\n","Eval num_timesteps=3362400, episode_reward=2.17 +/- 0.44\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.17     |\n","| time/              |          |\n","|    total_timesteps | 3362400  |\n","| train/             |          |\n","|    actor_loss      | 2.97     |\n","|    critic_loss     | 0.649    |\n","|    ent_coef        | 0.00346  |\n","|    ent_coef_loss   | 2.27     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140095   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6210     |\n","|    fps             | 279      |\n","|    time_elapsed    | 12027    |\n","|    total_timesteps | 3364176  |\n","| train/             |          |\n","|    actor_loss      | 3.11     |\n","|    critic_loss     | 0.388    |\n","|    ent_coef        | 0.00352  |\n","|    ent_coef_loss   | 1.5      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140169   |\n","---------------------------------\n","Eval num_timesteps=3364800, episode_reward=-32.20 +/- 38.89\n","Episode length: 319.60 +/- 220.94\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 320      |\n","|    mean_reward     | -32.2    |\n","| time/              |          |\n","|    total_timesteps | 3364800  |\n","| train/             |          |\n","|    actor_loss      | 3.9      |\n","|    critic_loss     | 0.491    |\n","|    ent_coef        | 0.00353  |\n","|    ent_coef_loss   | 1.39     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6220     |\n","|    fps             | 279      |\n","|    time_elapsed    | 12033    |\n","|    total_timesteps | 3365928  |\n","| train/             |          |\n","|    actor_loss      | 3.51     |\n","|    critic_loss     | 0.464    |\n","|    ent_coef        | 0.00358  |\n","|    ent_coef_loss   | 3.54     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140242   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6230     |\n","|    fps             | 279      |\n","|    time_elapsed    | 12034    |\n","|    total_timesteps | 3366528  |\n","| train/             |          |\n","|    actor_loss      | 4.18     |\n","|    critic_loss     | 27.2     |\n","|    ent_coef        | 0.00362  |\n","|    ent_coef_loss   | 2.34     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140267   |\n","---------------------------------\n","Eval num_timesteps=3367200, episode_reward=-47.59 +/- 41.19\n","Episode length: 235.40 +/- 216.04\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 235      |\n","|    mean_reward     | -47.6    |\n","| time/              |          |\n","|    total_timesteps | 3367200  |\n","| train/             |          |\n","|    actor_loss      | 3.66     |\n","|    critic_loss     | 0.673    |\n","|    ent_coef        | 0.00363  |\n","|    ent_coef_loss   | -1.05    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6240     |\n","|    fps             | 279      |\n","|    time_elapsed    | 12045    |\n","|    total_timesteps | 3369024  |\n","| train/             |          |\n","|    actor_loss      | 3.02     |\n","|    critic_loss     | 0.423    |\n","|    ent_coef        | 0.00372  |\n","|    ent_coef_loss   | 1.46     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140371   |\n","---------------------------------\n","Eval num_timesteps=3369600, episode_reward=-50.25 +/- 38.14\n","Episode length: 236.00 +/- 215.56\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 236      |\n","|    mean_reward     | -50.3    |\n","| time/              |          |\n","|    total_timesteps | 3369600  |\n","| train/             |          |\n","|    actor_loss      | 3.17     |\n","|    critic_loss     | 0.584    |\n","|    ent_coef        | 0.00374  |\n","|    ent_coef_loss   | -1.11    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6250     |\n","|    fps             | 279      |\n","|    time_elapsed    | 12052    |\n","|    total_timesteps | 3371256  |\n","| train/             |          |\n","|    actor_loss      | 3.57     |\n","|    critic_loss     | 1.7      |\n","|    ent_coef        | 0.00378  |\n","|    ent_coef_loss   | 1.2      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140464   |\n","---------------------------------\n","Eval num_timesteps=3372000, episode_reward=-2.06 +/- 0.94\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -2.06    |\n","| time/              |          |\n","|    total_timesteps | 3372000  |\n","| train/             |          |\n","|    actor_loss      | 3.57     |\n","|    critic_loss     | 4.31     |\n","|    ent_coef        | 0.00381  |\n","|    ent_coef_loss   | -0.223   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140495   |\n","---------------------------------\n","Eval num_timesteps=3374400, episode_reward=-82.64 +/- 2.73\n","Episode length: 122.00 +/- 68.59\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 122      |\n","|    mean_reward     | -82.6    |\n","| time/              |          |\n","|    total_timesteps | 3374400  |\n","| train/             |          |\n","|    actor_loss      | 3        |\n","|    critic_loss     | 38.5     |\n","|    ent_coef        | 0.00384  |\n","|    ent_coef_loss   | -3.94    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140595   |\n","---------------------------------\n","Eval num_timesteps=3376800, episode_reward=-78.99 +/- 1.80\n","Episode length: 141.60 +/- 47.03\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 142      |\n","|    mean_reward     | -79      |\n","| time/              |          |\n","|    total_timesteps | 3376800  |\n","| train/             |          |\n","|    actor_loss      | 3.8      |\n","|    critic_loss     | 0.557    |\n","|    ent_coef        | 0.00377  |\n","|    ent_coef_loss   | -0.715   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6260     |\n","|    fps             | 279      |\n","|    time_elapsed    | 12073    |\n","|    total_timesteps | 3377208  |\n","| train/             |          |\n","|    actor_loss      | 3.78     |\n","|    critic_loss     | 0.381    |\n","|    ent_coef        | 0.00376  |\n","|    ent_coef_loss   | -1.06    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140712   |\n","---------------------------------\n","Eval num_timesteps=3379200, episode_reward=-5.28 +/- 1.78\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -5.28    |\n","| time/              |          |\n","|    total_timesteps | 3379200  |\n","| train/             |          |\n","|    actor_loss      | 3.1      |\n","|    critic_loss     | 0.417    |\n","|    ent_coef        | 0.00377  |\n","|    ent_coef_loss   | -0.82    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6270     |\n","|    fps             | 279      |\n","|    time_elapsed    | 12084    |\n","|    total_timesteps | 3381384  |\n","| train/             |          |\n","|    actor_loss      | 3.9      |\n","|    critic_loss     | 0.66     |\n","|    ent_coef        | 0.00373  |\n","|    ent_coef_loss   | -0.482   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140886   |\n","---------------------------------\n","Eval num_timesteps=3381600, episode_reward=1.12 +/- 5.72\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.12     |\n","| time/              |          |\n","|    total_timesteps | 3381600  |\n","| train/             |          |\n","|    actor_loss      | 3.8      |\n","|    critic_loss     | 0.322    |\n","|    ent_coef        | 0.00372  |\n","|    ent_coef_loss   | -1.92    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140895   |\n","---------------------------------\n","Eval num_timesteps=3384000, episode_reward=-41.16 +/- 34.08\n","Episode length: 304.40 +/- 159.71\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 304      |\n","|    mean_reward     | -41.2    |\n","| time/              |          |\n","|    total_timesteps | 3384000  |\n","| train/             |          |\n","|    actor_loss      | 3.67     |\n","|    critic_loss     | 0.291    |\n","|    ent_coef        | 0.00366  |\n","|    ent_coef_loss   | 0.559    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140995   |\n","---------------------------------\n","Eval num_timesteps=3386400, episode_reward=-51.44 +/- 38.52\n","Episode length: 326.60 +/- 141.58\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 327      |\n","|    mean_reward     | -51.4    |\n","| time/              |          |\n","|    total_timesteps | 3386400  |\n","| train/             |          |\n","|    actor_loss      | 3.4      |\n","|    critic_loss     | 0.305    |\n","|    ent_coef        | 0.0036   |\n","|    ent_coef_loss   | -2.35    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141095   |\n","---------------------------------\n","Eval num_timesteps=3388800, episode_reward=-3.49 +/- 2.73\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -3.49    |\n","| time/              |          |\n","|    total_timesteps | 3388800  |\n","| train/             |          |\n","|    actor_loss      | 3.39     |\n","|    critic_loss     | 0.214    |\n","|    ent_coef        | 0.00366  |\n","|    ent_coef_loss   | 0.363    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141195   |\n","---------------------------------\n","Eval num_timesteps=3391200, episode_reward=-70.39 +/- 4.81\n","Episode length: 374.20 +/- 82.30\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 374      |\n","|    mean_reward     | -70.4    |\n","| time/              |          |\n","|    total_timesteps | 3391200  |\n","| train/             |          |\n","|    actor_loss      | 4.06     |\n","|    critic_loss     | 0.519    |\n","|    ent_coef        | 0.00367  |\n","|    ent_coef_loss   | 1.02     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6280     |\n","|    fps             | 279      |\n","|    time_elapsed    | 12128    |\n","|    total_timesteps | 3391800  |\n","| train/             |          |\n","|    actor_loss      | 3.21     |\n","|    critic_loss     | 0.368    |\n","|    ent_coef        | 0.00366  |\n","|    ent_coef_loss   | -2.53    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141320   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6290     |\n","|    fps             | 279      |\n","|    time_elapsed    | 12129    |\n","|    total_timesteps | 3392448  |\n","| train/             |          |\n","|    actor_loss      | 3.61     |\n","|    critic_loss     | 19.1     |\n","|    ent_coef        | 0.00365  |\n","|    ent_coef_loss   | 0.873    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141347   |\n","---------------------------------\n","Eval num_timesteps=3393600, episode_reward=0.75 +/- 3.86\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.753    |\n","| time/              |          |\n","|    total_timesteps | 3393600  |\n","| train/             |          |\n","|    actor_loss      | 3.51     |\n","|    critic_loss     | 23       |\n","|    ent_coef        | 0.00365  |\n","|    ent_coef_loss   | 0.0141   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141395   |\n","---------------------------------\n","Eval num_timesteps=3396000, episode_reward=-47.69 +/- 39.74\n","Episode length: 371.00 +/- 105.33\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 371      |\n","|    mean_reward     | -47.7    |\n","| time/              |          |\n","|    total_timesteps | 3396000  |\n","| train/             |          |\n","|    actor_loss      | 2.99     |\n","|    critic_loss     | 0.444    |\n","|    ent_coef        | 0.00365  |\n","|    ent_coef_loss   | -1.56    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141495   |\n","---------------------------------\n","Eval num_timesteps=3398400, episode_reward=-50.53 +/- 36.89\n","Episode length: 411.80 +/- 72.01\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 412      |\n","|    mean_reward     | -50.5    |\n","| time/              |          |\n","|    total_timesteps | 3398400  |\n","| train/             |          |\n","|    actor_loss      | 3.4      |\n","|    critic_loss     | 0.905    |\n","|    ent_coef        | 0.00362  |\n","|    ent_coef_loss   | -0.263   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6300     |\n","|    fps             | 279      |\n","|    time_elapsed    | 12156    |\n","|    total_timesteps | 3400776  |\n","| train/             |          |\n","|    actor_loss      | 3.21     |\n","|    critic_loss     | 0.523    |\n","|    ent_coef        | 0.00376  |\n","|    ent_coef_loss   | -0.498   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141694   |\n","---------------------------------\n","Eval num_timesteps=3400800, episode_reward=2.55 +/- 2.58\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.55     |\n","| time/              |          |\n","|    total_timesteps | 3400800  |\n","| train/             |          |\n","|    actor_loss      | 3.6      |\n","|    critic_loss     | 0.434    |\n","|    ent_coef        | 0.00376  |\n","|    ent_coef_loss   | 1.39     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141695   |\n","---------------------------------\n","Eval num_timesteps=3403200, episode_reward=-75.79 +/- 2.51\n","Episode length: 201.40 +/- 38.70\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 201      |\n","|    mean_reward     | -75.8    |\n","| time/              |          |\n","|    total_timesteps | 3403200  |\n","| train/             |          |\n","|    actor_loss      | 3.8      |\n","|    critic_loss     | 0.322    |\n","|    ent_coef        | 0.00378  |\n","|    ent_coef_loss   | 0.543    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141795   |\n","---------------------------------\n","Eval num_timesteps=3405600, episode_reward=-70.58 +/- 1.41\n","Episode length: 359.00 +/- 44.09\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 359      |\n","|    mean_reward     | -70.6    |\n","| time/              |          |\n","|    total_timesteps | 3405600  |\n","| train/             |          |\n","|    actor_loss      | 4.13     |\n","|    critic_loss     | 4.76     |\n","|    ent_coef        | 0.00382  |\n","|    ent_coef_loss   | 3.65     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141895   |\n","---------------------------------\n","Eval num_timesteps=3408000, episode_reward=-49.56 +/- 37.29\n","Episode length: 450.80 +/- 40.17\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 451      |\n","|    mean_reward     | -49.6    |\n","| time/              |          |\n","|    total_timesteps | 3408000  |\n","| train/             |          |\n","|    actor_loss      | 3.95     |\n","|    critic_loss     | 0.659    |\n","|    ent_coef        | 0.00385  |\n","|    ent_coef_loss   | -0.755   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141995   |\n","---------------------------------\n","Eval num_timesteps=3410400, episode_reward=4.22 +/- 2.83\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.22     |\n","| time/              |          |\n","|    total_timesteps | 3410400  |\n","| train/             |          |\n","|    actor_loss      | 3.68     |\n","|    critic_loss     | 0.438    |\n","|    ent_coef        | 0.0039   |\n","|    ent_coef_loss   | 2.64     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 142095   |\n","---------------------------------\n","Eval num_timesteps=3412800, episode_reward=-33.99 +/- 33.50\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -34      |\n","| time/              |          |\n","|    total_timesteps | 3412800  |\n","| train/             |          |\n","|    actor_loss      | 3.85     |\n","|    critic_loss     | 0.507    |\n","|    ent_coef        | 0.00396  |\n","|    ent_coef_loss   | 1.35     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 142195   |\n","---------------------------------\n","Eval num_timesteps=3415200, episode_reward=-28.44 +/- 33.46\n","Episode length: 398.00 +/- 124.92\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 398      |\n","|    mean_reward     | -28.4    |\n","| time/              |          |\n","|    total_timesteps | 3415200  |\n","| train/             |          |\n","|    actor_loss      | 4.24     |\n","|    critic_loss     | 17.8     |\n","|    ent_coef        | 0.00388  |\n","|    ent_coef_loss   | -2.64    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 142295   |\n","---------------------------------\n","Eval num_timesteps=3417600, episode_reward=-6.11 +/- 0.63\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -6.11    |\n","| time/              |          |\n","|    total_timesteps | 3417600  |\n","| train/             |          |\n","|    actor_loss      | 3.17     |\n","|    critic_loss     | 0.722    |\n","|    ent_coef        | 0.00389  |\n","|    ent_coef_loss   | -0.291   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 142395   |\n","---------------------------------\n","Eval num_timesteps=3420000, episode_reward=-46.81 +/- 40.83\n","Episode length: 363.80 +/- 111.21\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 364      |\n","|    mean_reward     | -46.8    |\n","| time/              |          |\n","|    total_timesteps | 3420000  |\n","| train/             |          |\n","|    actor_loss      | 4.27     |\n","|    critic_loss     | 0.701    |\n","|    ent_coef        | 0.00392  |\n","|    ent_coef_loss   | 4        |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 142495   |\n","---------------------------------\n","Eval num_timesteps=3422400, episode_reward=-29.05 +/- 46.41\n","Episode length: 499.20 +/- 0.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 499      |\n","|    mean_reward     | -29      |\n","| time/              |          |\n","|    total_timesteps | 3422400  |\n","| train/             |          |\n","|    actor_loss      | 3.8      |\n","|    critic_loss     | 15.5     |\n","|    ent_coef        | 0.00399  |\n","|    ent_coef_loss   | 1.78     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 142595   |\n","---------------------------------\n","Eval num_timesteps=3424800, episode_reward=-2.62 +/- 1.61\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -2.62    |\n","| time/              |          |\n","|    total_timesteps | 3424800  |\n","| train/             |          |\n","|    actor_loss      | 4.05     |\n","|    critic_loss     | 1.98     |\n","|    ent_coef        | 0.00404  |\n","|    ent_coef_loss   | -0.694   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 142695   |\n","---------------------------------\n","Eval num_timesteps=3427200, episode_reward=4.05 +/- 0.71\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.05     |\n","| time/              |          |\n","|    total_timesteps | 3427200  |\n","| train/             |          |\n","|    actor_loss      | 3.28     |\n","|    critic_loss     | 0.633    |\n","|    ent_coef        | 0.00398  |\n","|    ent_coef_loss   | -1.25    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 142795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6310     |\n","|    fps             | 279      |\n","|    time_elapsed    | 12262    |\n","|    total_timesteps | 3428712  |\n","| train/             |          |\n","|    actor_loss      | 3.55     |\n","|    critic_loss     | 4.67     |\n","|    ent_coef        | 0.00396  |\n","|    ent_coef_loss   | -2.87    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 142858   |\n","---------------------------------\n","Eval num_timesteps=3429600, episode_reward=-0.48 +/- 0.27\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.475   |\n","| time/              |          |\n","|    total_timesteps | 3429600  |\n","| train/             |          |\n","|    actor_loss      | 3.65     |\n","|    critic_loss     | 0.509    |\n","|    ent_coef        | 0.00395  |\n","|    ent_coef_loss   | 0.948    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 142895   |\n","---------------------------------\n","Eval num_timesteps=3432000, episode_reward=2.03 +/- 2.71\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.03     |\n","| time/              |          |\n","|    total_timesteps | 3432000  |\n","| train/             |          |\n","|    actor_loss      | 3.86     |\n","|    critic_loss     | 1.54     |\n","|    ent_coef        | 0.00396  |\n","|    ent_coef_loss   | 0.631    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 142995   |\n","---------------------------------\n","Eval num_timesteps=3434400, episode_reward=-32.42 +/- 40.15\n","Episode length: 336.80 +/- 199.88\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 337      |\n","|    mean_reward     | -32.4    |\n","| time/              |          |\n","|    total_timesteps | 3434400  |\n","| train/             |          |\n","|    actor_loss      | 3.42     |\n","|    critic_loss     | 20.5     |\n","|    ent_coef        | 0.00392  |\n","|    ent_coef_loss   | 0.492    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 143095   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6320     |\n","|    fps             | 279      |\n","|    time_elapsed    | 12293    |\n","|    total_timesteps | 3436752  |\n","| train/             |          |\n","|    actor_loss      | 4.07     |\n","|    critic_loss     | 2.42     |\n","|    ent_coef        | 0.00389  |\n","|    ent_coef_loss   | 1.21     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 143193   |\n","---------------------------------\n","Eval num_timesteps=3436800, episode_reward=-82.76 +/- 2.76\n","Episode length: 127.60 +/- 24.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 128      |\n","|    mean_reward     | -82.8    |\n","| time/              |          |\n","|    total_timesteps | 3436800  |\n","| train/             |          |\n","|    actor_loss      | 3.68     |\n","|    critic_loss     | 22.5     |\n","|    ent_coef        | 0.00389  |\n","|    ent_coef_loss   | 0.0509   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 143195   |\n","---------------------------------\n","Eval num_timesteps=3439200, episode_reward=-32.90 +/- 39.12\n","Episode length: 354.40 +/- 178.32\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 354      |\n","|    mean_reward     | -32.9    |\n","| time/              |          |\n","|    total_timesteps | 3439200  |\n","| train/             |          |\n","|    actor_loss      | 3.64     |\n","|    critic_loss     | 0.423    |\n","|    ent_coef        | 0.00392  |\n","|    ent_coef_loss   | -2.47    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 143295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6330     |\n","|    fps             | 279      |\n","|    time_elapsed    | 12302    |\n","|    total_timesteps | 3440472  |\n","| train/             |          |\n","|    actor_loss      | 3.52     |\n","|    critic_loss     | 0.275    |\n","|    ent_coef        | 0.00389  |\n","|    ent_coef_loss   | -0.00264 |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 143348   |\n","---------------------------------\n","Eval num_timesteps=3441600, episode_reward=-78.59 +/- 5.27\n","Episode length: 351.80 +/- 118.56\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 352      |\n","|    mean_reward     | -78.6    |\n","| time/              |          |\n","|    total_timesteps | 3441600  |\n","| train/             |          |\n","|    actor_loss      | 3.61     |\n","|    critic_loss     | 16.9     |\n","|    ent_coef        | 0.00385  |\n","|    ent_coef_loss   | -1.55    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 143395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6340     |\n","|    fps             | 279      |\n","|    time_elapsed    | 12312    |\n","|    total_timesteps | 3442104  |\n","| train/             |          |\n","|    actor_loss      | 3.9      |\n","|    critic_loss     | 1.36     |\n","|    ent_coef        | 0.00386  |\n","|    ent_coef_loss   | 0.0485   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 143416   |\n","---------------------------------\n","Eval num_timesteps=3444000, episode_reward=-73.63 +/- 3.66\n","Episode length: 236.20 +/- 65.16\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 236      |\n","|    mean_reward     | -73.6    |\n","| time/              |          |\n","|    total_timesteps | 3444000  |\n","| train/             |          |\n","|    actor_loss      | 3.52     |\n","|    critic_loss     | 0.259    |\n","|    ent_coef        | 0.00393  |\n","|    ent_coef_loss   | 0.567    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 143495   |\n","---------------------------------\n","Eval num_timesteps=3446400, episode_reward=1.54 +/- 1.72\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.54     |\n","| time/              |          |\n","|    total_timesteps | 3446400  |\n","| train/             |          |\n","|    actor_loss      | 3.49     |\n","|    critic_loss     | 0.463    |\n","|    ent_coef        | 0.00395  |\n","|    ent_coef_loss   | -2.67    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 143595   |\n","---------------------------------\n","Eval num_timesteps=3448800, episode_reward=-48.39 +/- 37.02\n","Episode length: 259.40 +/- 196.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 259      |\n","|    mean_reward     | -48.4    |\n","| time/              |          |\n","|    total_timesteps | 3448800  |\n","| train/             |          |\n","|    actor_loss      | 3.71     |\n","|    critic_loss     | 0.284    |\n","|    ent_coef        | 0.00389  |\n","|    ent_coef_loss   | -1.68    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 143695   |\n","---------------------------------\n","Eval num_timesteps=3451200, episode_reward=-79.55 +/- 2.55\n","Episode length: 107.20 +/- 21.07\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 107      |\n","|    mean_reward     | -79.5    |\n","| time/              |          |\n","|    total_timesteps | 3451200  |\n","| train/             |          |\n","|    actor_loss      | 3.25     |\n","|    critic_loss     | 0.522    |\n","|    ent_coef        | 0.0039   |\n","|    ent_coef_loss   | -0.394   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 143795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6350     |\n","|    fps             | 279      |\n","|    time_elapsed    | 12340    |\n","|    total_timesteps | 3451440  |\n","| train/             |          |\n","|    actor_loss      | 3.84     |\n","|    critic_loss     | 20.3     |\n","|    ent_coef        | 0.00391  |\n","|    ent_coef_loss   | 3.07     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 143805   |\n","---------------------------------\n","Eval num_timesteps=3453600, episode_reward=-73.89 +/- 4.94\n","Episode length: 236.80 +/- 138.64\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 237      |\n","|    mean_reward     | -73.9    |\n","| time/              |          |\n","|    total_timesteps | 3453600  |\n","| train/             |          |\n","|    actor_loss      | 4.76     |\n","|    critic_loss     | 0.378    |\n","|    ent_coef        | 0.00391  |\n","|    ent_coef_loss   | 1.21     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 143895   |\n","---------------------------------\n","Eval num_timesteps=3456000, episode_reward=0.42 +/- 1.48\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.422    |\n","| time/              |          |\n","|    total_timesteps | 3456000  |\n","| train/             |          |\n","|    actor_loss      | 3.79     |\n","|    critic_loss     | 0.817    |\n","|    ent_coef        | 0.0039   |\n","|    ent_coef_loss   | -1.76    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 143995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6360     |\n","|    fps             | 279      |\n","|    time_elapsed    | 12357    |\n","|    total_timesteps | 3456864  |\n","| train/             |          |\n","|    actor_loss      | 3.92     |\n","|    critic_loss     | 1.63     |\n","|    ent_coef        | 0.00388  |\n","|    ent_coef_loss   | -0.526   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 144031   |\n","---------------------------------\n","Eval num_timesteps=3458400, episode_reward=-45.12 +/- 39.46\n","Episode length: 297.20 +/- 165.59\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 297      |\n","|    mean_reward     | -45.1    |\n","| time/              |          |\n","|    total_timesteps | 3458400  |\n","| train/             |          |\n","|    actor_loss      | 4.29     |\n","|    critic_loss     | 0.369    |\n","|    ent_coef        | 0.0039   |\n","|    ent_coef_loss   | 0.0419   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 144095   |\n","---------------------------------\n","Eval num_timesteps=3460800, episode_reward=-0.01 +/- 0.52\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.0134  |\n","| time/              |          |\n","|    total_timesteps | 3460800  |\n","| train/             |          |\n","|    actor_loss      | 4.39     |\n","|    critic_loss     | 0.695    |\n","|    ent_coef        | 0.00396  |\n","|    ent_coef_loss   | 0.912    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 144195   |\n","---------------------------------\n","Eval num_timesteps=3463200, episode_reward=-34.65 +/- 37.28\n","Episode length: 390.40 +/- 134.23\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 390      |\n","|    mean_reward     | -34.7    |\n","| time/              |          |\n","|    total_timesteps | 3463200  |\n","| train/             |          |\n","|    actor_loss      | 4.02     |\n","|    critic_loss     | 0.778    |\n","|    ent_coef        | 0.00403  |\n","|    ent_coef_loss   | 1.88     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 144295   |\n","---------------------------------\n","Eval num_timesteps=3465600, episode_reward=-15.86 +/- 32.73\n","Episode length: 492.00 +/- 9.80\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 492      |\n","|    mean_reward     | -15.9    |\n","| time/              |          |\n","|    total_timesteps | 3465600  |\n","| train/             |          |\n","|    actor_loss      | 4.28     |\n","|    critic_loss     | 0.525    |\n","|    ent_coef        | 0.00409  |\n","|    ent_coef_loss   | -1.42    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 144395   |\n","---------------------------------\n","Eval num_timesteps=3468000, episode_reward=-81.00 +/- 2.47\n","Episode length: 90.00 +/- 4.90\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 90       |\n","|    mean_reward     | -81      |\n","| time/              |          |\n","|    total_timesteps | 3468000  |\n","| train/             |          |\n","|    actor_loss      | 3.69     |\n","|    critic_loss     | 2.7      |\n","|    ent_coef        | 0.00411  |\n","|    ent_coef_loss   | -0.768   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 144495   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6370     |\n","|    fps             | 279      |\n","|    time_elapsed    | 12398    |\n","|    total_timesteps | 3468120  |\n","| train/             |          |\n","|    actor_loss      | 4.14     |\n","|    critic_loss     | 0.307    |\n","|    ent_coef        | 0.00411  |\n","|    ent_coef_loss   | -0.311   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 144500   |\n","---------------------------------\n","Eval num_timesteps=3470400, episode_reward=-78.07 +/- 3.99\n","Episode length: 286.60 +/- 149.91\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 287      |\n","|    mean_reward     | -78.1    |\n","| time/              |          |\n","|    total_timesteps | 3470400  |\n","| train/             |          |\n","|    actor_loss      | 9.38     |\n","|    critic_loss     | 2.04     |\n","|    ent_coef        | 0.00415  |\n","|    ent_coef_loss   | -0.229   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 144595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6380     |\n","|    fps             | 279      |\n","|    time_elapsed    | 12406    |\n","|    total_timesteps | 3472584  |\n","| train/             |          |\n","|    actor_loss      | 3.67     |\n","|    critic_loss     | 0.59     |\n","|    ent_coef        | 0.00414  |\n","|    ent_coef_loss   | -2.38    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 144686   |\n","---------------------------------\n","Eval num_timesteps=3472800, episode_reward=-15.55 +/- 54.01\n","Episode length: 365.20 +/- 165.10\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 365      |\n","|    mean_reward     | -15.6    |\n","| time/              |          |\n","|    total_timesteps | 3472800  |\n","| train/             |          |\n","|    actor_loss      | 3.67     |\n","|    critic_loss     | 16.5     |\n","|    ent_coef        | 0.00414  |\n","|    ent_coef_loss   | -3.7     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 144695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6390     |\n","|    fps             | 279      |\n","|    time_elapsed    | 12413    |\n","|    total_timesteps | 3473784  |\n","| train/             |          |\n","|    actor_loss      | 4.41     |\n","|    critic_loss     | 0.66     |\n","|    ent_coef        | 0.0041   |\n","|    ent_coef_loss   | -0.284   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 144736   |\n","---------------------------------\n","Eval num_timesteps=3475200, episode_reward=-10.51 +/- 52.88\n","Episode length: 409.60 +/- 110.72\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 410      |\n","|    mean_reward     | -10.5    |\n","| time/              |          |\n","|    total_timesteps | 3475200  |\n","| train/             |          |\n","|    actor_loss      | 4.11     |\n","|    critic_loss     | 0.763    |\n","|    ent_coef        | 0.00407  |\n","|    ent_coef_loss   | 0.132    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 144795   |\n","---------------------------------\n","Eval num_timesteps=3477600, episode_reward=-13.09 +/- 35.39\n","Episode length: 444.00 +/- 68.59\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 444      |\n","|    mean_reward     | -13.1    |\n","| time/              |          |\n","|    total_timesteps | 3477600  |\n","| train/             |          |\n","|    actor_loss      | 8.22     |\n","|    critic_loss     | 0.681    |\n","|    ent_coef        | 0.00402  |\n","|    ent_coef_loss   | -3.78    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 144895   |\n","---------------------------------\n","Eval num_timesteps=3480000, episode_reward=-67.13 +/- 6.49\n","Episode length: 229.00 +/- 107.78\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 229      |\n","|    mean_reward     | -67.1    |\n","| time/              |          |\n","|    total_timesteps | 3480000  |\n","| train/             |          |\n","|    actor_loss      | 4.41     |\n","|    critic_loss     | 0.53     |\n","|    ent_coef        | 0.00389  |\n","|    ent_coef_loss   | 0.684    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 144995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6400     |\n","|    fps             | 279      |\n","|    time_elapsed    | 12440    |\n","|    total_timesteps | 3482184  |\n","| train/             |          |\n","|    actor_loss      | 3.91     |\n","|    critic_loss     | 2.91     |\n","|    ent_coef        | 0.0038   |\n","|    ent_coef_loss   | 0.993    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 145086   |\n","---------------------------------\n","Eval num_timesteps=3482400, episode_reward=-25.12 +/- 38.37\n","Episode length: 444.00 +/- 68.59\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 444      |\n","|    mean_reward     | -25.1    |\n","| time/              |          |\n","|    total_timesteps | 3482400  |\n","| train/             |          |\n","|    actor_loss      | 4.39     |\n","|    critic_loss     | 0.512    |\n","|    ent_coef        | 0.0038   |\n","|    ent_coef_loss   | 0.939    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 145095   |\n","---------------------------------\n","Eval num_timesteps=3484800, episode_reward=12.80 +/- 2.83\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 12.8     |\n","| time/              |          |\n","|    total_timesteps | 3484800  |\n","| train/             |          |\n","|    actor_loss      | 4.14     |\n","|    critic_loss     | 0.484    |\n","|    ent_coef        | 0.00377  |\n","|    ent_coef_loss   | 1.04     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 145195   |\n","---------------------------------\n","Eval num_timesteps=3487200, episode_reward=-76.06 +/- 2.00\n","Episode length: 186.60 +/- 58.30\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 187      |\n","|    mean_reward     | -76.1    |\n","| time/              |          |\n","|    total_timesteps | 3487200  |\n","| train/             |          |\n","|    actor_loss      | 4.78     |\n","|    critic_loss     | 14.8     |\n","|    ent_coef        | 0.00376  |\n","|    ent_coef_loss   | 2.87     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 145295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6410     |\n","|    fps             | 279      |\n","|    time_elapsed    | 12467    |\n","|    total_timesteps | 3488112  |\n","| train/             |          |\n","|    actor_loss      | 4.69     |\n","|    critic_loss     | 0.471    |\n","|    ent_coef        | 0.0038   |\n","|    ent_coef_loss   | 1.99     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 145333   |\n","---------------------------------\n","Eval num_timesteps=3489600, episode_reward=-64.14 +/- 6.78\n","Episode length: 294.40 +/- 122.96\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 294      |\n","|    mean_reward     | -64.1    |\n","| time/              |          |\n","|    total_timesteps | 3489600  |\n","| train/             |          |\n","|    actor_loss      | 4.35     |\n","|    critic_loss     | 0.23     |\n","|    ent_coef        | 0.00387  |\n","|    ent_coef_loss   | 1.02     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 145395   |\n","---------------------------------\n","Eval num_timesteps=3492000, episode_reward=-74.59 +/- 3.49\n","Episode length: 148.60 +/- 31.35\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 149      |\n","|    mean_reward     | -74.6    |\n","| time/              |          |\n","|    total_timesteps | 3492000  |\n","| train/             |          |\n","|    actor_loss      | 4.16     |\n","|    critic_loss     | 0.36     |\n","|    ent_coef        | 0.00388  |\n","|    ent_coef_loss   | -2.88    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 145495   |\n","---------------------------------\n","Eval num_timesteps=3494400, episode_reward=17.08 +/- 2.05\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 17.1     |\n","| time/              |          |\n","|    total_timesteps | 3494400  |\n","| train/             |          |\n","|    actor_loss      | 4.24     |\n","|    critic_loss     | 0.449    |\n","|    ent_coef        | 0.00387  |\n","|    ent_coef_loss   | -2.13    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 145595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6420     |\n","|    fps             | 279      |\n","|    time_elapsed    | 12487    |\n","|    total_timesteps | 3494448  |\n","| train/             |          |\n","|    actor_loss      | 4.46     |\n","|    critic_loss     | 11       |\n","|    ent_coef        | 0.00387  |\n","|    ent_coef_loss   | 4.06     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 145597   |\n","---------------------------------\n","Eval num_timesteps=3496800, episode_reward=-17.67 +/- 44.60\n","Episode length: 405.60 +/- 115.62\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 406      |\n","|    mean_reward     | -17.7    |\n","| time/              |          |\n","|    total_timesteps | 3496800  |\n","| train/             |          |\n","|    actor_loss      | 3.98     |\n","|    critic_loss     | 22.7     |\n","|    ent_coef        | 0.00389  |\n","|    ent_coef_loss   | -4       |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 145695   |\n","---------------------------------\n","Eval num_timesteps=3499200, episode_reward=27.47 +/- 0.99\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 27.5     |\n","| time/              |          |\n","|    total_timesteps | 3499200  |\n","| train/             |          |\n","|    actor_loss      | 3.7      |\n","|    critic_loss     | 0.284    |\n","|    ent_coef        | 0.00389  |\n","|    ent_coef_loss   | -1.58    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 145795   |\n","---------------------------------\n","Eval num_timesteps=3501600, episode_reward=-29.87 +/- 39.93\n","Episode length: 424.00 +/- 93.08\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 424      |\n","|    mean_reward     | -29.9    |\n","| time/              |          |\n","|    total_timesteps | 3501600  |\n","| train/             |          |\n","|    actor_loss      | 3.99     |\n","|    critic_loss     | 0.609    |\n","|    ent_coef        | 0.0039   |\n","|    ent_coef_loss   | 2.28     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 145895   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6430     |\n","|    fps             | 279      |\n","|    time_elapsed    | 12517    |\n","|    total_timesteps | 3502872  |\n","| train/             |          |\n","|    actor_loss      | 4.41     |\n","|    critic_loss     | 14.4     |\n","|    ent_coef        | 0.00389  |\n","|    ent_coef_loss   | -0.00735 |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 145948   |\n","---------------------------------\n","Eval num_timesteps=3504000, episode_reward=-72.13 +/- 3.98\n","Episode length: 232.80 +/- 43.11\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 233      |\n","|    mean_reward     | -72.1    |\n","| time/              |          |\n","|    total_timesteps | 3504000  |\n","| train/             |          |\n","|    actor_loss      | 4.62     |\n","|    critic_loss     | 12.1     |\n","|    ent_coef        | 0.0039   |\n","|    ent_coef_loss   | 3.52     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 145995   |\n","---------------------------------\n","Eval num_timesteps=3506400, episode_reward=20.14 +/- 3.02\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 20.1     |\n","| time/              |          |\n","|    total_timesteps | 3506400  |\n","| train/             |          |\n","|    actor_loss      | 3.73     |\n","|    critic_loss     | 0.62     |\n","|    ent_coef        | 0.00398  |\n","|    ent_coef_loss   | -1.77    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 146095   |\n","---------------------------------\n","Eval num_timesteps=3508800, episode_reward=-18.02 +/- 36.01\n","Episode length: 491.60 +/- 10.29\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 492      |\n","|    mean_reward     | -18      |\n","| time/              |          |\n","|    total_timesteps | 3508800  |\n","| train/             |          |\n","|    actor_loss      | 3.2      |\n","|    critic_loss     | 0.222    |\n","|    ent_coef        | 0.004    |\n","|    ent_coef_loss   | -3.5     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 146195   |\n","---------------------------------\n","Eval num_timesteps=3511200, episode_reward=-34.91 +/- 43.84\n","Episode length: 388.40 +/- 91.12\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 388      |\n","|    mean_reward     | -34.9    |\n","| time/              |          |\n","|    total_timesteps | 3511200  |\n","| train/             |          |\n","|    actor_loss      | 3.99     |\n","|    critic_loss     | 0.546    |\n","|    ent_coef        | 0.00393  |\n","|    ent_coef_loss   | -1.42    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 146295   |\n","---------------------------------\n","Eval num_timesteps=3513600, episode_reward=-8.10 +/- 45.53\n","Episode length: 425.60 +/- 91.12\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 426      |\n","|    mean_reward     | -8.1     |\n","| time/              |          |\n","|    total_timesteps | 3513600  |\n","| train/             |          |\n","|    actor_loss      | 3.68     |\n","|    critic_loss     | 0.764    |\n","|    ent_coef        | 0.00385  |\n","|    ent_coef_loss   | -3.6     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 146395   |\n","---------------------------------\n","Eval num_timesteps=3516000, episode_reward=-27.48 +/- 31.32\n","Episode length: 450.20 +/- 40.66\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 450      |\n","|    mean_reward     | -27.5    |\n","| time/              |          |\n","|    total_timesteps | 3516000  |\n","| train/             |          |\n","|    actor_loss      | 6.41     |\n","|    critic_loss     | 0.694    |\n","|    ent_coef        | 0.00376  |\n","|    ent_coef_loss   | -3.7     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 146495   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6440     |\n","|    fps             | 279      |\n","|    time_elapsed    | 12568    |\n","|    total_timesteps | 3518328  |\n","| train/             |          |\n","|    actor_loss      | 3.78     |\n","|    critic_loss     | 0.341    |\n","|    ent_coef        | 0.00364  |\n","|    ent_coef_loss   | -0.851   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 146592   |\n","---------------------------------\n","Eval num_timesteps=3518400, episode_reward=38.76 +/- 0.40\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 38.8     |\n","| time/              |          |\n","|    total_timesteps | 3518400  |\n","| train/             |          |\n","|    actor_loss      | 3.86     |\n","|    critic_loss     | 0.487    |\n","|    ent_coef        | 0.00364  |\n","|    ent_coef_loss   | -2.29    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 146595   |\n","---------------------------------\n","New best mean reward!\n","Eval num_timesteps=3520800, episode_reward=-73.20 +/- 2.18\n","Episode length: 221.20 +/- 1.47\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 221      |\n","|    mean_reward     | -73.2    |\n","| time/              |          |\n","|    total_timesteps | 3520800  |\n","| train/             |          |\n","|    actor_loss      | 4.7      |\n","|    critic_loss     | 1.9      |\n","|    ent_coef        | 0.00358  |\n","|    ent_coef_loss   | 1.83     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 146695   |\n","---------------------------------\n","Eval num_timesteps=3523200, episode_reward=-43.54 +/- 42.16\n","Episode length: 255.20 +/- 199.88\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 255      |\n","|    mean_reward     | -43.5    |\n","| time/              |          |\n","|    total_timesteps | 3523200  |\n","| train/             |          |\n","|    actor_loss      | 4.21     |\n","|    critic_loss     | 0.951    |\n","|    ent_coef        | 0.00361  |\n","|    ent_coef_loss   | 0.35     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 146795   |\n","---------------------------------\n","Eval num_timesteps=3525600, episode_reward=-77.12 +/- 1.72\n","Episode length: 257.00 +/- 149.42\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 257      |\n","|    mean_reward     | -77.1    |\n","| time/              |          |\n","|    total_timesteps | 3525600  |\n","| train/             |          |\n","|    actor_loss      | 3.76     |\n","|    critic_loss     | 0.775    |\n","|    ent_coef        | 0.00368  |\n","|    ent_coef_loss   | 0.18     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 146895   |\n","---------------------------------\n","Eval num_timesteps=3528000, episode_reward=-73.85 +/- 4.97\n","Episode length: 172.80 +/- 59.77\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 173      |\n","|    mean_reward     | -73.8    |\n","| time/              |          |\n","|    total_timesteps | 3528000  |\n","| train/             |          |\n","|    actor_loss      | 3.27     |\n","|    critic_loss     | 0.769    |\n","|    ent_coef        | 0.00365  |\n","|    ent_coef_loss   | -3.09    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 146995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6450     |\n","|    fps             | 279      |\n","|    time_elapsed    | 12602    |\n","|    total_timesteps | 3528624  |\n","| train/             |          |\n","|    actor_loss      | 4.04     |\n","|    critic_loss     | 0.949    |\n","|    ent_coef        | 0.00363  |\n","|    ent_coef_loss   | 1.24     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 147021   |\n","---------------------------------\n","Eval num_timesteps=3530400, episode_reward=-74.62 +/- 4.48\n","Episode length: 219.20 +/- 106.31\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 219      |\n","|    mean_reward     | -74.6    |\n","| time/              |          |\n","|    total_timesteps | 3530400  |\n","| train/             |          |\n","|    actor_loss      | 3.84     |\n","|    critic_loss     | 0.649    |\n","|    ent_coef        | 0.00359  |\n","|    ent_coef_loss   | -1.92    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 147095   |\n","---------------------------------\n","Eval num_timesteps=3532800, episode_reward=-23.40 +/- 30.26\n","Episode length: 488.80 +/- 13.72\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 489      |\n","|    mean_reward     | -23.4    |\n","| time/              |          |\n","|    total_timesteps | 3532800  |\n","| train/             |          |\n","|    actor_loss      | 4.54     |\n","|    critic_loss     | 18.1     |\n","|    ent_coef        | 0.00351  |\n","|    ent_coef_loss   | 4.69     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 147195   |\n","---------------------------------\n","Eval num_timesteps=3535200, episode_reward=19.19 +/- 3.42\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 19.2     |\n","| time/              |          |\n","|    total_timesteps | 3535200  |\n","| train/             |          |\n","|    actor_loss      | 3.99     |\n","|    critic_loss     | 1.39     |\n","|    ent_coef        | 0.00355  |\n","|    ent_coef_loss   | 1.51     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 147295   |\n","---------------------------------\n","Eval num_timesteps=3537600, episode_reward=15.80 +/- 2.92\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 15.8     |\n","| time/              |          |\n","|    total_timesteps | 3537600  |\n","| train/             |          |\n","|    actor_loss      | 5.48     |\n","|    critic_loss     | 0.619    |\n","|    ent_coef        | 0.00355  |\n","|    ent_coef_loss   | 3.27     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 147395   |\n","---------------------------------\n","Eval num_timesteps=3540000, episode_reward=22.22 +/- 5.01\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 22.2     |\n","| time/              |          |\n","|    total_timesteps | 3540000  |\n","| train/             |          |\n","|    actor_loss      | 3.89     |\n","|    critic_loss     | 0.525    |\n","|    ent_coef        | 0.00355  |\n","|    ent_coef_loss   | 0.517    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 147495   |\n","---------------------------------\n","Eval num_timesteps=3542400, episode_reward=-1.75 +/- 0.12\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.75    |\n","| time/              |          |\n","|    total_timesteps | 3542400  |\n","| train/             |          |\n","|    actor_loss      | 3.34     |\n","|    critic_loss     | 0.857    |\n","|    ent_coef        | 0.00352  |\n","|    ent_coef_loss   | 0.891    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 147595   |\n","---------------------------------\n","Eval num_timesteps=3544800, episode_reward=-38.83 +/- 34.52\n","Episode length: 425.00 +/- 61.24\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 425      |\n","|    mean_reward     | -38.8    |\n","| time/              |          |\n","|    total_timesteps | 3544800  |\n","| train/             |          |\n","|    actor_loss      | 3.45     |\n","|    critic_loss     | 0.289    |\n","|    ent_coef        | 0.00358  |\n","|    ent_coef_loss   | -1.86    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 147695   |\n","---------------------------------\n","Eval num_timesteps=3547200, episode_reward=-32.58 +/- 38.70\n","Episode length: 395.00 +/- 85.73\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 395      |\n","|    mean_reward     | -32.6    |\n","| time/              |          |\n","|    total_timesteps | 3547200  |\n","| train/             |          |\n","|    actor_loss      | 4.19     |\n","|    critic_loss     | 0.364    |\n","|    ent_coef        | 0.00363  |\n","|    ent_coef_loss   | 2.04     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 147795   |\n","---------------------------------\n","Eval num_timesteps=3549600, episode_reward=-14.02 +/- 44.85\n","Episode length: 475.60 +/- 29.88\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 476      |\n","|    mean_reward     | -14      |\n","| time/              |          |\n","|    total_timesteps | 3549600  |\n","| train/             |          |\n","|    actor_loss      | 3.75     |\n","|    critic_loss     | 0.332    |\n","|    ent_coef        | 0.00364  |\n","|    ent_coef_loss   | -0.638   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 147895   |\n","---------------------------------\n","Eval num_timesteps=3552000, episode_reward=0.37 +/- 6.49\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.372    |\n","| time/              |          |\n","|    total_timesteps | 3552000  |\n","| train/             |          |\n","|    actor_loss      | 3.53     |\n","|    critic_loss     | 0.426    |\n","|    ent_coef        | 0.00371  |\n","|    ent_coef_loss   | 1.61     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 147995   |\n","---------------------------------\n","Eval num_timesteps=3554400, episode_reward=8.27 +/- 0.62\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 8.27     |\n","| time/              |          |\n","|    total_timesteps | 3554400  |\n","| train/             |          |\n","|    actor_loss      | 3.83     |\n","|    critic_loss     | 0.561    |\n","|    ent_coef        | 0.00382  |\n","|    ent_coef_loss   | 2.18     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148095   |\n","---------------------------------\n","Eval num_timesteps=3556800, episode_reward=10.57 +/- 1.15\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 10.6     |\n","| time/              |          |\n","|    total_timesteps | 3556800  |\n","| train/             |          |\n","|    actor_loss      | 4.18     |\n","|    critic_loss     | 0.725    |\n","|    ent_coef        | 0.00383  |\n","|    ent_coef_loss   | 1.21     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148195   |\n","---------------------------------\n","Eval num_timesteps=3559200, episode_reward=2.69 +/- 3.53\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.69     |\n","| time/              |          |\n","|    total_timesteps | 3559200  |\n","| train/             |          |\n","|    actor_loss      | 3.6      |\n","|    critic_loss     | 0.547    |\n","|    ent_coef        | 0.00382  |\n","|    ent_coef_loss   | -0.466   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148295   |\n","---------------------------------\n","Eval num_timesteps=3561600, episode_reward=21.30 +/- 1.57\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 21.3     |\n","| time/              |          |\n","|    total_timesteps | 3561600  |\n","| train/             |          |\n","|    actor_loss      | 3.79     |\n","|    critic_loss     | 0.63     |\n","|    ent_coef        | 0.00384  |\n","|    ent_coef_loss   | 0.847    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148395   |\n","---------------------------------\n","Eval num_timesteps=3564000, episode_reward=28.62 +/- 1.03\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 28.6     |\n","| time/              |          |\n","|    total_timesteps | 3564000  |\n","| train/             |          |\n","|    actor_loss      | 3.85     |\n","|    critic_loss     | 1.04     |\n","|    ent_coef        | 0.00378  |\n","|    ent_coef_loss   | -3.47    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148495   |\n","---------------------------------\n","Eval num_timesteps=3566400, episode_reward=-20.77 +/- 38.65\n","Episode length: 366.00 +/- 164.12\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 366      |\n","|    mean_reward     | -20.8    |\n","| time/              |          |\n","|    total_timesteps | 3566400  |\n","| train/             |          |\n","|    actor_loss      | 4.25     |\n","|    critic_loss     | 3.72     |\n","|    ent_coef        | 0.00375  |\n","|    ent_coef_loss   | 0.855    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6460     |\n","|    fps             | 279      |\n","|    time_elapsed    | 12751    |\n","|    total_timesteps | 3567576  |\n","| train/             |          |\n","|    actor_loss      | 4.65     |\n","|    critic_loss     | 0.647    |\n","|    ent_coef        | 0.00376  |\n","|    ent_coef_loss   | 0.862    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148644   |\n","---------------------------------\n","Eval num_timesteps=3568800, episode_reward=14.53 +/- 4.52\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 14.5     |\n","| time/              |          |\n","|    total_timesteps | 3568800  |\n","| train/             |          |\n","|    actor_loss      | 3.44     |\n","|    critic_loss     | 0.39     |\n","|    ent_coef        | 0.00376  |\n","|    ent_coef_loss   | -1.15    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148695   |\n","---------------------------------\n","Eval num_timesteps=3571200, episode_reward=-37.19 +/- 48.96\n","Episode length: 263.60 +/- 193.02\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 264      |\n","|    mean_reward     | -37.2    |\n","| time/              |          |\n","|    total_timesteps | 3571200  |\n","| train/             |          |\n","|    actor_loss      | 4.22     |\n","|    critic_loss     | 0.683    |\n","|    ent_coef        | 0.00378  |\n","|    ent_coef_loss   | -0.638   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6470     |\n","|    fps             | 279      |\n","|    time_elapsed    | 12768    |\n","|    total_timesteps | 3572184  |\n","| train/             |          |\n","|    actor_loss      | 4.07     |\n","|    critic_loss     | 0.648    |\n","|    ent_coef        | 0.00378  |\n","|    ent_coef_loss   | -0.12    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148836   |\n","---------------------------------\n","Eval num_timesteps=3573600, episode_reward=-68.28 +/- 3.41\n","Episode length: 199.40 +/- 4.41\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 199      |\n","|    mean_reward     | -68.3    |\n","| time/              |          |\n","|    total_timesteps | 3573600  |\n","| train/             |          |\n","|    actor_loss      | 4.51     |\n","|    critic_loss     | 1.07     |\n","|    ent_coef        | 0.00381  |\n","|    ent_coef_loss   | -2.55    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148895   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6480     |\n","|    fps             | 279      |\n","|    time_elapsed    | 12776    |\n","|    total_timesteps | 3575616  |\n","| train/             |          |\n","|    actor_loss      | 3.93     |\n","|    critic_loss     | 20.2     |\n","|    ent_coef        | 0.00382  |\n","|    ent_coef_loss   | 2.18     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148979   |\n","---------------------------------\n","Eval num_timesteps=3576000, episode_reward=9.61 +/- 3.06\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 9.61     |\n","| time/              |          |\n","|    total_timesteps | 3576000  |\n","| train/             |          |\n","|    actor_loss      | 3.8      |\n","|    critic_loss     | 0.487    |\n","|    ent_coef        | 0.00384  |\n","|    ent_coef_loss   | 0.0619   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148995   |\n","---------------------------------\n","Eval num_timesteps=3578400, episode_reward=-48.45 +/- 42.17\n","Episode length: 462.80 +/- 30.37\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 463      |\n","|    mean_reward     | -48.5    |\n","| time/              |          |\n","|    total_timesteps | 3578400  |\n","| train/             |          |\n","|    actor_loss      | 4.29     |\n","|    critic_loss     | 19.1     |\n","|    ent_coef        | 0.00391  |\n","|    ent_coef_loss   | 0.205    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149095   |\n","---------------------------------\n","Eval num_timesteps=3580800, episode_reward=-32.78 +/- 28.89\n","Episode length: 408.20 +/- 74.95\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 408      |\n","|    mean_reward     | -32.8    |\n","| time/              |          |\n","|    total_timesteps | 3580800  |\n","| train/             |          |\n","|    actor_loss      | 3.88     |\n","|    critic_loss     | 17.5     |\n","|    ent_coef        | 0.00399  |\n","|    ent_coef_loss   | 0.137    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149195   |\n","---------------------------------\n","Eval num_timesteps=3583200, episode_reward=16.70 +/- 1.89\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 16.7     |\n","| time/              |          |\n","|    total_timesteps | 3583200  |\n","| train/             |          |\n","|    actor_loss      | 4.68     |\n","|    critic_loss     | 11       |\n","|    ent_coef        | 0.00399  |\n","|    ent_coef_loss   | 3.08     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149295   |\n","---------------------------------\n","Eval num_timesteps=3585600, episode_reward=18.57 +/- 3.67\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 18.6     |\n","| time/              |          |\n","|    total_timesteps | 3585600  |\n","| train/             |          |\n","|    actor_loss      | 4.11     |\n","|    critic_loss     | 1.39     |\n","|    ent_coef        | 0.00408  |\n","|    ent_coef_loss   | 1.61     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149395   |\n","---------------------------------\n","Eval num_timesteps=3588000, episode_reward=-76.40 +/- 4.16\n","Episode length: 336.20 +/- 10.78\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 336      |\n","|    mean_reward     | -76.4    |\n","| time/              |          |\n","|    total_timesteps | 3588000  |\n","| train/             |          |\n","|    actor_loss      | 4.31     |\n","|    critic_loss     | 0.955    |\n","|    ent_coef        | 0.00421  |\n","|    ent_coef_loss   | 3.61     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149495   |\n","---------------------------------\n","Eval num_timesteps=3590400, episode_reward=12.18 +/- 6.27\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 12.2     |\n","| time/              |          |\n","|    total_timesteps | 3590400  |\n","| train/             |          |\n","|    actor_loss      | 3.94     |\n","|    critic_loss     | 16.2     |\n","|    ent_coef        | 0.0043   |\n","|    ent_coef_loss   | -0.385   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6490     |\n","|    fps             | 279      |\n","|    time_elapsed    | 12834    |\n","|    total_timesteps | 3591168  |\n","| train/             |          |\n","|    actor_loss      | 4.54     |\n","|    critic_loss     | 12.4     |\n","|    ent_coef        | 0.0043   |\n","|    ent_coef_loss   | 2.26     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149627   |\n","---------------------------------\n","Eval num_timesteps=3592800, episode_reward=-48.95 +/- 32.35\n","Episode length: 449.00 +/- 41.64\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 449      |\n","|    mean_reward     | -48.9    |\n","| time/              |          |\n","|    total_timesteps | 3592800  |\n","| train/             |          |\n","|    actor_loss      | 3.99     |\n","|    critic_loss     | 1.83     |\n","|    ent_coef        | 0.00434  |\n","|    ent_coef_loss   | 3.69     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149695   |\n","---------------------------------\n","Eval num_timesteps=3595200, episode_reward=2.03 +/- 0.98\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.03     |\n","| time/              |          |\n","|    total_timesteps | 3595200  |\n","| train/             |          |\n","|    actor_loss      | 3.65     |\n","|    critic_loss     | 0.39     |\n","|    ent_coef        | 0.00451  |\n","|    ent_coef_loss   | 0.997    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149795   |\n","---------------------------------\n","Eval num_timesteps=3597600, episode_reward=-74.73 +/- 3.81\n","Episode length: 161.00 +/- 39.19\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 161      |\n","|    mean_reward     | -74.7    |\n","| time/              |          |\n","|    total_timesteps | 3597600  |\n","| train/             |          |\n","|    actor_loss      | 3.99     |\n","|    critic_loss     | 0.652    |\n","|    ent_coef        | 0.00471  |\n","|    ent_coef_loss   | 5.84     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149895   |\n","---------------------------------\n","Eval num_timesteps=3600000, episode_reward=-32.93 +/- 35.31\n","Episode length: 371.20 +/- 157.75\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 371      |\n","|    mean_reward     | -32.9    |\n","| time/              |          |\n","|    total_timesteps | 3600000  |\n","| train/             |          |\n","|    actor_loss      | 3.69     |\n","|    critic_loss     | 0.24     |\n","|    ent_coef        | 0.00488  |\n","|    ent_coef_loss   | -2.03    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149995   |\n","---------------------------------\n","Eval num_timesteps=3602400, episode_reward=0.56 +/- 0.63\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.564    |\n","| time/              |          |\n","|    total_timesteps | 3602400  |\n","| train/             |          |\n","|    actor_loss      | 3.75     |\n","|    critic_loss     | 0.304    |\n","|    ent_coef        | 0.00488  |\n","|    ent_coef_loss   | -1.92    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150095   |\n","---------------------------------\n","Eval num_timesteps=3604800, episode_reward=-48.82 +/- 38.24\n","Episode length: 288.20 +/- 172.93\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 288      |\n","|    mean_reward     | -48.8    |\n","| time/              |          |\n","|    total_timesteps | 3604800  |\n","| train/             |          |\n","|    actor_loss      | 3.47     |\n","|    critic_loss     | 1.15     |\n","|    ent_coef        | 0.00488  |\n","|    ent_coef_loss   | -2.34    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150195   |\n","---------------------------------\n","Eval num_timesteps=3607200, episode_reward=-2.81 +/- 2.84\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -2.81    |\n","| time/              |          |\n","|    total_timesteps | 3607200  |\n","| train/             |          |\n","|    actor_loss      | 4.19     |\n","|    critic_loss     | 0.665    |\n","|    ent_coef        | 0.0049   |\n","|    ent_coef_loss   | 1.33     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150295   |\n","---------------------------------\n","Eval num_timesteps=3609600, episode_reward=-30.67 +/- 33.10\n","Episode length: 370.40 +/- 158.73\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 370      |\n","|    mean_reward     | -30.7    |\n","| time/              |          |\n","|    total_timesteps | 3609600  |\n","| train/             |          |\n","|    actor_loss      | 3.76     |\n","|    critic_loss     | 0.474    |\n","|    ent_coef        | 0.0049   |\n","|    ent_coef_loss   | -2.09    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150395   |\n","---------------------------------\n","Eval num_timesteps=3612000, episode_reward=-40.19 +/- 40.75\n","Episode length: 290.00 +/- 171.46\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 290      |\n","|    mean_reward     | -40.2    |\n","| time/              |          |\n","|    total_timesteps | 3612000  |\n","| train/             |          |\n","|    actor_loss      | 3.58     |\n","|    critic_loss     | 22.2     |\n","|    ent_coef        | 0.00491  |\n","|    ent_coef_loss   | 0.396    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150495   |\n","---------------------------------\n","Eval num_timesteps=3614400, episode_reward=-1.79 +/- 5.62\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.79    |\n","| time/              |          |\n","|    total_timesteps | 3614400  |\n","| train/             |          |\n","|    actor_loss      | 3.43     |\n","|    critic_loss     | 0.511    |\n","|    ent_coef        | 0.00497  |\n","|    ent_coef_loss   | -2.03    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150595   |\n","---------------------------------\n","Eval num_timesteps=3616800, episode_reward=5.63 +/- 9.82\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.63     |\n","| time/              |          |\n","|    total_timesteps | 3616800  |\n","| train/             |          |\n","|    actor_loss      | 4.17     |\n","|    critic_loss     | 0.566    |\n","|    ent_coef        | 0.00494  |\n","|    ent_coef_loss   | 0.0757   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150695   |\n","---------------------------------\n","Eval num_timesteps=3619200, episode_reward=-1.44 +/- 0.28\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.44    |\n","| time/              |          |\n","|    total_timesteps | 3619200  |\n","| train/             |          |\n","|    actor_loss      | 3.75     |\n","|    critic_loss     | 0.514    |\n","|    ent_coef        | 0.00501  |\n","|    ent_coef_loss   | -1.68    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150795   |\n","---------------------------------\n","Eval num_timesteps=3621600, episode_reward=-41.16 +/- 31.37\n","Episode length: 366.20 +/- 109.25\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 366      |\n","|    mean_reward     | -41.2    |\n","| time/              |          |\n","|    total_timesteps | 3621600  |\n","| train/             |          |\n","|    actor_loss      | 4        |\n","|    critic_loss     | 0.859    |\n","|    ent_coef        | 0.00503  |\n","|    ent_coef_loss   | 0.973    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150895   |\n","---------------------------------\n","Eval num_timesteps=3624000, episode_reward=2.96 +/- 3.63\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.96     |\n","| time/              |          |\n","|    total_timesteps | 3624000  |\n","| train/             |          |\n","|    actor_loss      | 3.69     |\n","|    critic_loss     | 1.15     |\n","|    ent_coef        | 0.00504  |\n","|    ent_coef_loss   | -3.34    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150995   |\n","---------------------------------\n","Eval num_timesteps=3626400, episode_reward=6.28 +/- 2.04\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.28     |\n","| time/              |          |\n","|    total_timesteps | 3626400  |\n","| train/             |          |\n","|    actor_loss      | 4.69     |\n","|    critic_loss     | 0.273    |\n","|    ent_coef        | 0.00494  |\n","|    ent_coef_loss   | -1.98    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151095   |\n","---------------------------------\n","Eval num_timesteps=3628800, episode_reward=22.16 +/- 1.81\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 22.2     |\n","| time/              |          |\n","|    total_timesteps | 3628800  |\n","| train/             |          |\n","|    actor_loss      | 4.28     |\n","|    critic_loss     | 0.52     |\n","|    ent_coef        | 0.00484  |\n","|    ent_coef_loss   | -0.786   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151195   |\n","---------------------------------\n","Eval num_timesteps=3631200, episode_reward=-71.59 +/- 9.24\n","Episode length: 232.80 +/- 97.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 233      |\n","|    mean_reward     | -71.6    |\n","| time/              |          |\n","|    total_timesteps | 3631200  |\n","| train/             |          |\n","|    actor_loss      | 3.71     |\n","|    critic_loss     | 13.8     |\n","|    ent_coef        | 0.00481  |\n","|    ent_coef_loss   | -3.38    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151295   |\n","---------------------------------\n","Eval num_timesteps=3633600, episode_reward=13.32 +/- 2.92\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 13.3     |\n","| time/              |          |\n","|    total_timesteps | 3633600  |\n","| train/             |          |\n","|    actor_loss      | 3.89     |\n","|    critic_loss     | 20.1     |\n","|    ent_coef        | 0.00471  |\n","|    ent_coef_loss   | -0.777   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151395   |\n","---------------------------------\n","Eval num_timesteps=3636000, episode_reward=18.38 +/- 5.31\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 18.4     |\n","| time/              |          |\n","|    total_timesteps | 3636000  |\n","| train/             |          |\n","|    actor_loss      | 3.54     |\n","|    critic_loss     | 2.52     |\n","|    ent_coef        | 0.00472  |\n","|    ent_coef_loss   | -0.585   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151495   |\n","---------------------------------\n","Eval num_timesteps=3638400, episode_reward=-8.32 +/- 49.35\n","Episode length: 374.00 +/- 154.32\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 374      |\n","|    mean_reward     | -8.32    |\n","| time/              |          |\n","|    total_timesteps | 3638400  |\n","| train/             |          |\n","|    actor_loss      | 3.86     |\n","|    critic_loss     | 18.8     |\n","|    ent_coef        | 0.00462  |\n","|    ent_coef_loss   | -3.35    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151595   |\n","---------------------------------\n","Eval num_timesteps=3640800, episode_reward=-41.23 +/- 36.47\n","Episode length: 395.00 +/- 85.73\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 395      |\n","|    mean_reward     | -41.2    |\n","| time/              |          |\n","|    total_timesteps | 3640800  |\n","| train/             |          |\n","|    actor_loss      | 4.02     |\n","|    critic_loss     | 0.462    |\n","|    ent_coef        | 0.00454  |\n","|    ent_coef_loss   | 0.742    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6500     |\n","|    fps             | 279      |\n","|    time_elapsed    | 13026    |\n","|    total_timesteps | 3641928  |\n","| train/             |          |\n","|    actor_loss      | 4        |\n","|    critic_loss     | 0.426    |\n","|    ent_coef        | 0.00454  |\n","|    ent_coef_loss   | 3.61     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151742   |\n","---------------------------------\n","Eval num_timesteps=3643200, episode_reward=-25.75 +/- 39.65\n","Episode length: 371.20 +/- 157.75\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 371      |\n","|    mean_reward     | -25.8    |\n","| time/              |          |\n","|    total_timesteps | 3643200  |\n","| train/             |          |\n","|    actor_loss      | 3.58     |\n","|    critic_loss     | 0.457    |\n","|    ent_coef        | 0.00459  |\n","|    ent_coef_loss   | 0.0298   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151795   |\n","---------------------------------\n","Eval num_timesteps=3645600, episode_reward=-17.23 +/- 41.52\n","Episode length: 481.40 +/- 15.19\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 481      |\n","|    mean_reward     | -17.2    |\n","| time/              |          |\n","|    total_timesteps | 3645600  |\n","| train/             |          |\n","|    actor_loss      | 3.69     |\n","|    critic_loss     | 0.341    |\n","|    ent_coef        | 0.00458  |\n","|    ent_coef_loss   | -3.1     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151895   |\n","---------------------------------\n","Eval num_timesteps=3648000, episode_reward=-7.69 +/- 50.35\n","Episode length: 363.20 +/- 167.55\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 363      |\n","|    mean_reward     | -7.69    |\n","| time/              |          |\n","|    total_timesteps | 3648000  |\n","| train/             |          |\n","|    actor_loss      | 3.37     |\n","|    critic_loss     | 0.378    |\n","|    ent_coef        | 0.00451  |\n","|    ent_coef_loss   | -3.7     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151995   |\n","---------------------------------\n","Eval num_timesteps=3650400, episode_reward=-20.09 +/- 43.99\n","Episode length: 377.60 +/- 99.94\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 378      |\n","|    mean_reward     | -20.1    |\n","| time/              |          |\n","|    total_timesteps | 3650400  |\n","| train/             |          |\n","|    actor_loss      | 3.81     |\n","|    critic_loss     | 18.4     |\n","|    ent_coef        | 0.00439  |\n","|    ent_coef_loss   | -1.34    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152095   |\n","---------------------------------\n","Eval num_timesteps=3652800, episode_reward=-31.63 +/- 35.62\n","Episode length: 392.80 +/- 131.29\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 393      |\n","|    mean_reward     | -31.6    |\n","| time/              |          |\n","|    total_timesteps | 3652800  |\n","| train/             |          |\n","|    actor_loss      | 3.88     |\n","|    critic_loss     | 0.753    |\n","|    ent_coef        | 0.00434  |\n","|    ent_coef_loss   | 0.62     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152195   |\n","---------------------------------\n","Eval num_timesteps=3655200, episode_reward=20.89 +/- 0.36\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 20.9     |\n","| time/              |          |\n","|    total_timesteps | 3655200  |\n","| train/             |          |\n","|    actor_loss      | 3.75     |\n","|    critic_loss     | 20.5     |\n","|    ent_coef        | 0.00436  |\n","|    ent_coef_loss   | -1.08    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152295   |\n","---------------------------------\n","Eval num_timesteps=3657600, episode_reward=-75.40 +/- 3.38\n","Episode length: 189.00 +/- 24.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 189      |\n","|    mean_reward     | -75.4    |\n","| time/              |          |\n","|    total_timesteps | 3657600  |\n","| train/             |          |\n","|    actor_loss      | 3.63     |\n","|    critic_loss     | 0.273    |\n","|    ent_coef        | 0.00442  |\n","|    ent_coef_loss   | 1.23     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152395   |\n","---------------------------------\n","Eval num_timesteps=3660000, episode_reward=-34.48 +/- 36.04\n","Episode length: 401.60 +/- 80.34\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 402      |\n","|    mean_reward     | -34.5    |\n","| time/              |          |\n","|    total_timesteps | 3660000  |\n","| train/             |          |\n","|    actor_loss      | 3.21     |\n","|    critic_loss     | 3.81     |\n","|    ent_coef        | 0.00448  |\n","|    ent_coef_loss   | -0.25    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152495   |\n","---------------------------------\n","Eval num_timesteps=3662400, episode_reward=-71.97 +/- 2.35\n","Episode length: 258.00 +/- 97.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 258      |\n","|    mean_reward     | -72      |\n","| time/              |          |\n","|    total_timesteps | 3662400  |\n","| train/             |          |\n","|    actor_loss      | 3.56     |\n","|    critic_loss     | 0.531    |\n","|    ent_coef        | 0.00451  |\n","|    ent_coef_loss   | -1.24    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6510     |\n","|    fps             | 279      |\n","|    time_elapsed    | 13104    |\n","|    total_timesteps | 3662520  |\n","| train/             |          |\n","|    actor_loss      | 3.66     |\n","|    critic_loss     | 8        |\n","|    ent_coef        | 0.00451  |\n","|    ent_coef_loss   | -1.35    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152600   |\n","---------------------------------\n","Eval num_timesteps=3664800, episode_reward=19.69 +/- 4.77\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 19.7     |\n","| time/              |          |\n","|    total_timesteps | 3664800  |\n","| train/             |          |\n","|    actor_loss      | 4.15     |\n","|    critic_loss     | 0.385    |\n","|    ent_coef        | 0.00453  |\n","|    ent_coef_loss   | -1.22    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152695   |\n","---------------------------------\n","Eval num_timesteps=3667200, episode_reward=-59.41 +/- 12.11\n","Episode length: 298.40 +/- 135.21\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 298      |\n","|    mean_reward     | -59.4    |\n","| time/              |          |\n","|    total_timesteps | 3667200  |\n","| train/             |          |\n","|    actor_loss      | 3.96     |\n","|    critic_loss     | 21.2     |\n","|    ent_coef        | 0.00445  |\n","|    ent_coef_loss   | 1.55     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6520     |\n","|    fps             | 279      |\n","|    time_elapsed    | 13123    |\n","|    total_timesteps | 3668640  |\n","| train/             |          |\n","|    actor_loss      | 4.52     |\n","|    critic_loss     | 0.675    |\n","|    ent_coef        | 0.00443  |\n","|    ent_coef_loss   | 0.858    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152855   |\n","---------------------------------\n","Eval num_timesteps=3669600, episode_reward=-40.89 +/- 44.13\n","Episode length: 453.80 +/- 37.72\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 454      |\n","|    mean_reward     | -40.9    |\n","| time/              |          |\n","|    total_timesteps | 3669600  |\n","| train/             |          |\n","|    actor_loss      | 4.52     |\n","|    critic_loss     | 1.07     |\n","|    ent_coef        | 0.00443  |\n","|    ent_coef_loss   | 2.18     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152895   |\n","---------------------------------\n","Eval num_timesteps=3672000, episode_reward=-28.95 +/- 27.47\n","Episode length: 433.60 +/- 81.32\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 434      |\n","|    mean_reward     | -28.9    |\n","| time/              |          |\n","|    total_timesteps | 3672000  |\n","| train/             |          |\n","|    actor_loss      | 3.4      |\n","|    critic_loss     | 0.916    |\n","|    ent_coef        | 0.00452  |\n","|    ent_coef_loss   | -0.656   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152995   |\n","---------------------------------\n","Eval num_timesteps=3674400, episode_reward=0.01 +/- 0.57\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.00985  |\n","| time/              |          |\n","|    total_timesteps | 3674400  |\n","| train/             |          |\n","|    actor_loss      | 4.3      |\n","|    critic_loss     | 20.4     |\n","|    ent_coef        | 0.00456  |\n","|    ent_coef_loss   | 1.51     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153095   |\n","---------------------------------\n","Eval num_timesteps=3676800, episode_reward=-40.70 +/- 29.74\n","Episode length: 489.80 +/- 8.33\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 490      |\n","|    mean_reward     | -40.7    |\n","| time/              |          |\n","|    total_timesteps | 3676800  |\n","| train/             |          |\n","|    actor_loss      | 4.04     |\n","|    critic_loss     | 0.503    |\n","|    ent_coef        | 0.00461  |\n","|    ent_coef_loss   | -1.45    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153195   |\n","---------------------------------\n","Eval num_timesteps=3679200, episode_reward=-2.64 +/- 0.75\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -2.64    |\n","| time/              |          |\n","|    total_timesteps | 3679200  |\n","| train/             |          |\n","|    actor_loss      | 4.52     |\n","|    critic_loss     | 18.7     |\n","|    ent_coef        | 0.00464  |\n","|    ent_coef_loss   | 1.92     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153295   |\n","---------------------------------\n","Eval num_timesteps=3681600, episode_reward=0.97 +/- 0.84\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.972    |\n","| time/              |          |\n","|    total_timesteps | 3681600  |\n","| train/             |          |\n","|    actor_loss      | 4.37     |\n","|    critic_loss     | 2.13     |\n","|    ent_coef        | 0.00464  |\n","|    ent_coef_loss   | -3.56    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153395   |\n","---------------------------------\n","Eval num_timesteps=3684000, episode_reward=-75.46 +/- 3.79\n","Episode length: 137.40 +/- 4.41\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 137      |\n","|    mean_reward     | -75.5    |\n","| time/              |          |\n","|    total_timesteps | 3684000  |\n","| train/             |          |\n","|    actor_loss      | 3.88     |\n","|    critic_loss     | 0.447    |\n","|    ent_coef        | 0.0046   |\n","|    ent_coef_loss   | 0.058    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153495   |\n","---------------------------------\n","Eval num_timesteps=3686400, episode_reward=23.74 +/- 8.43\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 23.7     |\n","| time/              |          |\n","|    total_timesteps | 3686400  |\n","| train/             |          |\n","|    actor_loss      | 3.57     |\n","|    critic_loss     | 11.9     |\n","|    ent_coef        | 0.00451  |\n","|    ent_coef_loss   | -3.18    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153595   |\n","---------------------------------\n","Eval num_timesteps=3688800, episode_reward=34.55 +/- 2.26\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 34.5     |\n","| time/              |          |\n","|    total_timesteps | 3688800  |\n","| train/             |          |\n","|    actor_loss      | 3.4      |\n","|    critic_loss     | 0.605    |\n","|    ent_coef        | 0.0045   |\n","|    ent_coef_loss   | -1.44    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153695   |\n","---------------------------------\n","Eval num_timesteps=3691200, episode_reward=0.93 +/- 47.70\n","Episode length: 455.20 +/- 54.87\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 455      |\n","|    mean_reward     | 0.928    |\n","| time/              |          |\n","|    total_timesteps | 3691200  |\n","| train/             |          |\n","|    actor_loss      | 4.15     |\n","|    critic_loss     | 0.486    |\n","|    ent_coef        | 0.00447  |\n","|    ent_coef_loss   | 0.817    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153795   |\n","---------------------------------\n","Eval num_timesteps=3693600, episode_reward=-41.92 +/- 34.69\n","Episode length: 445.40 +/- 44.58\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 445      |\n","|    mean_reward     | -41.9    |\n","| time/              |          |\n","|    total_timesteps | 3693600  |\n","| train/             |          |\n","|    actor_loss      | 3.82     |\n","|    critic_loss     | 36.7     |\n","|    ent_coef        | 0.00446  |\n","|    ent_coef_loss   | -1.38    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153895   |\n","---------------------------------\n","Eval num_timesteps=3696000, episode_reward=-78.57 +/- 2.02\n","Episode length: 129.60 +/- 11.76\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 130      |\n","|    mean_reward     | -78.6    |\n","| time/              |          |\n","|    total_timesteps | 3696000  |\n","| train/             |          |\n","|    actor_loss      | 3.71     |\n","|    critic_loss     | 0.487    |\n","|    ent_coef        | 0.00442  |\n","|    ent_coef_loss   | -1.41    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6530     |\n","|    fps             | 279      |\n","|    time_elapsed    | 13229    |\n","|    total_timesteps | 3697776  |\n","| train/             |          |\n","|    actor_loss      | 3.84     |\n","|    critic_loss     | 0.305    |\n","|    ent_coef        | 0.00444  |\n","|    ent_coef_loss   | -0.216   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154069   |\n","---------------------------------\n","Eval num_timesteps=3698400, episode_reward=-50.59 +/- 38.05\n","Episode length: 281.60 +/- 178.32\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 282      |\n","|    mean_reward     | -50.6    |\n","| time/              |          |\n","|    total_timesteps | 3698400  |\n","| train/             |          |\n","|    actor_loss      | 4.74     |\n","|    critic_loss     | 1.46     |\n","|    ent_coef        | 0.00444  |\n","|    ent_coef_loss   | 6.07     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154095   |\n","---------------------------------\n","Eval num_timesteps=3700800, episode_reward=-63.16 +/- 2.82\n","Episode length: 274.40 +/- 15.19\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 274      |\n","|    mean_reward     | -63.2    |\n","| time/              |          |\n","|    total_timesteps | 3700800  |\n","| train/             |          |\n","|    actor_loss      | 3.96     |\n","|    critic_loss     | 0.325    |\n","|    ent_coef        | 0.0044   |\n","|    ent_coef_loss   | -4.74    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6540     |\n","|    fps             | 279      |\n","|    time_elapsed    | 13241    |\n","|    total_timesteps | 3701664  |\n","| train/             |          |\n","|    actor_loss      | 3.91     |\n","|    critic_loss     | 16.2     |\n","|    ent_coef        | 0.00432  |\n","|    ent_coef_loss   | -2.87    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154231   |\n","---------------------------------\n","Eval num_timesteps=3703200, episode_reward=-34.41 +/- 38.47\n","Episode length: 413.60 +/- 105.82\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 414      |\n","|    mean_reward     | -34.4    |\n","| time/              |          |\n","|    total_timesteps | 3703200  |\n","| train/             |          |\n","|    actor_loss      | 4.3      |\n","|    critic_loss     | 0.418    |\n","|    ent_coef        | 0.00422  |\n","|    ent_coef_loss   | -0.151   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154295   |\n","---------------------------------\n","Eval num_timesteps=3705600, episode_reward=2.40 +/- 1.01\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.4      |\n","| time/              |          |\n","|    total_timesteps | 3705600  |\n","| train/             |          |\n","|    actor_loss      | 4.08     |\n","|    critic_loss     | 0.525    |\n","|    ent_coef        | 0.00415  |\n","|    ent_coef_loss   | 1.73     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6550     |\n","|    fps             | 279      |\n","|    time_elapsed    | 13263    |\n","|    total_timesteps | 3706968  |\n","| train/             |          |\n","|    actor_loss      | 4.11     |\n","|    critic_loss     | 0.467    |\n","|    ent_coef        | 0.00419  |\n","|    ent_coef_loss   | 0.127    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154452   |\n","---------------------------------\n","Eval num_timesteps=3708000, episode_reward=-76.22 +/- 1.49\n","Episode length: 396.40 +/- 95.04\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 396      |\n","|    mean_reward     | -76.2    |\n","| time/              |          |\n","|    total_timesteps | 3708000  |\n","| train/             |          |\n","|    actor_loss      | 3.86     |\n","|    critic_loss     | 0.901    |\n","|    ent_coef        | 0.00422  |\n","|    ent_coef_loss   | 0.291    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154495   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6560     |\n","|    fps             | 279      |\n","|    time_elapsed    | 13272    |\n","|    total_timesteps | 3708624  |\n","| train/             |          |\n","|    actor_loss      | 3.8      |\n","|    critic_loss     | 1        |\n","|    ent_coef        | 0.00423  |\n","|    ent_coef_loss   | 0.839    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154521   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6570     |\n","|    fps             | 279      |\n","|    time_elapsed    | 13274    |\n","|    total_timesteps | 3709680  |\n","| train/             |          |\n","|    actor_loss      | 3.82     |\n","|    critic_loss     | 22.3     |\n","|    ent_coef        | 0.00422  |\n","|    ent_coef_loss   | 0.0335   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154565   |\n","---------------------------------\n","Eval num_timesteps=3710400, episode_reward=-29.44 +/- 35.14\n","Episode length: 410.80 +/- 109.25\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 411      |\n","|    mean_reward     | -29.4    |\n","| time/              |          |\n","|    total_timesteps | 3710400  |\n","| train/             |          |\n","|    actor_loss      | 3.99     |\n","|    critic_loss     | 0.483    |\n","|    ent_coef        | 0.00421  |\n","|    ent_coef_loss   | 0.583    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154595   |\n","---------------------------------\n","Eval num_timesteps=3712800, episode_reward=6.02 +/- 3.26\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.02     |\n","| time/              |          |\n","|    total_timesteps | 3712800  |\n","| train/             |          |\n","|    actor_loss      | 3.36     |\n","|    critic_loss     | 0.532    |\n","|    ent_coef        | 0.00422  |\n","|    ent_coef_loss   | -1.95    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154695   |\n","---------------------------------\n","Eval num_timesteps=3715200, episode_reward=-8.06 +/- 51.52\n","Episode length: 413.60 +/- 105.82\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 414      |\n","|    mean_reward     | -8.06    |\n","| time/              |          |\n","|    total_timesteps | 3715200  |\n","| train/             |          |\n","|    actor_loss      | 4.24     |\n","|    critic_loss     | 0.757    |\n","|    ent_coef        | 0.00423  |\n","|    ent_coef_loss   | -1.39    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154795   |\n","---------------------------------\n","Eval num_timesteps=3717600, episode_reward=-80.26 +/- 2.47\n","Episode length: 159.80 +/- 21.07\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 160      |\n","|    mean_reward     | -80.3    |\n","| time/              |          |\n","|    total_timesteps | 3717600  |\n","| train/             |          |\n","|    actor_loss      | 3.82     |\n","|    critic_loss     | 12.3     |\n","|    ent_coef        | 0.00417  |\n","|    ent_coef_loss   | -1.79    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154895   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6580     |\n","|    fps             | 279      |\n","|    time_elapsed    | 13305    |\n","|    total_timesteps | 3719808  |\n","| train/             |          |\n","|    actor_loss      | 4.43     |\n","|    critic_loss     | 13.2     |\n","|    ent_coef        | 0.00416  |\n","|    ent_coef_loss   | 1.25     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154987   |\n","---------------------------------\n","Eval num_timesteps=3720000, episode_reward=-87.33 +/- 5.94\n","Episode length: 307.20 +/- 103.86\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 307      |\n","|    mean_reward     | -87.3    |\n","| time/              |          |\n","|    total_timesteps | 3720000  |\n","| train/             |          |\n","|    actor_loss      | 3.98     |\n","|    critic_loss     | 0.305    |\n","|    ent_coef        | 0.00416  |\n","|    ent_coef_loss   | -0.241   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6590     |\n","|    fps             | 279      |\n","|    time_elapsed    | 13315    |\n","|    total_timesteps | 3721896  |\n","| train/             |          |\n","|    actor_loss      | 4.36     |\n","|    critic_loss     | 0.46     |\n","|    ent_coef        | 0.0042   |\n","|    ent_coef_loss   | 0.0869   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155074   |\n","---------------------------------\n","Eval num_timesteps=3722400, episode_reward=-72.85 +/- 0.95\n","Episode length: 242.60 +/- 91.12\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 243      |\n","|    mean_reward     | -72.9    |\n","| time/              |          |\n","|    total_timesteps | 3722400  |\n","| train/             |          |\n","|    actor_loss      | 4.67     |\n","|    critic_loss     | 1.22     |\n","|    ent_coef        | 0.00421  |\n","|    ent_coef_loss   | 1.37     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155095   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6600     |\n","|    fps             | 279      |\n","|    time_elapsed    | 13321    |\n","|    total_timesteps | 3723264  |\n","| train/             |          |\n","|    actor_loss      | 3.9      |\n","|    critic_loss     | 0.417    |\n","|    ent_coef        | 0.00418  |\n","|    ent_coef_loss   | -2.8     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155131   |\n","---------------------------------\n","Eval num_timesteps=3724800, episode_reward=-25.04 +/- 40.44\n","Episode length: 355.60 +/- 176.85\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 356      |\n","|    mean_reward     | -25      |\n","| time/              |          |\n","|    total_timesteps | 3724800  |\n","| train/             |          |\n","|    actor_loss      | 4.15     |\n","|    critic_loss     | 0.92     |\n","|    ent_coef        | 0.00415  |\n","|    ent_coef_loss   | 0.464    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155195   |\n","---------------------------------\n","Eval num_timesteps=3727200, episode_reward=-29.30 +/- 37.44\n","Episode length: 339.60 +/- 196.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 340      |\n","|    mean_reward     | -29.3    |\n","| time/              |          |\n","|    total_timesteps | 3727200  |\n","| train/             |          |\n","|    actor_loss      | 4.01     |\n","|    critic_loss     | 0.267    |\n","|    ent_coef        | 0.0041   |\n","|    ent_coef_loss   | -1.29    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155295   |\n","---------------------------------\n","Eval num_timesteps=3729600, episode_reward=-72.89 +/- 1.96\n","Episode length: 108.40 +/- 15.19\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 108      |\n","|    mean_reward     | -72.9    |\n","| time/              |          |\n","|    total_timesteps | 3729600  |\n","| train/             |          |\n","|    actor_loss      | 6.2      |\n","|    critic_loss     | 1.31     |\n","|    ent_coef        | 0.00401  |\n","|    ent_coef_loss   | -0.911   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6610     |\n","|    fps             | 279      |\n","|    time_elapsed    | 13343    |\n","|    total_timesteps | 3731664  |\n","| train/             |          |\n","|    actor_loss      | 3.37     |\n","|    critic_loss     | 0.196    |\n","|    ent_coef        | 0.00397  |\n","|    ent_coef_loss   | -2.94    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155481   |\n","---------------------------------\n","Eval num_timesteps=3732000, episode_reward=-75.21 +/- 2.34\n","Episode length: 117.20 +/- 25.96\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 117      |\n","|    mean_reward     | -75.2    |\n","| time/              |          |\n","|    total_timesteps | 3732000  |\n","| train/             |          |\n","|    actor_loss      | 3.97     |\n","|    critic_loss     | 2.36     |\n","|    ent_coef        | 0.00397  |\n","|    ent_coef_loss   | -0.695   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155495   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6620     |\n","|    fps             | 279      |\n","|    time_elapsed    | 13346    |\n","|    total_timesteps | 3733440  |\n","| train/             |          |\n","|    actor_loss      | 4.09     |\n","|    critic_loss     | 0.806    |\n","|    ent_coef        | 0.00396  |\n","|    ent_coef_loss   | 0.762    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155555   |\n","---------------------------------\n","Eval num_timesteps=3734400, episode_reward=-74.21 +/- 2.10\n","Episode length: 109.00 +/- 2.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 109      |\n","|    mean_reward     | -74.2    |\n","| time/              |          |\n","|    total_timesteps | 3734400  |\n","| train/             |          |\n","|    actor_loss      | 3.85     |\n","|    critic_loss     | 1.14     |\n","|    ent_coef        | 0.00397  |\n","|    ent_coef_loss   | -1.94    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6630     |\n","|    fps             | 279      |\n","|    time_elapsed    | 13349    |\n","|    total_timesteps | 3735960  |\n","| train/             |          |\n","|    actor_loss      | 4.2      |\n","|    critic_loss     | 0.488    |\n","|    ent_coef        | 0.00397  |\n","|    ent_coef_loss   | 1.68     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155660   |\n","---------------------------------\n","Eval num_timesteps=3736800, episode_reward=-26.55 +/- 39.08\n","Episode length: 352.40 +/- 180.77\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 352      |\n","|    mean_reward     | -26.6    |\n","| time/              |          |\n","|    total_timesteps | 3736800  |\n","| train/             |          |\n","|    actor_loss      | 4.25     |\n","|    critic_loss     | 0.411    |\n","|    ent_coef        | 0.00397  |\n","|    ent_coef_loss   | 1.25     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155695   |\n","---------------------------------\n","Eval num_timesteps=3739200, episode_reward=-28.45 +/- 38.60\n","Episode length: 342.40 +/- 193.02\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 342      |\n","|    mean_reward     | -28.5    |\n","| time/              |          |\n","|    total_timesteps | 3739200  |\n","| train/             |          |\n","|    actor_loss      | 3.98     |\n","|    critic_loss     | 0.219    |\n","|    ent_coef        | 0.00398  |\n","|    ent_coef_loss   | 0.293    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6640     |\n","|    fps             | 279      |\n","|    time_elapsed    | 13372    |\n","|    total_timesteps | 3741000  |\n","| train/             |          |\n","|    actor_loss      | 3.88     |\n","|    critic_loss     | 0.54     |\n","|    ent_coef        | 0.004    |\n","|    ent_coef_loss   | -0.769   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155870   |\n","---------------------------------\n","Eval num_timesteps=3741600, episode_reward=-77.05 +/- 0.98\n","Episode length: 103.00 +/- 12.25\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 103      |\n","|    mean_reward     | -77.1    |\n","| time/              |          |\n","|    total_timesteps | 3741600  |\n","| train/             |          |\n","|    actor_loss      | 4.24     |\n","|    critic_loss     | 0.609    |\n","|    ent_coef        | 0.00404  |\n","|    ent_coef_loss   | 5        |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155895   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6650     |\n","|    fps             | 279      |\n","|    time_elapsed    | 13374    |\n","|    total_timesteps | 3742848  |\n","| train/             |          |\n","|    actor_loss      | 4.46     |\n","|    critic_loss     | 0.431    |\n","|    ent_coef        | 0.0041   |\n","|    ent_coef_loss   | 1.92     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155947   |\n","---------------------------------\n","Eval num_timesteps=3744000, episode_reward=-75.49 +/- 1.97\n","Episode length: 105.00 +/- 19.60\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 105      |\n","|    mean_reward     | -75.5    |\n","| time/              |          |\n","|    total_timesteps | 3744000  |\n","| train/             |          |\n","|    actor_loss      | 4.16     |\n","|    critic_loss     | 0.522    |\n","|    ent_coef        | 0.00414  |\n","|    ent_coef_loss   | -4.99    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155995   |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    episodes        | 6660    |\n","|    fps             | 279     |\n","|    time_elapsed    | 13377   |\n","|    total_timesteps | 3744000 |\n","--------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6670     |\n","|    fps             | 279      |\n","|    time_elapsed    | 13379    |\n","|    total_timesteps | 3745296  |\n","| train/             |          |\n","|    actor_loss      | 3.89     |\n","|    critic_loss     | 0.958    |\n","|    ent_coef        | 0.00409  |\n","|    ent_coef_loss   | -1.45    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156049   |\n","---------------------------------\n","Eval num_timesteps=3746400, episode_reward=-22.39 +/- 41.46\n","Episode length: 353.20 +/- 179.79\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 353      |\n","|    mean_reward     | -22.4    |\n","| time/              |          |\n","|    total_timesteps | 3746400  |\n","| train/             |          |\n","|    actor_loss      | 4.57     |\n","|    critic_loss     | 1.19     |\n","|    ent_coef        | 0.00408  |\n","|    ent_coef_loss   | 2.41     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156095   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6680     |\n","|    fps             | 279      |\n","|    time_elapsed    | 13390    |\n","|    total_timesteps | 3748392  |\n","| train/             |          |\n","|    actor_loss      | 4.4      |\n","|    critic_loss     | 0.282    |\n","|    ent_coef        | 0.00413  |\n","|    ent_coef_loss   | 1.52     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156178   |\n","---------------------------------\n","Eval num_timesteps=3748800, episode_reward=-33.74 +/- 40.84\n","Episode length: 490.80 +/- 11.27\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 491      |\n","|    mean_reward     | -33.7    |\n","| time/              |          |\n","|    total_timesteps | 3748800  |\n","| train/             |          |\n","|    actor_loss      | 4.27     |\n","|    critic_loss     | 1.06     |\n","|    ent_coef        | 0.00415  |\n","|    ent_coef_loss   | -2.54    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156195   |\n","---------------------------------\n","Eval num_timesteps=3751200, episode_reward=-23.01 +/- 42.10\n","Episode length: 386.00 +/- 139.62\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 386      |\n","|    mean_reward     | -23      |\n","| time/              |          |\n","|    total_timesteps | 3751200  |\n","| train/             |          |\n","|    actor_loss      | 4.66     |\n","|    critic_loss     | 0.892    |\n","|    ent_coef        | 0.00419  |\n","|    ent_coef_loss   | 3.07     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6690     |\n","|    fps             | 279      |\n","|    time_elapsed    | 13407    |\n","|    total_timesteps | 3752928  |\n","| train/             |          |\n","|    actor_loss      | 4.68     |\n","|    critic_loss     | 0.518    |\n","|    ent_coef        | 0.00417  |\n","|    ent_coef_loss   | -3.43    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156367   |\n","---------------------------------\n","Eval num_timesteps=3753600, episode_reward=-39.40 +/- 40.60\n","Episode length: 258.80 +/- 196.94\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 259      |\n","|    mean_reward     | -39.4    |\n","| time/              |          |\n","|    total_timesteps | 3753600  |\n","| train/             |          |\n","|    actor_loss      | 4.6      |\n","|    critic_loss     | 10.6     |\n","|    ent_coef        | 0.00413  |\n","|    ent_coef_loss   | 1.33     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156395   |\n","---------------------------------\n","Eval num_timesteps=3756000, episode_reward=-72.19 +/- 2.11\n","Episode length: 189.60 +/- 20.09\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 190      |\n","|    mean_reward     | -72.2    |\n","| time/              |          |\n","|    total_timesteps | 3756000  |\n","| train/             |          |\n","|    actor_loss      | 4.23     |\n","|    critic_loss     | 0.586    |\n","|    ent_coef        | 0.00412  |\n","|    ent_coef_loss   | 1.06     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156495   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6700     |\n","|    fps             | 279      |\n","|    time_elapsed    | 13423    |\n","|    total_timesteps | 3756768  |\n","| train/             |          |\n","|    actor_loss      | 4.39     |\n","|    critic_loss     | 0.448    |\n","|    ent_coef        | 0.00414  |\n","|    ent_coef_loss   | 5.97     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156527   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6710     |\n","|    fps             | 279      |\n","|    time_elapsed    | 13425    |\n","|    total_timesteps | 3758112  |\n","| train/             |          |\n","|    actor_loss      | 4.76     |\n","|    critic_loss     | 0.574    |\n","|    ent_coef        | 0.00416  |\n","|    ent_coef_loss   | 0.247    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156583   |\n","---------------------------------\n","Eval num_timesteps=3758400, episode_reward=-69.03 +/- 4.42\n","Episode length: 286.60 +/- 15.19\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 287      |\n","|    mean_reward     | -69      |\n","| time/              |          |\n","|    total_timesteps | 3758400  |\n","| train/             |          |\n","|    actor_loss      | 4.19     |\n","|    critic_loss     | 0.311    |\n","|    ent_coef        | 0.00417  |\n","|    ent_coef_loss   | -0.22    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6720     |\n","|    fps             | 279      |\n","|    time_elapsed    | 13432    |\n","|    total_timesteps | 3760176  |\n","| train/             |          |\n","|    actor_loss      | 4.31     |\n","|    critic_loss     | 28.5     |\n","|    ent_coef        | 0.00419  |\n","|    ent_coef_loss   | -0.496   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156669   |\n","---------------------------------\n","Eval num_timesteps=3760800, episode_reward=-56.53 +/- 39.52\n","Episode length: 353.00 +/- 120.02\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 353      |\n","|    mean_reward     | -56.5    |\n","| time/              |          |\n","|    total_timesteps | 3760800  |\n","| train/             |          |\n","|    actor_loss      | 4.72     |\n","|    critic_loss     | 21.5     |\n","|    ent_coef        | 0.00421  |\n","|    ent_coef_loss   | 6.39     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156695   |\n","---------------------------------\n","Eval num_timesteps=3763200, episode_reward=-29.54 +/- 37.82\n","Episode length: 330.00 +/- 208.21\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 330      |\n","|    mean_reward     | -29.5    |\n","| time/              |          |\n","|    total_timesteps | 3763200  |\n","| train/             |          |\n","|    actor_loss      | 4.45     |\n","|    critic_loss     | 0.345    |\n","|    ent_coef        | 0.00437  |\n","|    ent_coef_loss   | 3.15     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6730     |\n","|    fps             | 279      |\n","|    time_elapsed    | 13454    |\n","|    total_timesteps | 3765000  |\n","| train/             |          |\n","|    actor_loss      | 3.89     |\n","|    critic_loss     | 1.72     |\n","|    ent_coef        | 0.00442  |\n","|    ent_coef_loss   | 0.456    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156870   |\n","---------------------------------\n","Eval num_timesteps=3765600, episode_reward=-77.54 +/- 4.17\n","Episode length: 87.80 +/- 8.82\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 87.8     |\n","|    mean_reward     | -77.5    |\n","| time/              |          |\n","|    total_timesteps | 3765600  |\n","| train/             |          |\n","|    actor_loss      | 4.39     |\n","|    critic_loss     | 10.3     |\n","|    ent_coef        | 0.00444  |\n","|    ent_coef_loss   | 1.61     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156895   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6740     |\n","|    fps             | 279      |\n","|    time_elapsed    | 13457    |\n","|    total_timesteps | 3766872  |\n","| train/             |          |\n","|    actor_loss      | 3.79     |\n","|    critic_loss     | 0.786    |\n","|    ent_coef        | 0.00451  |\n","|    ent_coef_loss   | -0.243   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156948   |\n","---------------------------------\n","Eval num_timesteps=3768000, episode_reward=-47.15 +/- 39.25\n","Episode length: 446.60 +/- 43.60\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 447      |\n","|    mean_reward     | -47.1    |\n","| time/              |          |\n","|    total_timesteps | 3768000  |\n","| train/             |          |\n","|    actor_loss      | 4.26     |\n","|    critic_loss     | 1.13     |\n","|    ent_coef        | 0.00455  |\n","|    ent_coef_loss   | 0.932    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156995   |\n","---------------------------------\n","Eval num_timesteps=3770400, episode_reward=-69.22 +/- 2.90\n","Episode length: 356.20 +/- 157.75\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 356      |\n","|    mean_reward     | -69.2    |\n","| time/              |          |\n","|    total_timesteps | 3770400  |\n","| train/             |          |\n","|    actor_loss      | 4.11     |\n","|    critic_loss     | 0.69     |\n","|    ent_coef        | 0.00457  |\n","|    ent_coef_loss   | -0.195   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157095   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6750     |\n","|    fps             | 279      |\n","|    time_elapsed    | 13475    |\n","|    total_timesteps | 3770760  |\n","| train/             |          |\n","|    actor_loss      | 4.67     |\n","|    critic_loss     | 0.899    |\n","|    ent_coef        | 0.00458  |\n","|    ent_coef_loss   | 3.02     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157110   |\n","---------------------------------\n","Eval num_timesteps=3772800, episode_reward=3.34 +/- 3.01\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.34     |\n","| time/              |          |\n","|    total_timesteps | 3772800  |\n","| train/             |          |\n","|    actor_loss      | 4.26     |\n","|    critic_loss     | 4.06     |\n","|    ent_coef        | 0.00459  |\n","|    ent_coef_loss   | 2.75     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157195   |\n","---------------------------------\n","Eval num_timesteps=3775200, episode_reward=-35.35 +/- 37.91\n","Episode length: 497.20 +/- 3.43\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 497      |\n","|    mean_reward     | -35.3    |\n","| time/              |          |\n","|    total_timesteps | 3775200  |\n","| train/             |          |\n","|    actor_loss      | 4.48     |\n","|    critic_loss     | 0.437    |\n","|    ent_coef        | 0.00461  |\n","|    ent_coef_loss   | 2.61     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157295   |\n","---------------------------------\n","Eval num_timesteps=3777600, episode_reward=-1.61 +/- 2.20\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.61    |\n","| time/              |          |\n","|    total_timesteps | 3777600  |\n","| train/             |          |\n","|    actor_loss      | 5.24     |\n","|    critic_loss     | 5.38     |\n","|    ent_coef        | 0.00463  |\n","|    ent_coef_loss   | 1.44     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157395   |\n","---------------------------------\n","Eval num_timesteps=3780000, episode_reward=-35.86 +/- 47.00\n","Episode length: 437.60 +/- 50.95\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 438      |\n","|    mean_reward     | -35.9    |\n","| time/              |          |\n","|    total_timesteps | 3780000  |\n","| train/             |          |\n","|    actor_loss      | 4        |\n","|    critic_loss     | 0.556    |\n","|    ent_coef        | 0.00472  |\n","|    ent_coef_loss   | 0.925    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157495   |\n","---------------------------------\n","Eval num_timesteps=3782400, episode_reward=-41.75 +/- 35.30\n","Episode length: 446.40 +/- 65.65\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 446      |\n","|    mean_reward     | -41.7    |\n","| time/              |          |\n","|    total_timesteps | 3782400  |\n","| train/             |          |\n","|    actor_loss      | 3.99     |\n","|    critic_loss     | 2.22     |\n","|    ent_coef        | 0.00492  |\n","|    ent_coef_loss   | 0.894    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6760     |\n","|    fps             | 279      |\n","|    time_elapsed    | 13519    |\n","|    total_timesteps | 3784488  |\n","| train/             |          |\n","|    actor_loss      | 3.53     |\n","|    critic_loss     | 0.307    |\n","|    ent_coef        | 0.00496  |\n","|    ent_coef_loss   | 0.306    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157682   |\n","---------------------------------\n","Eval num_timesteps=3784800, episode_reward=-43.12 +/- 37.80\n","Episode length: 432.40 +/- 82.79\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 432      |\n","|    mean_reward     | -43.1    |\n","| time/              |          |\n","|    total_timesteps | 3784800  |\n","| train/             |          |\n","|    actor_loss      | 4.87     |\n","|    critic_loss     | 0.423    |\n","|    ent_coef        | 0.00497  |\n","|    ent_coef_loss   | 0.702    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157695   |\n","---------------------------------\n","Eval num_timesteps=3787200, episode_reward=-45.03 +/- 42.37\n","Episode length: 399.20 +/- 82.30\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 399      |\n","|    mean_reward     | -45      |\n","| time/              |          |\n","|    total_timesteps | 3787200  |\n","| train/             |          |\n","|    actor_loss      | 4.22     |\n","|    critic_loss     | 20.5     |\n","|    ent_coef        | 0.00493  |\n","|    ent_coef_loss   | -1.01    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157795   |\n","---------------------------------\n","Eval num_timesteps=3789600, episode_reward=-38.64 +/- 32.36\n","Episode length: 486.20 +/- 11.27\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 486      |\n","|    mean_reward     | -38.6    |\n","| time/              |          |\n","|    total_timesteps | 3789600  |\n","| train/             |          |\n","|    actor_loss      | 4.51     |\n","|    critic_loss     | 0.416    |\n","|    ent_coef        | 0.00492  |\n","|    ent_coef_loss   | -0.216   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157895   |\n","---------------------------------\n","Eval num_timesteps=3792000, episode_reward=-71.81 +/- 2.59\n","Episode length: 171.80 +/- 11.27\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 172      |\n","|    mean_reward     | -71.8    |\n","| time/              |          |\n","|    total_timesteps | 3792000  |\n","| train/             |          |\n","|    actor_loss      | 4.92     |\n","|    critic_loss     | 4.47     |\n","|    ent_coef        | 0.00486  |\n","|    ent_coef_loss   | -1.02    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6770     |\n","|    fps             | 279      |\n","|    time_elapsed    | 13551    |\n","|    total_timesteps | 3793992  |\n","| train/             |          |\n","|    actor_loss      | 3.99     |\n","|    critic_loss     | 0.586    |\n","|    ent_coef        | 0.0048   |\n","|    ent_coef_loss   | 1.12     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158078   |\n","---------------------------------\n","Eval num_timesteps=3794400, episode_reward=-70.70 +/- 0.73\n","Episode length: 281.80 +/- 57.81\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 282      |\n","|    mean_reward     | -70.7    |\n","| time/              |          |\n","|    total_timesteps | 3794400  |\n","| train/             |          |\n","|    actor_loss      | 4.4      |\n","|    critic_loss     | 0.342    |\n","|    ent_coef        | 0.00481  |\n","|    ent_coef_loss   | 1.14     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158095   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6780     |\n","|    fps             | 280      |\n","|    time_elapsed    | 13557    |\n","|    total_timesteps | 3796392  |\n","| train/             |          |\n","|    actor_loss      | 4.88     |\n","|    critic_loss     | 0.523    |\n","|    ent_coef        | 0.00485  |\n","|    ent_coef_loss   | -0.973   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158178   |\n","---------------------------------\n","Eval num_timesteps=3796800, episode_reward=-71.74 +/- 3.00\n","Episode length: 364.40 +/- 26.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 364      |\n","|    mean_reward     | -71.7    |\n","| time/              |          |\n","|    total_timesteps | 3796800  |\n","| train/             |          |\n","|    actor_loss      | 4.72     |\n","|    critic_loss     | 0.491    |\n","|    ent_coef        | 0.00485  |\n","|    ent_coef_loss   | -0.514   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158195   |\n","---------------------------------\n","Eval num_timesteps=3799200, episode_reward=-75.98 +/- 1.58\n","Episode length: 222.20 +/- 18.13\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 222      |\n","|    mean_reward     | -76      |\n","| time/              |          |\n","|    total_timesteps | 3799200  |\n","| train/             |          |\n","|    actor_loss      | 4.21     |\n","|    critic_loss     | 0.554    |\n","|    ent_coef        | 0.0049   |\n","|    ent_coef_loss   | -1.05    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158295   |\n","---------------------------------\n","Eval num_timesteps=3801600, episode_reward=-20.46 +/- 36.68\n","Episode length: 490.40 +/- 11.76\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 490      |\n","|    mean_reward     | -20.5    |\n","| time/              |          |\n","|    total_timesteps | 3801600  |\n","| train/             |          |\n","|    actor_loss      | 4.48     |\n","|    critic_loss     | 1.79     |\n","|    ent_coef        | 0.00487  |\n","|    ent_coef_loss   | -3.68    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158395   |\n","---------------------------------\n","Eval num_timesteps=3804000, episode_reward=-44.25 +/- 41.70\n","Episode length: 494.00 +/- 4.90\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 494      |\n","|    mean_reward     | -44.3    |\n","| time/              |          |\n","|    total_timesteps | 3804000  |\n","| train/             |          |\n","|    actor_loss      | 4.03     |\n","|    critic_loss     | 0.471    |\n","|    ent_coef        | 0.00486  |\n","|    ent_coef_loss   | -4.58    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158495   |\n","---------------------------------\n","Eval num_timesteps=3806400, episode_reward=-72.30 +/- 7.14\n","Episode length: 286.80 +/- 123.94\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 287      |\n","|    mean_reward     | -72.3    |\n","| time/              |          |\n","|    total_timesteps | 3806400  |\n","| train/             |          |\n","|    actor_loss      | 4.91     |\n","|    critic_loss     | 21.4     |\n","|    ent_coef        | 0.0048   |\n","|    ent_coef_loss   | 2.36     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6790     |\n","|    fps             | 280      |\n","|    time_elapsed    | 13597    |\n","|    total_timesteps | 3808296  |\n","| train/             |          |\n","|    actor_loss      | 4.86     |\n","|    critic_loss     | 0.666    |\n","|    ent_coef        | 0.00476  |\n","|    ent_coef_loss   | 1.69     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158674   |\n","---------------------------------\n","Eval num_timesteps=3808800, episode_reward=-51.23 +/- 39.77\n","Episode length: 299.60 +/- 163.63\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 300      |\n","|    mean_reward     | -51.2    |\n","| time/              |          |\n","|    total_timesteps | 3808800  |\n","| train/             |          |\n","|    actor_loss      | 4.97     |\n","|    critic_loss     | 3.83     |\n","|    ent_coef        | 0.00476  |\n","|    ent_coef_loss   | -0.234   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158695   |\n","---------------------------------\n","Eval num_timesteps=3811200, episode_reward=-36.16 +/- 40.27\n","Episode length: 457.60 +/- 51.93\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 458      |\n","|    mean_reward     | -36.2    |\n","| time/              |          |\n","|    total_timesteps | 3811200  |\n","| train/             |          |\n","|    actor_loss      | 4.72     |\n","|    critic_loss     | 11.7     |\n","|    ent_coef        | 0.00477  |\n","|    ent_coef_loss   | 6.89     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158795   |\n","---------------------------------\n","Eval num_timesteps=3813600, episode_reward=-75.04 +/- 2.57\n","Episode length: 281.00 +/- 71.04\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 281      |\n","|    mean_reward     | -75      |\n","| time/              |          |\n","|    total_timesteps | 3813600  |\n","| train/             |          |\n","|    actor_loss      | 4.63     |\n","|    critic_loss     | 0.663    |\n","|    ent_coef        | 0.00489  |\n","|    ent_coef_loss   | 1.25     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158895   |\n","---------------------------------\n","Eval num_timesteps=3816000, episode_reward=-81.56 +/- 1.84\n","Episode length: 260.40 +/- 46.05\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 260      |\n","|    mean_reward     | -81.6    |\n","| time/              |          |\n","|    total_timesteps | 3816000  |\n","| train/             |          |\n","|    actor_loss      | 4.41     |\n","|    critic_loss     | 0.337    |\n","|    ent_coef        | 0.00486  |\n","|    ent_coef_loss   | 0.53     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158995   |\n","---------------------------------\n","Eval num_timesteps=3818400, episode_reward=-62.74 +/- 3.46\n","Episode length: 425.00 +/- 4.90\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 425      |\n","|    mean_reward     | -62.7    |\n","| time/              |          |\n","|    total_timesteps | 3818400  |\n","| train/             |          |\n","|    actor_loss      | 5.12     |\n","|    critic_loss     | 3.61     |\n","|    ent_coef        | 0.00487  |\n","|    ent_coef_loss   | -0.152   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159095   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6800     |\n","|    fps             | 280      |\n","|    time_elapsed    | 13638    |\n","|    total_timesteps | 3819768  |\n","| train/             |          |\n","|    actor_loss      | 4.7      |\n","|    critic_loss     | 3.26     |\n","|    ent_coef        | 0.00487  |\n","|    ent_coef_loss   | -0.775   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159152   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6810     |\n","|    fps             | 280      |\n","|    time_elapsed    | 13639    |\n","|    total_timesteps | 3820104  |\n","| train/             |          |\n","|    actor_loss      | 4.35     |\n","|    critic_loss     | 0.235    |\n","|    ent_coef        | 0.00486  |\n","|    ent_coef_loss   | -2.63    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159166   |\n","---------------------------------\n","Eval num_timesteps=3820800, episode_reward=-80.38 +/- 2.39\n","Episode length: 251.40 +/- 21.56\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 251      |\n","|    mean_reward     | -80.4    |\n","| time/              |          |\n","|    total_timesteps | 3820800  |\n","| train/             |          |\n","|    actor_loss      | 5.05     |\n","|    critic_loss     | 14.1     |\n","|    ent_coef        | 0.00485  |\n","|    ent_coef_loss   | 4.17     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159195   |\n","---------------------------------\n","Eval num_timesteps=3823200, episode_reward=-72.98 +/- 1.82\n","Episode length: 184.00 +/- 4.90\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 184      |\n","|    mean_reward     | -73      |\n","| time/              |          |\n","|    total_timesteps | 3823200  |\n","| train/             |          |\n","|    actor_loss      | 5.12     |\n","|    critic_loss     | 1.08     |\n","|    ent_coef        | 0.00495  |\n","|    ent_coef_loss   | -3.22    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159295   |\n","---------------------------------\n","Eval num_timesteps=3825600, episode_reward=-73.15 +/- 5.20\n","Episode length: 172.80 +/- 70.06\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 173      |\n","|    mean_reward     | -73.2    |\n","| time/              |          |\n","|    total_timesteps | 3825600  |\n","| train/             |          |\n","|    actor_loss      | 4.91     |\n","|    critic_loss     | 1.99     |\n","|    ent_coef        | 0.00496  |\n","|    ent_coef_loss   | 1.11     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6820     |\n","|    fps             | 280      |\n","|    time_elapsed    | 13655    |\n","|    total_timesteps | 3826896  |\n","| train/             |          |\n","|    actor_loss      | 4.88     |\n","|    critic_loss     | 0.847    |\n","|    ent_coef        | 0.0049   |\n","|    ent_coef_loss   | -0.791   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159449   |\n","---------------------------------\n","Eval num_timesteps=3828000, episode_reward=-33.05 +/- 48.48\n","Episode length: 444.80 +/- 45.07\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 445      |\n","|    mean_reward     | -33      |\n","| time/              |          |\n","|    total_timesteps | 3828000  |\n","| train/             |          |\n","|    actor_loss      | 4.37     |\n","|    critic_loss     | 0.644    |\n","|    ent_coef        | 0.00487  |\n","|    ent_coef_loss   | 0.182    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159495   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6830     |\n","|    fps             | 280      |\n","|    time_elapsed    | 13662    |\n","|    total_timesteps | 3829704  |\n","| train/             |          |\n","|    actor_loss      | 4.34     |\n","|    critic_loss     | 0.464    |\n","|    ent_coef        | 0.00488  |\n","|    ent_coef_loss   | -1.38    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159566   |\n","---------------------------------\n","Eval num_timesteps=3830400, episode_reward=-72.39 +/- 3.47\n","Episode length: 183.20 +/- 11.27\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 183      |\n","|    mean_reward     | -72.4    |\n","| time/              |          |\n","|    total_timesteps | 3830400  |\n","| train/             |          |\n","|    actor_loss      | 5.25     |\n","|    critic_loss     | 1.01     |\n","|    ent_coef        | 0.00488  |\n","|    ent_coef_loss   | -1.07    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159595   |\n","---------------------------------\n","Eval num_timesteps=3832800, episode_reward=-72.12 +/- 2.41\n","Episode length: 250.40 +/- 95.04\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 250      |\n","|    mean_reward     | -72.1    |\n","| time/              |          |\n","|    total_timesteps | 3832800  |\n","| train/             |          |\n","|    actor_loss      | 5.17     |\n","|    critic_loss     | 1.04     |\n","|    ent_coef        | 0.0049   |\n","|    ent_coef_loss   | -0.16    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6840     |\n","|    fps             | 280      |\n","|    time_elapsed    | 13673    |\n","|    total_timesteps | 3833520  |\n","| train/             |          |\n","|    actor_loss      | 5.22     |\n","|    critic_loss     | 14.5     |\n","|    ent_coef        | 0.00488  |\n","|    ent_coef_loss   | 1.06     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159725   |\n","---------------------------------\n","Eval num_timesteps=3835200, episode_reward=-44.89 +/- 43.54\n","Episode length: 270.80 +/- 187.14\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 271      |\n","|    mean_reward     | -44.9    |\n","| time/              |          |\n","|    total_timesteps | 3835200  |\n","| train/             |          |\n","|    actor_loss      | 4.65     |\n","|    critic_loss     | 15.4     |\n","|    ent_coef        | 0.00483  |\n","|    ent_coef_loss   | -0.8     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6850     |\n","|    fps             | 280      |\n","|    time_elapsed    | 13684    |\n","|    total_timesteps | 3837168  |\n","| train/             |          |\n","|    actor_loss      | 4.78     |\n","|    critic_loss     | 0.574    |\n","|    ent_coef        | 0.00493  |\n","|    ent_coef_loss   | 1.62     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159877   |\n","---------------------------------\n","Eval num_timesteps=3837600, episode_reward=-77.21 +/- 3.86\n","Episode length: 186.60 +/- 80.34\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 187      |\n","|    mean_reward     | -77.2    |\n","| time/              |          |\n","|    total_timesteps | 3837600  |\n","| train/             |          |\n","|    actor_loss      | 4.94     |\n","|    critic_loss     | 0.44     |\n","|    ent_coef        | 0.00495  |\n","|    ent_coef_loss   | 0.286    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159895   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6860     |\n","|    fps             | 280      |\n","|    time_elapsed    | 13693    |\n","|    total_timesteps | 3839520  |\n","| train/             |          |\n","|    actor_loss      | 4.39     |\n","|    critic_loss     | 0.503    |\n","|    ent_coef        | 0.00499  |\n","|    ent_coef_loss   | -0.177   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159975   |\n","---------------------------------\n","Eval num_timesteps=3840000, episode_reward=-65.14 +/- 3.40\n","Episode length: 312.40 +/- 117.09\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 312      |\n","|    mean_reward     | -65.1    |\n","| time/              |          |\n","|    total_timesteps | 3840000  |\n","| train/             |          |\n","|    actor_loss      | 4.07     |\n","|    critic_loss     | 0.359    |\n","|    ent_coef        | 0.00499  |\n","|    ent_coef_loss   | -1.87    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159995   |\n","---------------------------------\n","Eval num_timesteps=3842400, episode_reward=-73.11 +/- 4.09\n","Episode length: 112.60 +/- 9.31\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 113      |\n","|    mean_reward     | -73.1    |\n","| time/              |          |\n","|    total_timesteps | 3842400  |\n","| train/             |          |\n","|    actor_loss      | 4.45     |\n","|    critic_loss     | 0.55     |\n","|    ent_coef        | 0.0049   |\n","|    ent_coef_loss   | -0.5     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160095   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6870     |\n","|    fps             | 280      |\n","|    time_elapsed    | 13704    |\n","|    total_timesteps | 3842760  |\n","| train/             |          |\n","|    actor_loss      | 5.12     |\n","|    critic_loss     | 0.66     |\n","|    ent_coef        | 0.00489  |\n","|    ent_coef_loss   | 0.298    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160110   |\n","---------------------------------\n","Eval num_timesteps=3844800, episode_reward=-28.87 +/- 49.27\n","Episode length: 285.80 +/- 174.89\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 286      |\n","|    mean_reward     | -28.9    |\n","| time/              |          |\n","|    total_timesteps | 3844800  |\n","| train/             |          |\n","|    actor_loss      | 5.02     |\n","|    critic_loss     | 1.18     |\n","|    ent_coef        | 0.00485  |\n","|    ent_coef_loss   | -2.17    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160195   |\n","---------------------------------\n","Eval num_timesteps=3847200, episode_reward=-63.10 +/- 7.71\n","Episode length: 154.40 +/- 48.50\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 154      |\n","|    mean_reward     | -63.1    |\n","| time/              |          |\n","|    total_timesteps | 3847200  |\n","| train/             |          |\n","|    actor_loss      | 4.54     |\n","|    critic_loss     | 0.478    |\n","|    ent_coef        | 0.00488  |\n","|    ent_coef_loss   | -0.393   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6880     |\n","|    fps             | 280      |\n","|    time_elapsed    | 13719    |\n","|    total_timesteps | 3849264  |\n","| train/             |          |\n","|    actor_loss      | 5.02     |\n","|    critic_loss     | 1.77     |\n","|    ent_coef        | 0.00504  |\n","|    ent_coef_loss   | 4.48     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160381   |\n","---------------------------------\n","Eval num_timesteps=3849600, episode_reward=-41.40 +/- 22.22\n","Episode length: 294.60 +/- 137.66\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 295      |\n","|    mean_reward     | -41.4    |\n","| time/              |          |\n","|    total_timesteps | 3849600  |\n","| train/             |          |\n","|    actor_loss      | 4.83     |\n","|    critic_loss     | 0.425    |\n","|    ent_coef        | 0.00508  |\n","|    ent_coef_loss   | 0.995    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6890     |\n","|    fps             | 280      |\n","|    time_elapsed    | 13725    |\n","|    total_timesteps | 3850896  |\n","| train/             |          |\n","|    actor_loss      | 5.08     |\n","|    critic_loss     | 0.759    |\n","|    ent_coef        | 0.0051   |\n","|    ent_coef_loss   | -2.31    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160449   |\n","---------------------------------\n","Eval num_timesteps=3852000, episode_reward=-13.15 +/- 42.10\n","Episode length: 447.60 +/- 64.18\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 448      |\n","|    mean_reward     | -13.1    |\n","| time/              |          |\n","|    total_timesteps | 3852000  |\n","| train/             |          |\n","|    actor_loss      | 5.23     |\n","|    critic_loss     | 0.57     |\n","|    ent_coef        | 0.00503  |\n","|    ent_coef_loss   | -0.122   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160495   |\n","---------------------------------\n","Eval num_timesteps=3854400, episode_reward=-0.82 +/- 3.99\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.818   |\n","| time/              |          |\n","|    total_timesteps | 3854400  |\n","| train/             |          |\n","|    actor_loss      | 4.55     |\n","|    critic_loss     | 0.694    |\n","|    ent_coef        | 0.00509  |\n","|    ent_coef_loss   | 2.8      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160595   |\n","---------------------------------\n","Eval num_timesteps=3856800, episode_reward=-81.03 +/- 3.01\n","Episode length: 223.60 +/- 95.04\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 224      |\n","|    mean_reward     | -81      |\n","| time/              |          |\n","|    total_timesteps | 3856800  |\n","| train/             |          |\n","|    actor_loss      | 4.42     |\n","|    critic_loss     | 0.493    |\n","|    ent_coef        | 0.00514  |\n","|    ent_coef_loss   | -0.379   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160695   |\n","---------------------------------\n","Eval num_timesteps=3859200, episode_reward=-48.97 +/- 41.16\n","Episode length: 440.00 +/- 48.99\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 440      |\n","|    mean_reward     | -49      |\n","| time/              |          |\n","|    total_timesteps | 3859200  |\n","| train/             |          |\n","|    actor_loss      | 4.8      |\n","|    critic_loss     | 0.608    |\n","|    ent_coef        | 0.00509  |\n","|    ent_coef_loss   | -1.24    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160795   |\n","---------------------------------\n","Eval num_timesteps=3861600, episode_reward=-20.13 +/- 30.57\n","Episode length: 447.60 +/- 64.18\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 448      |\n","|    mean_reward     | -20.1    |\n","| time/              |          |\n","|    total_timesteps | 3861600  |\n","| train/             |          |\n","|    actor_loss      | 4.88     |\n","|    critic_loss     | 24.4     |\n","|    ent_coef        | 0.005    |\n","|    ent_coef_loss   | -1.38    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160895   |\n","---------------------------------\n","Eval num_timesteps=3864000, episode_reward=16.16 +/- 8.19\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 16.2     |\n","| time/              |          |\n","|    total_timesteps | 3864000  |\n","| train/             |          |\n","|    actor_loss      | 6.21     |\n","|    critic_loss     | 4.33     |\n","|    ent_coef        | 0.00493  |\n","|    ent_coef_loss   | 2.89     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160995   |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    episodes        | 6900    |\n","|    fps             | 280     |\n","|    time_elapsed    | 13776   |\n","|    total_timesteps | 3864000 |\n","--------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n"]},{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-22-8d06615c32b6>\", line 10, in <cell line: 10>\n","    model.learn(total_timesteps=4000000, log_interval=10,callback=eval_callback)\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/sac/sac.py\", line 307, in learn\n","    return super().learn(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 312, in learn\n","    rollout = self.collect_rollouts(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 552, in collect_rollouts\n","    if callback.on_step() is False:\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 104, in on_step\n","    return self._on_step()\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 471, in _on_step\n","    np.savez(\n","  File \"<__array_function__ internals>\", line 180, in savez\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 615, in savez\n","    _savez(file, args, kwds, False)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 712, in _savez\n","    zipf = zipfile_factory(file, mode=\"w\", compression=compression)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 103, in zipfile_factory\n","    return zipfile.ZipFile(file, *args, **kwargs)\n","  File \"/usr/lib/python3.10/zipfile.py\", line 1251, in __init__\n","    self.fp = io.open(file, filemode)\n","OSError: [Errno 107] Transport endpoint is not connected: './multiwalker_sac_04_log_eval/evaluations.npz'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'OSError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-22-8d06615c32b6>\", line 10, in <cell line: 10>\n","    model.learn(total_timesteps=4000000, log_interval=10,callback=eval_callback)\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/sac/sac.py\", line 307, in learn\n","    return super().learn(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 312, in learn\n","    rollout = self.collect_rollouts(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 552, in collect_rollouts\n","    if callback.on_step() is False:\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 104, in on_step\n","    return self._on_step()\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 471, in _on_step\n","    np.savez(\n","  File \"<__array_function__ internals>\", line 180, in savez\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 615, in savez\n","    _savez(file, args, kwds, False)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 712, in _savez\n","    zipf = zipfile_factory(file, mode=\"w\", compression=compression)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 103, in zipfile_factory\n","    return zipfile.ZipFile(file, *args, **kwargs)\n","  File \"/usr/lib/python3.10/zipfile.py\", line 1251, in __init__\n","    self.fp = io.open(file, filemode)\n","OSError: [Errno 107] Transport endpoint is not connected: './multiwalker_sac_04_log_eval/evaluations.npz'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'OSError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n","    if (await self.run_code(code, result,  async_=asy)):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n","    self.showtraceback(running_compiled_code=True)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n","    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-22-8d06615c32b6>\", line 10, in <cell line: 10>\n","    model.learn(total_timesteps=4000000, log_interval=10,callback=eval_callback)\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/sac/sac.py\", line 307, in learn\n","    return super().learn(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 312, in learn\n","    rollout = self.collect_rollouts(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 552, in collect_rollouts\n","    if callback.on_step() is False:\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 104, in on_step\n","    return self._on_step()\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 471, in _on_step\n","    np.savez(\n","  File \"<__array_function__ internals>\", line 180, in savez\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 615, in savez\n","    _savez(file, args, kwds, False)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 712, in _savez\n","    zipf = zipfile_factory(file, mode=\"w\", compression=compression)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 103, in zipfile_factory\n","    return zipfile.ZipFile(file, *args, **kwargs)\n","  File \"/usr/lib/python3.10/zipfile.py\", line 1251, in __init__\n","    self.fp = io.open(file, filemode)\n","OSError: [Errno 107] Transport endpoint is not connected: './multiwalker_sac_04_log_eval/evaluations.npz'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'OSError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n","    if (await self.run_code(code, result,  async_=asy)):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n","    self.showtraceback(running_compiled_code=True)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n","    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n","    return runner(coro)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n","    coro.send(None)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n","    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3492, in run_ast_nodes\n","    self.showtraceback()\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1142, in structured_traceback\n","    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n"]}],"source":["from stable_baselines3.common.logger import configure\n","\n","tmp_path = \"multiwalker_sac_04_log_eval/\"\n","# set up logger\n","new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n","\n","model = SAC(\"MlpPolicy\", env, verbose=3) #,train_freq=1\n","\n","model.set_logger(new_logger)\n","model.learn(total_timesteps=4000000, log_interval=10,callback=eval_callback)\n","model.save(\"multiwalker_sac_04\")"]},{"cell_type":"code","source":["from stable_baselines3 import SAC\n","from pettingzoo.sisl import multiwalker_v9\n","import supersuit as ss\n","from stable_baselines3.common.monitor import Monitor"],"metadata":{"id":"-6BKh_FuTZzN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HTOVAZKHTdeV","executionInfo":{"status":"ok","timestamp":1697725712497,"user_tz":-120,"elapsed":334,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"67c4f770-06db-4aed-c296-ff714445355d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["'=2.13'\t\t\t\t     multiwalker_sac_log_eval\n"," DQN_new_pettingzoo_gym_cap.ipynb    multiwalker_td3_log_eval\n"," DQN_policies\t\t\t     policy_log_eval\n","'Entrenamientos antiguos sin logs'   PPO_policies\n"," Entrenamientos_log_no_eval\t     results_rllib\n"," MCR_TFM.ipynb\t\t\t     TFM_Multiwalker_DDPG_PPO_gym_cap.ipynb\n"," multi_car_racing\t\t     TFM_Multiwalker_SAC_gym_cap.ipynb\n"," multiwalker_ddpg_log_eval\t     TFM_Multiwalker_TD3_gym_cap.ipynb\n"," multiwalker_ddpg.zip\t\t     TFM_PPO_new_pettingzoo_gym_cap.ipynb\n"," multiwalker_ppo_log_eval\t     TFM_PPO_pettingzoo_gym_cap.ipynb\n"," multiwalker_ppo.zip\n"]}]},{"cell_type":"code","source":["env = multiwalker_v9.parallel_env(render_mode=\"rgb_array\",shared_reward=0.4)\n","\n","# #---------------------------------\n","# log_dir = \"./policy_log\"\n","# os.makedirs(log_dir, exist_ok=True)\n","# env = Monitor(env, log_dir)\n","# #---------------------------------\n","# env = ss.color_reduction_v0(env, mode='B')\n","# env = ss.black_death_v3(env)\n","# env = ss.resize_v1(env, x_size=84, y_size=84)\n","env = ss.frame_stack_v1(env, 3)\n","env = ss.pettingzoo_env_to_vec_env_v1(env)\n","env = ss.concat_vec_envs_v1(env, 8, num_cpus=4, base_class='stable_baselines3')"],"metadata":{"id":"U6TtjGBcTedF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3.common.callbacks import EvalCallback\n","eval_callback = EvalCallback(env, best_model_save_path=\"./multiwalker_sac2_04_log_eval/\",\n","                             log_path=\"./multiwalker_sac2_04_log_eval/\", eval_freq=150,\n","                             deterministic=False, render=False)\n"],"metadata":{"id":"1ah3u9HuTga_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3.common.logger import configure\n","\n","tmp_path = \"multiwalker_sac2_04_log_eval/\"\n","# set up logger\n","new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n","\n","model = SAC(\"MlpPolicy\", env, verbose=3,batch_size=512, learning_rate=0.0005) #,train_freq=1\n","\n","model.set_logger(new_logger)\n","model.learn(total_timesteps=4000000, log_interval=10,callback=eval_callback)\n","model.save(\"multiwalker_sac2_04\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"muMFxxDZTg_Z","outputId":"0f9e7699-6025-44e3-ba08-fd604525060b","executionInfo":{"status":"ok","timestamp":1699026698170,"user_tz":-60,"elapsed":11666946,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Logging to multiwalker_sac2_04_log_eval/\n","Using cuda device\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 10       |\n","|    fps             | 204      |\n","|    time_elapsed    | 7        |\n","|    total_timesteps | 1440     |\n","| train/             |          |\n","|    actor_loss      | -5.35    |\n","|    critic_loss     | 38.7     |\n","|    ent_coef        | 0.973    |\n","|    ent_coef_loss   | -0.182   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 55       |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 20       |\n","|    fps             | 219      |\n","|    time_elapsed    | 7        |\n","|    total_timesteps | 1632     |\n","| train/             |          |\n","|    actor_loss      | -4.81    |\n","|    critic_loss     | 87.1     |\n","|    ent_coef        | 0.969    |\n","|    ent_coef_loss   | -0.209   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 63       |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 30       |\n","|    fps             | 309      |\n","|    time_elapsed    | 9        |\n","|    total_timesteps | 2928     |\n","| train/             |          |\n","|    actor_loss      | -5.03    |\n","|    critic_loss     | 86.8     |\n","|    ent_coef        | 0.944    |\n","|    ent_coef_loss   | -0.391   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 117      |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40       |\n","|    fps             | 353      |\n","|    time_elapsed    | 10       |\n","|    total_timesteps | 3576     |\n","| train/             |          |\n","|    actor_loss      | -5.19    |\n","|    critic_loss     | 25.4     |\n","|    ent_coef        | 0.931    |\n","|    ent_coef_loss   | -0.482   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 144      |\n","---------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.76     |\n","| time/              |          |\n","|    total_timesteps | 3078000  |\n","| train/             |          |\n","|    actor_loss      | 0.395    |\n","|    critic_loss     | 0.0786   |\n","|    ent_coef        | 0.00218  |\n","|    ent_coef_loss   | -0.024   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 128245   |\n","---------------------------------\n","Eval num_timesteps=3081600, episode_reward=2.43 +/- 0.14\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.43     |\n","| time/              |          |\n","|    total_timesteps | 3081600  |\n","| train/             |          |\n","|    actor_loss      | 0.575    |\n","|    critic_loss     | 11.4     |\n","|    ent_coef        | 0.00217  |\n","|    ent_coef_loss   | 1.96     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 128395   |\n","---------------------------------\n","Eval num_timesteps=3085200, episode_reward=-28.59 +/- 39.20\n","Episode length: 370.40 +/- 158.73\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 370      |\n","|    mean_reward     | -28.6    |\n","| time/              |          |\n","|    total_timesteps | 3085200  |\n","| train/             |          |\n","|    actor_loss      | 0.41     |\n","|    critic_loss     | 0.152    |\n","|    ent_coef        | 0.00212  |\n","|    ent_coef_loss   | 1.36     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 128545   |\n","---------------------------------\n","Eval num_timesteps=3088800, episode_reward=-26.72 +/- 37.71\n","Episode length: 369.20 +/- 160.20\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 369      |\n","|    mean_reward     | -26.7    |\n","| time/              |          |\n","|    total_timesteps | 3088800  |\n","| train/             |          |\n","|    actor_loss      | 0.381    |\n","|    critic_loss     | 1.1      |\n","|    ent_coef        | 0.00218  |\n","|    ent_coef_loss   | 3.43     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 128695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4230     |\n","|    fps             | 343      |\n","|    time_elapsed    | 9007     |\n","|    total_timesteps | 3091248  |\n","| train/             |          |\n","|    actor_loss      | 0.352    |\n","|    critic_loss     | 0.101    |\n","|    ent_coef        | 0.00222  |\n","|    ent_coef_loss   | 0.894    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 128797   |\n","---------------------------------\n","Eval num_timesteps=3092400, episode_reward=7.64 +/- 0.19\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.64     |\n","| time/              |          |\n","|    total_timesteps | 3092400  |\n","| train/             |          |\n","|    actor_loss      | 0.427    |\n","|    critic_loss     | 0.73     |\n","|    ent_coef        | 0.0022   |\n","|    ent_coef_loss   | -2.89    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 128845   |\n","---------------------------------\n","Eval num_timesteps=3096000, episode_reward=-21.21 +/- 40.19\n","Episode length: 362.40 +/- 168.52\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 362      |\n","|    mean_reward     | -21.2    |\n","| time/              |          |\n","|    total_timesteps | 3096000  |\n","| train/             |          |\n","|    actor_loss      | 0.575    |\n","|    critic_loss     | 0.0675   |\n","|    ent_coef        | 0.00203  |\n","|    ent_coef_loss   | -2.02    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 128995   |\n","---------------------------------\n","Eval num_timesteps=3099600, episode_reward=-0.45 +/- 6.37\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.449   |\n","| time/              |          |\n","|    total_timesteps | 3099600  |\n","| train/             |          |\n","|    actor_loss      | 0.316    |\n","|    critic_loss     | 0.0648   |\n","|    ent_coef        | 0.00211  |\n","|    ent_coef_loss   | -3.99    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 129145   |\n","---------------------------------\n","Eval num_timesteps=3103200, episode_reward=7.14 +/- 2.47\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.14     |\n","| time/              |          |\n","|    total_timesteps | 3103200  |\n","| train/             |          |\n","|    actor_loss      | 0.451    |\n","|    critic_loss     | 0.124    |\n","|    ent_coef        | 0.00211  |\n","|    ent_coef_loss   | 0.0912   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 129295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4240     |\n","|    fps             | 343      |\n","|    time_elapsed    | 9050     |\n","|    total_timesteps | 3105192  |\n","| train/             |          |\n","|    actor_loss      | 0.403    |\n","|    critic_loss     | 0.101    |\n","|    ent_coef        | 0.00216  |\n","|    ent_coef_loss   | 0.23     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 129378   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4250     |\n","|    fps             | 343      |\n","|    time_elapsed    | 9051     |\n","|    total_timesteps | 3105480  |\n","| train/             |          |\n","|    actor_loss      | 0.346    |\n","|    critic_loss     | 0.191    |\n","|    ent_coef        | 0.00217  |\n","|    ent_coef_loss   | -2.31    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 129390   |\n","---------------------------------\n","Eval num_timesteps=3106800, episode_reward=1.11 +/- 1.04\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.11     |\n","| time/              |          |\n","|    total_timesteps | 3106800  |\n","| train/             |          |\n","|    actor_loss      | 0.275    |\n","|    critic_loss     | 0.0758   |\n","|    ent_coef        | 0.00215  |\n","|    ent_coef_loss   | -3.85    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 129445   |\n","---------------------------------\n","Eval num_timesteps=3110400, episode_reward=1.49 +/- 0.71\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.49     |\n","| time/              |          |\n","|    total_timesteps | 3110400  |\n","| train/             |          |\n","|    actor_loss      | 0.344    |\n","|    critic_loss     | 0.114    |\n","|    ent_coef        | 0.00204  |\n","|    ent_coef_loss   | -3.24    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 129595   |\n","---------------------------------\n","Eval num_timesteps=3114000, episode_reward=-43.94 +/- 36.36\n","Episode length: 303.20 +/- 160.69\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 303      |\n","|    mean_reward     | -43.9    |\n","| time/              |          |\n","|    total_timesteps | 3114000  |\n","| train/             |          |\n","|    actor_loss      | 0.303    |\n","|    critic_loss     | 11.3     |\n","|    ent_coef        | 0.00205  |\n","|    ent_coef_loss   | -1.03    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 129745   |\n","---------------------------------\n","Eval num_timesteps=3117600, episode_reward=2.82 +/- 1.02\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.82     |\n","| time/              |          |\n","|    total_timesteps | 3117600  |\n","| train/             |          |\n","|    actor_loss      | 0.26     |\n","|    critic_loss     | 0.218    |\n","|    ent_coef        | 0.00194  |\n","|    ent_coef_loss   | 1.47     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 129895   |\n","---------------------------------\n","Eval num_timesteps=3121200, episode_reward=-41.02 +/- 36.17\n","Episode length: 495.20 +/- 3.92\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 495      |\n","|    mean_reward     | -41      |\n","| time/              |          |\n","|    total_timesteps | 3121200  |\n","| train/             |          |\n","|    actor_loss      | 0.32     |\n","|    critic_loss     | 0.136    |\n","|    ent_coef        | 0.00193  |\n","|    ent_coef_loss   | -1.68    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 130045   |\n","---------------------------------\n","Eval num_timesteps=3124800, episode_reward=-37.55 +/- 43.46\n","Episode length: 365.00 +/- 110.23\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 365      |\n","|    mean_reward     | -37.5    |\n","| time/              |          |\n","|    total_timesteps | 3124800  |\n","| train/             |          |\n","|    actor_loss      | 0.328    |\n","|    critic_loss     | 0.132    |\n","|    ent_coef        | 0.002    |\n","|    ent_coef_loss   | 2.94     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 130195   |\n","---------------------------------\n","Eval num_timesteps=3128400, episode_reward=2.50 +/- 0.17\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.5      |\n","| time/              |          |\n","|    total_timesteps | 3128400  |\n","| train/             |          |\n","|    actor_loss      | 0.173    |\n","|    critic_loss     | 0.12     |\n","|    ent_coef        | 0.00203  |\n","|    ent_coef_loss   | 0.736    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 130345   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4260     |\n","|    fps             | 343      |\n","|    time_elapsed    | 9126     |\n","|    total_timesteps | 3131760  |\n","| train/             |          |\n","|    actor_loss      | 0.536    |\n","|    critic_loss     | 0.304    |\n","|    ent_coef        | 0.00204  |\n","|    ent_coef_loss   | -3.2     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 130485   |\n","---------------------------------\n","Eval num_timesteps=3132000, episode_reward=1.04 +/- 0.16\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.04     |\n","| time/              |          |\n","|    total_timesteps | 3132000  |\n","| train/             |          |\n","|    actor_loss      | 0.326    |\n","|    critic_loss     | 0.095    |\n","|    ent_coef        | 0.00203  |\n","|    ent_coef_loss   | -0.503   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 130495   |\n","---------------------------------\n","Eval num_timesteps=3135600, episode_reward=-78.84 +/- 3.36\n","Episode length: 64.20 +/- 3.92\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 64.2     |\n","|    mean_reward     | -78.8    |\n","| time/              |          |\n","|    total_timesteps | 3135600  |\n","| train/             |          |\n","|    actor_loss      | 0.34     |\n","|    critic_loss     | 0.087    |\n","|    ent_coef        | 0.00198  |\n","|    ent_coef_loss   | 0.578    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 130645   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4270     |\n","|    fps             | 343      |\n","|    time_elapsed    | 9141     |\n","|    total_timesteps | 3135912  |\n","| train/             |          |\n","|    actor_loss      | 0.504    |\n","|    critic_loss     | 0.694    |\n","|    ent_coef        | 0.00198  |\n","|    ent_coef_loss   | -1.1     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 130658   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4280     |\n","|    fps             | 343      |\n","|    time_elapsed    | 9141     |\n","|    total_timesteps | 3136224  |\n","| train/             |          |\n","|    actor_loss      | 0.166    |\n","|    critic_loss     | 0.0981   |\n","|    ent_coef        | 0.00197  |\n","|    ent_coef_loss   | 0.308    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 130671   |\n","---------------------------------\n","Eval num_timesteps=3139200, episode_reward=-31.39 +/- 40.09\n","Episode length: 495.20 +/- 5.88\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 495      |\n","|    mean_reward     | -31.4    |\n","| time/              |          |\n","|    total_timesteps | 3139200  |\n","| train/             |          |\n","|    actor_loss      | 0.467    |\n","|    critic_loss     | 0.319    |\n","|    ent_coef        | 0.00191  |\n","|    ent_coef_loss   | -5.58    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 130795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4290     |\n","|    fps             | 343      |\n","|    time_elapsed    | 9153     |\n","|    total_timesteps | 3142584  |\n","| train/             |          |\n","|    actor_loss      | 0.393    |\n","|    critic_loss     | 0.137    |\n","|    ent_coef        | 0.00184  |\n","|    ent_coef_loss   | 0.554    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 130936   |\n","---------------------------------\n","Eval num_timesteps=3142800, episode_reward=-37.46 +/- 43.41\n","Episode length: 444.80 +/- 67.61\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 445      |\n","|    mean_reward     | -37.5    |\n","| time/              |          |\n","|    total_timesteps | 3142800  |\n","| train/             |          |\n","|    actor_loss      | 0.283    |\n","|    critic_loss     | 0.152    |\n","|    ent_coef        | 0.00184  |\n","|    ent_coef_loss   | -1.95    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 130945   |\n","---------------------------------\n","Eval num_timesteps=3146400, episode_reward=-0.68 +/- 0.81\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.678   |\n","| time/              |          |\n","|    total_timesteps | 3146400  |\n","| train/             |          |\n","|    actor_loss      | 0.125    |\n","|    critic_loss     | 0.0791   |\n","|    ent_coef        | 0.0018   |\n","|    ent_coef_loss   | -1.15    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 131095   |\n","---------------------------------\n","Eval num_timesteps=3150000, episode_reward=-77.02 +/- 4.03\n","Episode length: 448.80 +/- 13.72\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 449      |\n","|    mean_reward     | -77      |\n","| time/              |          |\n","|    total_timesteps | 3150000  |\n","| train/             |          |\n","|    actor_loss      | 0.352    |\n","|    critic_loss     | 0.199    |\n","|    ent_coef        | 0.00173  |\n","|    ent_coef_loss   | -0.26    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 131245   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4300     |\n","|    fps             | 343      |\n","|    time_elapsed    | 9184     |\n","|    total_timesteps | 3150960  |\n","| train/             |          |\n","|    actor_loss      | 0.151    |\n","|    critic_loss     | 0.103    |\n","|    ent_coef        | 0.00172  |\n","|    ent_coef_loss   | -2.59    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 131285   |\n","---------------------------------\n","Eval num_timesteps=3153600, episode_reward=2.63 +/- 0.66\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.63     |\n","| time/              |          |\n","|    total_timesteps | 3153600  |\n","| train/             |          |\n","|    actor_loss      | 0.32     |\n","|    critic_loss     | 0.109    |\n","|    ent_coef        | 0.00171  |\n","|    ent_coef_loss   | -0.0157  |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 131395   |\n","---------------------------------\n","Eval num_timesteps=3157200, episode_reward=-0.31 +/- 1.62\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.309   |\n","| time/              |          |\n","|    total_timesteps | 3157200  |\n","| train/             |          |\n","|    actor_loss      | 0.14     |\n","|    critic_loss     | 0.104    |\n","|    ent_coef        | 0.00192  |\n","|    ent_coef_loss   | 4.18     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 131545   |\n","---------------------------------\n","Eval num_timesteps=3160800, episode_reward=1.45 +/- 0.61\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.45     |\n","| time/              |          |\n","|    total_timesteps | 3160800  |\n","| train/             |          |\n","|    actor_loss      | 0.0467   |\n","|    critic_loss     | 0.084    |\n","|    ent_coef        | 0.00199  |\n","|    ent_coef_loss   | 0.355    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 131695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4310     |\n","|    fps             | 343      |\n","|    time_elapsed    | 9215     |\n","|    total_timesteps | 3162144  |\n","| train/             |          |\n","|    actor_loss      | 0.238    |\n","|    critic_loss     | 0.172    |\n","|    ent_coef        | 0.00198  |\n","|    ent_coef_loss   | -2.15    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 131751   |\n","---------------------------------\n","Eval num_timesteps=3164400, episode_reward=0.48 +/- 0.93\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.482    |\n","| time/              |          |\n","|    total_timesteps | 3164400  |\n","| train/             |          |\n","|    actor_loss      | 0.26     |\n","|    critic_loss     | 0.0951   |\n","|    ent_coef        | 0.00195  |\n","|    ent_coef_loss   | -3.99    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 131845   |\n","---------------------------------\n","Eval num_timesteps=3168000, episode_reward=5.07 +/- 1.42\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.07     |\n","| time/              |          |\n","|    total_timesteps | 3168000  |\n","| train/             |          |\n","|    actor_loss      | 0.417    |\n","|    critic_loss     | 0.118    |\n","|    ent_coef        | 0.0019   |\n","|    ent_coef_loss   | -2.37    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 131995   |\n","---------------------------------\n","Eval num_timesteps=3171600, episode_reward=1.96 +/- 0.25\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.96     |\n","| time/              |          |\n","|    total_timesteps | 3171600  |\n","| train/             |          |\n","|    actor_loss      | 0.384    |\n","|    critic_loss     | 11.6     |\n","|    ent_coef        | 0.00185  |\n","|    ent_coef_loss   | -1.26    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 132145   |\n","---------------------------------\n","Eval num_timesteps=3175200, episode_reward=-0.19 +/- 2.55\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.187   |\n","| time/              |          |\n","|    total_timesteps | 3175200  |\n","| train/             |          |\n","|    actor_loss      | 0.0983   |\n","|    critic_loss     | 0.0957   |\n","|    ent_coef        | 0.00173  |\n","|    ent_coef_loss   | -0.899   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 132295   |\n","---------------------------------\n","Eval num_timesteps=3178800, episode_reward=-24.84 +/- 41.96\n","Episode length: 346.80 +/- 187.63\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 347      |\n","|    mean_reward     | -24.8    |\n","| time/              |          |\n","|    total_timesteps | 3178800  |\n","| train/             |          |\n","|    actor_loss      | 0.222    |\n","|    critic_loss     | 0.0755   |\n","|    ent_coef        | 0.00169  |\n","|    ent_coef_loss   | -0.435   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 132445   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4320     |\n","|    fps             | 343      |\n","|    time_elapsed    | 9273     |\n","|    total_timesteps | 3181608  |\n","| train/             |          |\n","|    actor_loss      | 0.409    |\n","|    critic_loss     | 11.4     |\n","|    ent_coef        | 0.00174  |\n","|    ent_coef_loss   | -1.23    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 132562   |\n","---------------------------------\n","Eval num_timesteps=3182400, episode_reward=-26.35 +/- 41.95\n","Episode length: 362.40 +/- 168.52\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 362      |\n","|    mean_reward     | -26.3    |\n","| time/              |          |\n","|    total_timesteps | 3182400  |\n","| train/             |          |\n","|    actor_loss      | 0.133    |\n","|    critic_loss     | 0.0588   |\n","|    ent_coef        | 0.00173  |\n","|    ent_coef_loss   | -2.73    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 132595   |\n","---------------------------------\n","Eval num_timesteps=3186000, episode_reward=1.77 +/- 0.26\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.77     |\n","| time/              |          |\n","|    total_timesteps | 3186000  |\n","| train/             |          |\n","|    actor_loss      | 0.181    |\n","|    critic_loss     | 0.254    |\n","|    ent_coef        | 0.00165  |\n","|    ent_coef_loss   | -2.84    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 132745   |\n","---------------------------------\n","Eval num_timesteps=3189600, episode_reward=2.59 +/- 1.44\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.59     |\n","| time/              |          |\n","|    total_timesteps | 3189600  |\n","| train/             |          |\n","|    actor_loss      | 0.264    |\n","|    critic_loss     | 0.354    |\n","|    ent_coef        | 0.00155  |\n","|    ent_coef_loss   | -3.57    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 132895   |\n","---------------------------------\n","Eval num_timesteps=3193200, episode_reward=2.16 +/- 0.35\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.16     |\n","| time/              |          |\n","|    total_timesteps | 3193200  |\n","| train/             |          |\n","|    actor_loss      | 0.233    |\n","|    critic_loss     | 0.196    |\n","|    ent_coef        | 0.00149  |\n","|    ent_coef_loss   | 1.14     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 133045   |\n","---------------------------------\n","Eval num_timesteps=3196800, episode_reward=-0.79 +/- 1.85\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.792   |\n","| time/              |          |\n","|    total_timesteps | 3196800  |\n","| train/             |          |\n","|    actor_loss      | 0.186    |\n","|    critic_loss     | 0.262    |\n","|    ent_coef        | 0.00154  |\n","|    ent_coef_loss   | -2.62    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 133195   |\n","---------------------------------\n","Eval num_timesteps=3200400, episode_reward=1.78 +/- 0.38\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.78     |\n","| time/              |          |\n","|    total_timesteps | 3200400  |\n","| train/             |          |\n","|    actor_loss      | 0.306    |\n","|    critic_loss     | 0.164    |\n","|    ent_coef        | 0.0017   |\n","|    ent_coef_loss   | 3.73     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 133345   |\n","---------------------------------\n","Eval num_timesteps=3204000, episode_reward=-41.97 +/- 35.80\n","Episode length: 332.60 +/- 136.68\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 333      |\n","|    mean_reward     | -42      |\n","| time/              |          |\n","|    total_timesteps | 3204000  |\n","| train/             |          |\n","|    actor_loss      | 0.266    |\n","|    critic_loss     | 0.291    |\n","|    ent_coef        | 0.00177  |\n","|    ent_coef_loss   | 4.56     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 133495   |\n","---------------------------------\n","Eval num_timesteps=3207600, episode_reward=3.12 +/- 0.76\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.12     |\n","| time/              |          |\n","|    total_timesteps | 3207600  |\n","| train/             |          |\n","|    actor_loss      | 0.304    |\n","|    critic_loss     | 0.335    |\n","|    ent_coef        | 0.00177  |\n","|    ent_coef_loss   | -3.55    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 133645   |\n","---------------------------------\n","Eval num_timesteps=3211200, episode_reward=1.76 +/- 0.52\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.76     |\n","| time/              |          |\n","|    total_timesteps | 3211200  |\n","| train/             |          |\n","|    actor_loss      | 0.184    |\n","|    critic_loss     | 0.0741   |\n","|    ent_coef        | 0.00171  |\n","|    ent_coef_loss   | -2.02    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 133795   |\n","---------------------------------\n","Eval num_timesteps=3214800, episode_reward=-74.59 +/- 3.75\n","Episode length: 188.00 +/- 93.08\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 188      |\n","|    mean_reward     | -74.6    |\n","| time/              |          |\n","|    total_timesteps | 3214800  |\n","| train/             |          |\n","|    actor_loss      | 0.154    |\n","|    critic_loss     | 0.176    |\n","|    ent_coef        | 0.00172  |\n","|    ent_coef_loss   | 0.585    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 133945   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4330     |\n","|    fps             | 342      |\n","|    time_elapsed    | 9376     |\n","|    total_timesteps | 3215640  |\n","| train/             |          |\n","|    actor_loss      | 0.314    |\n","|    critic_loss     | 0.152    |\n","|    ent_coef        | 0.00174  |\n","|    ent_coef_loss   | 1.81     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 133980   |\n","---------------------------------\n","Eval num_timesteps=3218400, episode_reward=3.10 +/- 0.10\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.1      |\n","| time/              |          |\n","|    total_timesteps | 3218400  |\n","| train/             |          |\n","|    actor_loss      | 0.266    |\n","|    critic_loss     | 0.0978   |\n","|    ent_coef        | 0.0017   |\n","|    ent_coef_loss   | -3.3     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 134095   |\n","---------------------------------\n","Eval num_timesteps=3222000, episode_reward=1.91 +/- 0.49\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.91     |\n","| time/              |          |\n","|    total_timesteps | 3222000  |\n","| train/             |          |\n","|    actor_loss      | 0.295    |\n","|    critic_loss     | 1.28     |\n","|    ent_coef        | 0.00164  |\n","|    ent_coef_loss   | -2.31    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 134245   |\n","---------------------------------\n","Eval num_timesteps=3225600, episode_reward=2.68 +/- 0.50\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.68     |\n","| time/              |          |\n","|    total_timesteps | 3225600  |\n","| train/             |          |\n","|    actor_loss      | 0.151    |\n","|    critic_loss     | 0.0608   |\n","|    ent_coef        | 0.00163  |\n","|    ent_coef_loss   | -2.55    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 134395   |\n","---------------------------------\n","Eval num_timesteps=3229200, episode_reward=1.59 +/- 0.02\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.59     |\n","| time/              |          |\n","|    total_timesteps | 3229200  |\n","| train/             |          |\n","|    actor_loss      | 0.138    |\n","|    critic_loss     | 0.0415   |\n","|    ent_coef        | 0.00164  |\n","|    ent_coef_loss   | 4.11     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 134545   |\n","---------------------------------\n","Eval num_timesteps=3232800, episode_reward=2.53 +/- 0.09\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.53     |\n","| time/              |          |\n","|    total_timesteps | 3232800  |\n","| train/             |          |\n","|    actor_loss      | 0.297    |\n","|    critic_loss     | 0.123    |\n","|    ent_coef        | 0.00174  |\n","|    ent_coef_loss   | 2.14     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 134695   |\n","---------------------------------\n","Eval num_timesteps=3236400, episode_reward=-22.53 +/- 43.31\n","Episode length: 347.20 +/- 187.14\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 347      |\n","|    mean_reward     | -22.5    |\n","| time/              |          |\n","|    total_timesteps | 3236400  |\n","| train/             |          |\n","|    actor_loss      | 0.234    |\n","|    critic_loss     | 0.0891   |\n","|    ent_coef        | 0.00176  |\n","|    ent_coef_loss   | -2.25    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 134845   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4340     |\n","|    fps             | 342      |\n","|    time_elapsed    | 9443     |\n","|    total_timesteps | 3239040  |\n","| train/             |          |\n","|    actor_loss      | 0.356    |\n","|    critic_loss     | 0.137    |\n","|    ent_coef        | 0.00175  |\n","|    ent_coef_loss   | 0.102    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 134955   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4350     |\n","|    fps             | 343      |\n","|    time_elapsed    | 9445     |\n","|    total_timesteps | 3239880  |\n","| train/             |          |\n","|    actor_loss      | 0.138    |\n","|    critic_loss     | 0.0688   |\n","|    ent_coef        | 0.00176  |\n","|    ent_coef_loss   | -0.403   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 134990   |\n","---------------------------------\n","Eval num_timesteps=3240000, episode_reward=2.03 +/- 0.16\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.03     |\n","| time/              |          |\n","|    total_timesteps | 3240000  |\n","| train/             |          |\n","|    actor_loss      | 0.212    |\n","|    critic_loss     | 0.145    |\n","|    ent_coef        | 0.00176  |\n","|    ent_coef_loss   | -0.221   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 134995   |\n","---------------------------------\n","Eval num_timesteps=3243600, episode_reward=2.12 +/- 0.04\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.12     |\n","| time/              |          |\n","|    total_timesteps | 3243600  |\n","| train/             |          |\n","|    actor_loss      | 0.192    |\n","|    critic_loss     | 0.121    |\n","|    ent_coef        | 0.00181  |\n","|    ent_coef_loss   | -1.12    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 135145   |\n","---------------------------------\n","Eval num_timesteps=3247200, episode_reward=1.56 +/- 0.21\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.56     |\n","| time/              |          |\n","|    total_timesteps | 3247200  |\n","| train/             |          |\n","|    actor_loss      | 0.206    |\n","|    critic_loss     | 0.145    |\n","|    ent_coef        | 0.0018   |\n","|    ent_coef_loss   | -0.27    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 135295   |\n","---------------------------------\n","Eval num_timesteps=3250800, episode_reward=3.09 +/- 0.10\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.09     |\n","| time/              |          |\n","|    total_timesteps | 3250800  |\n","| train/             |          |\n","|    actor_loss      | -0.00613 |\n","|    critic_loss     | 0.0632   |\n","|    ent_coef        | 0.00176  |\n","|    ent_coef_loss   | -0.166   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 135445   |\n","---------------------------------\n","Eval num_timesteps=3254400, episode_reward=-75.78 +/- 5.33\n","Episode length: 261.60 +/- 34.78\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 262      |\n","|    mean_reward     | -75.8    |\n","| time/              |          |\n","|    total_timesteps | 3254400  |\n","| train/             |          |\n","|    actor_loss      | 0.475    |\n","|    critic_loss     | 0.272    |\n","|    ent_coef        | 0.0018   |\n","|    ent_coef_loss   | -0.902   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 135595   |\n","---------------------------------\n","Eval num_timesteps=3258000, episode_reward=3.02 +/- 0.57\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.02     |\n","| time/              |          |\n","|    total_timesteps | 3258000  |\n","| train/             |          |\n","|    actor_loss      | 0.0846   |\n","|    critic_loss     | 0.0776   |\n","|    ent_coef        | 0.00171  |\n","|    ent_coef_loss   | -1.26    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 135745   |\n","---------------------------------\n","Eval num_timesteps=3261600, episode_reward=10.42 +/- 0.12\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 10.4     |\n","| time/              |          |\n","|    total_timesteps | 3261600  |\n","| train/             |          |\n","|    actor_loss      | 0.29     |\n","|    critic_loss     | 0.0728   |\n","|    ent_coef        | 0.00177  |\n","|    ent_coef_loss   | 1.7      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 135895   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4360     |\n","|    fps             | 343      |\n","|    time_elapsed    | 9517     |\n","|    total_timesteps | 3265152  |\n","| train/             |          |\n","|    actor_loss      | 0.299    |\n","|    critic_loss     | 0.105    |\n","|    ent_coef        | 0.0019   |\n","|    ent_coef_loss   | -1.13    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 136043   |\n","---------------------------------\n","Eval num_timesteps=3265200, episode_reward=0.90 +/- 0.26\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.897    |\n","| time/              |          |\n","|    total_timesteps | 3265200  |\n","| train/             |          |\n","|    actor_loss      | 0.071    |\n","|    critic_loss     | 0.0877   |\n","|    ent_coef        | 0.0019   |\n","|    ent_coef_loss   | -0.171   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 136045   |\n","---------------------------------\n","Eval num_timesteps=3268800, episode_reward=-30.32 +/- 39.16\n","Episode length: 347.60 +/- 186.65\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 348      |\n","|    mean_reward     | -30.3    |\n","| time/              |          |\n","|    total_timesteps | 3268800  |\n","| train/             |          |\n","|    actor_loss      | 0.263    |\n","|    critic_loss     | 7.26     |\n","|    ent_coef        | 0.00191  |\n","|    ent_coef_loss   | -0.49    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 136195   |\n","---------------------------------\n","Eval num_timesteps=3272400, episode_reward=0.90 +/- 0.41\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.896    |\n","| time/              |          |\n","|    total_timesteps | 3272400  |\n","| train/             |          |\n","|    actor_loss      | 0.0824   |\n","|    critic_loss     | 0.125    |\n","|    ent_coef        | 0.002    |\n","|    ent_coef_loss   | 2.88     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 136345   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4370     |\n","|    fps             | 343      |\n","|    time_elapsed    | 9547     |\n","|    total_timesteps | 3275952  |\n","| train/             |          |\n","|    actor_loss      | 0.0818   |\n","|    critic_loss     | 0.112    |\n","|    ent_coef        | 0.00211  |\n","|    ent_coef_loss   | 5.59     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 136493   |\n","---------------------------------\n","Eval num_timesteps=3276000, episode_reward=1.18 +/- 0.63\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.18     |\n","| time/              |          |\n","|    total_timesteps | 3276000  |\n","| train/             |          |\n","|    actor_loss      | 0.179    |\n","|    critic_loss     | 0.11     |\n","|    ent_coef        | 0.00211  |\n","|    ent_coef_loss   | 4.09     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 136495   |\n","---------------------------------\n","Eval num_timesteps=3279600, episode_reward=-0.17 +/- 0.47\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.169   |\n","| time/              |          |\n","|    total_timesteps | 3279600  |\n","| train/             |          |\n","|    actor_loss      | 0.184    |\n","|    critic_loss     | 8.68     |\n","|    ent_coef        | 0.00228  |\n","|    ent_coef_loss   | 0.258    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 136645   |\n","---------------------------------\n","Eval num_timesteps=3283200, episode_reward=7.94 +/- 1.23\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.94     |\n","| time/              |          |\n","|    total_timesteps | 3283200  |\n","| train/             |          |\n","|    actor_loss      | -0.0172  |\n","|    critic_loss     | 0.112    |\n","|    ent_coef        | 0.00216  |\n","|    ent_coef_loss   | -2       |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 136795   |\n","---------------------------------\n","Eval num_timesteps=3286800, episode_reward=-79.58 +/- 7.63\n","Episode length: 204.60 +/- 28.90\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 205      |\n","|    mean_reward     | -79.6    |\n","| time/              |          |\n","|    total_timesteps | 3286800  |\n","| train/             |          |\n","|    actor_loss      | 0.165    |\n","|    critic_loss     | 0.114    |\n","|    ent_coef        | 0.00218  |\n","|    ent_coef_loss   | -0.385   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 136945   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4380     |\n","|    fps             | 343      |\n","|    time_elapsed    | 9585     |\n","|    total_timesteps | 3288072  |\n","| train/             |          |\n","|    actor_loss      | 0.122    |\n","|    critic_loss     | 0.107    |\n","|    ent_coef        | 0.00215  |\n","|    ent_coef_loss   | 2.6      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 136998   |\n","---------------------------------\n","Eval num_timesteps=3290400, episode_reward=-42.68 +/- 38.26\n","Episode length: 259.40 +/- 196.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 259      |\n","|    mean_reward     | -42.7    |\n","| time/              |          |\n","|    total_timesteps | 3290400  |\n","| train/             |          |\n","|    actor_loss      | 0.299    |\n","|    critic_loss     | 9.84     |\n","|    ent_coef        | 0.00215  |\n","|    ent_coef_loss   | -5.07    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 137095   |\n","---------------------------------\n","Eval num_timesteps=3294000, episode_reward=-29.11 +/- 37.92\n","Episode length: 348.00 +/- 186.16\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 348      |\n","|    mean_reward     | -29.1    |\n","| time/              |          |\n","|    total_timesteps | 3294000  |\n","| train/             |          |\n","|    actor_loss      | 0.0303   |\n","|    critic_loss     | 0.089    |\n","|    ent_coef        | 0.00222  |\n","|    ent_coef_loss   | 25.5     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 137245   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4390     |\n","|    fps             | 343      |\n","|    time_elapsed    | 9608     |\n","|    total_timesteps | 3296880  |\n","| train/             |          |\n","|    actor_loss      | 0.0125   |\n","|    critic_loss     | 10.8     |\n","|    ent_coef        | 0.00241  |\n","|    ent_coef_loss   | -1       |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 137365   |\n","---------------------------------\n","Eval num_timesteps=3297600, episode_reward=0.41 +/- 1.36\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.409    |\n","| time/              |          |\n","|    total_timesteps | 3297600  |\n","| train/             |          |\n","|    actor_loss      | 0.335    |\n","|    critic_loss     | 0.182    |\n","|    ent_coef        | 0.0024   |\n","|    ent_coef_loss   | 1.01     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 137395   |\n","---------------------------------\n","Eval num_timesteps=3301200, episode_reward=4.15 +/- 1.86\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.15     |\n","| time/              |          |\n","|    total_timesteps | 3301200  |\n","| train/             |          |\n","|    actor_loss      | 0.132    |\n","|    critic_loss     | 0.0885   |\n","|    ent_coef        | 0.00241  |\n","|    ent_coef_loss   | -5.89    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 137545   |\n","---------------------------------\n","Eval num_timesteps=3304800, episode_reward=7.56 +/- 4.65\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.56     |\n","| time/              |          |\n","|    total_timesteps | 3304800  |\n","| train/             |          |\n","|    actor_loss      | 0.338    |\n","|    critic_loss     | 10.3     |\n","|    ent_coef        | 0.00225  |\n","|    ent_coef_loss   | -4.66    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 137695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4400     |\n","|    fps             | 343      |\n","|    time_elapsed    | 9641     |\n","|    total_timesteps | 3307152  |\n","| train/             |          |\n","|    actor_loss      | 0.205    |\n","|    critic_loss     | 0.171    |\n","|    ent_coef        | 0.00221  |\n","|    ent_coef_loss   | 5.65     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 137793   |\n","---------------------------------\n","Eval num_timesteps=3308400, episode_reward=1.46 +/- 0.16\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.46     |\n","| time/              |          |\n","|    total_timesteps | 3308400  |\n","| train/             |          |\n","|    actor_loss      | 0.321    |\n","|    critic_loss     | 0.162    |\n","|    ent_coef        | 0.00228  |\n","|    ent_coef_loss   | 3.55     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 137845   |\n","---------------------------------\n","Eval num_timesteps=3312000, episode_reward=1.99 +/- 0.13\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.99     |\n","| time/              |          |\n","|    total_timesteps | 3312000  |\n","| train/             |          |\n","|    actor_loss      | 0.628    |\n","|    critic_loss     | 0.453    |\n","|    ent_coef        | 0.00238  |\n","|    ent_coef_loss   | 3.76     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 137995   |\n","---------------------------------\n","Eval num_timesteps=3315600, episode_reward=-75.91 +/- 3.54\n","Episode length: 106.40 +/- 14.21\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 106      |\n","|    mean_reward     | -75.9    |\n","| time/              |          |\n","|    total_timesteps | 3315600  |\n","| train/             |          |\n","|    actor_loss      | 0.159    |\n","|    critic_loss     | 0.149    |\n","|    ent_coef        | 0.00232  |\n","|    ent_coef_loss   | -1.39    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 138145   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4410     |\n","|    fps             | 343      |\n","|    time_elapsed    | 9668     |\n","|    total_timesteps | 3317184  |\n","| train/             |          |\n","|    actor_loss      | 0.293    |\n","|    critic_loss     | 9.45     |\n","|    ent_coef        | 0.0023   |\n","|    ent_coef_loss   | 0.548    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 138211   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4420     |\n","|    fps             | 343      |\n","|    time_elapsed    | 9669     |\n","|    total_timesteps | 3318312  |\n","| train/             |          |\n","|    actor_loss      | 0.111    |\n","|    critic_loss     | 0.168    |\n","|    ent_coef        | 0.00232  |\n","|    ent_coef_loss   | 0.0152   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 138258   |\n","---------------------------------\n","Eval num_timesteps=3319200, episode_reward=-75.04 +/- 1.95\n","Episode length: 282.20 +/- 23.52\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 282      |\n","|    mean_reward     | -75      |\n","| time/              |          |\n","|    total_timesteps | 3319200  |\n","| train/             |          |\n","|    actor_loss      | 0.09     |\n","|    critic_loss     | 0.0984   |\n","|    ent_coef        | 0.0023   |\n","|    ent_coef_loss   | -3.46    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 138295   |\n","---------------------------------\n","Eval num_timesteps=3322800, episode_reward=1.65 +/- 0.27\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.65     |\n","| time/              |          |\n","|    total_timesteps | 3322800  |\n","| train/             |          |\n","|    actor_loss      | 0.313    |\n","|    critic_loss     | 0.12     |\n","|    ent_coef        | 0.00226  |\n","|    ent_coef_loss   | 2.03     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 138445   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4430     |\n","|    fps             | 343      |\n","|    time_elapsed    | 9689     |\n","|    total_timesteps | 3325728  |\n","| train/             |          |\n","|    actor_loss      | 0.133    |\n","|    critic_loss     | 0.124    |\n","|    ent_coef        | 0.00226  |\n","|    ent_coef_loss   | -2.55    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 138567   |\n","---------------------------------\n","Eval num_timesteps=3326400, episode_reward=-71.59 +/- 5.33\n","Episode length: 127.00 +/- 19.60\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 127      |\n","|    mean_reward     | -71.6    |\n","| time/              |          |\n","|    total_timesteps | 3326400  |\n","| train/             |          |\n","|    actor_loss      | 0.185    |\n","|    critic_loss     | 12.1     |\n","|    ent_coef        | 0.00224  |\n","|    ent_coef_loss   | -2.19    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 138595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4440     |\n","|    fps             | 343      |\n","|    time_elapsed    | 9694     |\n","|    total_timesteps | 3328944  |\n","| train/             |          |\n","|    actor_loss      | 0.242    |\n","|    critic_loss     | 0.19     |\n","|    ent_coef        | 0.0022   |\n","|    ent_coef_loss   | -1.09    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 138701   |\n","---------------------------------\n","Eval num_timesteps=3330000, episode_reward=2.30 +/- 0.48\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.3      |\n","| time/              |          |\n","|    total_timesteps | 3330000  |\n","| train/             |          |\n","|    actor_loss      | 0.066    |\n","|    critic_loss     | 7.83     |\n","|    ent_coef        | 0.00222  |\n","|    ent_coef_loss   | 3.11     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 138745   |\n","---------------------------------\n","Eval num_timesteps=3333600, episode_reward=-75.99 +/- 2.83\n","Episode length: 110.40 +/- 47.03\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 110      |\n","|    mean_reward     | -76      |\n","| time/              |          |\n","|    total_timesteps | 3333600  |\n","| train/             |          |\n","|    actor_loss      | 0.175    |\n","|    critic_loss     | 0.137    |\n","|    ent_coef        | 0.00222  |\n","|    ent_coef_loss   | 2.2      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 138895   |\n","---------------------------------\n","Eval num_timesteps=3337200, episode_reward=-44.15 +/- 37.31\n","Episode length: 475.40 +/- 20.09\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 475      |\n","|    mean_reward     | -44.2    |\n","| time/              |          |\n","|    total_timesteps | 3337200  |\n","| train/             |          |\n","|    actor_loss      | 0.294    |\n","|    critic_loss     | 0.388    |\n","|    ent_coef        | 0.00235  |\n","|    ent_coef_loss   | -1.49    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 139045   |\n","---------------------------------\n","Eval num_timesteps=3340800, episode_reward=4.16 +/- 1.01\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.16     |\n","| time/              |          |\n","|    total_timesteps | 3340800  |\n","| train/             |          |\n","|    actor_loss      | 0.257    |\n","|    critic_loss     | 24.4     |\n","|    ent_coef        | 0.0023   |\n","|    ent_coef_loss   | -2.19    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 139195   |\n","---------------------------------\n","Eval num_timesteps=3344400, episode_reward=2.79 +/- 0.50\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.79     |\n","| time/              |          |\n","|    total_timesteps | 3344400  |\n","| train/             |          |\n","|    actor_loss      | 0.063    |\n","|    critic_loss     | 0.296    |\n","|    ent_coef        | 0.00223  |\n","|    ent_coef_loss   | 0.355    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 139345   |\n","---------------------------------\n","Eval num_timesteps=3348000, episode_reward=-41.88 +/- 40.81\n","Episode length: 254.60 +/- 200.37\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 255      |\n","|    mean_reward     | -41.9    |\n","| time/              |          |\n","|    total_timesteps | 3348000  |\n","| train/             |          |\n","|    actor_loss      | 0.483    |\n","|    critic_loss     | 0.181    |\n","|    ent_coef        | 0.0023   |\n","|    ent_coef_loss   | -3.34    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 139495   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4450     |\n","|    fps             | 343      |\n","|    time_elapsed    | 9755     |\n","|    total_timesteps | 3350472  |\n","| train/             |          |\n","|    actor_loss      | 0.468    |\n","|    critic_loss     | 0.245    |\n","|    ent_coef        | 0.00226  |\n","|    ent_coef_loss   | -1.73    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 139598   |\n","---------------------------------\n","Eval num_timesteps=3351600, episode_reward=0.96 +/- 0.31\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.957    |\n","| time/              |          |\n","|    total_timesteps | 3351600  |\n","| train/             |          |\n","|    actor_loss      | 0.445    |\n","|    critic_loss     | 0.271    |\n","|    ent_coef        | 0.00225  |\n","|    ent_coef_loss   | -1.97    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 139645   |\n","---------------------------------\n","Eval num_timesteps=3355200, episode_reward=2.83 +/- 0.60\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.83     |\n","| time/              |          |\n","|    total_timesteps | 3355200  |\n","| train/             |          |\n","|    actor_loss      | 0.293    |\n","|    critic_loss     | 0.136    |\n","|    ent_coef        | 0.00209  |\n","|    ent_coef_loss   | 0.974    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 139795   |\n","---------------------------------\n","Eval num_timesteps=3358800, episode_reward=4.92 +/- 3.44\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.92     |\n","| time/              |          |\n","|    total_timesteps | 3358800  |\n","| train/             |          |\n","|    actor_loss      | 0.366    |\n","|    critic_loss     | 0.545    |\n","|    ent_coef        | 0.00207  |\n","|    ent_coef_loss   | -4.16    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 139945   |\n","---------------------------------\n","Eval num_timesteps=3362400, episode_reward=2.13 +/- 0.53\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.13     |\n","| time/              |          |\n","|    total_timesteps | 3362400  |\n","| train/             |          |\n","|    actor_loss      | 0.131    |\n","|    critic_loss     | 0.134    |\n","|    ent_coef        | 0.0022   |\n","|    ent_coef_loss   | -0.155   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 140095   |\n","---------------------------------\n","Eval num_timesteps=3366000, episode_reward=-42.53 +/- 36.77\n","Episode length: 286.40 +/- 174.40\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 286      |\n","|    mean_reward     | -42.5    |\n","| time/              |          |\n","|    total_timesteps | 3366000  |\n","| train/             |          |\n","|    actor_loss      | 0.0903   |\n","|    critic_loss     | 0.0811   |\n","|    ent_coef        | 0.00227  |\n","|    ent_coef_loss   | 0.819    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 140245   |\n","---------------------------------\n","Eval num_timesteps=3369600, episode_reward=-69.79 +/- 4.07\n","Episode length: 154.20 +/- 40.66\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 154      |\n","|    mean_reward     | -69.8    |\n","| time/              |          |\n","|    total_timesteps | 3369600  |\n","| train/             |          |\n","|    actor_loss      | 0.342    |\n","|    critic_loss     | 12.9     |\n","|    ent_coef        | 0.00219  |\n","|    ent_coef_loss   | -3.23    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 140395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4460     |\n","|    fps             | 343      |\n","|    time_elapsed    | 9811     |\n","|    total_timesteps | 3370488  |\n","| train/             |          |\n","|    actor_loss      | 0.12     |\n","|    critic_loss     | 0.0631   |\n","|    ent_coef        | 0.00215  |\n","|    ent_coef_loss   | -4.23    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 140432   |\n","---------------------------------\n","Eval num_timesteps=3373200, episode_reward=2.59 +/- 0.11\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.59     |\n","| time/              |          |\n","|    total_timesteps | 3373200  |\n","| train/             |          |\n","|    actor_loss      | 0.334    |\n","|    critic_loss     | 0.23     |\n","|    ent_coef        | 0.00213  |\n","|    ent_coef_loss   | 7.2      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 140545   |\n","---------------------------------\n","Eval num_timesteps=3376800, episode_reward=-31.05 +/- 39.14\n","Episode length: 372.80 +/- 155.79\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 373      |\n","|    mean_reward     | -31.1    |\n","| time/              |          |\n","|    total_timesteps | 3376800  |\n","| train/             |          |\n","|    actor_loss      | 0.342    |\n","|    critic_loss     | 0.227    |\n","|    ent_coef        | 0.00226  |\n","|    ent_coef_loss   | -1.97    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 140695   |\n","---------------------------------\n","Eval num_timesteps=3380400, episode_reward=2.26 +/- 0.03\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.26     |\n","| time/              |          |\n","|    total_timesteps | 3380400  |\n","| train/             |          |\n","|    actor_loss      | 0.297    |\n","|    critic_loss     | 0.4      |\n","|    ent_coef        | 0.00231  |\n","|    ent_coef_loss   | 4        |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 140845   |\n","---------------------------------\n","Eval num_timesteps=3384000, episode_reward=2.93 +/- 0.05\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.93     |\n","| time/              |          |\n","|    total_timesteps | 3384000  |\n","| train/             |          |\n","|    actor_loss      | 0.223    |\n","|    critic_loss     | 0.246    |\n","|    ent_coef        | 0.00226  |\n","|    ent_coef_loss   | -2.22    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 140995   |\n","---------------------------------\n","Eval num_timesteps=3387600, episode_reward=2.65 +/- 0.05\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.65     |\n","| time/              |          |\n","|    total_timesteps | 3387600  |\n","| train/             |          |\n","|    actor_loss      | 0.235    |\n","|    critic_loss     | 0.257    |\n","|    ent_coef        | 0.00223  |\n","|    ent_coef_loss   | -0.675   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 141145   |\n","---------------------------------\n","Eval num_timesteps=3391200, episode_reward=3.28 +/- 0.42\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.28     |\n","| time/              |          |\n","|    total_timesteps | 3391200  |\n","| train/             |          |\n","|    actor_loss      | 0.391    |\n","|    critic_loss     | 0.28     |\n","|    ent_coef        | 0.0022   |\n","|    ent_coef_loss   | 5.32     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 141295   |\n","---------------------------------\n","Eval num_timesteps=3394800, episode_reward=3.30 +/- 0.24\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.3      |\n","| time/              |          |\n","|    total_timesteps | 3394800  |\n","| train/             |          |\n","|    actor_loss      | 0.365    |\n","|    critic_loss     | 11.2     |\n","|    ent_coef        | 0.00225  |\n","|    ent_coef_loss   | -3.53    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 141445   |\n","---------------------------------\n","Eval num_timesteps=3398400, episode_reward=4.19 +/- 1.59\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.19     |\n","| time/              |          |\n","|    total_timesteps | 3398400  |\n","| train/             |          |\n","|    actor_loss      | 0.457    |\n","|    critic_loss     | 0.178    |\n","|    ent_coef        | 0.00217  |\n","|    ent_coef_loss   | 8.06     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 141595   |\n","---------------------------------\n","Eval num_timesteps=3402000, episode_reward=2.28 +/- 0.23\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.28     |\n","| time/              |          |\n","|    total_timesteps | 3402000  |\n","| train/             |          |\n","|    actor_loss      | 0.22     |\n","|    critic_loss     | 0.112    |\n","|    ent_coef        | 0.00231  |\n","|    ent_coef_loss   | -2.82    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 141745   |\n","---------------------------------\n","Eval num_timesteps=3405600, episode_reward=-24.88 +/- 39.46\n","Episode length: 390.00 +/- 134.72\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 390      |\n","|    mean_reward     | -24.9    |\n","| time/              |          |\n","|    total_timesteps | 3405600  |\n","| train/             |          |\n","|    actor_loss      | 0.151    |\n","|    critic_loss     | 0.116    |\n","|    ent_coef        | 0.00231  |\n","|    ent_coef_loss   | 1.44     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 141895   |\n","---------------------------------\n","Eval num_timesteps=3409200, episode_reward=3.08 +/- 0.22\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.08     |\n","| time/              |          |\n","|    total_timesteps | 3409200  |\n","| train/             |          |\n","|    actor_loss      | 0.162    |\n","|    critic_loss     | 0.0559   |\n","|    ent_coef        | 0.00232  |\n","|    ent_coef_loss   | -4.24    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 142045   |\n","---------------------------------\n","Eval num_timesteps=3412800, episode_reward=3.16 +/- 0.59\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.16     |\n","| time/              |          |\n","|    total_timesteps | 3412800  |\n","| train/             |          |\n","|    actor_loss      | 0.141    |\n","|    critic_loss     | 0.117    |\n","|    ent_coef        | 0.00222  |\n","|    ent_coef_loss   | -1.01    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 142195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4470     |\n","|    fps             | 343      |\n","|    time_elapsed    | 9941     |\n","|    total_timesteps | 3415872  |\n","| train/             |          |\n","|    actor_loss      | 0.696    |\n","|    critic_loss     | 0.282    |\n","|    ent_coef        | 0.00219  |\n","|    ent_coef_loss   | 6.37     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 142323   |\n","---------------------------------\n","Eval num_timesteps=3416400, episode_reward=-26.32 +/- 38.49\n","Episode length: 387.20 +/- 138.15\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 387      |\n","|    mean_reward     | -26.3    |\n","| time/              |          |\n","|    total_timesteps | 3416400  |\n","| train/             |          |\n","|    actor_loss      | 0.439    |\n","|    critic_loss     | 0.938    |\n","|    ent_coef        | 0.00222  |\n","|    ent_coef_loss   | 8.44     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 142345   |\n","---------------------------------\n","Eval num_timesteps=3420000, episode_reward=-28.43 +/- 39.50\n","Episode length: 367.60 +/- 162.16\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 368      |\n","|    mean_reward     | -28.4    |\n","| time/              |          |\n","|    total_timesteps | 3420000  |\n","| train/             |          |\n","|    actor_loss      | 0.333    |\n","|    critic_loss     | 5        |\n","|    ent_coef        | 0.00237  |\n","|    ent_coef_loss   | -1.6     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 142495   |\n","---------------------------------\n","Eval num_timesteps=3423600, episode_reward=-45.47 +/- 38.98\n","Episode length: 403.40 +/- 78.87\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 403      |\n","|    mean_reward     | -45.5    |\n","| time/              |          |\n","|    total_timesteps | 3423600  |\n","| train/             |          |\n","|    actor_loss      | 0.174    |\n","|    critic_loss     | 0.802    |\n","|    ent_coef        | 0.00243  |\n","|    ent_coef_loss   | -0.69    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 142645   |\n","---------------------------------\n","Eval num_timesteps=3427200, episode_reward=-76.03 +/- 1.46\n","Episode length: 130.80 +/- 21.07\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 131      |\n","|    mean_reward     | -76      |\n","| time/              |          |\n","|    total_timesteps | 3427200  |\n","| train/             |          |\n","|    actor_loss      | 0.181    |\n","|    critic_loss     | 0.195    |\n","|    ent_coef        | 0.00235  |\n","|    ent_coef_loss   | -2.83    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 142795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4480     |\n","|    fps             | 343      |\n","|    time_elapsed    | 9981     |\n","|    total_timesteps | 3429984  |\n","| train/             |          |\n","|    actor_loss      | 0.463    |\n","|    critic_loss     | 0.191    |\n","|    ent_coef        | 0.00224  |\n","|    ent_coef_loss   | 2.77     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 142911   |\n","---------------------------------\n","Eval num_timesteps=3430800, episode_reward=-39.36 +/- 34.19\n","Episode length: 373.40 +/- 103.37\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 373      |\n","|    mean_reward     | -39.4    |\n","| time/              |          |\n","|    total_timesteps | 3430800  |\n","| train/             |          |\n","|    actor_loss      | 0.243    |\n","|    critic_loss     | 0.287    |\n","|    ent_coef        | 0.00227  |\n","|    ent_coef_loss   | 1.17     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 142945   |\n","---------------------------------\n","Eval num_timesteps=3434400, episode_reward=3.09 +/- 0.57\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.09     |\n","| time/              |          |\n","|    total_timesteps | 3434400  |\n","| train/             |          |\n","|    actor_loss      | 0.132    |\n","|    critic_loss     | 0.0943   |\n","|    ent_coef        | 0.00237  |\n","|    ent_coef_loss   | 2.23     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 143095   |\n","---------------------------------\n","Eval num_timesteps=3438000, episode_reward=1.82 +/- 0.83\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.82     |\n","| time/              |          |\n","|    total_timesteps | 3438000  |\n","| train/             |          |\n","|    actor_loss      | 0.6      |\n","|    critic_loss     | 0.329    |\n","|    ent_coef        | 0.00244  |\n","|    ent_coef_loss   | 0.755    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 143245   |\n","---------------------------------\n","Eval num_timesteps=3441600, episode_reward=2.57 +/- 0.76\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.57     |\n","| time/              |          |\n","|    total_timesteps | 3441600  |\n","| train/             |          |\n","|    actor_loss      | 0.464    |\n","|    critic_loss     | 0.676    |\n","|    ent_coef        | 0.00227  |\n","|    ent_coef_loss   | -1.95    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 143395   |\n","---------------------------------\n","Eval num_timesteps=3445200, episode_reward=2.69 +/- 0.19\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.69     |\n","| time/              |          |\n","|    total_timesteps | 3445200  |\n","| train/             |          |\n","|    actor_loss      | 0.305    |\n","|    critic_loss     | 0.14     |\n","|    ent_coef        | 0.00226  |\n","|    ent_coef_loss   | -0.45    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 143545   |\n","---------------------------------\n","Eval num_timesteps=3448800, episode_reward=2.22 +/- 0.20\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.22     |\n","| time/              |          |\n","|    total_timesteps | 3448800  |\n","| train/             |          |\n","|    actor_loss      | 0.574    |\n","|    critic_loss     | 0.216    |\n","|    ent_coef        | 0.00229  |\n","|    ent_coef_loss   | 2.05     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 143695   |\n","---------------------------------\n","Eval num_timesteps=3452400, episode_reward=-46.84 +/- 40.56\n","Episode length: 404.00 +/- 78.38\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 404      |\n","|    mean_reward     | -46.8    |\n","| time/              |          |\n","|    total_timesteps | 3452400  |\n","| train/             |          |\n","|    actor_loss      | 0.288    |\n","|    critic_loss     | 0.111    |\n","|    ent_coef        | 0.00226  |\n","|    ent_coef_loss   | -1.93    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 143845   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4490     |\n","|    fps             | 343      |\n","|    time_elapsed    | 10058    |\n","|    total_timesteps | 3455952  |\n","| train/             |          |\n","|    actor_loss      | 0.186    |\n","|    critic_loss     | 11.4     |\n","|    ent_coef        | 0.0022   |\n","|    ent_coef_loss   | -0.321   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 143993   |\n","---------------------------------\n","Eval num_timesteps=3456000, episode_reward=0.96 +/- 1.18\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.962    |\n","| time/              |          |\n","|    total_timesteps | 3456000  |\n","| train/             |          |\n","|    actor_loss      | 0.467    |\n","|    critic_loss     | 0.515    |\n","|    ent_coef        | 0.0022   |\n","|    ent_coef_loss   | 1.23     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 143995   |\n","---------------------------------\n","Eval num_timesteps=3459600, episode_reward=-74.32 +/- 2.21\n","Episode length: 189.20 +/- 0.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 189      |\n","|    mean_reward     | -74.3    |\n","| time/              |          |\n","|    total_timesteps | 3459600  |\n","| train/             |          |\n","|    actor_loss      | 0.41     |\n","|    critic_loss     | 0.196    |\n","|    ent_coef        | 0.00223  |\n","|    ent_coef_loss   | -2.58    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 144145   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4500     |\n","|    fps             | 343      |\n","|    time_elapsed    | 10071    |\n","|    total_timesteps | 3461328  |\n","| train/             |          |\n","|    actor_loss      | 0.2      |\n","|    critic_loss     | 0.176    |\n","|    ent_coef        | 0.00213  |\n","|    ent_coef_loss   | -4.2     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 144217   |\n","---------------------------------\n","Eval num_timesteps=3463200, episode_reward=-46.10 +/- 39.47\n","Episode length: 359.00 +/- 115.13\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 359      |\n","|    mean_reward     | -46.1    |\n","| time/              |          |\n","|    total_timesteps | 3463200  |\n","| train/             |          |\n","|    actor_loss      | 0.29     |\n","|    critic_loss     | 0.0965   |\n","|    ent_coef        | 0.00205  |\n","|    ent_coef_loss   | 8.19     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 144295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4510     |\n","|    fps             | 343      |\n","|    time_elapsed    | 10085    |\n","|    total_timesteps | 3465096  |\n","| train/             |          |\n","|    actor_loss      | 0.484    |\n","|    critic_loss     | 0.235    |\n","|    ent_coef        | 0.00214  |\n","|    ent_coef_loss   | 3.46     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 144374   |\n","---------------------------------\n","Eval num_timesteps=3466800, episode_reward=2.35 +/- 0.63\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.35     |\n","| time/              |          |\n","|    total_timesteps | 3466800  |\n","| train/             |          |\n","|    actor_loss      | 0.604    |\n","|    critic_loss     | 0.215    |\n","|    ent_coef        | 0.00214  |\n","|    ent_coef_loss   | 2.57     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 144445   |\n","---------------------------------\n","Eval num_timesteps=3470400, episode_reward=7.13 +/- 2.50\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.13     |\n","| time/              |          |\n","|    total_timesteps | 3470400  |\n","| train/             |          |\n","|    actor_loss      | 0.465    |\n","|    critic_loss     | 0.231    |\n","|    ent_coef        | 0.00211  |\n","|    ent_coef_loss   | -0.0332  |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 144595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4520     |\n","|    fps             | 343      |\n","|    time_elapsed    | 10107    |\n","|    total_timesteps | 3472080  |\n","| train/             |          |\n","|    actor_loss      | 0.37     |\n","|    critic_loss     | 0.226    |\n","|    ent_coef        | 0.00211  |\n","|    ent_coef_loss   | 3.51     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 144665   |\n","---------------------------------\n","Eval num_timesteps=3474000, episode_reward=6.14 +/- 4.74\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.14     |\n","| time/              |          |\n","|    total_timesteps | 3474000  |\n","| train/             |          |\n","|    actor_loss      | 0.572    |\n","|    critic_loss     | 3.97     |\n","|    ent_coef        | 0.00223  |\n","|    ent_coef_loss   | 5.62     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 144745   |\n","---------------------------------\n","Eval num_timesteps=3477600, episode_reward=1.19 +/- 0.71\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.19     |\n","| time/              |          |\n","|    total_timesteps | 3477600  |\n","| train/             |          |\n","|    actor_loss      | 0.197    |\n","|    critic_loss     | 0.115    |\n","|    ent_coef        | 0.0023   |\n","|    ent_coef_loss   | -2.77    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 144895   |\n","---------------------------------\n","Eval num_timesteps=3481200, episode_reward=3.40 +/- 0.87\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.4      |\n","| time/              |          |\n","|    total_timesteps | 3481200  |\n","| train/             |          |\n","|    actor_loss      | 0.494    |\n","|    critic_loss     | 0.185    |\n","|    ent_coef        | 0.00218  |\n","|    ent_coef_loss   | 1.9      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 145045   |\n","---------------------------------\n","Eval num_timesteps=3484800, episode_reward=-27.71 +/- 39.78\n","Episode length: 377.20 +/- 150.40\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 377      |\n","|    mean_reward     | -27.7    |\n","| time/              |          |\n","|    total_timesteps | 3484800  |\n","| train/             |          |\n","|    actor_loss      | 0.238    |\n","|    critic_loss     | 0.124    |\n","|    ent_coef        | 0.00223  |\n","|    ent_coef_loss   | 1.63     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 145195   |\n","---------------------------------\n","Eval num_timesteps=3488400, episode_reward=4.91 +/- 1.58\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.91     |\n","| time/              |          |\n","|    total_timesteps | 3488400  |\n","| train/             |          |\n","|    actor_loss      | 0.429    |\n","|    critic_loss     | 0.114    |\n","|    ent_coef        | 0.0023   |\n","|    ent_coef_loss   | -0.811   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 145345   |\n","---------------------------------\n","Eval num_timesteps=3492000, episode_reward=-29.52 +/- 39.28\n","Episode length: 408.40 +/- 112.19\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 408      |\n","|    mean_reward     | -29.5    |\n","| time/              |          |\n","|    total_timesteps | 3492000  |\n","| train/             |          |\n","|    actor_loss      | 0.296    |\n","|    critic_loss     | 0.168    |\n","|    ent_coef        | 0.00224  |\n","|    ent_coef_loss   | 6.77     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 145495   |\n","---------------------------------\n","Eval num_timesteps=3495600, episode_reward=3.56 +/- 0.72\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.56     |\n","| time/              |          |\n","|    total_timesteps | 3495600  |\n","| train/             |          |\n","|    actor_loss      | 0.483    |\n","|    critic_loss     | 0.318    |\n","|    ent_coef        | 0.0023   |\n","|    ent_coef_loss   | 4.24     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 145645   |\n","---------------------------------\n","Eval num_timesteps=3499200, episode_reward=2.56 +/- 0.21\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.56     |\n","| time/              |          |\n","|    total_timesteps | 3499200  |\n","| train/             |          |\n","|    actor_loss      | 0.558    |\n","|    critic_loss     | 0.211    |\n","|    ent_coef        | 0.00241  |\n","|    ent_coef_loss   | -0.687   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 145795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4530     |\n","|    fps             | 343      |\n","|    time_elapsed    | 10195    |\n","|    total_timesteps | 3502776  |\n","| train/             |          |\n","|    actor_loss      | 0.236    |\n","|    critic_loss     | 0.0988   |\n","|    ent_coef        | 0.00221  |\n","|    ent_coef_loss   | -4.23    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 145944   |\n","---------------------------------\n","Eval num_timesteps=3502800, episode_reward=2.00 +/- 0.63\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2        |\n","| time/              |          |\n","|    total_timesteps | 3502800  |\n","| train/             |          |\n","|    actor_loss      | 0.304    |\n","|    critic_loss     | 0.118    |\n","|    ent_coef        | 0.00221  |\n","|    ent_coef_loss   | -4.09    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 145945   |\n","---------------------------------\n","Eval num_timesteps=3506400, episode_reward=2.61 +/- 0.20\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.61     |\n","| time/              |          |\n","|    total_timesteps | 3506400  |\n","| train/             |          |\n","|    actor_loss      | 0.305    |\n","|    critic_loss     | 0.122    |\n","|    ent_coef        | 0.00205  |\n","|    ent_coef_loss   | 1.51     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 146095   |\n","---------------------------------\n","Eval num_timesteps=3510000, episode_reward=2.87 +/- 1.18\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.87     |\n","| time/              |          |\n","|    total_timesteps | 3510000  |\n","| train/             |          |\n","|    actor_loss      | 0.279    |\n","|    critic_loss     | 9.33     |\n","|    ent_coef        | 0.00198  |\n","|    ent_coef_loss   | 1.03     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 146245   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4540     |\n","|    fps             | 343      |\n","|    time_elapsed    | 10224    |\n","|    total_timesteps | 3511848  |\n","| train/             |          |\n","|    actor_loss      | 0.569    |\n","|    critic_loss     | 0.181    |\n","|    ent_coef        | 0.00195  |\n","|    ent_coef_loss   | 3.01     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 146322   |\n","---------------------------------\n","Eval num_timesteps=3513600, episode_reward=2.11 +/- 0.69\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.11     |\n","| time/              |          |\n","|    total_timesteps | 3513600  |\n","| train/             |          |\n","|    actor_loss      | 0.297    |\n","|    critic_loss     | 0.146    |\n","|    ent_coef        | 0.00199  |\n","|    ent_coef_loss   | 4.5      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 146395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4550     |\n","|    fps             | 343      |\n","|    time_elapsed    | 10239    |\n","|    total_timesteps | 3517152  |\n","| train/             |          |\n","|    actor_loss      | 0.194    |\n","|    critic_loss     | 9.24     |\n","|    ent_coef        | 0.00217  |\n","|    ent_coef_loss   | 8.88     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 146543   |\n","---------------------------------\n","Eval num_timesteps=3517200, episode_reward=-22.84 +/- 37.07\n","Episode length: 418.80 +/- 99.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 419      |\n","|    mean_reward     | -22.8    |\n","| time/              |          |\n","|    total_timesteps | 3517200  |\n","| train/             |          |\n","|    actor_loss      | 0.321    |\n","|    critic_loss     | 0.182    |\n","|    ent_coef        | 0.00218  |\n","|    ent_coef_loss   | 7.09     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 146545   |\n","---------------------------------\n","Eval num_timesteps=3520800, episode_reward=5.88 +/- 3.95\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.88     |\n","| time/              |          |\n","|    total_timesteps | 3520800  |\n","| train/             |          |\n","|    actor_loss      | 0.63     |\n","|    critic_loss     | 0.185    |\n","|    ent_coef        | 0.00236  |\n","|    ent_coef_loss   | 4.49     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 146695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4560     |\n","|    fps             | 343      |\n","|    time_elapsed    | 10259    |\n","|    total_timesteps | 3523248  |\n","| train/             |          |\n","|    actor_loss      | 0.218    |\n","|    critic_loss     | 0.124    |\n","|    ent_coef        | 0.00238  |\n","|    ent_coef_loss   | 0.633    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 146797   |\n","---------------------------------\n","Eval num_timesteps=3524400, episode_reward=3.07 +/- 0.08\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.07     |\n","| time/              |          |\n","|    total_timesteps | 3524400  |\n","| train/             |          |\n","|    actor_loss      | 0.455    |\n","|    critic_loss     | 0.566    |\n","|    ent_coef        | 0.00236  |\n","|    ent_coef_loss   | -2.39    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 146845   |\n","---------------------------------\n","Eval num_timesteps=3528000, episode_reward=-44.06 +/- 40.12\n","Episode length: 323.60 +/- 144.03\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 324      |\n","|    mean_reward     | -44.1    |\n","| time/              |          |\n","|    total_timesteps | 3528000  |\n","| train/             |          |\n","|    actor_loss      | 0.452    |\n","|    critic_loss     | 11       |\n","|    ent_coef        | 0.00237  |\n","|    ent_coef_loss   | -0.448   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 146995   |\n","---------------------------------\n","Eval num_timesteps=3531600, episode_reward=4.16 +/- 0.79\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.16     |\n","| time/              |          |\n","|    total_timesteps | 3531600  |\n","| train/             |          |\n","|    actor_loss      | 0.621    |\n","|    critic_loss     | 0.291    |\n","|    ent_coef        | 0.00253  |\n","|    ent_coef_loss   | 1.8      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 147145   |\n","---------------------------------\n","Eval num_timesteps=3535200, episode_reward=3.63 +/- 0.94\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.63     |\n","| time/              |          |\n","|    total_timesteps | 3535200  |\n","| train/             |          |\n","|    actor_loss      | 0.375    |\n","|    critic_loss     | 0.638    |\n","|    ent_coef        | 0.00261  |\n","|    ent_coef_loss   | -1.23    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 147295   |\n","---------------------------------\n","Eval num_timesteps=3538800, episode_reward=2.42 +/- 0.53\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.42     |\n","| time/              |          |\n","|    total_timesteps | 3538800  |\n","| train/             |          |\n","|    actor_loss      | 0.393    |\n","|    critic_loss     | 0.579    |\n","|    ent_coef        | 0.00261  |\n","|    ent_coef_loss   | -3.18    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 147445   |\n","---------------------------------\n","Eval num_timesteps=3542400, episode_reward=3.91 +/- 1.88\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.91     |\n","| time/              |          |\n","|    total_timesteps | 3542400  |\n","| train/             |          |\n","|    actor_loss      | 0.431    |\n","|    critic_loss     | 0.547    |\n","|    ent_coef        | 0.00264  |\n","|    ent_coef_loss   | 4.31     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 147595   |\n","---------------------------------\n","Eval num_timesteps=3546000, episode_reward=-29.76 +/- 41.14\n","Episode length: 391.60 +/- 132.76\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 392      |\n","|    mean_reward     | -29.8    |\n","| time/              |          |\n","|    total_timesteps | 3546000  |\n","| train/             |          |\n","|    actor_loss      | 0.71     |\n","|    critic_loss     | 13.3     |\n","|    ent_coef        | 0.00268  |\n","|    ent_coef_loss   | -1.06    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 147745   |\n","---------------------------------\n","Eval num_timesteps=3549600, episode_reward=5.31 +/- 0.88\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.31     |\n","| time/              |          |\n","|    total_timesteps | 3549600  |\n","| train/             |          |\n","|    actor_loss      | 0.407    |\n","|    critic_loss     | 0.237    |\n","|    ent_coef        | 0.00254  |\n","|    ent_coef_loss   | -3.12    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 147895   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4570     |\n","|    fps             | 343      |\n","|    time_elapsed    | 10348    |\n","|    total_timesteps | 3552264  |\n","| train/             |          |\n","|    actor_loss      | 0.483    |\n","|    critic_loss     | 0.207    |\n","|    ent_coef        | 0.00246  |\n","|    ent_coef_loss   | 1.73     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 148006   |\n","---------------------------------\n","Eval num_timesteps=3553200, episode_reward=-26.84 +/- 38.35\n","Episode length: 467.60 +/- 39.68\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 468      |\n","|    mean_reward     | -26.8    |\n","| time/              |          |\n","|    total_timesteps | 3553200  |\n","| train/             |          |\n","|    actor_loss      | 0.327    |\n","|    critic_loss     | 0.21     |\n","|    ent_coef        | 0.00248  |\n","|    ent_coef_loss   | -1.64    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 148045   |\n","---------------------------------\n","Eval num_timesteps=3556800, episode_reward=1.39 +/- 0.18\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.39     |\n","| time/              |          |\n","|    total_timesteps | 3556800  |\n","| train/             |          |\n","|    actor_loss      | 0.377    |\n","|    critic_loss     | 0.279    |\n","|    ent_coef        | 0.00247  |\n","|    ent_coef_loss   | -0.967   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 148195   |\n","---------------------------------\n","Eval num_timesteps=3560400, episode_reward=-74.59 +/- 3.60\n","Episode length: 141.60 +/- 24.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 142      |\n","|    mean_reward     | -74.6    |\n","| time/              |          |\n","|    total_timesteps | 3560400  |\n","| train/             |          |\n","|    actor_loss      | 0.452    |\n","|    critic_loss     | 6.27     |\n","|    ent_coef        | 0.00249  |\n","|    ent_coef_loss   | -1.19    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 148345   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4580     |\n","|    fps             | 343      |\n","|    time_elapsed    | 10375    |\n","|    total_timesteps | 3562296  |\n","| train/             |          |\n","|    actor_loss      | 0.779    |\n","|    critic_loss     | 0.612    |\n","|    ent_coef        | 0.00246  |\n","|    ent_coef_loss   | 0.617    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 148424   |\n","---------------------------------\n","Eval num_timesteps=3564000, episode_reward=4.30 +/- 0.25\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.3      |\n","| time/              |          |\n","|    total_timesteps | 3564000  |\n","| train/             |          |\n","|    actor_loss      | 0.534    |\n","|    critic_loss     | 0.492    |\n","|    ent_coef        | 0.00241  |\n","|    ent_coef_loss   | -1.22    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 148495   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4590     |\n","|    fps             | 343      |\n","|    time_elapsed    | 10387    |\n","|    total_timesteps | 3566880  |\n","| train/             |          |\n","|    actor_loss      | 0.278    |\n","|    critic_loss     | 0.13     |\n","|    ent_coef        | 0.00231  |\n","|    ent_coef_loss   | -0.967   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 148615   |\n","---------------------------------\n","Eval num_timesteps=3567600, episode_reward=-75.78 +/- 2.51\n","Episode length: 430.40 +/- 31.35\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 430      |\n","|    mean_reward     | -75.8    |\n","| time/              |          |\n","|    total_timesteps | 3567600  |\n","| train/             |          |\n","|    actor_loss      | 0.379    |\n","|    critic_loss     | 0.182    |\n","|    ent_coef        | 0.00228  |\n","|    ent_coef_loss   | -2.89    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 148645   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4600     |\n","|    fps             | 343      |\n","|    time_elapsed    | 10395    |\n","|    total_timesteps | 3568656  |\n","| train/             |          |\n","|    actor_loss      | 0.66     |\n","|    critic_loss     | 0.316    |\n","|    ent_coef        | 0.00223  |\n","|    ent_coef_loss   | 1.47     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 148689   |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    episodes        | 4610    |\n","|    fps             | 343     |\n","|    time_elapsed    | 10395   |\n","|    total_timesteps | 3568656 |\n","--------------------------------\n","Eval num_timesteps=3571200, episode_reward=-69.20 +/- 2.06\n","Episode length: 179.20 +/- 8.33\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 179      |\n","|    mean_reward     | -69.2    |\n","| time/              |          |\n","|    total_timesteps | 3571200  |\n","| train/             |          |\n","|    actor_loss      | 0.342    |\n","|    critic_loss     | 0.121    |\n","|    ent_coef        | 0.00223  |\n","|    ent_coef_loss   | -2.42    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 148795   |\n","---------------------------------\n","Eval num_timesteps=3574800, episode_reward=-68.77 +/- 8.90\n","Episode length: 197.20 +/- 94.55\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 197      |\n","|    mean_reward     | -68.8    |\n","| time/              |          |\n","|    total_timesteps | 3574800  |\n","| train/             |          |\n","|    actor_loss      | 0.563    |\n","|    critic_loss     | 0.24     |\n","|    ent_coef        | 0.00224  |\n","|    ent_coef_loss   | 0.182    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 148945   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4620     |\n","|    fps             | 343      |\n","|    time_elapsed    | 10411    |\n","|    total_timesteps | 3576264  |\n","| train/             |          |\n","|    actor_loss      | 0.187    |\n","|    critic_loss     | 0.114    |\n","|    ent_coef        | 0.00235  |\n","|    ent_coef_loss   | 1.72     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 149006   |\n","---------------------------------\n","Eval num_timesteps=3578400, episode_reward=-65.75 +/- 7.63\n","Episode length: 320.40 +/- 50.95\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 320      |\n","|    mean_reward     | -65.7    |\n","| time/              |          |\n","|    total_timesteps | 3578400  |\n","| train/             |          |\n","|    actor_loss      | 0.414    |\n","|    critic_loss     | 0.437    |\n","|    ent_coef        | 0.00242  |\n","|    ent_coef_loss   | 0.519    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 149095   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4630     |\n","|    fps             | 343      |\n","|    time_elapsed    | 10419    |\n","|    total_timesteps | 3580104  |\n","| train/             |          |\n","|    actor_loss      | 0.403    |\n","|    critic_loss     | 0.226    |\n","|    ent_coef        | 0.00243  |\n","|    ent_coef_loss   | 1.35     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 149166   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4640     |\n","|    fps             | 343      |\n","|    time_elapsed    | 10420    |\n","|    total_timesteps | 3581544  |\n","| train/             |          |\n","|    actor_loss      | 0.261    |\n","|    critic_loss     | 0.286    |\n","|    ent_coef        | 0.00243  |\n","|    ent_coef_loss   | -1.41    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 149226   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4650     |\n","|    fps             | 343      |\n","|    time_elapsed    | 10421    |\n","|    total_timesteps | 3581712  |\n","| train/             |          |\n","|    actor_loss      | 0.355    |\n","|    critic_loss     | 0.102    |\n","|    ent_coef        | 0.00242  |\n","|    ent_coef_loss   | -0.954   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 149233   |\n","---------------------------------\n","Eval num_timesteps=3582000, episode_reward=-76.25 +/- 1.39\n","Episode length: 323.00 +/- 31.84\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 323      |\n","|    mean_reward     | -76.3    |\n","| time/              |          |\n","|    total_timesteps | 3582000  |\n","| train/             |          |\n","|    actor_loss      | 0.364    |\n","|    critic_loss     | 0.103    |\n","|    ent_coef        | 0.00242  |\n","|    ent_coef_loss   | -0.371   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 149245   |\n","---------------------------------\n","Eval num_timesteps=3585600, episode_reward=2.53 +/- 0.29\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.53     |\n","| time/              |          |\n","|    total_timesteps | 3585600  |\n","| train/             |          |\n","|    actor_loss      | 0.251    |\n","|    critic_loss     | 0.416    |\n","|    ent_coef        | 0.00259  |\n","|    ent_coef_loss   | 6.53     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 149395   |\n","---------------------------------\n","Eval num_timesteps=3589200, episode_reward=2.94 +/- 0.57\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.94     |\n","| time/              |          |\n","|    total_timesteps | 3589200  |\n","| train/             |          |\n","|    actor_loss      | 0.41     |\n","|    critic_loss     | 0.457    |\n","|    ent_coef        | 0.00275  |\n","|    ent_coef_loss   | 0.1      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 149545   |\n","---------------------------------\n","Eval num_timesteps=3592800, episode_reward=4.20 +/- 1.76\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.2      |\n","| time/              |          |\n","|    total_timesteps | 3592800  |\n","| train/             |          |\n","|    actor_loss      | 0.366    |\n","|    critic_loss     | 0.204    |\n","|    ent_coef        | 0.00279  |\n","|    ent_coef_loss   | -1.12    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 149695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4660     |\n","|    fps             | 343      |\n","|    time_elapsed    | 10462    |\n","|    total_timesteps | 3596304  |\n","| train/             |          |\n","|    actor_loss      | 0.484    |\n","|    critic_loss     | 0.131    |\n","|    ent_coef        | 0.00275  |\n","|    ent_coef_loss   | 0.597    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 149841   |\n","---------------------------------\n","Eval num_timesteps=3596400, episode_reward=4.82 +/- 0.28\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.82     |\n","| time/              |          |\n","|    total_timesteps | 3596400  |\n","| train/             |          |\n","|    actor_loss      | 0.468    |\n","|    critic_loss     | 13.2     |\n","|    ent_coef        | 0.00275  |\n","|    ent_coef_loss   | 0.991    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 149845   |\n","---------------------------------\n","Eval num_timesteps=3600000, episode_reward=6.06 +/- 3.14\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.06     |\n","| time/              |          |\n","|    total_timesteps | 3600000  |\n","| train/             |          |\n","|    actor_loss      | 0.581    |\n","|    critic_loss     | 10.3     |\n","|    ent_coef        | 0.00262  |\n","|    ent_coef_loss   | -1.44    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 149995   |\n","---------------------------------\n","Eval num_timesteps=3603600, episode_reward=3.72 +/- 0.77\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.72     |\n","| time/              |          |\n","|    total_timesteps | 3603600  |\n","| train/             |          |\n","|    actor_loss      | 0.571    |\n","|    critic_loss     | 0.181    |\n","|    ent_coef        | 0.00263  |\n","|    ent_coef_loss   | 0.353    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 150145   |\n","---------------------------------\n","Eval num_timesteps=3607200, episode_reward=-40.52 +/- 34.62\n","Episode length: 360.20 +/- 114.15\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 360      |\n","|    mean_reward     | -40.5    |\n","| time/              |          |\n","|    total_timesteps | 3607200  |\n","| train/             |          |\n","|    actor_loss      | 0.398    |\n","|    critic_loss     | 0.19     |\n","|    ent_coef        | 0.00264  |\n","|    ent_coef_loss   | 5.5      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 150295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4670     |\n","|    fps             | 343      |\n","|    time_elapsed    | 10503    |\n","|    total_timesteps | 3608088  |\n","| train/             |          |\n","|    actor_loss      | 0.464    |\n","|    critic_loss     | 12       |\n","|    ent_coef        | 0.00273  |\n","|    ent_coef_loss   | -0.0689  |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 150332   |\n","---------------------------------\n","Eval num_timesteps=3610800, episode_reward=6.11 +/- 0.33\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.11     |\n","| time/              |          |\n","|    total_timesteps | 3610800  |\n","| train/             |          |\n","|    actor_loss      | 0.538    |\n","|    critic_loss     | 0.2      |\n","|    ent_coef        | 0.00272  |\n","|    ent_coef_loss   | 0.481    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 150445   |\n","---------------------------------\n","Eval num_timesteps=3614400, episode_reward=-42.92 +/- 39.98\n","Episode length: 284.60 +/- 175.87\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 285      |\n","|    mean_reward     | -42.9    |\n","| time/              |          |\n","|    total_timesteps | 3614400  |\n","| train/             |          |\n","|    actor_loss      | 1.01     |\n","|    critic_loss     | 12.7     |\n","|    ent_coef        | 0.00279  |\n","|    ent_coef_loss   | 0.614    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 150595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4680     |\n","|    fps             | 343      |\n","|    time_elapsed    | 10526    |\n","|    total_timesteps | 3617448  |\n","| train/             |          |\n","|    actor_loss      | 0.65     |\n","|    critic_loss     | 0.263    |\n","|    ent_coef        | 0.00272  |\n","|    ent_coef_loss   | -2.3     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 150722   |\n","---------------------------------\n","Eval num_timesteps=3618000, episode_reward=1.57 +/- 0.25\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.57     |\n","| time/              |          |\n","|    total_timesteps | 3618000  |\n","| train/             |          |\n","|    actor_loss      | 0.679    |\n","|    critic_loss     | 0.612    |\n","|    ent_coef        | 0.00269  |\n","|    ent_coef_loss   | 0.805    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 150745   |\n","---------------------------------\n","Eval num_timesteps=3621600, episode_reward=-27.88 +/- 35.80\n","Episode length: 484.40 +/- 19.11\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 484      |\n","|    mean_reward     | -27.9    |\n","| time/              |          |\n","|    total_timesteps | 3621600  |\n","| train/             |          |\n","|    actor_loss      | 0.139    |\n","|    critic_loss     | 0.0811   |\n","|    ent_coef        | 0.00265  |\n","|    ent_coef_loss   | -1.76    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 150895   |\n","---------------------------------\n","Eval num_timesteps=3625200, episode_reward=-31.26 +/- 39.03\n","Episode length: 412.80 +/- 106.80\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 413      |\n","|    mean_reward     | -31.3    |\n","| time/              |          |\n","|    total_timesteps | 3625200  |\n","| train/             |          |\n","|    actor_loss      | 0.365    |\n","|    critic_loss     | 0.46     |\n","|    ent_coef        | 0.00242  |\n","|    ent_coef_loss   | -4.73    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 151045   |\n","---------------------------------\n","Eval num_timesteps=3628800, episode_reward=1.30 +/- 0.09\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.3      |\n","| time/              |          |\n","|    total_timesteps | 3628800  |\n","| train/             |          |\n","|    actor_loss      | 0.347    |\n","|    critic_loss     | 0.209    |\n","|    ent_coef        | 0.00227  |\n","|    ent_coef_loss   | -5.76    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 151195   |\n","---------------------------------\n","Eval num_timesteps=3632400, episode_reward=-21.72 +/- 39.23\n","Episode length: 446.00 +/- 66.14\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 446      |\n","|    mean_reward     | -21.7    |\n","| time/              |          |\n","|    total_timesteps | 3632400  |\n","| train/             |          |\n","|    actor_loss      | 0.697    |\n","|    critic_loss     | 0.681    |\n","|    ent_coef        | 0.00212  |\n","|    ent_coef_loss   | 1.66     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 151345   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4690     |\n","|    fps             | 343      |\n","|    time_elapsed    | 10579    |\n","|    total_timesteps | 3635928  |\n","| train/             |          |\n","|    actor_loss      | 0.45     |\n","|    critic_loss     | 0.213    |\n","|    ent_coef        | 0.0022   |\n","|    ent_coef_loss   | 1.21     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 151492   |\n","---------------------------------\n","Eval num_timesteps=3636000, episode_reward=5.24 +/- 2.75\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.24     |\n","| time/              |          |\n","|    total_timesteps | 3636000  |\n","| train/             |          |\n","|    actor_loss      | 0.436    |\n","|    critic_loss     | 0.205    |\n","|    ent_coef        | 0.00221  |\n","|    ent_coef_loss   | 0.687    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 151495   |\n","---------------------------------\n","Eval num_timesteps=3639600, episode_reward=-39.43 +/- 34.61\n","Episode length: 321.80 +/- 145.50\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 322      |\n","|    mean_reward     | -39.4    |\n","| time/              |          |\n","|    total_timesteps | 3639600  |\n","| train/             |          |\n","|    actor_loss      | 0.474    |\n","|    critic_loss     | 0.176    |\n","|    ent_coef        | 0.00223  |\n","|    ent_coef_loss   | 0.749    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 151645   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4700     |\n","|    fps             | 343      |\n","|    time_elapsed    | 10600    |\n","|    total_timesteps | 3642504  |\n","| train/             |          |\n","|    actor_loss      | 0.529    |\n","|    critic_loss     | 0.225    |\n","|    ent_coef        | 0.00222  |\n","|    ent_coef_loss   | -0.967   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 151766   |\n","---------------------------------\n","Eval num_timesteps=3643200, episode_reward=3.07 +/- 1.11\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.07     |\n","| time/              |          |\n","|    total_timesteps | 3643200  |\n","| train/             |          |\n","|    actor_loss      | 0.833    |\n","|    critic_loss     | 0.844    |\n","|    ent_coef        | 0.00222  |\n","|    ent_coef_loss   | 2.61     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 151795   |\n","---------------------------------\n","Eval num_timesteps=3646800, episode_reward=1.12 +/- 0.53\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.12     |\n","| time/              |          |\n","|    total_timesteps | 3646800  |\n","| train/             |          |\n","|    actor_loss      | 0.584    |\n","|    critic_loss     | 0.287    |\n","|    ent_coef        | 0.0022   |\n","|    ent_coef_loss   | 2.76     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 151945   |\n","---------------------------------\n","Eval num_timesteps=3650400, episode_reward=-76.92 +/- 2.11\n","Episode length: 146.60 +/- 41.15\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 147      |\n","|    mean_reward     | -76.9    |\n","| time/              |          |\n","|    total_timesteps | 3650400  |\n","| train/             |          |\n","|    actor_loss      | 0.338    |\n","|    critic_loss     | 0.161    |\n","|    ent_coef        | 0.00231  |\n","|    ent_coef_loss   | 0.375    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 152095   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4710     |\n","|    fps             | 343      |\n","|    time_elapsed    | 10628    |\n","|    total_timesteps | 3651960  |\n","| train/             |          |\n","|    actor_loss      | 0.781    |\n","|    critic_loss     | 23.1     |\n","|    ent_coef        | 0.00231  |\n","|    ent_coef_loss   | -2.98    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 152160   |\n","---------------------------------\n","Eval num_timesteps=3654000, episode_reward=-41.66 +/- 36.33\n","Episode length: 320.60 +/- 146.48\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 321      |\n","|    mean_reward     | -41.7    |\n","| time/              |          |\n","|    total_timesteps | 3654000  |\n","| train/             |          |\n","|    actor_loss      | 0.339    |\n","|    critic_loss     | 0.212    |\n","|    ent_coef        | 0.00224  |\n","|    ent_coef_loss   | -2.95    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 152245   |\n","---------------------------------\n","Eval num_timesteps=3657600, episode_reward=2.34 +/- 0.21\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.34     |\n","| time/              |          |\n","|    total_timesteps | 3657600  |\n","| train/             |          |\n","|    actor_loss      | 0.827    |\n","|    critic_loss     | 0.391    |\n","|    ent_coef        | 0.00217  |\n","|    ent_coef_loss   | 0.503    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 152395   |\n","---------------------------------\n","Eval num_timesteps=3661200, episode_reward=-30.40 +/- 38.62\n","Episode length: 402.80 +/- 119.05\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 403      |\n","|    mean_reward     | -30.4    |\n","| time/              |          |\n","|    total_timesteps | 3661200  |\n","| train/             |          |\n","|    actor_loss      | 0.55     |\n","|    critic_loss     | 7.35     |\n","|    ent_coef        | 0.00208  |\n","|    ent_coef_loss   | -1.5     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 152545   |\n","---------------------------------\n","Eval num_timesteps=3664800, episode_reward=3.07 +/- 1.48\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.07     |\n","| time/              |          |\n","|    total_timesteps | 3664800  |\n","| train/             |          |\n","|    actor_loss      | 0.473    |\n","|    critic_loss     | 0.272    |\n","|    ent_coef        | 0.0021   |\n","|    ent_coef_loss   | -0.289   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 152695   |\n","---------------------------------\n","Eval num_timesteps=3668400, episode_reward=1.83 +/- 0.03\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.83     |\n","| time/              |          |\n","|    total_timesteps | 3668400  |\n","| train/             |          |\n","|    actor_loss      | 0.651    |\n","|    critic_loss     | 0.478    |\n","|    ent_coef        | 0.002    |\n","|    ent_coef_loss   | -0.374   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 152845   |\n","---------------------------------\n","Eval num_timesteps=3672000, episode_reward=1.24 +/- 0.22\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.24     |\n","| time/              |          |\n","|    total_timesteps | 3672000  |\n","| train/             |          |\n","|    actor_loss      | 0.703    |\n","|    critic_loss     | 0.185    |\n","|    ent_coef        | 0.00201  |\n","|    ent_coef_loss   | -0.508   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 152995   |\n","---------------------------------\n","Eval num_timesteps=3675600, episode_reward=-40.96 +/- 42.13\n","Episode length: 338.60 +/- 131.78\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 339      |\n","|    mean_reward     | -41      |\n","| time/              |          |\n","|    total_timesteps | 3675600  |\n","| train/             |          |\n","|    actor_loss      | 0.68     |\n","|    critic_loss     | 0.235    |\n","|    ent_coef        | 0.00201  |\n","|    ent_coef_loss   | 2.78     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 153145   |\n","---------------------------------\n","Eval num_timesteps=3679200, episode_reward=1.81 +/- 0.72\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.81     |\n","| time/              |          |\n","|    total_timesteps | 3679200  |\n","| train/             |          |\n","|    actor_loss      | 0.512    |\n","|    critic_loss     | 0.137    |\n","|    ent_coef        | 0.00233  |\n","|    ent_coef_loss   | 9.7      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 153295   |\n","---------------------------------\n","Eval num_timesteps=3682800, episode_reward=2.45 +/- 0.19\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.45     |\n","| time/              |          |\n","|    total_timesteps | 3682800  |\n","| train/             |          |\n","|    actor_loss      | 0.46     |\n","|    critic_loss     | 0.192    |\n","|    ent_coef        | 0.00248  |\n","|    ent_coef_loss   | -1.26    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 153445   |\n","---------------------------------\n","Eval num_timesteps=3686400, episode_reward=3.42 +/- 0.12\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.42     |\n","| time/              |          |\n","|    total_timesteps | 3686400  |\n","| train/             |          |\n","|    actor_loss      | 0.99     |\n","|    critic_loss     | 0.255    |\n","|    ent_coef        | 0.00252  |\n","|    ent_coef_loss   | 2.67     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 153595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4720     |\n","|    fps             | 343      |\n","|    time_elapsed    | 10737    |\n","|    total_timesteps | 3689328  |\n","| train/             |          |\n","|    actor_loss      | 0.588    |\n","|    critic_loss     | 0.134    |\n","|    ent_coef        | 0.00257  |\n","|    ent_coef_loss   | 0.853    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 153717   |\n","---------------------------------\n","Eval num_timesteps=3690000, episode_reward=3.62 +/- 1.15\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.62     |\n","| time/              |          |\n","|    total_timesteps | 3690000  |\n","| train/             |          |\n","|    actor_loss      | 0.625    |\n","|    critic_loss     | 3.61     |\n","|    ent_coef        | 0.00257  |\n","|    ent_coef_loss   | 0.791    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 153745   |\n","---------------------------------\n","Eval num_timesteps=3693600, episode_reward=2.76 +/- 0.35\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.76     |\n","| time/              |          |\n","|    total_timesteps | 3693600  |\n","| train/             |          |\n","|    actor_loss      | 0.391    |\n","|    critic_loss     | 0.106    |\n","|    ent_coef        | 0.00258  |\n","|    ent_coef_loss   | -3.39    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 153895   |\n","---------------------------------\n","Eval num_timesteps=3697200, episode_reward=-44.96 +/- 39.84\n","Episode length: 297.80 +/- 165.10\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 298      |\n","|    mean_reward     | -45      |\n","| time/              |          |\n","|    total_timesteps | 3697200  |\n","| train/             |          |\n","|    actor_loss      | 0.662    |\n","|    critic_loss     | 0.312    |\n","|    ent_coef        | 0.00257  |\n","|    ent_coef_loss   | -0.337   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 154045   |\n","---------------------------------\n","Eval num_timesteps=3700800, episode_reward=-28.64 +/- 37.98\n","Episode length: 382.40 +/- 144.03\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 382      |\n","|    mean_reward     | -28.6    |\n","| time/              |          |\n","|    total_timesteps | 3700800  |\n","| train/             |          |\n","|    actor_loss      | 0.39     |\n","|    critic_loss     | 1.34     |\n","|    ent_coef        | 0.00236  |\n","|    ent_coef_loss   | -2.26    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 154195   |\n","---------------------------------\n","Eval num_timesteps=3704400, episode_reward=0.66 +/- 0.35\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.665    |\n","| time/              |          |\n","|    total_timesteps | 3704400  |\n","| train/             |          |\n","|    actor_loss      | 0.571    |\n","|    critic_loss     | 0.361    |\n","|    ent_coef        | 0.0023   |\n","|    ent_coef_loss   | 2.31     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 154345   |\n","---------------------------------\n","Eval num_timesteps=3708000, episode_reward=1.62 +/- 0.26\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.62     |\n","| time/              |          |\n","|    total_timesteps | 3708000  |\n","| train/             |          |\n","|    actor_loss      | 0.562    |\n","|    critic_loss     | 0.27     |\n","|    ent_coef        | 0.0023   |\n","|    ent_coef_loss   | -1.05    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 154495   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4730     |\n","|    fps             | 343      |\n","|    time_elapsed    | 10802    |\n","|    total_timesteps | 3711000  |\n","| train/             |          |\n","|    actor_loss      | 0.661    |\n","|    critic_loss     | 0.481    |\n","|    ent_coef        | 0.00246  |\n","|    ent_coef_loss   | 5.41     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 154620   |\n","---------------------------------\n","Eval num_timesteps=3711600, episode_reward=1.86 +/- 0.61\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.86     |\n","| time/              |          |\n","|    total_timesteps | 3711600  |\n","| train/             |          |\n","|    actor_loss      | 0.858    |\n","|    critic_loss     | 0.214    |\n","|    ent_coef        | 0.00251  |\n","|    ent_coef_loss   | 3.6      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 154645   |\n","---------------------------------\n","Eval num_timesteps=3715200, episode_reward=-43.28 +/- 37.48\n","Episode length: 317.60 +/- 148.93\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 318      |\n","|    mean_reward     | -43.3    |\n","| time/              |          |\n","|    total_timesteps | 3715200  |\n","| train/             |          |\n","|    actor_loss      | 0.449    |\n","|    critic_loss     | 0.408    |\n","|    ent_coef        | 0.00271  |\n","|    ent_coef_loss   | -1.67    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 154795   |\n","---------------------------------\n","Eval num_timesteps=3718800, episode_reward=3.12 +/- 0.28\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.12     |\n","| time/              |          |\n","|    total_timesteps | 3718800  |\n","| train/             |          |\n","|    actor_loss      | 0.184    |\n","|    critic_loss     | 0.142    |\n","|    ent_coef        | 0.00271  |\n","|    ent_coef_loss   | 0.557    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 154945   |\n","---------------------------------\n","Eval num_timesteps=3722400, episode_reward=4.96 +/- 3.48\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.96     |\n","| time/              |          |\n","|    total_timesteps | 3722400  |\n","| train/             |          |\n","|    actor_loss      | 0.462    |\n","|    critic_loss     | 0.116    |\n","|    ent_coef        | 0.00278  |\n","|    ent_coef_loss   | -2.83    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 155095   |\n","---------------------------------\n","Eval num_timesteps=3726000, episode_reward=1.50 +/- 0.16\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.5      |\n","| time/              |          |\n","|    total_timesteps | 3726000  |\n","| train/             |          |\n","|    actor_loss      | 1.06     |\n","|    critic_loss     | 10.7     |\n","|    ent_coef        | 0.00271  |\n","|    ent_coef_loss   | -0.724   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 155245   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4740     |\n","|    fps             | 343      |\n","|    time_elapsed    | 10853    |\n","|    total_timesteps | 3727680  |\n","| train/             |          |\n","|    actor_loss      | 0.927    |\n","|    critic_loss     | 0.276    |\n","|    ent_coef        | 0.00266  |\n","|    ent_coef_loss   | 0.484    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 155315   |\n","---------------------------------\n","Eval num_timesteps=3729600, episode_reward=2.18 +/- 0.54\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.18     |\n","| time/              |          |\n","|    total_timesteps | 3729600  |\n","| train/             |          |\n","|    actor_loss      | 0.757    |\n","|    critic_loss     | 1.44     |\n","|    ent_coef        | 0.00263  |\n","|    ent_coef_loss   | -2.17    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 155395   |\n","---------------------------------\n","Eval num_timesteps=3733200, episode_reward=-43.62 +/- 37.20\n","Episode length: 312.80 +/- 152.85\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 313      |\n","|    mean_reward     | -43.6    |\n","| time/              |          |\n","|    total_timesteps | 3733200  |\n","| train/             |          |\n","|    actor_loss      | 0.34     |\n","|    critic_loss     | 11.5     |\n","|    ent_coef        | 0.00253  |\n","|    ent_coef_loss   | -0.403   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 155545   |\n","---------------------------------\n","Eval num_timesteps=3736800, episode_reward=1.87 +/- 0.32\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.87     |\n","| time/              |          |\n","|    total_timesteps | 3736800  |\n","| train/             |          |\n","|    actor_loss      | 0.381    |\n","|    critic_loss     | 0.268    |\n","|    ent_coef        | 0.00248  |\n","|    ent_coef_loss   | -2.45    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 155695   |\n","---------------------------------\n","Eval num_timesteps=3740400, episode_reward=1.31 +/- 1.35\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.31     |\n","| time/              |          |\n","|    total_timesteps | 3740400  |\n","| train/             |          |\n","|    actor_loss      | 0.467    |\n","|    critic_loss     | 12.1     |\n","|    ent_coef        | 0.00243  |\n","|    ent_coef_loss   | -0.372   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 155845   |\n","---------------------------------\n","Eval num_timesteps=3744000, episode_reward=-44.35 +/- 38.80\n","Episode length: 353.60 +/- 119.54\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 354      |\n","|    mean_reward     | -44.4    |\n","| time/              |          |\n","|    total_timesteps | 3744000  |\n","| train/             |          |\n","|    actor_loss      | 0.658    |\n","|    critic_loss     | 0.32     |\n","|    ent_coef        | 0.00261  |\n","|    ent_coef_loss   | 5.66     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 155995   |\n","---------------------------------\n","Eval num_timesteps=3747600, episode_reward=1.88 +/- 0.19\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.88     |\n","| time/              |          |\n","|    total_timesteps | 3747600  |\n","| train/             |          |\n","|    actor_loss      | 0.72     |\n","|    critic_loss     | 0.192    |\n","|    ent_coef        | 0.00281  |\n","|    ent_coef_loss   | 2.65     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 156145   |\n","---------------------------------\n","Eval num_timesteps=3751200, episode_reward=5.79 +/- 2.31\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.79     |\n","| time/              |          |\n","|    total_timesteps | 3751200  |\n","| train/             |          |\n","|    actor_loss      | 0.512    |\n","|    critic_loss     | 0.19     |\n","|    ent_coef        | 0.00291  |\n","|    ent_coef_loss   | -1.07    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 156295   |\n","---------------------------------\n","Eval num_timesteps=3754800, episode_reward=2.65 +/- 0.22\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.65     |\n","| time/              |          |\n","|    total_timesteps | 3754800  |\n","| train/             |          |\n","|    actor_loss      | 0.788    |\n","|    critic_loss     | 0.217    |\n","|    ent_coef        | 0.00283  |\n","|    ent_coef_loss   | 2.72     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 156445   |\n","---------------------------------\n","Eval num_timesteps=3758400, episode_reward=2.12 +/- 1.62\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.12     |\n","| time/              |          |\n","|    total_timesteps | 3758400  |\n","| train/             |          |\n","|    actor_loss      | 0.456    |\n","|    critic_loss     | 0.18     |\n","|    ent_coef        | 0.00276  |\n","|    ent_coef_loss   | -4.31    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 156595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4750     |\n","|    fps             | 343      |\n","|    time_elapsed    | 10951    |\n","|    total_timesteps | 3761952  |\n","| train/             |          |\n","|    actor_loss      | 0.595    |\n","|    critic_loss     | 10.4     |\n","|    ent_coef        | 0.00267  |\n","|    ent_coef_loss   | 6.61     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 156743   |\n","---------------------------------\n","Eval num_timesteps=3762000, episode_reward=2.30 +/- 0.32\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.3      |\n","| time/              |          |\n","|    total_timesteps | 3762000  |\n","| train/             |          |\n","|    actor_loss      | 0.837    |\n","|    critic_loss     | 0.264    |\n","|    ent_coef        | 0.00267  |\n","|    ent_coef_loss   | 5.82     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 156745   |\n","---------------------------------\n","Eval num_timesteps=3765600, episode_reward=0.93 +/- 0.21\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.929    |\n","| time/              |          |\n","|    total_timesteps | 3765600  |\n","| train/             |          |\n","|    actor_loss      | 0.257    |\n","|    critic_loss     | 0.136    |\n","|    ent_coef        | 0.003    |\n","|    ent_coef_loss   | 0.058    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 156895   |\n","---------------------------------\n","Eval num_timesteps=3769200, episode_reward=3.45 +/- 0.62\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.45     |\n","| time/              |          |\n","|    total_timesteps | 3769200  |\n","| train/             |          |\n","|    actor_loss      | 0.735    |\n","|    critic_loss     | 0.228    |\n","|    ent_coef        | 0.00294  |\n","|    ent_coef_loss   | -3.55    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 157045   |\n","---------------------------------\n","Eval num_timesteps=3772800, episode_reward=-46.60 +/- 41.33\n","Episode length: 341.00 +/- 129.82\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 341      |\n","|    mean_reward     | -46.6    |\n","| time/              |          |\n","|    total_timesteps | 3772800  |\n","| train/             |          |\n","|    actor_loss      | 0.601    |\n","|    critic_loss     | 0.223    |\n","|    ent_coef        | 0.0029   |\n","|    ent_coef_loss   | -2.35    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 157195   |\n","---------------------------------\n","Eval num_timesteps=3776400, episode_reward=4.65 +/- 4.06\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.65     |\n","| time/              |          |\n","|    total_timesteps | 3776400  |\n","| train/             |          |\n","|    actor_loss      | 0.996    |\n","|    critic_loss     | 0.43     |\n","|    ent_coef        | 0.0027   |\n","|    ent_coef_loss   | -2.22    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 157345   |\n","---------------------------------\n","Eval num_timesteps=3780000, episode_reward=-29.99 +/- 40.36\n","Episode length: 373.20 +/- 155.30\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 373      |\n","|    mean_reward     | -30      |\n","| time/              |          |\n","|    total_timesteps | 3780000  |\n","| train/             |          |\n","|    actor_loss      | 0.788    |\n","|    critic_loss     | 0.522    |\n","|    ent_coef        | 0.0026   |\n","|    ent_coef_loss   | 1.6      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 157495   |\n","---------------------------------\n","Eval num_timesteps=3783600, episode_reward=-70.16 +/- 2.86\n","Episode length: 190.60 +/- 6.86\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 191      |\n","|    mean_reward     | -70.2    |\n","| time/              |          |\n","|    total_timesteps | 3783600  |\n","| train/             |          |\n","|    actor_loss      | 0.606    |\n","|    critic_loss     | 0.491    |\n","|    ent_coef        | 0.0026   |\n","|    ent_coef_loss   | -0.259   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 157645   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4760     |\n","|    fps             | 343      |\n","|    time_elapsed    | 11023    |\n","|    total_timesteps | 3786456  |\n","| train/             |          |\n","|    actor_loss      | 0.614    |\n","|    critic_loss     | 0.49     |\n","|    ent_coef        | 0.00278  |\n","|    ent_coef_loss   | -1.69    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 157764   |\n","---------------------------------\n","Eval num_timesteps=3787200, episode_reward=1.88 +/- 0.83\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.88     |\n","| time/              |          |\n","|    total_timesteps | 3787200  |\n","| train/             |          |\n","|    actor_loss      | 0.818    |\n","|    critic_loss     | 0.284    |\n","|    ent_coef        | 0.00277  |\n","|    ent_coef_loss   | -0.501   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 157795   |\n","---------------------------------\n","Eval num_timesteps=3790800, episode_reward=3.89 +/- 1.78\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.89     |\n","| time/              |          |\n","|    total_timesteps | 3790800  |\n","| train/             |          |\n","|    actor_loss      | 0.682    |\n","|    critic_loss     | 3.09     |\n","|    ent_coef        | 0.00276  |\n","|    ent_coef_loss   | -1.19    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 157945   |\n","---------------------------------\n","Eval num_timesteps=3794400, episode_reward=-77.07 +/- 2.51\n","Episode length: 121.20 +/- 8.33\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 121      |\n","|    mean_reward     | -77.1    |\n","| time/              |          |\n","|    total_timesteps | 3794400  |\n","| train/             |          |\n","|    actor_loss      | 0.898    |\n","|    critic_loss     | 0.46     |\n","|    ent_coef        | 0.0028   |\n","|    ent_coef_loss   | -1.02    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 158095   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4770     |\n","|    fps             | 343      |\n","|    time_elapsed    | 11050    |\n","|    total_timesteps | 3796560  |\n","| train/             |          |\n","|    actor_loss      | 0.6      |\n","|    critic_loss     | 0.165    |\n","|    ent_coef        | 0.00274  |\n","|    ent_coef_loss   | -1.38    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 158185   |\n","---------------------------------\n","Eval num_timesteps=3798000, episode_reward=-46.06 +/- 39.93\n","Episode length: 270.20 +/- 187.63\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 270      |\n","|    mean_reward     | -46.1    |\n","| time/              |          |\n","|    total_timesteps | 3798000  |\n","| train/             |          |\n","|    actor_loss      | 0.463    |\n","|    critic_loss     | 12.6     |\n","|    ent_coef        | 0.00269  |\n","|    ent_coef_loss   | 1.26     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 158245   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4780     |\n","|    fps             | 343      |\n","|    time_elapsed    | 11060    |\n","|    total_timesteps | 3799632  |\n","| train/             |          |\n","|    actor_loss      | 1.22     |\n","|    critic_loss     | 0.567    |\n","|    ent_coef        | 0.00273  |\n","|    ent_coef_loss   | 0.504    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 158313   |\n","---------------------------------\n","Eval num_timesteps=3801600, episode_reward=1.97 +/- 0.18\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.97     |\n","| time/              |          |\n","|    total_timesteps | 3801600  |\n","| train/             |          |\n","|    actor_loss      | 0.763    |\n","|    critic_loss     | 0.83     |\n","|    ent_coef        | 0.00271  |\n","|    ent_coef_loss   | 1.09     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 158395   |\n","---------------------------------\n","Eval num_timesteps=3805200, episode_reward=1.40 +/- 0.49\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.4      |\n","| time/              |          |\n","|    total_timesteps | 3805200  |\n","| train/             |          |\n","|    actor_loss      | 0.384    |\n","|    critic_loss     | 0.175    |\n","|    ent_coef        | 0.00305  |\n","|    ent_coef_loss   | 0.964    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 158545   |\n","---------------------------------\n","Eval num_timesteps=3808800, episode_reward=2.76 +/- 1.16\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.76     |\n","| time/              |          |\n","|    total_timesteps | 3808800  |\n","| train/             |          |\n","|    actor_loss      | 0.742    |\n","|    critic_loss     | 0.395    |\n","|    ent_coef        | 0.0031   |\n","|    ent_coef_loss   | -0.779   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 158695   |\n","---------------------------------\n","Eval num_timesteps=3812400, episode_reward=1.08 +/- 0.72\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.08     |\n","| time/              |          |\n","|    total_timesteps | 3812400  |\n","| train/             |          |\n","|    actor_loss      | 0.476    |\n","|    critic_loss     | 0.42     |\n","|    ent_coef        | 0.00286  |\n","|    ent_coef_loss   | -3.72    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 158845   |\n","---------------------------------\n","Eval num_timesteps=3816000, episode_reward=1.72 +/- 0.42\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.72     |\n","| time/              |          |\n","|    total_timesteps | 3816000  |\n","| train/             |          |\n","|    actor_loss      | 0.654    |\n","|    critic_loss     | 0.372    |\n","|    ent_coef        | 0.00284  |\n","|    ent_coef_loss   | -1.19    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 158995   |\n","---------------------------------\n","Eval num_timesteps=3819600, episode_reward=2.58 +/- 0.08\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.58     |\n","| time/              |          |\n","|    total_timesteps | 3819600  |\n","| train/             |          |\n","|    actor_loss      | 0.623    |\n","|    critic_loss     | 0.343    |\n","|    ent_coef        | 0.00274  |\n","|    ent_coef_loss   | -0.974   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 159145   |\n","---------------------------------\n","Eval num_timesteps=3823200, episode_reward=-28.20 +/- 36.98\n","Episode length: 406.00 +/- 115.13\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 406      |\n","|    mean_reward     | -28.2    |\n","| time/              |          |\n","|    total_timesteps | 3823200  |\n","| train/             |          |\n","|    actor_loss      | 0.842    |\n","|    critic_loss     | 0.715    |\n","|    ent_coef        | 0.00274  |\n","|    ent_coef_loss   | -3.2     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 159295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4790     |\n","|    fps             | 343      |\n","|    time_elapsed    | 11135    |\n","|    total_timesteps | 3824352  |\n","| train/             |          |\n","|    actor_loss      | 0.785    |\n","|    critic_loss     | 0.384    |\n","|    ent_coef        | 0.00266  |\n","|    ent_coef_loss   | -4       |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 159343   |\n","---------------------------------\n","Eval num_timesteps=3826800, episode_reward=-44.81 +/- 38.60\n","Episode length: 287.00 +/- 173.91\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 287      |\n","|    mean_reward     | -44.8    |\n","| time/              |          |\n","|    total_timesteps | 3826800  |\n","| train/             |          |\n","|    actor_loss      | 0.572    |\n","|    critic_loss     | 0.647    |\n","|    ent_coef        | 0.00255  |\n","|    ent_coef_loss   | -0.91    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 159445   |\n","---------------------------------\n","Eval num_timesteps=3830400, episode_reward=-28.38 +/- 37.52\n","Episode length: 357.60 +/- 174.40\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 358      |\n","|    mean_reward     | -28.4    |\n","| time/              |          |\n","|    total_timesteps | 3830400  |\n","| train/             |          |\n","|    actor_loss      | 1.29     |\n","|    critic_loss     | 0.507    |\n","|    ent_coef        | 0.00258  |\n","|    ent_coef_loss   | 1.6      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 159595   |\n","---------------------------------\n","Eval num_timesteps=3834000, episode_reward=1.88 +/- 0.06\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.88     |\n","| time/              |          |\n","|    total_timesteps | 3834000  |\n","| train/             |          |\n","|    actor_loss      | 0.662    |\n","|    critic_loss     | 0.568    |\n","|    ent_coef        | 0.00272  |\n","|    ent_coef_loss   | -0.339   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 159745   |\n","---------------------------------\n","Eval num_timesteps=3837600, episode_reward=2.55 +/- 0.08\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.55     |\n","| time/              |          |\n","|    total_timesteps | 3837600  |\n","| train/             |          |\n","|    actor_loss      | 0.559    |\n","|    critic_loss     | 0.304    |\n","|    ent_coef        | 0.00274  |\n","|    ent_coef_loss   | 0.658    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 159895   |\n","---------------------------------\n","Eval num_timesteps=3841200, episode_reward=1.81 +/- 0.04\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.81     |\n","| time/              |          |\n","|    total_timesteps | 3841200  |\n","| train/             |          |\n","|    actor_loss      | 1.06     |\n","|    critic_loss     | 0.339    |\n","|    ent_coef        | 0.00274  |\n","|    ent_coef_loss   | -0.314   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 160045   |\n","---------------------------------\n","Eval num_timesteps=3844800, episode_reward=3.09 +/- 1.03\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.09     |\n","| time/              |          |\n","|    total_timesteps | 3844800  |\n","| train/             |          |\n","|    actor_loss      | 0.641    |\n","|    critic_loss     | 0.418    |\n","|    ent_coef        | 0.0026   |\n","|    ent_coef_loss   | -0.169   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 160195   |\n","---------------------------------\n","Eval num_timesteps=3848400, episode_reward=3.14 +/- 1.52\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.14     |\n","| time/              |          |\n","|    total_timesteps | 3848400  |\n","| train/             |          |\n","|    actor_loss      | 0.562    |\n","|    critic_loss     | 0.413    |\n","|    ent_coef        | 0.00265  |\n","|    ent_coef_loss   | -2.67    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 160345   |\n","---------------------------------\n","Eval num_timesteps=3852000, episode_reward=3.38 +/- 0.92\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.38     |\n","| time/              |          |\n","|    total_timesteps | 3852000  |\n","| train/             |          |\n","|    actor_loss      | 1.26     |\n","|    critic_loss     | 0.981    |\n","|    ent_coef        | 0.00266  |\n","|    ent_coef_loss   | 2.23     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 160495   |\n","---------------------------------\n","Eval num_timesteps=3855600, episode_reward=-44.72 +/- 38.02\n","Episode length: 395.00 +/- 85.73\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 395      |\n","|    mean_reward     | -44.7    |\n","| time/              |          |\n","|    total_timesteps | 3855600  |\n","| train/             |          |\n","|    actor_loss      | 0.686    |\n","|    critic_loss     | 0.47     |\n","|    ent_coef        | 0.00263  |\n","|    ent_coef_loss   | -0.477   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 160645   |\n","---------------------------------\n","Eval num_timesteps=3859200, episode_reward=1.19 +/- 1.28\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.19     |\n","| time/              |          |\n","|    total_timesteps | 3859200  |\n","| train/             |          |\n","|    actor_loss      | 0.533    |\n","|    critic_loss     | 0.388    |\n","|    ent_coef        | 0.00268  |\n","|    ent_coef_loss   | 1.72     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 160795   |\n","---------------------------------\n","Eval num_timesteps=3862800, episode_reward=9.87 +/- 0.89\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 9.87     |\n","| time/              |          |\n","|    total_timesteps | 3862800  |\n","| train/             |          |\n","|    actor_loss      | 0.64     |\n","|    critic_loss     | 0.49     |\n","|    ent_coef        | 0.00278  |\n","|    ent_coef_loss   | 2.33     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 160945   |\n","---------------------------------\n","Eval num_timesteps=3866400, episode_reward=2.02 +/- 0.11\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.02     |\n","| time/              |          |\n","|    total_timesteps | 3866400  |\n","| train/             |          |\n","|    actor_loss      | 0.571    |\n","|    critic_loss     | 1.9      |\n","|    ent_coef        | 0.00301  |\n","|    ent_coef_loss   | 1.52     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 161095   |\n","---------------------------------\n","Eval num_timesteps=3870000, episode_reward=9.69 +/- 5.22\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 9.69     |\n","| time/              |          |\n","|    total_timesteps | 3870000  |\n","| train/             |          |\n","|    actor_loss      | 1.4      |\n","|    critic_loss     | 0.615    |\n","|    ent_coef        | 0.00299  |\n","|    ent_coef_loss   | 3.54     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 161245   |\n","---------------------------------\n","Eval num_timesteps=3873600, episode_reward=-21.63 +/- 44.72\n","Episode length: 368.40 +/- 161.18\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 368      |\n","|    mean_reward     | -21.6    |\n","| time/              |          |\n","|    total_timesteps | 3873600  |\n","| train/             |          |\n","|    actor_loss      | 0.492    |\n","|    critic_loss     | 0.271    |\n","|    ent_coef        | 0.003    |\n","|    ent_coef_loss   | -0.00425 |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 161395   |\n","---------------------------------\n","Eval num_timesteps=3877200, episode_reward=3.23 +/- 0.88\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.23     |\n","| time/              |          |\n","|    total_timesteps | 3877200  |\n","| train/             |          |\n","|    actor_loss      | 1.08     |\n","|    critic_loss     | 0.5      |\n","|    ent_coef        | 0.0031   |\n","|    ent_coef_loss   | 2.19     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 161545   |\n","---------------------------------\n","Eval num_timesteps=3880800, episode_reward=3.11 +/- 0.92\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.11     |\n","| time/              |          |\n","|    total_timesteps | 3880800  |\n","| train/             |          |\n","|    actor_loss      | 0.905    |\n","|    critic_loss     | 0.45     |\n","|    ent_coef        | 0.00303  |\n","|    ent_coef_loss   | 0.583    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 161695   |\n","---------------------------------\n","Eval num_timesteps=3884400, episode_reward=7.55 +/- 1.59\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.55     |\n","| time/              |          |\n","|    total_timesteps | 3884400  |\n","| train/             |          |\n","|    actor_loss      | 1.03     |\n","|    critic_loss     | 0.27     |\n","|    ent_coef        | 0.00321  |\n","|    ent_coef_loss   | 4.13     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 161845   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4800     |\n","|    fps             | 343      |\n","|    time_elapsed    | 11315    |\n","|    total_timesteps | 3885192  |\n","| train/             |          |\n","|    actor_loss      | 0.925    |\n","|    critic_loss     | 0.367    |\n","|    ent_coef        | 0.00325  |\n","|    ent_coef_loss   | 1.68     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 161878   |\n","---------------------------------\n","Eval num_timesteps=3888000, episode_reward=3.70 +/- 1.96\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.7      |\n","| time/              |          |\n","|    total_timesteps | 3888000  |\n","| train/             |          |\n","|    actor_loss      | 0.659    |\n","|    critic_loss     | 0.244    |\n","|    ent_coef        | 0.0031   |\n","|    ent_coef_loss   | -4.52    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 161995   |\n","---------------------------------\n","Eval num_timesteps=3891600, episode_reward=4.81 +/- 0.39\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.81     |\n","| time/              |          |\n","|    total_timesteps | 3891600  |\n","| train/             |          |\n","|    actor_loss      | 0.852    |\n","|    critic_loss     | 0.919    |\n","|    ent_coef        | 0.00287  |\n","|    ent_coef_loss   | 1.89     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 162145   |\n","---------------------------------\n","Eval num_timesteps=3895200, episode_reward=7.79 +/- 3.11\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.79     |\n","| time/              |          |\n","|    total_timesteps | 3895200  |\n","| train/             |          |\n","|    actor_loss      | 0.654    |\n","|    critic_loss     | 0.165    |\n","|    ent_coef        | 0.00292  |\n","|    ent_coef_loss   | -0.428   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 162295   |\n","---------------------------------\n","Eval num_timesteps=3898800, episode_reward=-65.18 +/- 3.52\n","Episode length: 329.60 +/- 22.54\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 330      |\n","|    mean_reward     | -65.2    |\n","| time/              |          |\n","|    total_timesteps | 3898800  |\n","| train/             |          |\n","|    actor_loss      | 0.968    |\n","|    critic_loss     | 0.346    |\n","|    ent_coef        | 0.00299  |\n","|    ent_coef_loss   | 0.618    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 162445   |\n","---------------------------------\n","Eval num_timesteps=3902400, episode_reward=2.49 +/- 0.93\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.49     |\n","| time/              |          |\n","|    total_timesteps | 3902400  |\n","| train/             |          |\n","|    actor_loss      | 0.807    |\n","|    critic_loss     | 0.167    |\n","|    ent_coef        | 0.00293  |\n","|    ent_coef_loss   | -2.03    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 162595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4810     |\n","|    fps             | 343      |\n","|    time_elapsed    | 11371    |\n","|    total_timesteps | 3905784  |\n","| train/             |          |\n","|    actor_loss      | 0.923    |\n","|    critic_loss     | 0.229    |\n","|    ent_coef        | 0.00279  |\n","|    ent_coef_loss   | -1.63    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 162736   |\n","---------------------------------\n","Eval num_timesteps=3906000, episode_reward=2.53 +/- 0.83\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.53     |\n","| time/              |          |\n","|    total_timesteps | 3906000  |\n","| train/             |          |\n","|    actor_loss      | 0.55     |\n","|    critic_loss     | 0.225    |\n","|    ent_coef        | 0.00279  |\n","|    ent_coef_loss   | -1.25    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 162745   |\n","---------------------------------\n","Eval num_timesteps=3909600, episode_reward=-31.33 +/- 39.63\n","Episode length: 407.20 +/- 113.66\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 407      |\n","|    mean_reward     | -31.3    |\n","| time/              |          |\n","|    total_timesteps | 3909600  |\n","| train/             |          |\n","|    actor_loss      | 0.932    |\n","|    critic_loss     | 0.205    |\n","|    ent_coef        | 0.00294  |\n","|    ent_coef_loss   | 3.4      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 162895   |\n","---------------------------------\n","Eval num_timesteps=3913200, episode_reward=-27.21 +/- 39.53\n","Episode length: 446.80 +/- 65.16\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 447      |\n","|    mean_reward     | -27.2    |\n","| time/              |          |\n","|    total_timesteps | 3913200  |\n","| train/             |          |\n","|    actor_loss      | 1.21     |\n","|    critic_loss     | 3.23     |\n","|    ent_coef        | 0.00338  |\n","|    ent_coef_loss   | -0.522   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 163045   |\n","---------------------------------\n","Eval num_timesteps=3916800, episode_reward=2.45 +/- 3.97\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.45     |\n","| time/              |          |\n","|    total_timesteps | 3916800  |\n","| train/             |          |\n","|    actor_loss      | 0.614    |\n","|    critic_loss     | 0.275    |\n","|    ent_coef        | 0.00311  |\n","|    ent_coef_loss   | -1.89    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 163195   |\n","---------------------------------\n","Eval num_timesteps=3920400, episode_reward=-73.87 +/- 6.11\n","Episode length: 213.00 +/- 24.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 213      |\n","|    mean_reward     | -73.9    |\n","| time/              |          |\n","|    total_timesteps | 3920400  |\n","| train/             |          |\n","|    actor_loss      | 0.83     |\n","|    critic_loss     | 0.53     |\n","|    ent_coef        | 0.00315  |\n","|    ent_coef_loss   | -2.33    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 163345   |\n","---------------------------------\n","Eval num_timesteps=3924000, episode_reward=1.50 +/- 0.04\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.5      |\n","| time/              |          |\n","|    total_timesteps | 3924000  |\n","| train/             |          |\n","|    actor_loss      | 1.22     |\n","|    critic_loss     | 1.24     |\n","|    ent_coef        | 0.00308  |\n","|    ent_coef_loss   | -0.668   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 163495   |\n","---------------------------------\n","Eval num_timesteps=3927600, episode_reward=7.62 +/- 1.22\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.62     |\n","| time/              |          |\n","|    total_timesteps | 3927600  |\n","| train/             |          |\n","|    actor_loss      | 0.903    |\n","|    critic_loss     | 0.191    |\n","|    ent_coef        | 0.00297  |\n","|    ent_coef_loss   | -2.9     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 163645   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4820     |\n","|    fps             | 343      |\n","|    time_elapsed    | 11443    |\n","|    total_timesteps | 3930240  |\n","| train/             |          |\n","|    actor_loss      | 0.796    |\n","|    critic_loss     | 0.32     |\n","|    ent_coef        | 0.00283  |\n","|    ent_coef_loss   | 2.42     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 163755   |\n","---------------------------------\n","Eval num_timesteps=3931200, episode_reward=5.86 +/- 2.89\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.86     |\n","| time/              |          |\n","|    total_timesteps | 3931200  |\n","| train/             |          |\n","|    actor_loss      | 1.08     |\n","|    critic_loss     | 0.366    |\n","|    ent_coef        | 0.00286  |\n","|    ent_coef_loss   | 2.05     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 163795   |\n","---------------------------------\n","Eval num_timesteps=3934800, episode_reward=-69.46 +/- 4.94\n","Episode length: 357.20 +/- 169.99\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 357      |\n","|    mean_reward     | -69.5    |\n","| time/              |          |\n","|    total_timesteps | 3934800  |\n","| train/             |          |\n","|    actor_loss      | 0.857    |\n","|    critic_loss     | 0.179    |\n","|    ent_coef        | 0.00284  |\n","|    ent_coef_loss   | -1.25    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 163945   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4830     |\n","|    fps             | 343      |\n","|    time_elapsed    | 11462    |\n","|    total_timesteps | 3934896  |\n","| train/             |          |\n","|    actor_loss      | 0.919    |\n","|    critic_loss     | 0.174    |\n","|    ent_coef        | 0.00283  |\n","|    ent_coef_loss   | -0.296   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 163949   |\n","---------------------------------\n","Eval num_timesteps=3938400, episode_reward=-44.88 +/- 39.74\n","Episode length: 324.20 +/- 143.54\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 324      |\n","|    mean_reward     | -44.9    |\n","| time/              |          |\n","|    total_timesteps | 3938400  |\n","| train/             |          |\n","|    actor_loss      | 1.37     |\n","|    critic_loss     | 0.543    |\n","|    ent_coef        | 0.00292  |\n","|    ent_coef_loss   | -0.866   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 164095   |\n","---------------------------------\n","Eval num_timesteps=3942000, episode_reward=-43.67 +/- 38.53\n","Episode length: 297.20 +/- 165.59\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 297      |\n","|    mean_reward     | -43.7    |\n","| time/              |          |\n","|    total_timesteps | 3942000  |\n","| train/             |          |\n","|    actor_loss      | 0.748    |\n","|    critic_loss     | 2.38     |\n","|    ent_coef        | 0.00298  |\n","|    ent_coef_loss   | -0.844   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 164245   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4840     |\n","|    fps             | 343      |\n","|    time_elapsed    | 11487    |\n","|    total_timesteps | 3944376  |\n","| train/             |          |\n","|    actor_loss      | 0.924    |\n","|    critic_loss     | 0.242    |\n","|    ent_coef        | 0.00292  |\n","|    ent_coef_loss   | -1.11    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 164344   |\n","---------------------------------\n","Eval num_timesteps=3945600, episode_reward=-42.29 +/- 36.27\n","Episode length: 266.60 +/- 190.57\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 267      |\n","|    mean_reward     | -42.3    |\n","| time/              |          |\n","|    total_timesteps | 3945600  |\n","| train/             |          |\n","|    actor_loss      | 0.928    |\n","|    critic_loss     | 0.351    |\n","|    ent_coef        | 0.00291  |\n","|    ent_coef_loss   | -1.91    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 164395   |\n","---------------------------------\n","Eval num_timesteps=3949200, episode_reward=-14.87 +/- 27.39\n","Episode length: 421.20 +/- 96.51\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 421      |\n","|    mean_reward     | -14.9    |\n","| time/              |          |\n","|    total_timesteps | 3949200  |\n","| train/             |          |\n","|    actor_loss      | 0.886    |\n","|    critic_loss     | 1.08     |\n","|    ent_coef        | 0.00276  |\n","|    ent_coef_loss   | -0.942   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 164545   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4850     |\n","|    fps             | 343      |\n","|    time_elapsed    | 11509    |\n","|    total_timesteps | 3952344  |\n","| train/             |          |\n","|    actor_loss      | 1.01     |\n","|    critic_loss     | 1.17     |\n","|    ent_coef        | 0.00275  |\n","|    ent_coef_loss   | 0.953    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 164676   |\n","---------------------------------\n","Eval num_timesteps=3952800, episode_reward=-43.28 +/- 41.09\n","Episode length: 306.80 +/- 157.75\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 307      |\n","|    mean_reward     | -43.3    |\n","| time/              |          |\n","|    total_timesteps | 3952800  |\n","| train/             |          |\n","|    actor_loss      | 1.12     |\n","|    critic_loss     | 0.293    |\n","|    ent_coef        | 0.00275  |\n","|    ent_coef_loss   | -2.85    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 164695   |\n","---------------------------------\n","Eval num_timesteps=3956400, episode_reward=6.21 +/- 4.25\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.21     |\n","| time/              |          |\n","|    total_timesteps | 3956400  |\n","| train/             |          |\n","|    actor_loss      | 0.842    |\n","|    critic_loss     | 0.179    |\n","|    ent_coef        | 0.00282  |\n","|    ent_coef_loss   | 0.53     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 164845   |\n","---------------------------------\n","Eval num_timesteps=3960000, episode_reward=1.43 +/- 0.46\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.43     |\n","| time/              |          |\n","|    total_timesteps | 3960000  |\n","| train/             |          |\n","|    actor_loss      | 0.701    |\n","|    critic_loss     | 0.223    |\n","|    ent_coef        | 0.00293  |\n","|    ent_coef_loss   | -1.07    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 164995   |\n","---------------------------------\n","Eval num_timesteps=3963600, episode_reward=1.81 +/- 0.30\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.81     |\n","| time/              |          |\n","|    total_timesteps | 3963600  |\n","| train/             |          |\n","|    actor_loss      | 0.826    |\n","|    critic_loss     | 0.217    |\n","|    ent_coef        | 0.00277  |\n","|    ent_coef_loss   | -1.42    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 165145   |\n","---------------------------------\n","Eval num_timesteps=3967200, episode_reward=-78.41 +/- 4.94\n","Episode length: 242.20 +/- 70.06\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 242      |\n","|    mean_reward     | -78.4    |\n","| time/              |          |\n","|    total_timesteps | 3967200  |\n","| train/             |          |\n","|    actor_loss      | 1.03     |\n","|    critic_loss     | 0.355    |\n","|    ent_coef        | 0.00254  |\n","|    ent_coef_loss   | -2.23    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 165295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4860     |\n","|    fps             | 343      |\n","|    time_elapsed    | 11558    |\n","|    total_timesteps | 3967848  |\n","| train/             |          |\n","|    actor_loss      | 0.846    |\n","|    critic_loss     | 0.203    |\n","|    ent_coef        | 0.00251  |\n","|    ent_coef_loss   | -2.14    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 165322   |\n","---------------------------------\n","Eval num_timesteps=3970800, episode_reward=-46.13 +/- 40.28\n","Episode length: 303.80 +/- 160.20\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 304      |\n","|    mean_reward     | -46.1    |\n","| time/              |          |\n","|    total_timesteps | 3970800  |\n","| train/             |          |\n","|    actor_loss      | 1.26     |\n","|    critic_loss     | 1.03     |\n","|    ent_coef        | 0.00241  |\n","|    ent_coef_loss   | -0.22    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 165445   |\n","---------------------------------\n","Eval num_timesteps=3974400, episode_reward=7.88 +/- 2.75\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.88     |\n","| time/              |          |\n","|    total_timesteps | 3974400  |\n","| train/             |          |\n","|    actor_loss      | 1.54     |\n","|    critic_loss     | 7.67     |\n","|    ent_coef        | 0.00247  |\n","|    ent_coef_loss   | 0.42     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 165595   |\n","---------------------------------\n","Eval num_timesteps=3978000, episode_reward=1.24 +/- 0.25\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.24     |\n","| time/              |          |\n","|    total_timesteps | 3978000  |\n","| train/             |          |\n","|    actor_loss      | 1.2      |\n","|    critic_loss     | 0.427    |\n","|    ent_coef        | 0.00261  |\n","|    ent_coef_loss   | 1.41     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 165745   |\n","---------------------------------\n","Eval num_timesteps=3981600, episode_reward=1.65 +/- 0.37\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.65     |\n","| time/              |          |\n","|    total_timesteps | 3981600  |\n","| train/             |          |\n","|    actor_loss      | 1.3      |\n","|    critic_loss     | 0.444    |\n","|    ent_coef        | 0.00275  |\n","|    ent_coef_loss   | 2.33     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 165895   |\n","---------------------------------\n","Eval num_timesteps=3985200, episode_reward=2.65 +/- 0.30\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.65     |\n","| time/              |          |\n","|    total_timesteps | 3985200  |\n","| train/             |          |\n","|    actor_loss      | 1.16     |\n","|    critic_loss     | 1.03     |\n","|    ent_coef        | 0.00282  |\n","|    ent_coef_loss   | 1.3      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 166045   |\n","---------------------------------\n","Eval num_timesteps=3988800, episode_reward=1.57 +/- 0.07\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.57     |\n","| time/              |          |\n","|    total_timesteps | 3988800  |\n","| train/             |          |\n","|    actor_loss      | 1.16     |\n","|    critic_loss     | 0.567    |\n","|    ent_coef        | 0.00281  |\n","|    ent_coef_loss   | -1.5     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 166195   |\n","---------------------------------\n","Eval num_timesteps=3992400, episode_reward=2.39 +/- 0.07\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.39     |\n","| time/              |          |\n","|    total_timesteps | 3992400  |\n","| train/             |          |\n","|    actor_loss      | 0.908    |\n","|    critic_loss     | 0.198    |\n","|    ent_coef        | 0.00268  |\n","|    ent_coef_loss   | -1.73    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 166345   |\n","---------------------------------\n","Eval num_timesteps=3996000, episode_reward=3.27 +/- 0.24\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.27     |\n","| time/              |          |\n","|    total_timesteps | 3996000  |\n","| train/             |          |\n","|    actor_loss      | 1.25     |\n","|    critic_loss     | 0.57     |\n","|    ent_coef        | 0.00269  |\n","|    ent_coef_loss   | 3.79     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 166495   |\n","---------------------------------\n","Eval num_timesteps=3999600, episode_reward=2.69 +/- 0.31\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.69     |\n","| time/              |          |\n","|    total_timesteps | 3999600  |\n","| train/             |          |\n","|    actor_loss      | 1.1      |\n","|    critic_loss     | 0.286    |\n","|    ent_coef        | 0.00262  |\n","|    ent_coef_loss   | 0.15     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 166645   |\n","---------------------------------\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"OooX9WwN9AGO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3 import SAC\n","from pettingzoo.sisl import multiwalker_v9\n","import supersuit as ss\n","from stable_baselines3.common.monitor import Monitor"],"metadata":{"id":"5ypcPewx9AxR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697887381542,"user_tz":-120,"elapsed":59,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"bcbc999c-9da8-47cf-d68d-f554b75a1956","id":"mqyCzx7d9AxS"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["'=2.13'\t\t\t\t     multiwalker_sac2_log_eval\n"," DQN_new_pettingzoo_gym_cap.ipynb    multiwalker_sac_log_eval\n"," DQN_policies\t\t\t     multiwalker_td3_log_eval\n","'Entrenamientos antiguos sin logs'   policy_log_eval\n"," Entrenamientos_log_no_eval\t     PPO_policies\n"," MCR_TFM.ipynb\t\t\t     results_rllib\n"," multi_car_racing\t\t     TFM_Multiwalker_DDPG_PPO_gym_cap.ipynb\n"," multiwalker_ddpg_log_eval\t     TFM_Multiwalker_SAC_gym_cap.ipynb\n"," multiwalker_ddpg.zip\t\t     TFM_Multiwalker_TD3_gym_cap.ipynb\n"," multiwalker_ppo_log_eval\t     TFM_PPO_new_pettingzoo_gym_cap.ipynb\n"," multiwalker_ppo.zip\t\t     TFM_PPO_pettingzoo_gym_cap.ipynb\n"]}]},{"cell_type":"code","source":["env = multiwalker_v9.parallel_env(render_mode=\"rgb_array\",shared_reward=0.4)\n","\n","env = ss.frame_stack_v1(env, 3)\n","env = ss.pettingzoo_env_to_vec_env_v1(env)\n","env = ss.concat_vec_envs_v1(env, 8, num_cpus=4, base_class='stable_baselines3')"],"metadata":{"id":"oBQ_pARy9AxT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3.common.callbacks import EvalCallback\n","eval_callback = EvalCallback(env, best_model_save_path=\"./multiwalker_sac3_04_log_eval/\",\n","                             log_path=\"./multiwalker_sac3_04_log_eval/\", eval_freq=200,\n","                             deterministic=False, render=False)\n"],"metadata":{"id":"iN3G9S1T9AxU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3.common.logger import configure\n","\n","tmp_path = \"multiwalker_sac3_04_log_eval/\"\n","# set up logger\n","new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n","\n","model = SAC(\"MlpPolicy\", env, verbose=3,batch_size=512, learning_rate=0.001,gamma=0.9,tau=0.01) #,train_freq=1\n","\n","model.set_logger(new_logger)\n","model.learn(total_timesteps=4000000, log_interval=10,callback=eval_callback)\n","model.save(\"multiwalker_sac3_04\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"caf85c78-cb94-4072-97a5-601b7adbe5ae","id":"eyrgIrJl9AxV","executionInfo":{"status":"ok","timestamp":1699034279141,"user_tz":-60,"elapsed":6177476,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Logging to multiwalker_sac3_04_log_eval/\n","Using cuda device\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 10       |\n","|    fps             | 203      |\n","|    time_elapsed    | 7        |\n","|    total_timesteps | 1464     |\n","| train/             |          |\n","|    actor_loss      | -5.52    |\n","|    critic_loss     | 37.1     |\n","|    ent_coef        | 0.946    |\n","|    ent_coef_loss   | -0.371   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 56       |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 20       |\n","|    fps             | 227      |\n","|    time_elapsed    | 7        |\n","|    total_timesteps | 1728     |\n","| train/             |          |\n","|    actor_loss      | -4.55    |\n","|    critic_loss     | 51.9     |\n","|    ent_coef        | 0.936    |\n","|    ent_coef_loss   | -0.445   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 67       |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 30       |\n","|    fps             | 343      |\n","|    time_elapsed    | 8        |\n","|    total_timesteps | 3024     |\n","| train/             |          |\n","|    actor_loss      | -5.41    |\n","|    critic_loss     | 37.2     |\n","|    ent_coef        | 0.887    |\n","|    ent_coef_loss   | -0.799   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 121      |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40       |\n","|    fps             | 365      |\n","|    time_elapsed    | 9        |\n","|    total_timesteps | 3336     |\n","| train/             |          |\n","|    actor_loss      | -5       |\n","|    critic_loss     | 97.8     |\n","|    ent_coef        | 0.876    |\n","|    ent_coef_loss   | -0.893   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 134      |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 50       |\n","|    fps             | 427      |\n","|    time_elapsed    | 10       |\n","|    total_timesteps | 4560     |\n","| train/             |          |\n","|    actor_loss      | -5.21    |\n","|    critic_loss     | 26.4     |\n","|    ent_coef        | 0.832    |\n","|    ent_coef_loss   | -1.21    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 185      |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 60       |\n","|    fps             | 427      |\n","|    time_elapsed    | 10       |\n","|    total_timesteps | 4584     |\n","| train/             |          |\n","|    actor_loss      | -5.13    |\n","|    critic_loss     | 79.4     |\n","|    ent_coef        | 0.831    |\n","|    ent_coef_loss   | -1.22    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 186      |\n","---------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n","|    actor_loss      | 9.64     |\n","|    critic_loss     | 2.61     |\n","|    ent_coef        | 0.00742  |\n","|    ent_coef_loss   | -2.49    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155285   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 37620    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5754     |\n","|    total_timesteps | 3728280  |\n","| train/             |          |\n","|    actor_loss      | 9.61     |\n","|    critic_loss     | 4.3      |\n","|    ent_coef        | 0.00726  |\n","|    ent_coef_loss   | 0.302    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155340   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 37630    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5754     |\n","|    total_timesteps | 3728736  |\n","| train/             |          |\n","|    actor_loss      | 9.59     |\n","|    critic_loss     | 17.3     |\n","|    ent_coef        | 0.00731  |\n","|    ent_coef_loss   | 0.221    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155359   |\n","---------------------------------\n","Eval num_timesteps=3729600, episode_reward=-89.46 +/- 4.38\n","Episode length: 92.20 +/- 8.33\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 92.2     |\n","|    mean_reward     | -89.5    |\n","| time/              |          |\n","|    total_timesteps | 3729600  |\n","| train/             |          |\n","|    actor_loss      | 9.72     |\n","|    critic_loss     | 12.8     |\n","|    ent_coef        | 0.00748  |\n","|    ent_coef_loss   | 3.08     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 37640    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5758     |\n","|    total_timesteps | 3730560  |\n","| train/             |          |\n","|    actor_loss      | 8.73     |\n","|    critic_loss     | 4.45     |\n","|    ent_coef        | 0.00782  |\n","|    ent_coef_loss   | 0.0136   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155435   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 37650    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5758     |\n","|    total_timesteps | 3730752  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 22.6     |\n","|    ent_coef        | 0.00786  |\n","|    ent_coef_loss   | 2.53     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155443   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 37660    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5759     |\n","|    total_timesteps | 3731232  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 14.6     |\n","|    ent_coef        | 0.00804  |\n","|    ent_coef_loss   | 0.173    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155463   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 37670    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5760     |\n","|    total_timesteps | 3732384  |\n","| train/             |          |\n","|    actor_loss      | 10.1     |\n","|    critic_loss     | 17.4     |\n","|    ent_coef        | 0.00824  |\n","|    ent_coef_loss   | -2.16    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155511   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 37680    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5761     |\n","|    total_timesteps | 3732624  |\n","| train/             |          |\n","|    actor_loss      | 10.1     |\n","|    critic_loss     | 4.59     |\n","|    ent_coef        | 0.00819  |\n","|    ent_coef_loss   | -0.967   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155521   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 37690    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5763     |\n","|    total_timesteps | 3733944  |\n","| train/             |          |\n","|    actor_loss      | 8.49     |\n","|    critic_loss     | 16.5     |\n","|    ent_coef        | 0.0083   |\n","|    ent_coef_loss   | -2.9     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155576   |\n","---------------------------------\n","Eval num_timesteps=3734400, episode_reward=-55.23 +/- 40.40\n","Episode length: 246.20 +/- 207.23\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 246      |\n","|    mean_reward     | -55.2    |\n","| time/              |          |\n","|    total_timesteps | 3734400  |\n","| train/             |          |\n","|    actor_loss      | 8.11     |\n","|    critic_loss     | 11.2     |\n","|    ent_coef        | 0.0082   |\n","|    ent_coef_loss   | -4.14    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155595   |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    episodes        | 37700   |\n","|    fps             | 647     |\n","|    time_elapsed    | 5771    |\n","|    total_timesteps | 3734400 |\n","--------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 37710    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5771     |\n","|    total_timesteps | 3735216  |\n","| train/             |          |\n","|    actor_loss      | 9.66     |\n","|    critic_loss     | 25.4     |\n","|    ent_coef        | 0.00801  |\n","|    ent_coef_loss   | 0.0503   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155629   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 37720    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5772     |\n","|    total_timesteps | 3736224  |\n","| train/             |          |\n","|    actor_loss      | 9.88     |\n","|    critic_loss     | 14.3     |\n","|    ent_coef        | 0.00802  |\n","|    ent_coef_loss   | -0.97    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155671   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 37730    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5773     |\n","|    total_timesteps | 3736896  |\n","| train/             |          |\n","|    actor_loss      | 10.4     |\n","|    critic_loss     | 23.2     |\n","|    ent_coef        | 0.00793  |\n","|    ent_coef_loss   | -1       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155699   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 37740    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5774     |\n","|    total_timesteps | 3738120  |\n","| train/             |          |\n","|    actor_loss      | 10.8     |\n","|    critic_loss     | 15.2     |\n","|    ent_coef        | 0.00746  |\n","|    ent_coef_loss   | 1.01     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155750   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 37750    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5775     |\n","|    total_timesteps | 3739176  |\n","| train/             |          |\n","|    actor_loss      | 9.14     |\n","|    critic_loss     | 13.2     |\n","|    ent_coef        | 0.00727  |\n","|    ent_coef_loss   | 0.475    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155794   |\n","---------------------------------\n","Eval num_timesteps=3739200, episode_reward=-85.05 +/- 3.50\n","Episode length: 84.60 +/- 7.84\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 84.6     |\n","|    mean_reward     | -85      |\n","| time/              |          |\n","|    total_timesteps | 3739200  |\n","| train/             |          |\n","|    actor_loss      | 9.02     |\n","|    critic_loss     | 15.4     |\n","|    ent_coef        | 0.00727  |\n","|    ent_coef_loss   | 0.688    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 37760    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5777     |\n","|    total_timesteps | 3740616  |\n","| train/             |          |\n","|    actor_loss      | 8.91     |\n","|    critic_loss     | 15       |\n","|    ent_coef        | 0.00731  |\n","|    ent_coef_loss   | -0.106   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155854   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 37770    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5778     |\n","|    total_timesteps | 3741456  |\n","| train/             |          |\n","|    actor_loss      | 9.75     |\n","|    critic_loss     | 7.38     |\n","|    ent_coef        | 0.00723  |\n","|    ent_coef_loss   | -0.435   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155889   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 37780    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5778     |\n","|    total_timesteps | 3742128  |\n","| train/             |          |\n","|    actor_loss      | 10.4     |\n","|    critic_loss     | 19.3     |\n","|    ent_coef        | 0.00729  |\n","|    ent_coef_loss   | -0.237   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155917   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 37790    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5780     |\n","|    total_timesteps | 3743376  |\n","| train/             |          |\n","|    actor_loss      | 9.78     |\n","|    critic_loss     | 18.6     |\n","|    ent_coef        | 0.00732  |\n","|    ent_coef_loss   | -1.49    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155969   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 37800    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5780     |\n","|    total_timesteps | 3743616  |\n","| train/             |          |\n","|    actor_loss      | 9.92     |\n","|    critic_loss     | 2.86     |\n","|    ent_coef        | 0.00731  |\n","|    ent_coef_loss   | -1.29    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155979   |\n","---------------------------------\n","Eval num_timesteps=3744000, episode_reward=-86.43 +/- 3.38\n","Episode length: 121.20 +/- 5.88\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 121      |\n","|    mean_reward     | -86.4    |\n","| time/              |          |\n","|    total_timesteps | 3744000  |\n","| train/             |          |\n","|    actor_loss      | 9.99     |\n","|    critic_loss     | 11.3     |\n","|    ent_coef        | 0.00723  |\n","|    ent_coef_loss   | -0.948   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 37810    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5785     |\n","|    total_timesteps | 3745032  |\n","| train/             |          |\n","|    actor_loss      | 9.46     |\n","|    critic_loss     | 20.3     |\n","|    ent_coef        | 0.00713  |\n","|    ent_coef_loss   | -0.311   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156038   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 37820    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5787     |\n","|    total_timesteps | 3746016  |\n","| train/             |          |\n","|    actor_loss      | 9.54     |\n","|    critic_loss     | 11.6     |\n","|    ent_coef        | 0.00711  |\n","|    ent_coef_loss   | -0.369   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156079   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 37830    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5788     |\n","|    total_timesteps | 3746736  |\n","| train/             |          |\n","|    actor_loss      | 11.1     |\n","|    critic_loss     | 7.55     |\n","|    ent_coef        | 0.0071   |\n","|    ent_coef_loss   | -0.581   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156109   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 37840    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5788     |\n","|    total_timesteps | 3747240  |\n","| train/             |          |\n","|    actor_loss      | 9.69     |\n","|    critic_loss     | 11       |\n","|    ent_coef        | 0.00704  |\n","|    ent_coef_loss   | 0.76     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156130   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 37850    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5790     |\n","|    total_timesteps | 3748536  |\n","| train/             |          |\n","|    actor_loss      | 9.13     |\n","|    critic_loss     | 2.25     |\n","|    ent_coef        | 0.0069   |\n","|    ent_coef_loss   | -0.341   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156184   |\n","---------------------------------\n","Eval num_timesteps=3748800, episode_reward=-83.48 +/- 2.51\n","Episode length: 78.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 78       |\n","|    mean_reward     | -83.5    |\n","| time/              |          |\n","|    total_timesteps | 3748800  |\n","| train/             |          |\n","|    actor_loss      | 10.4     |\n","|    critic_loss     | 28.1     |\n","|    ent_coef        | 0.00694  |\n","|    ent_coef_loss   | 2.41     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 37860    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5792     |\n","|    total_timesteps | 3748848  |\n","| train/             |          |\n","|    actor_loss      | 9.86     |\n","|    critic_loss     | 22.3     |\n","|    ent_coef        | 0.00695  |\n","|    ent_coef_loss   | 1.61     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156197   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 37870    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5794     |\n","|    total_timesteps | 3750192  |\n","| train/             |          |\n","|    actor_loss      | 9.75     |\n","|    critic_loss     | 5.87     |\n","|    ent_coef        | 0.00693  |\n","|    ent_coef_loss   | -0.0375  |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156253   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 37880    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5795     |\n","|    total_timesteps | 3750864  |\n","| train/             |          |\n","|    actor_loss      | 8.85     |\n","|    critic_loss     | 12.7     |\n","|    ent_coef        | 0.00691  |\n","|    ent_coef_loss   | -1.43    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156281   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 37890    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5797     |\n","|    total_timesteps | 3751968  |\n","| train/             |          |\n","|    actor_loss      | 10.1     |\n","|    critic_loss     | 20.1     |\n","|    ent_coef        | 0.0069   |\n","|    ent_coef_loss   | 3.47     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156327   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 37900    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5798     |\n","|    total_timesteps | 3752760  |\n","| train/             |          |\n","|    actor_loss      | 9.33     |\n","|    critic_loss     | 14.2     |\n","|    ent_coef        | 0.00716  |\n","|    ent_coef_loss   | -0.197   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156360   |\n","---------------------------------\n","Eval num_timesteps=3753600, episode_reward=-80.92 +/- 3.25\n","Episode length: 79.40 +/- 9.31\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 79.4     |\n","|    mean_reward     | -80.9    |\n","| time/              |          |\n","|    total_timesteps | 3753600  |\n","| train/             |          |\n","|    actor_loss      | 9.47     |\n","|    critic_loss     | 5.92     |\n","|    ent_coef        | 0.00742  |\n","|    ent_coef_loss   | 0.155    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 37910    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5800     |\n","|    total_timesteps | 3754584  |\n","| train/             |          |\n","|    actor_loss      | 10.3     |\n","|    critic_loss     | 12.2     |\n","|    ent_coef        | 0.00752  |\n","|    ent_coef_loss   | -0.602   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156436   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 37920    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5801     |\n","|    total_timesteps | 3755568  |\n","| train/             |          |\n","|    actor_loss      | 9.33     |\n","|    critic_loss     | 17.5     |\n","|    ent_coef        | 0.00757  |\n","|    ent_coef_loss   | 0.0457   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156477   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 37930    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5801     |\n","|    total_timesteps | 3755784  |\n","| train/             |          |\n","|    actor_loss      | 9.85     |\n","|    critic_loss     | 10.2     |\n","|    ent_coef        | 0.00762  |\n","|    ent_coef_loss   | -0.776   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156486   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 37940    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5803     |\n","|    total_timesteps | 3757200  |\n","| train/             |          |\n","|    actor_loss      | 8.65     |\n","|    critic_loss     | 6.14     |\n","|    ent_coef        | 0.00752  |\n","|    ent_coef_loss   | -2.93    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156545   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 37950    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5803     |\n","|    total_timesteps | 3757368  |\n","| train/             |          |\n","|    actor_loss      | 8.96     |\n","|    critic_loss     | 15       |\n","|    ent_coef        | 0.0075   |\n","|    ent_coef_loss   | -0.633   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156552   |\n","---------------------------------\n","Eval num_timesteps=3758400, episode_reward=-80.93 +/- 2.70\n","Episode length: 189.60 +/- 50.95\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 190      |\n","|    mean_reward     | -80.9    |\n","| time/              |          |\n","|    total_timesteps | 3758400  |\n","| train/             |          |\n","|    actor_loss      | 8.57     |\n","|    critic_loss     | 8.63     |\n","|    ent_coef        | 0.00738  |\n","|    ent_coef_loss   | -0.192   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 37960    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5808     |\n","|    total_timesteps | 3760344  |\n","| train/             |          |\n","|    actor_loss      | 10.3     |\n","|    critic_loss     | 31.2     |\n","|    ent_coef        | 0.00751  |\n","|    ent_coef_loss   | 1.62     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156676   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 37970    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5811     |\n","|    total_timesteps | 3762408  |\n","| train/             |          |\n","|    actor_loss      | 9.87     |\n","|    critic_loss     | 21.8     |\n","|    ent_coef        | 0.00733  |\n","|    ent_coef_loss   | -1.21    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156762   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 37980    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5811     |\n","|    total_timesteps | 3762696  |\n","| train/             |          |\n","|    actor_loss      | 8.61     |\n","|    critic_loss     | 13.9     |\n","|    ent_coef        | 0.00724  |\n","|    ent_coef_loss   | -2.78    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156774   |\n","---------------------------------\n","Eval num_timesteps=3763200, episode_reward=-85.84 +/- 6.41\n","Episode length: 103.80 +/- 25.96\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 104      |\n","|    mean_reward     | -85.8    |\n","| time/              |          |\n","|    total_timesteps | 3763200  |\n","| train/             |          |\n","|    actor_loss      | 9.91     |\n","|    critic_loss     | 14.6     |\n","|    ent_coef        | 0.00721  |\n","|    ent_coef_loss   | 1.42     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 37990    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5815     |\n","|    total_timesteps | 3764088  |\n","| train/             |          |\n","|    actor_loss      | 9.11     |\n","|    critic_loss     | 6.61     |\n","|    ent_coef        | 0.00747  |\n","|    ent_coef_loss   | -0.993   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156832   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38000    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5817     |\n","|    total_timesteps | 3765096  |\n","| train/             |          |\n","|    actor_loss      | 10.8     |\n","|    critic_loss     | 22.9     |\n","|    ent_coef        | 0.00754  |\n","|    ent_coef_loss   | 1.73     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156874   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38010    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5818     |\n","|    total_timesteps | 3766632  |\n","| train/             |          |\n","|    actor_loss      | 8.79     |\n","|    critic_loss     | 11       |\n","|    ent_coef        | 0.00741  |\n","|    ent_coef_loss   | -2.49    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156938   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38020    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5819     |\n","|    total_timesteps | 3767784  |\n","| train/             |          |\n","|    actor_loss      | 10.3     |\n","|    critic_loss     | 6.86     |\n","|    ent_coef        | 0.00733  |\n","|    ent_coef_loss   | 0.23     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156986   |\n","---------------------------------\n","Eval num_timesteps=3768000, episode_reward=-84.68 +/- 3.83\n","Episode length: 70.80 +/- 3.43\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 70.8     |\n","|    mean_reward     | -84.7    |\n","| time/              |          |\n","|    total_timesteps | 3768000  |\n","| train/             |          |\n","|    actor_loss      | 9.09     |\n","|    critic_loss     | 5.12     |\n","|    ent_coef        | 0.00732  |\n","|    ent_coef_loss   | 0.795    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38030    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5821     |\n","|    total_timesteps | 3768624  |\n","| train/             |          |\n","|    actor_loss      | 9.66     |\n","|    critic_loss     | 22.6     |\n","|    ent_coef        | 0.00728  |\n","|    ent_coef_loss   | 0.262    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157021   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38040    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5822     |\n","|    total_timesteps | 3769560  |\n","| train/             |          |\n","|    actor_loss      | 9.26     |\n","|    critic_loss     | 5.93     |\n","|    ent_coef        | 0.00741  |\n","|    ent_coef_loss   | 1.41     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157060   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38050    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5822     |\n","|    total_timesteps | 3770328  |\n","| train/             |          |\n","|    actor_loss      | 9.1      |\n","|    critic_loss     | 4.24     |\n","|    ent_coef        | 0.00755  |\n","|    ent_coef_loss   | -1.19    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157092   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38060    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5823     |\n","|    total_timesteps | 3771192  |\n","| train/             |          |\n","|    actor_loss      | 9.17     |\n","|    critic_loss     | 10.4     |\n","|    ent_coef        | 0.00743  |\n","|    ent_coef_loss   | -2.96    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157128   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38070    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5823     |\n","|    total_timesteps | 3771336  |\n","| train/             |          |\n","|    actor_loss      | 9.89     |\n","|    critic_loss     | 13.5     |\n","|    ent_coef        | 0.00739  |\n","|    ent_coef_loss   | 0.517    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157134   |\n","---------------------------------\n","Eval num_timesteps=3772800, episode_reward=-84.71 +/- 3.16\n","Episode length: 85.20 +/- 3.43\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 85.2     |\n","|    mean_reward     | -84.7    |\n","| time/              |          |\n","|    total_timesteps | 3772800  |\n","| train/             |          |\n","|    actor_loss      | 10.1     |\n","|    critic_loss     | 3.08     |\n","|    ent_coef        | 0.00746  |\n","|    ent_coef_loss   | 1.91     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38080    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5827     |\n","|    total_timesteps | 3774336  |\n","| train/             |          |\n","|    actor_loss      | 10.1     |\n","|    critic_loss     | 11.9     |\n","|    ent_coef        | 0.00739  |\n","|    ent_coef_loss   | 2.47     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157259   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38090    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5827     |\n","|    total_timesteps | 3775104  |\n","| train/             |          |\n","|    actor_loss      | 10.7     |\n","|    critic_loss     | 12.2     |\n","|    ent_coef        | 0.00742  |\n","|    ent_coef_loss   | 0.818    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157291   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38100    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5829     |\n","|    total_timesteps | 3776064  |\n","| train/             |          |\n","|    actor_loss      | 9.02     |\n","|    critic_loss     | 3.79     |\n","|    ent_coef        | 0.00748  |\n","|    ent_coef_loss   | 1.51     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157331   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38110    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5829     |\n","|    total_timesteps | 3776592  |\n","| train/             |          |\n","|    actor_loss      | 10.7     |\n","|    critic_loss     | 4.86     |\n","|    ent_coef        | 0.0076   |\n","|    ent_coef_loss   | 0.196    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157353   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38120    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5830     |\n","|    total_timesteps | 3777120  |\n","| train/             |          |\n","|    actor_loss      | 10.5     |\n","|    critic_loss     | 21.1     |\n","|    ent_coef        | 0.00765  |\n","|    ent_coef_loss   | 1.15     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157375   |\n","---------------------------------\n","Eval num_timesteps=3777600, episode_reward=-88.05 +/- 2.94\n","Episode length: 85.60 +/- 4.41\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 85.6     |\n","|    mean_reward     | -88.1    |\n","| time/              |          |\n","|    total_timesteps | 3777600  |\n","| train/             |          |\n","|    actor_loss      | 8.66     |\n","|    critic_loss     | 9.87     |\n","|    ent_coef        | 0.00766  |\n","|    ent_coef_loss   | 0.132    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38130    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5834     |\n","|    total_timesteps | 3778800  |\n","| train/             |          |\n","|    actor_loss      | 8.56     |\n","|    critic_loss     | 12.2     |\n","|    ent_coef        | 0.00765  |\n","|    ent_coef_loss   | -2.38    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157445   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38140    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5835     |\n","|    total_timesteps | 3779232  |\n","| train/             |          |\n","|    actor_loss      | 10.1     |\n","|    critic_loss     | 32.6     |\n","|    ent_coef        | 0.00757  |\n","|    ent_coef_loss   | -1.14    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157463   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38150    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5837     |\n","|    total_timesteps | 3780816  |\n","| train/             |          |\n","|    actor_loss      | 10.6     |\n","|    critic_loss     | 24.1     |\n","|    ent_coef        | 0.00736  |\n","|    ent_coef_loss   | 3.09     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157529   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38160    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5838     |\n","|    total_timesteps | 3781152  |\n","| train/             |          |\n","|    actor_loss      | 10.4     |\n","|    critic_loss     | 11.1     |\n","|    ent_coef        | 0.00745  |\n","|    ent_coef_loss   | 3.64     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157543   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38170    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5839     |\n","|    total_timesteps | 3781920  |\n","| train/             |          |\n","|    actor_loss      | 9.38     |\n","|    critic_loss     | 12.7     |\n","|    ent_coef        | 0.00771  |\n","|    ent_coef_loss   | 1.34     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157575   |\n","---------------------------------\n","Eval num_timesteps=3782400, episode_reward=-85.75 +/- 4.07\n","Episode length: 65.00 +/- 4.90\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 65       |\n","|    mean_reward     | -85.8    |\n","| time/              |          |\n","|    total_timesteps | 3782400  |\n","| train/             |          |\n","|    actor_loss      | 9.06     |\n","|    critic_loss     | 13       |\n","|    ent_coef        | 0.00779  |\n","|    ent_coef_loss   | 0.016    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38180    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5841     |\n","|    total_timesteps | 3782616  |\n","| train/             |          |\n","|    actor_loss      | 10       |\n","|    critic_loss     | 21.4     |\n","|    ent_coef        | 0.00779  |\n","|    ent_coef_loss   | 2.11     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157604   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38190    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5843     |\n","|    total_timesteps | 3784272  |\n","| train/             |          |\n","|    actor_loss      | 9.59     |\n","|    critic_loss     | 17.9     |\n","|    ent_coef        | 0.0079   |\n","|    ent_coef_loss   | -1.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157673   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38200    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5844     |\n","|    total_timesteps | 3784416  |\n","| train/             |          |\n","|    actor_loss      | 9.1      |\n","|    critic_loss     | 16.4     |\n","|    ent_coef        | 0.00789  |\n","|    ent_coef_loss   | -3.34    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157679   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38210    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5845     |\n","|    total_timesteps | 3785520  |\n","| train/             |          |\n","|    actor_loss      | 9.16     |\n","|    critic_loss     | 23.6     |\n","|    ent_coef        | 0.0076   |\n","|    ent_coef_loss   | -0.208   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157725   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38220    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5846     |\n","|    total_timesteps | 3786288  |\n","| train/             |          |\n","|    actor_loss      | 9.54     |\n","|    critic_loss     | 3.66     |\n","|    ent_coef        | 0.0076   |\n","|    ent_coef_loss   | 0.199    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157757   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38230    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5847     |\n","|    total_timesteps | 3787032  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 6.93     |\n","|    ent_coef        | 0.00765  |\n","|    ent_coef_loss   | -0.381   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157788   |\n","---------------------------------\n","Eval num_timesteps=3787200, episode_reward=-87.17 +/- 3.13\n","Episode length: 105.40 +/- 20.09\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 105      |\n","|    mean_reward     | -87.2    |\n","| time/              |          |\n","|    total_timesteps | 3787200  |\n","| train/             |          |\n","|    actor_loss      | 9.13     |\n","|    critic_loss     | 2.77     |\n","|    ent_coef        | 0.00762  |\n","|    ent_coef_loss   | -3.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38240    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5852     |\n","|    total_timesteps | 3788496  |\n","| train/             |          |\n","|    actor_loss      | 9.22     |\n","|    critic_loss     | 23.9     |\n","|    ent_coef        | 0.0072   |\n","|    ent_coef_loss   | -0.486   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157849   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38250    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5853     |\n","|    total_timesteps | 3789048  |\n","| train/             |          |\n","|    actor_loss      | 9.98     |\n","|    critic_loss     | 24       |\n","|    ent_coef        | 0.00708  |\n","|    ent_coef_loss   | -0.675   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38260    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5854     |\n","|    total_timesteps | 3790344  |\n","| train/             |          |\n","|    actor_loss      | 10.4     |\n","|    critic_loss     | 21.5     |\n","|    ent_coef        | 0.00714  |\n","|    ent_coef_loss   | 1.22     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157926   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38270    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5856     |\n","|    total_timesteps | 3791520  |\n","| train/             |          |\n","|    actor_loss      | 9.68     |\n","|    critic_loss     | 26.3     |\n","|    ent_coef        | 0.00716  |\n","|    ent_coef_loss   | 1.09     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157975   |\n","---------------------------------\n","Eval num_timesteps=3792000, episode_reward=-84.12 +/- 3.86\n","Episode length: 85.40 +/- 14.21\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 85.4     |\n","|    mean_reward     | -84.1    |\n","| time/              |          |\n","|    total_timesteps | 3792000  |\n","| train/             |          |\n","|    actor_loss      | 9.57     |\n","|    critic_loss     | 16.8     |\n","|    ent_coef        | 0.00724  |\n","|    ent_coef_loss   | 1.78     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38280    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5859     |\n","|    total_timesteps | 3792432  |\n","| train/             |          |\n","|    actor_loss      | 8.13     |\n","|    critic_loss     | 18.2     |\n","|    ent_coef        | 0.00731  |\n","|    ent_coef_loss   | -1.06    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158013   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38290    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5861     |\n","|    total_timesteps | 3794112  |\n","| train/             |          |\n","|    actor_loss      | 9.86     |\n","|    critic_loss     | 4.68     |\n","|    ent_coef        | 0.0075   |\n","|    ent_coef_loss   | 0.475    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158083   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38300    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5861     |\n","|    total_timesteps | 3794520  |\n","| train/             |          |\n","|    actor_loss      | 10.7     |\n","|    critic_loss     | 27.3     |\n","|    ent_coef        | 0.00752  |\n","|    ent_coef_loss   | 1.5      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158100   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38310    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5861     |\n","|    total_timesteps | 3795072  |\n","| train/             |          |\n","|    actor_loss      | 9.35     |\n","|    critic_loss     | 12       |\n","|    ent_coef        | 0.00762  |\n","|    ent_coef_loss   | 1.83     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158123   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38320    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5862     |\n","|    total_timesteps | 3796248  |\n","| train/             |          |\n","|    actor_loss      | 10.6     |\n","|    critic_loss     | 8.71     |\n","|    ent_coef        | 0.00804  |\n","|    ent_coef_loss   | 0.137    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158172   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38330    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5863     |\n","|    total_timesteps | 3796560  |\n","| train/             |          |\n","|    actor_loss      | 10.4     |\n","|    critic_loss     | 26.4     |\n","|    ent_coef        | 0.00809  |\n","|    ent_coef_loss   | 1.31     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158185   |\n","---------------------------------\n","Eval num_timesteps=3796800, episode_reward=-84.94 +/- 3.16\n","Episode length: 146.00 +/- 19.60\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 146      |\n","|    mean_reward     | -84.9    |\n","| time/              |          |\n","|    total_timesteps | 3796800  |\n","| train/             |          |\n","|    actor_loss      | 9.44     |\n","|    critic_loss     | 20       |\n","|    ent_coef        | 0.00813  |\n","|    ent_coef_loss   | -1.9     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38340    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5866     |\n","|    total_timesteps | 3798168  |\n","| train/             |          |\n","|    actor_loss      | 10       |\n","|    critic_loss     | 24.9     |\n","|    ent_coef        | 0.00794  |\n","|    ent_coef_loss   | 3.59     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158252   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38350    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5867     |\n","|    total_timesteps | 3799272  |\n","| train/             |          |\n","|    actor_loss      | 10.8     |\n","|    critic_loss     | 11.3     |\n","|    ent_coef        | 0.00835  |\n","|    ent_coef_loss   | 0.646    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158298   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38360    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5867     |\n","|    total_timesteps | 3799752  |\n","| train/             |          |\n","|    actor_loss      | 10       |\n","|    critic_loss     | 23.4     |\n","|    ent_coef        | 0.00852  |\n","|    ent_coef_loss   | -0.594   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158318   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38370    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5868     |\n","|    total_timesteps | 3800304  |\n","| train/             |          |\n","|    actor_loss      | 10.1     |\n","|    critic_loss     | 15.9     |\n","|    ent_coef        | 0.0086   |\n","|    ent_coef_loss   | 1.63     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158341   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38380    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5869     |\n","|    total_timesteps | 3801528  |\n","| train/             |          |\n","|    actor_loss      | 9.61     |\n","|    critic_loss     | 13       |\n","|    ent_coef        | 0.00874  |\n","|    ent_coef_loss   | -1.23    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158392   |\n","---------------------------------\n","Eval num_timesteps=3801600, episode_reward=-86.21 +/- 3.79\n","Episode length: 73.20 +/- 8.82\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 73.2     |\n","|    mean_reward     | -86.2    |\n","| time/              |          |\n","|    total_timesteps | 3801600  |\n","| train/             |          |\n","|    actor_loss      | 10       |\n","|    critic_loss     | 22.3     |\n","|    ent_coef        | 0.00874  |\n","|    ent_coef_loss   | 0.447    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38390    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5872     |\n","|    total_timesteps | 3802968  |\n","| train/             |          |\n","|    actor_loss      | 10.1     |\n","|    critic_loss     | 8.3      |\n","|    ent_coef        | 0.00866  |\n","|    ent_coef_loss   | -1.23    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158452   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38400    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5873     |\n","|    total_timesteps | 3803352  |\n","| train/             |          |\n","|    actor_loss      | 9.99     |\n","|    critic_loss     | 18       |\n","|    ent_coef        | 0.00858  |\n","|    ent_coef_loss   | -1.32    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158468   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38410    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5875     |\n","|    total_timesteps | 3804744  |\n","| train/             |          |\n","|    actor_loss      | 9.93     |\n","|    critic_loss     | 22       |\n","|    ent_coef        | 0.00856  |\n","|    ent_coef_loss   | 0.0948   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158526   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38420    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5875     |\n","|    total_timesteps | 3805056  |\n","| train/             |          |\n","|    actor_loss      | 10.7     |\n","|    critic_loss     | 4.35     |\n","|    ent_coef        | 0.00856  |\n","|    ent_coef_loss   | 0.935    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158539   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38430    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5877     |\n","|    total_timesteps | 3806112  |\n","| train/             |          |\n","|    actor_loss      | 11       |\n","|    critic_loss     | 21.7     |\n","|    ent_coef        | 0.00838  |\n","|    ent_coef_loss   | 0.663    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158583   |\n","---------------------------------\n","Eval num_timesteps=3806400, episode_reward=-85.17 +/- 5.79\n","Episode length: 76.40 +/- 10.29\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 76.4     |\n","|    mean_reward     | -85.2    |\n","| time/              |          |\n","|    total_timesteps | 3806400  |\n","| train/             |          |\n","|    actor_loss      | 10.1     |\n","|    critic_loss     | 7.32     |\n","|    ent_coef        | 0.00835  |\n","|    ent_coef_loss   | -0.0739  |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38440    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5880     |\n","|    total_timesteps | 3807960  |\n","| train/             |          |\n","|    actor_loss      | 8.39     |\n","|    critic_loss     | 18.5     |\n","|    ent_coef        | 0.00852  |\n","|    ent_coef_loss   | -0.246   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158660   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38450    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5881     |\n","|    total_timesteps | 3808584  |\n","| train/             |          |\n","|    actor_loss      | 10.5     |\n","|    critic_loss     | 12.2     |\n","|    ent_coef        | 0.00863  |\n","|    ent_coef_loss   | 0.931    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158686   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38460    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5881     |\n","|    total_timesteps | 3809304  |\n","| train/             |          |\n","|    actor_loss      | 9.75     |\n","|    critic_loss     | 7.99     |\n","|    ent_coef        | 0.00872  |\n","|    ent_coef_loss   | -0.237   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158716   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38470    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5882     |\n","|    total_timesteps | 3810504  |\n","| train/             |          |\n","|    actor_loss      | 9.66     |\n","|    critic_loss     | 6.27     |\n","|    ent_coef        | 0.00881  |\n","|    ent_coef_loss   | 1.12     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158766   |\n","---------------------------------\n","Eval num_timesteps=3811200, episode_reward=-82.34 +/- 3.90\n","Episode length: 76.40 +/- 5.39\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 76.4     |\n","|    mean_reward     | -82.3    |\n","| time/              |          |\n","|    total_timesteps | 3811200  |\n","| train/             |          |\n","|    actor_loss      | 10.1     |\n","|    critic_loss     | 7.38     |\n","|    ent_coef        | 0.00891  |\n","|    ent_coef_loss   | 1.11     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38480    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5884     |\n","|    total_timesteps | 3811272  |\n","| train/             |          |\n","|    actor_loss      | 9.44     |\n","|    critic_loss     | 4.4      |\n","|    ent_coef        | 0.00892  |\n","|    ent_coef_loss   | -1.08    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158798   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38490    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5885     |\n","|    total_timesteps | 3813072  |\n","| train/             |          |\n","|    actor_loss      | 9.54     |\n","|    critic_loss     | 5.9      |\n","|    ent_coef        | 0.00837  |\n","|    ent_coef_loss   | -0.152   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158873   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38500    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5887     |\n","|    total_timesteps | 3814800  |\n","| train/             |          |\n","|    actor_loss      | 9.41     |\n","|    critic_loss     | 14.4     |\n","|    ent_coef        | 0.00864  |\n","|    ent_coef_loss   | -0.401   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158945   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38510    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5888     |\n","|    total_timesteps | 3815472  |\n","| train/             |          |\n","|    actor_loss      | 11.1     |\n","|    critic_loss     | 19.3     |\n","|    ent_coef        | 0.00876  |\n","|    ent_coef_loss   | 2.71     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158973   |\n","---------------------------------\n","Eval num_timesteps=3816000, episode_reward=-88.27 +/- 3.29\n","Episode length: 76.80 +/- 5.88\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 76.8     |\n","|    mean_reward     | -88.3    |\n","| time/              |          |\n","|    total_timesteps | 3816000  |\n","| train/             |          |\n","|    actor_loss      | 9.33     |\n","|    critic_loss     | 14.1     |\n","|    ent_coef        | 0.00891  |\n","|    ent_coef_loss   | -1.09    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38520    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5891     |\n","|    total_timesteps | 3817440  |\n","| train/             |          |\n","|    actor_loss      | 8.84     |\n","|    critic_loss     | 1.8      |\n","|    ent_coef        | 0.00878  |\n","|    ent_coef_loss   | -3.25    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159055   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38530    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5891     |\n","|    total_timesteps | 3817776  |\n","| train/             |          |\n","|    actor_loss      | 10.5     |\n","|    critic_loss     | 13.1     |\n","|    ent_coef        | 0.00863  |\n","|    ent_coef_loss   | -1.76    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159069   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38540    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5893     |\n","|    total_timesteps | 3819432  |\n","| train/             |          |\n","|    actor_loss      | 10       |\n","|    critic_loss     | 13       |\n","|    ent_coef        | 0.00809  |\n","|    ent_coef_loss   | -2.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159138   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38550    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5894     |\n","|    total_timesteps | 3819936  |\n","| train/             |          |\n","|    actor_loss      | 10.4     |\n","|    critic_loss     | 12.5     |\n","|    ent_coef        | 0.00798  |\n","|    ent_coef_loss   | 0.331    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159159   |\n","---------------------------------\n","Eval num_timesteps=3820800, episode_reward=-88.78 +/- 5.05\n","Episode length: 82.00 +/- 14.70\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 82       |\n","|    mean_reward     | -88.8    |\n","| time/              |          |\n","|    total_timesteps | 3820800  |\n","| train/             |          |\n","|    actor_loss      | 9.12     |\n","|    critic_loss     | 21.2     |\n","|    ent_coef        | 0.00811  |\n","|    ent_coef_loss   | -1.96    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159195   |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    episodes        | 38560   |\n","|    fps             | 647     |\n","|    time_elapsed    | 5897    |\n","|    total_timesteps | 3820800 |\n","--------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38570    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5898     |\n","|    total_timesteps | 3821160  |\n","| train/             |          |\n","|    actor_loss      | 10.3     |\n","|    critic_loss     | 31.6     |\n","|    ent_coef        | 0.00813  |\n","|    ent_coef_loss   | 1.94     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159210   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38580    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5900     |\n","|    total_timesteps | 3822336  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 11.2     |\n","|    ent_coef        | 0.00819  |\n","|    ent_coef_loss   | -1.46    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159259   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38590    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5903     |\n","|    total_timesteps | 3824136  |\n","| train/             |          |\n","|    actor_loss      | 8.91     |\n","|    critic_loss     | 9.65     |\n","|    ent_coef        | 0.00807  |\n","|    ent_coef_loss   | -2.18    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159334   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38600    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5904     |\n","|    total_timesteps | 3824880  |\n","| train/             |          |\n","|    actor_loss      | 9.99     |\n","|    critic_loss     | 16.6     |\n","|    ent_coef        | 0.00799  |\n","|    ent_coef_loss   | -2.09    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159365   |\n","---------------------------------\n","Eval num_timesteps=3825600, episode_reward=-84.36 +/- 3.36\n","Episode length: 102.40 +/- 0.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 102      |\n","|    mean_reward     | -84.4    |\n","| time/              |          |\n","|    total_timesteps | 3825600  |\n","| train/             |          |\n","|    actor_loss      | 9.96     |\n","|    critic_loss     | 4.39     |\n","|    ent_coef        | 0.00787  |\n","|    ent_coef_loss   | -1.62    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38610    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5907     |\n","|    total_timesteps | 3825648  |\n","| train/             |          |\n","|    actor_loss      | 9.1      |\n","|    critic_loss     | 3.88     |\n","|    ent_coef        | 0.00787  |\n","|    ent_coef_loss   | -1.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159397   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38620    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5909     |\n","|    total_timesteps | 3827448  |\n","| train/             |          |\n","|    actor_loss      | 9.72     |\n","|    critic_loss     | 26.9     |\n","|    ent_coef        | 0.00743  |\n","|    ent_coef_loss   | -0.619   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38630    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5910     |\n","|    total_timesteps | 3828096  |\n","| train/             |          |\n","|    actor_loss      | 10.4     |\n","|    critic_loss     | 7.51     |\n","|    ent_coef        | 0.00735  |\n","|    ent_coef_loss   | 0.96     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159499   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38640    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5911     |\n","|    total_timesteps | 3828936  |\n","| train/             |          |\n","|    actor_loss      | 11       |\n","|    critic_loss     | 16.5     |\n","|    ent_coef        | 0.0073   |\n","|    ent_coef_loss   | 0.455    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159534   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38650    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5911     |\n","|    total_timesteps | 3829680  |\n","| train/             |          |\n","|    actor_loss      | 10.6     |\n","|    critic_loss     | 4.07     |\n","|    ent_coef        | 0.00732  |\n","|    ent_coef_loss   | 1.07     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159565   |\n","---------------------------------\n","Eval num_timesteps=3830400, episode_reward=-84.79 +/- 3.60\n","Episode length: 74.40 +/- 4.41\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 74.4     |\n","|    mean_reward     | -84.8    |\n","| time/              |          |\n","|    total_timesteps | 3830400  |\n","| train/             |          |\n","|    actor_loss      | 10.3     |\n","|    critic_loss     | 20.1     |\n","|    ent_coef        | 0.0075   |\n","|    ent_coef_loss   | 1.98     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38660    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5914     |\n","|    total_timesteps | 3830592  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 10.4     |\n","|    ent_coef        | 0.00755  |\n","|    ent_coef_loss   | 1.1      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159603   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38670    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5916     |\n","|    total_timesteps | 3831960  |\n","| train/             |          |\n","|    actor_loss      | 9.01     |\n","|    critic_loss     | 7.07     |\n","|    ent_coef        | 0.00746  |\n","|    ent_coef_loss   | -3.17    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159660   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38680    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5916     |\n","|    total_timesteps | 3832272  |\n","| train/             |          |\n","|    actor_loss      | 10.6     |\n","|    critic_loss     | 21.2     |\n","|    ent_coef        | 0.0074   |\n","|    ent_coef_loss   | 0.89     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159673   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38690    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5918     |\n","|    total_timesteps | 3833304  |\n","| train/             |          |\n","|    actor_loss      | 9.64     |\n","|    critic_loss     | 7.99     |\n","|    ent_coef        | 0.00718  |\n","|    ent_coef_loss   | -1.49    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159716   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38700    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5918     |\n","|    total_timesteps | 3833808  |\n","| train/             |          |\n","|    actor_loss      | 9.32     |\n","|    critic_loss     | 9.63     |\n","|    ent_coef        | 0.00707  |\n","|    ent_coef_loss   | -1.86    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159737   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38710    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5920     |\n","|    total_timesteps | 3834624  |\n","| train/             |          |\n","|    actor_loss      | 9.74     |\n","|    critic_loss     | 21.7     |\n","|    ent_coef        | 0.00696  |\n","|    ent_coef_loss   | -1.58    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159771   |\n","---------------------------------\n","Eval num_timesteps=3835200, episode_reward=-85.62 +/- 3.17\n","Episode length: 91.80 +/- 11.27\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 91.8     |\n","|    mean_reward     | -85.6    |\n","| time/              |          |\n","|    total_timesteps | 3835200  |\n","| train/             |          |\n","|    actor_loss      | 10.5     |\n","|    critic_loss     | 31.5     |\n","|    ent_coef        | 0.00697  |\n","|    ent_coef_loss   | -0.638   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38720    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5923     |\n","|    total_timesteps | 3836448  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 6.73     |\n","|    ent_coef        | 0.00709  |\n","|    ent_coef_loss   | 0.0685   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159847   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38730    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5923     |\n","|    total_timesteps | 3836880  |\n","| train/             |          |\n","|    actor_loss      | 8.65     |\n","|    critic_loss     | 25.1     |\n","|    ent_coef        | 0.00706  |\n","|    ent_coef_loss   | 0.383    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159865   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38740    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5925     |\n","|    total_timesteps | 3838392  |\n","| train/             |          |\n","|    actor_loss      | 9.61     |\n","|    critic_loss     | 21       |\n","|    ent_coef        | 0.0073   |\n","|    ent_coef_loss   | 1.01     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159928   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38750    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5925     |\n","|    total_timesteps | 3838824  |\n","| train/             |          |\n","|    actor_loss      | 9.4      |\n","|    critic_loss     | 16.9     |\n","|    ent_coef        | 0.00739  |\n","|    ent_coef_loss   | -0.363   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159946   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38760    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5926     |\n","|    total_timesteps | 3839544  |\n","| train/             |          |\n","|    actor_loss      | 9.96     |\n","|    critic_loss     | 11.7     |\n","|    ent_coef        | 0.00746  |\n","|    ent_coef_loss   | 0.416    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159976   |\n","---------------------------------\n","Eval num_timesteps=3840000, episode_reward=-86.94 +/- 4.90\n","Episode length: 70.80 +/- 5.88\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 70.8     |\n","|    mean_reward     | -86.9    |\n","| time/              |          |\n","|    total_timesteps | 3840000  |\n","| train/             |          |\n","|    actor_loss      | 10.8     |\n","|    critic_loss     | 16.8     |\n","|    ent_coef        | 0.0074   |\n","|    ent_coef_loss   | -0.935   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38770    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5927     |\n","|    total_timesteps | 3840096  |\n","| train/             |          |\n","|    actor_loss      | 9.53     |\n","|    critic_loss     | 10.7     |\n","|    ent_coef        | 0.00737  |\n","|    ent_coef_loss   | -2.59    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159999   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38780    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5929     |\n","|    total_timesteps | 3842016  |\n","| train/             |          |\n","|    actor_loss      | 10.3     |\n","|    critic_loss     | 18.4     |\n","|    ent_coef        | 0.00714  |\n","|    ent_coef_loss   | -0.741   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160079   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38790    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5929     |\n","|    total_timesteps | 3842232  |\n","| train/             |          |\n","|    actor_loss      | 9.2      |\n","|    critic_loss     | 14.6     |\n","|    ent_coef        | 0.00711  |\n","|    ent_coef_loss   | -1.86    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160088   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38800    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5929     |\n","|    total_timesteps | 3842664  |\n","| train/             |          |\n","|    actor_loss      | 9.62     |\n","|    critic_loss     | 24.4     |\n","|    ent_coef        | 0.00704  |\n","|    ent_coef_loss   | -0.521   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160106   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38810    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5930     |\n","|    total_timesteps | 3844008  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 5.56     |\n","|    ent_coef        | 0.00696  |\n","|    ent_coef_loss   | -1.36    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160162   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38820    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5931     |\n","|    total_timesteps | 3844608  |\n","| train/             |          |\n","|    actor_loss      | 11.1     |\n","|    critic_loss     | 17.6     |\n","|    ent_coef        | 0.00698  |\n","|    ent_coef_loss   | -0.0264  |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160187   |\n","---------------------------------\n","Eval num_timesteps=3844800, episode_reward=-87.31 +/- 3.42\n","Episode length: 83.80 +/- 5.88\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 83.8     |\n","|    mean_reward     | -87.3    |\n","| time/              |          |\n","|    total_timesteps | 3844800  |\n","| train/             |          |\n","|    actor_loss      | 9.65     |\n","|    critic_loss     | 16.9     |\n","|    ent_coef        | 0.00698  |\n","|    ent_coef_loss   | -1.66    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38830    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5935     |\n","|    total_timesteps | 3846600  |\n","| train/             |          |\n","|    actor_loss      | 10.1     |\n","|    critic_loss     | 3.94     |\n","|    ent_coef        | 0.00673  |\n","|    ent_coef_loss   | 3.02     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160270   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38840    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5935     |\n","|    total_timesteps | 3846888  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 8.62     |\n","|    ent_coef        | 0.00676  |\n","|    ent_coef_loss   | 0.962    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160282   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38850    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5936     |\n","|    total_timesteps | 3847272  |\n","| train/             |          |\n","|    actor_loss      | 10.7     |\n","|    critic_loss     | 6.73     |\n","|    ent_coef        | 0.00685  |\n","|    ent_coef_loss   | 1.84     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160298   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38860    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5938     |\n","|    total_timesteps | 3848616  |\n","| train/             |          |\n","|    actor_loss      | 9.51     |\n","|    critic_loss     | 3.02     |\n","|    ent_coef        | 0.00671  |\n","|    ent_coef_loss   | -2.34    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160354   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38870    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5938     |\n","|    total_timesteps | 3849048  |\n","| train/             |          |\n","|    actor_loss      | 9.31     |\n","|    critic_loss     | 17.9     |\n","|    ent_coef        | 0.00667  |\n","|    ent_coef_loss   | -1.35    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160372   |\n","---------------------------------\n","Eval num_timesteps=3849600, episode_reward=-84.32 +/- 4.00\n","Episode length: 78.80 +/- 6.37\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 78.8     |\n","|    mean_reward     | -84.3    |\n","| time/              |          |\n","|    total_timesteps | 3849600  |\n","| train/             |          |\n","|    actor_loss      | 10.3     |\n","|    critic_loss     | 9.63     |\n","|    ent_coef        | 0.00659  |\n","|    ent_coef_loss   | -0.742   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38880    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5941     |\n","|    total_timesteps | 3849648  |\n","| train/             |          |\n","|    actor_loss      | 9.5      |\n","|    critic_loss     | 18.3     |\n","|    ent_coef        | 0.00658  |\n","|    ent_coef_loss   | -2.06    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160397   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38890    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5943     |\n","|    total_timesteps | 3851496  |\n","| train/             |          |\n","|    actor_loss      | 9        |\n","|    critic_loss     | 33.9     |\n","|    ent_coef        | 0.00641  |\n","|    ent_coef_loss   | -0.81    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160474   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38900    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5943     |\n","|    total_timesteps | 3851736  |\n","| train/             |          |\n","|    actor_loss      | 9.33     |\n","|    critic_loss     | 8.27     |\n","|    ent_coef        | 0.00643  |\n","|    ent_coef_loss   | -1.52    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160484   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38910    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5945     |\n","|    total_timesteps | 3853440  |\n","| train/             |          |\n","|    actor_loss      | 10.8     |\n","|    critic_loss     | 5.52     |\n","|    ent_coef        | 0.00677  |\n","|    ent_coef_loss   | 0.353    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160555   |\n","---------------------------------\n","Eval num_timesteps=3854400, episode_reward=-84.82 +/- 3.32\n","Episode length: 109.60 +/- 12.74\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 110      |\n","|    mean_reward     | -84.8    |\n","| time/              |          |\n","|    total_timesteps | 3854400  |\n","| train/             |          |\n","|    actor_loss      | 11.2     |\n","|    critic_loss     | 12.4     |\n","|    ent_coef        | 0.00695  |\n","|    ent_coef_loss   | 2.42     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38920    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5947     |\n","|    total_timesteps | 3854424  |\n","| train/             |          |\n","|    actor_loss      | 9.29     |\n","|    critic_loss     | 1.86     |\n","|    ent_coef        | 0.00696  |\n","|    ent_coef_loss   | 1.88     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160596   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38930    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5948     |\n","|    total_timesteps | 3856248  |\n","| train/             |          |\n","|    actor_loss      | 9.98     |\n","|    critic_loss     | 4.91     |\n","|    ent_coef        | 0.00707  |\n","|    ent_coef_loss   | 0.611    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38940    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5949     |\n","|    total_timesteps | 3857040  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 14.8     |\n","|    ent_coef        | 0.00731  |\n","|    ent_coef_loss   | 1.51     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160705   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38950    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5949     |\n","|    total_timesteps | 3857544  |\n","| train/             |          |\n","|    actor_loss      | 10.6     |\n","|    critic_loss     | 17.5     |\n","|    ent_coef        | 0.00741  |\n","|    ent_coef_loss   | 3.27     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160726   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38960    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5950     |\n","|    total_timesteps | 3858888  |\n","| train/             |          |\n","|    actor_loss      | 9.96     |\n","|    critic_loss     | 22.1     |\n","|    ent_coef        | 0.00767  |\n","|    ent_coef_loss   | -2.57    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160782   |\n","---------------------------------\n","Eval num_timesteps=3859200, episode_reward=-89.05 +/- 3.47\n","Episode length: 150.60 +/- 54.38\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 151      |\n","|    mean_reward     | -89.1    |\n","| time/              |          |\n","|    total_timesteps | 3859200  |\n","| train/             |          |\n","|    actor_loss      | 10.7     |\n","|    critic_loss     | 9.16     |\n","|    ent_coef        | 0.00765  |\n","|    ent_coef_loss   | 1        |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160795   |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    episodes        | 38970   |\n","|    fps             | 648     |\n","|    time_elapsed    | 5953    |\n","|    total_timesteps | 3859200 |\n","--------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38980    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5955     |\n","|    total_timesteps | 3860328  |\n","| train/             |          |\n","|    actor_loss      | 10.9     |\n","|    critic_loss     | 4.36     |\n","|    ent_coef        | 0.00766  |\n","|    ent_coef_loss   | 1.68     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160842   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 38990    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5955     |\n","|    total_timesteps | 3860568  |\n","| train/             |          |\n","|    actor_loss      | 10.5     |\n","|    critic_loss     | 15.7     |\n","|    ent_coef        | 0.00767  |\n","|    ent_coef_loss   | 0.553    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160852   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39000    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5957     |\n","|    total_timesteps | 3861744  |\n","| train/             |          |\n","|    actor_loss      | 9.58     |\n","|    critic_loss     | 7.67     |\n","|    ent_coef        | 0.0076   |\n","|    ent_coef_loss   | -0.384   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160901   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39010    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5958     |\n","|    total_timesteps | 3862512  |\n","| train/             |          |\n","|    actor_loss      | 10.3     |\n","|    critic_loss     | 5.47     |\n","|    ent_coef        | 0.00758  |\n","|    ent_coef_loss   | 0.794    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160933   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39020    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5960     |\n","|    total_timesteps | 3863256  |\n","| train/             |          |\n","|    actor_loss      | 9.97     |\n","|    critic_loss     | 3.77     |\n","|    ent_coef        | 0.00751  |\n","|    ent_coef_loss   | 0.127    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160964   |\n","---------------------------------\n","Eval num_timesteps=3864000, episode_reward=-83.56 +/- 2.63\n","Episode length: 89.20 +/- 1.47\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 89.2     |\n","|    mean_reward     | -83.6    |\n","| time/              |          |\n","|    total_timesteps | 3864000  |\n","| train/             |          |\n","|    actor_loss      | 8.24     |\n","|    critic_loss     | 5.65     |\n","|    ent_coef        | 0.0076   |\n","|    ent_coef_loss   | -1.66    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39030    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5963     |\n","|    total_timesteps | 3864120  |\n","| train/             |          |\n","|    actor_loss      | 10.3     |\n","|    critic_loss     | 31.2     |\n","|    ent_coef        | 0.00762  |\n","|    ent_coef_loss   | 1.44     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161000   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39040    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5965     |\n","|    total_timesteps | 3865536  |\n","| train/             |          |\n","|    actor_loss      | 9.96     |\n","|    critic_loss     | 19       |\n","|    ent_coef        | 0.0076   |\n","|    ent_coef_loss   | -1.36    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161059   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39050    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5965     |\n","|    total_timesteps | 3865680  |\n","| train/             |          |\n","|    actor_loss      | 10.6     |\n","|    critic_loss     | 19.5     |\n","|    ent_coef        | 0.00758  |\n","|    ent_coef_loss   | 0.597    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161065   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39060    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5967     |\n","|    total_timesteps | 3867096  |\n","| train/             |          |\n","|    actor_loss      | 11       |\n","|    critic_loss     | 18       |\n","|    ent_coef        | 0.00744  |\n","|    ent_coef_loss   | 0.421    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161124   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39070    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5968     |\n","|    total_timesteps | 3867480  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 6.13     |\n","|    ent_coef        | 0.00737  |\n","|    ent_coef_loss   | -0.638   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161140   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39080    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5970     |\n","|    total_timesteps | 3868656  |\n","| train/             |          |\n","|    actor_loss      | 11.2     |\n","|    critic_loss     | 15.1     |\n","|    ent_coef        | 0.00723  |\n","|    ent_coef_loss   | 1.12     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161189   |\n","---------------------------------\n","Eval num_timesteps=3868800, episode_reward=-86.59 +/- 3.46\n","Episode length: 74.20 +/- 8.82\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 74.2     |\n","|    mean_reward     | -86.6    |\n","| time/              |          |\n","|    total_timesteps | 3868800  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 3.56     |\n","|    ent_coef        | 0.00723  |\n","|    ent_coef_loss   | -0.0706  |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39090    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5971     |\n","|    total_timesteps | 3868920  |\n","| train/             |          |\n","|    actor_loss      | 9.8      |\n","|    critic_loss     | 6.67     |\n","|    ent_coef        | 0.00724  |\n","|    ent_coef_loss   | -0.487   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161200   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39100    |\n","|    fps             | 647      |\n","|    time_elapsed    | 5973     |\n","|    total_timesteps | 3870432  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 2.33     |\n","|    ent_coef        | 0.00728  |\n","|    ent_coef_loss   | 0.286    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161263   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39110    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5973     |\n","|    total_timesteps | 3870816  |\n","| train/             |          |\n","|    actor_loss      | 9.81     |\n","|    critic_loss     | 4.12     |\n","|    ent_coef        | 0.00728  |\n","|    ent_coef_loss   | -0.145   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161279   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39120    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5974     |\n","|    total_timesteps | 3871944  |\n","| train/             |          |\n","|    actor_loss      | 9.58     |\n","|    critic_loss     | 17.7     |\n","|    ent_coef        | 0.00736  |\n","|    ent_coef_loss   | -1.19    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161326   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39130    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5974     |\n","|    total_timesteps | 3872496  |\n","| train/             |          |\n","|    actor_loss      | 10.6     |\n","|    critic_loss     | 14.1     |\n","|    ent_coef        | 0.00727  |\n","|    ent_coef_loss   | -0.204   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161349   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39140    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5975     |\n","|    total_timesteps | 3873432  |\n","| train/             |          |\n","|    actor_loss      | 10.6     |\n","|    critic_loss     | 15.9     |\n","|    ent_coef        | 0.00708  |\n","|    ent_coef_loss   | -2.22    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161388   |\n","---------------------------------\n","Eval num_timesteps=3873600, episode_reward=-83.60 +/- 2.83\n","Episode length: 75.00 +/- 2.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 75       |\n","|    mean_reward     | -83.6    |\n","| time/              |          |\n","|    total_timesteps | 3873600  |\n","| train/             |          |\n","|    actor_loss      | 9.5      |\n","|    critic_loss     | 5.31     |\n","|    ent_coef        | 0.00705  |\n","|    ent_coef_loss   | -2.24    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39150    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5977     |\n","|    total_timesteps | 3874104  |\n","| train/             |          |\n","|    actor_loss      | 10       |\n","|    critic_loss     | 28.8     |\n","|    ent_coef        | 0.00696  |\n","|    ent_coef_loss   | 0.182    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161416   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39160    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5979     |\n","|    total_timesteps | 3875736  |\n","| train/             |          |\n","|    actor_loss      | 10.3     |\n","|    critic_loss     | 46.8     |\n","|    ent_coef        | 0.00699  |\n","|    ent_coef_loss   | 3.05     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161484   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39170    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5980     |\n","|    total_timesteps | 3876048  |\n","| train/             |          |\n","|    actor_loss      | 9.74     |\n","|    critic_loss     | 10.1     |\n","|    ent_coef        | 0.00703  |\n","|    ent_coef_loss   | -0.538   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161497   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39180    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5981     |\n","|    total_timesteps | 3877272  |\n","| train/             |          |\n","|    actor_loss      | 10.3     |\n","|    critic_loss     | 15.2     |\n","|    ent_coef        | 0.00707  |\n","|    ent_coef_loss   | 0.611    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161548   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39190    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5982     |\n","|    total_timesteps | 3877848  |\n","| train/             |          |\n","|    actor_loss      | 9.58     |\n","|    critic_loss     | 3.79     |\n","|    ent_coef        | 0.00704  |\n","|    ent_coef_loss   | -2.74    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161572   |\n","---------------------------------\n","Eval num_timesteps=3878400, episode_reward=-87.22 +/- 4.36\n","Episode length: 73.40 +/- 0.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 73.4     |\n","|    mean_reward     | -87.2    |\n","| time/              |          |\n","|    total_timesteps | 3878400  |\n","| train/             |          |\n","|    actor_loss      | 10.8     |\n","|    critic_loss     | 10.5     |\n","|    ent_coef        | 0.00692  |\n","|    ent_coef_loss   | 0.326    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39200    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5985     |\n","|    total_timesteps | 3878544  |\n","| train/             |          |\n","|    actor_loss      | 10.4     |\n","|    critic_loss     | 11.2     |\n","|    ent_coef        | 0.00689  |\n","|    ent_coef_loss   | -1.07    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161601   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39210    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5987     |\n","|    total_timesteps | 3879888  |\n","| train/             |          |\n","|    actor_loss      | 9.72     |\n","|    critic_loss     | 4.89     |\n","|    ent_coef        | 0.00672  |\n","|    ent_coef_loss   | 1.59     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161657   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39220    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5987     |\n","|    total_timesteps | 3880296  |\n","| train/             |          |\n","|    actor_loss      | 9.56     |\n","|    critic_loss     | 29.5     |\n","|    ent_coef        | 0.00682  |\n","|    ent_coef_loss   | -0.17    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161674   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39230    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5988     |\n","|    total_timesteps | 3881568  |\n","| train/             |          |\n","|    actor_loss      | 11.3     |\n","|    critic_loss     | 4.26     |\n","|    ent_coef        | 0.00701  |\n","|    ent_coef_loss   | 1.96     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161727   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39240    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5988     |\n","|    total_timesteps | 3881880  |\n","| train/             |          |\n","|    actor_loss      | 10.6     |\n","|    critic_loss     | 37.2     |\n","|    ent_coef        | 0.00702  |\n","|    ent_coef_loss   | -0.116   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161740   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39250    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5989     |\n","|    total_timesteps | 3882984  |\n","| train/             |          |\n","|    actor_loss      | 10.6     |\n","|    critic_loss     | 3.84     |\n","|    ent_coef        | 0.00686  |\n","|    ent_coef_loss   | 0.592    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161786   |\n","---------------------------------\n","Eval num_timesteps=3883200, episode_reward=-85.67 +/- 4.46\n","Episode length: 129.60 +/- 4.41\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 130      |\n","|    mean_reward     | -85.7    |\n","| time/              |          |\n","|    total_timesteps | 3883200  |\n","| train/             |          |\n","|    actor_loss      | 9.39     |\n","|    critic_loss     | 7.28     |\n","|    ent_coef        | 0.00685  |\n","|    ent_coef_loss   | 0.833    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39260    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5992     |\n","|    total_timesteps | 3884136  |\n","| train/             |          |\n","|    actor_loss      | 9.46     |\n","|    critic_loss     | 12.5     |\n","|    ent_coef        | 0.00685  |\n","|    ent_coef_loss   | -0.111   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161834   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39270    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5992     |\n","|    total_timesteps | 3884856  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 6.97     |\n","|    ent_coef        | 0.00688  |\n","|    ent_coef_loss   | 0.0653   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161864   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39280    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5993     |\n","|    total_timesteps | 3885672  |\n","| train/             |          |\n","|    actor_loss      | 9.48     |\n","|    critic_loss     | 23.1     |\n","|    ent_coef        | 0.00681  |\n","|    ent_coef_loss   | -2.1     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161898   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39290    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5994     |\n","|    total_timesteps | 3886392  |\n","| train/             |          |\n","|    actor_loss      | 10.1     |\n","|    critic_loss     | 7.71     |\n","|    ent_coef        | 0.00671  |\n","|    ent_coef_loss   | 0.205    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161928   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39300    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5994     |\n","|    total_timesteps | 3886704  |\n","| train/             |          |\n","|    actor_loss      | 9.7      |\n","|    critic_loss     | 12.2     |\n","|    ent_coef        | 0.00665  |\n","|    ent_coef_loss   | -1.15    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161941   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39310    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5995     |\n","|    total_timesteps | 3887952  |\n","| train/             |          |\n","|    actor_loss      | 9.95     |\n","|    critic_loss     | 11.8     |\n","|    ent_coef        | 0.00675  |\n","|    ent_coef_loss   | -1.06    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161993   |\n","---------------------------------\n","Eval num_timesteps=3888000, episode_reward=-83.53 +/- 4.02\n","Episode length: 79.60 +/- 1.96\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 79.6     |\n","|    mean_reward     | -83.5    |\n","| time/              |          |\n","|    total_timesteps | 3888000  |\n","| train/             |          |\n","|    actor_loss      | 10.1     |\n","|    critic_loss     | 8.08     |\n","|    ent_coef        | 0.00676  |\n","|    ent_coef_loss   | 0.966    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39320    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5996     |\n","|    total_timesteps | 3888048  |\n","| train/             |          |\n","|    actor_loss      | 11       |\n","|    critic_loss     | 19.3     |\n","|    ent_coef        | 0.00677  |\n","|    ent_coef_loss   | 1.77     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161997   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39330    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5996     |\n","|    total_timesteps | 3888480  |\n","| train/             |          |\n","|    actor_loss      | 9.46     |\n","|    critic_loss     | 5.76     |\n","|    ent_coef        | 0.00681  |\n","|    ent_coef_loss   | -0.584   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162015   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39340    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5998     |\n","|    total_timesteps | 3889872  |\n","| train/             |          |\n","|    actor_loss      | 10.1     |\n","|    critic_loss     | 17.2     |\n","|    ent_coef        | 0.00693  |\n","|    ent_coef_loss   | 0.836    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162073   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39350    |\n","|    fps             | 648      |\n","|    time_elapsed    | 5998     |\n","|    total_timesteps | 3889968  |\n","| train/             |          |\n","|    actor_loss      | 8.66     |\n","|    critic_loss     | 29.9     |\n","|    ent_coef        | 0.00695  |\n","|    ent_coef_loss   | -1.44    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162077   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39360    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6000     |\n","|    total_timesteps | 3891504  |\n","| train/             |          |\n","|    actor_loss      | 10.3     |\n","|    critic_loss     | 8.22     |\n","|    ent_coef        | 0.00736  |\n","|    ent_coef_loss   | 1.52     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162141   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39370    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6001     |\n","|    total_timesteps | 3891912  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 26.8     |\n","|    ent_coef        | 0.00755  |\n","|    ent_coef_loss   | 0.303    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162158   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39380    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6002     |\n","|    total_timesteps | 3892392  |\n","| train/             |          |\n","|    actor_loss      | 10.7     |\n","|    critic_loss     | 11.3     |\n","|    ent_coef        | 0.00763  |\n","|    ent_coef_loss   | -0.429   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162178   |\n","---------------------------------\n","Eval num_timesteps=3892800, episode_reward=-86.47 +/- 4.45\n","Episode length: 76.80 +/- 6.37\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 76.8     |\n","|    mean_reward     | -86.5    |\n","| time/              |          |\n","|    total_timesteps | 3892800  |\n","| train/             |          |\n","|    actor_loss      | 10.9     |\n","|    critic_loss     | 8.41     |\n","|    ent_coef        | 0.0076   |\n","|    ent_coef_loss   | -0.461   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39390    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6006     |\n","|    total_timesteps | 3894384  |\n","| train/             |          |\n","|    actor_loss      | 10.1     |\n","|    critic_loss     | 17.3     |\n","|    ent_coef        | 0.00743  |\n","|    ent_coef_loss   | 0.244    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162261   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39400    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6007     |\n","|    total_timesteps | 3894696  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 4.73     |\n","|    ent_coef        | 0.0074   |\n","|    ent_coef_loss   | 1.01     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162274   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39410    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6008     |\n","|    total_timesteps | 3896208  |\n","| train/             |          |\n","|    actor_loss      | 10.1     |\n","|    critic_loss     | 4.61     |\n","|    ent_coef        | 0.00704  |\n","|    ent_coef_loss   | -1.66    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162337   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39420    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6008     |\n","|    total_timesteps | 3896280  |\n","| train/             |          |\n","|    actor_loss      | 10.4     |\n","|    critic_loss     | 12.7     |\n","|    ent_coef        | 0.00702  |\n","|    ent_coef_loss   | -0.746   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162340   |\n","---------------------------------\n","Eval num_timesteps=3897600, episode_reward=-84.97 +/- 3.23\n","Episode length: 84.40 +/- 6.86\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 84.4     |\n","|    mean_reward     | -85      |\n","| time/              |          |\n","|    total_timesteps | 3897600  |\n","| train/             |          |\n","|    actor_loss      | 10.7     |\n","|    critic_loss     | 11.6     |\n","|    ent_coef        | 0.00688  |\n","|    ent_coef_loss   | 1.71     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39430    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6011     |\n","|    total_timesteps | 3897624  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 16.8     |\n","|    ent_coef        | 0.00688  |\n","|    ent_coef_loss   | 0.247    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162396   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39440    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6013     |\n","|    total_timesteps | 3898728  |\n","| train/             |          |\n","|    actor_loss      | 10.1     |\n","|    critic_loss     | 12.5     |\n","|    ent_coef        | 0.00684  |\n","|    ent_coef_loss   | -1.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162442   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39450    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6013     |\n","|    total_timesteps | 3899136  |\n","| train/             |          |\n","|    actor_loss      | 9.4      |\n","|    critic_loss     | 5.13     |\n","|    ent_coef        | 0.00681  |\n","|    ent_coef_loss   | -1.6     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162459   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39460    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6015     |\n","|    total_timesteps | 3900144  |\n","| train/             |          |\n","|    actor_loss      | 9.97     |\n","|    critic_loss     | 7.8      |\n","|    ent_coef        | 0.00684  |\n","|    ent_coef_loss   | -0.819   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162501   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39470    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6016     |\n","|    total_timesteps | 3901128  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 3.02     |\n","|    ent_coef        | 0.00681  |\n","|    ent_coef_loss   | 0.863    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162542   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39480    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6017     |\n","|    total_timesteps | 3901728  |\n","| train/             |          |\n","|    actor_loss      | 11.1     |\n","|    critic_loss     | 5.95     |\n","|    ent_coef        | 0.00688  |\n","|    ent_coef_loss   | 2.46     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162567   |\n","---------------------------------\n","Eval num_timesteps=3902400, episode_reward=-83.93 +/- 3.68\n","Episode length: 78.80 +/- 8.33\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 78.8     |\n","|    mean_reward     | -83.9    |\n","| time/              |          |\n","|    total_timesteps | 3902400  |\n","| train/             |          |\n","|    actor_loss      | 8.45     |\n","|    critic_loss     | 27.7     |\n","|    ent_coef        | 0.00688  |\n","|    ent_coef_loss   | -1.95    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39490    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6021     |\n","|    total_timesteps | 3903096  |\n","| train/             |          |\n","|    actor_loss      | 9.74     |\n","|    critic_loss     | 2.7      |\n","|    ent_coef        | 0.00679  |\n","|    ent_coef_loss   | 0.745    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162624   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39500    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6023     |\n","|    total_timesteps | 3904224  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 2.02     |\n","|    ent_coef        | 0.00665  |\n","|    ent_coef_loss   | -0.822   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162671   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39510    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6024     |\n","|    total_timesteps | 3904848  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 9.42     |\n","|    ent_coef        | 0.00656  |\n","|    ent_coef_loss   | -0.809   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162697   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39520    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6026     |\n","|    total_timesteps | 3906360  |\n","| train/             |          |\n","|    actor_loss      | 8.31     |\n","|    critic_loss     | 20       |\n","|    ent_coef        | 0.00677  |\n","|    ent_coef_loss   | 1.97     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162760   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39530    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6027     |\n","|    total_timesteps | 3906864  |\n","| train/             |          |\n","|    actor_loss      | 11       |\n","|    critic_loss     | 13       |\n","|    ent_coef        | 0.00693  |\n","|    ent_coef_loss   | 1.08     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162781   |\n","---------------------------------\n","Eval num_timesteps=3907200, episode_reward=-89.26 +/- 3.85\n","Episode length: 95.60 +/- 1.96\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 95.6     |\n","|    mean_reward     | -89.3    |\n","| time/              |          |\n","|    total_timesteps | 3907200  |\n","| train/             |          |\n","|    actor_loss      | 9.84     |\n","|    critic_loss     | 11.2     |\n","|    ent_coef        | 0.00697  |\n","|    ent_coef_loss   | 1.2      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39540    |\n","|    fps             | 647      |\n","|    time_elapsed    | 6030     |\n","|    total_timesteps | 3907512  |\n","| train/             |          |\n","|    actor_loss      | 11.2     |\n","|    critic_loss     | 9.44     |\n","|    ent_coef        | 0.00699  |\n","|    ent_coef_loss   | 0.739    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162808   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39550    |\n","|    fps             | 647      |\n","|    time_elapsed    | 6032     |\n","|    total_timesteps | 3908832  |\n","| train/             |          |\n","|    actor_loss      | 8.61     |\n","|    critic_loss     | 7.44     |\n","|    ent_coef        | 0.00729  |\n","|    ent_coef_loss   | -0.15    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162863   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39560    |\n","|    fps             | 647      |\n","|    time_elapsed    | 6033     |\n","|    total_timesteps | 3909144  |\n","| train/             |          |\n","|    actor_loss      | 10.6     |\n","|    critic_loss     | 2.66     |\n","|    ent_coef        | 0.00731  |\n","|    ent_coef_loss   | 1.14     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162876   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39570    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6035     |\n","|    total_timesteps | 3910920  |\n","| train/             |          |\n","|    actor_loss      | 10.5     |\n","|    critic_loss     | 3.89     |\n","|    ent_coef        | 0.00752  |\n","|    ent_coef_loss   | 2.73     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162950   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39580    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6035     |\n","|    total_timesteps | 3911136  |\n","| train/             |          |\n","|    actor_loss      | 11.1     |\n","|    critic_loss     | 30       |\n","|    ent_coef        | 0.00754  |\n","|    ent_coef_loss   | 1.94     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162959   |\n","---------------------------------\n","Eval num_timesteps=3912000, episode_reward=-87.81 +/- 2.86\n","Episode length: 99.40 +/- 15.19\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 99.4     |\n","|    mean_reward     | -87.8    |\n","| time/              |          |\n","|    total_timesteps | 3912000  |\n","| train/             |          |\n","|    actor_loss      | 10.5     |\n","|    critic_loss     | 14       |\n","|    ent_coef        | 0.00767  |\n","|    ent_coef_loss   | 1.54     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39590    |\n","|    fps             | 647      |\n","|    time_elapsed    | 6037     |\n","|    total_timesteps | 3912264  |\n","| train/             |          |\n","|    actor_loss      | 9.51     |\n","|    critic_loss     | 16.6     |\n","|    ent_coef        | 0.00769  |\n","|    ent_coef_loss   | -1.59    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163006   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39600    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6038     |\n","|    total_timesteps | 3913512  |\n","| train/             |          |\n","|    actor_loss      | 9.68     |\n","|    critic_loss     | 7.43     |\n","|    ent_coef        | 0.00756  |\n","|    ent_coef_loss   | -1.31    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163058   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39610    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6039     |\n","|    total_timesteps | 3914232  |\n","| train/             |          |\n","|    actor_loss      | 9.56     |\n","|    critic_loss     | 11.2     |\n","|    ent_coef        | 0.00761  |\n","|    ent_coef_loss   | -0.49    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163088   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39620    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6040     |\n","|    total_timesteps | 3915576  |\n","| train/             |          |\n","|    actor_loss      | 9.57     |\n","|    critic_loss     | 5.27     |\n","|    ent_coef        | 0.0076   |\n","|    ent_coef_loss   | -0.0501  |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163144   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39630    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6041     |\n","|    total_timesteps | 3916200  |\n","| train/             |          |\n","|    actor_loss      | 9.58     |\n","|    critic_loss     | 11.4     |\n","|    ent_coef        | 0.00771  |\n","|    ent_coef_loss   | 1.5      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163170   |\n","---------------------------------\n","Eval num_timesteps=3916800, episode_reward=-84.41 +/- 3.70\n","Episode length: 71.20 +/- 3.43\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 71.2     |\n","|    mean_reward     | -84.4    |\n","| time/              |          |\n","|    total_timesteps | 3916800  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 12.2     |\n","|    ent_coef        | 0.00791  |\n","|    ent_coef_loss   | 0.428    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39640    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6042     |\n","|    total_timesteps | 3916824  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 7.38     |\n","|    ent_coef        | 0.00791  |\n","|    ent_coef_loss   | -0.351   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163196   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39650    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6042     |\n","|    total_timesteps | 3917304  |\n","| train/             |          |\n","|    actor_loss      | 9.23     |\n","|    critic_loss     | 5.52     |\n","|    ent_coef        | 0.00793  |\n","|    ent_coef_loss   | -0.0339  |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163216   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39660    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6044     |\n","|    total_timesteps | 3918648  |\n","| train/             |          |\n","|    actor_loss      | 11.2     |\n","|    critic_loss     | 8.98     |\n","|    ent_coef        | 0.00791  |\n","|    ent_coef_loss   | 3.82     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39670    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6044     |\n","|    total_timesteps | 3919176  |\n","| train/             |          |\n","|    actor_loss      | 11.1     |\n","|    critic_loss     | 22.1     |\n","|    ent_coef        | 0.0081   |\n","|    ent_coef_loss   | 0.115    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163294   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39680    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6046     |\n","|    total_timesteps | 3920472  |\n","| train/             |          |\n","|    actor_loss      | 10.3     |\n","|    critic_loss     | 7.2      |\n","|    ent_coef        | 0.00829  |\n","|    ent_coef_loss   | 0.6      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163348   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39690    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6047     |\n","|    total_timesteps | 3921072  |\n","| train/             |          |\n","|    actor_loss      | 10.5     |\n","|    critic_loss     | 10.8     |\n","|    ent_coef        | 0.00828  |\n","|    ent_coef_loss   | -0.846   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163373   |\n","---------------------------------\n","Eval num_timesteps=3921600, episode_reward=-82.69 +/- 3.31\n","Episode length: 74.20 +/- 3.92\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 74.2     |\n","|    mean_reward     | -82.7    |\n","| time/              |          |\n","|    total_timesteps | 3921600  |\n","| train/             |          |\n","|    actor_loss      | 9.67     |\n","|    critic_loss     | 2.01     |\n","|    ent_coef        | 0.00825  |\n","|    ent_coef_loss   | 0.802    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39700    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6051     |\n","|    total_timesteps | 3922992  |\n","| train/             |          |\n","|    actor_loss      | 10.7     |\n","|    critic_loss     | 10.9     |\n","|    ent_coef        | 0.00806  |\n","|    ent_coef_loss   | -1.5     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163453   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39710    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6051     |\n","|    total_timesteps | 3923160  |\n","| train/             |          |\n","|    actor_loss      | 9.55     |\n","|    critic_loss     | 17.4     |\n","|    ent_coef        | 0.00801  |\n","|    ent_coef_loss   | -1.7     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163460   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39720    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6052     |\n","|    total_timesteps | 3923520  |\n","| train/             |          |\n","|    actor_loss      | 10.5     |\n","|    critic_loss     | 14.9     |\n","|    ent_coef        | 0.00794  |\n","|    ent_coef_loss   | -0.348   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163475   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39730    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6053     |\n","|    total_timesteps | 3924840  |\n","| train/             |          |\n","|    actor_loss      | 10       |\n","|    critic_loss     | 37.1     |\n","|    ent_coef        | 0.0081   |\n","|    ent_coef_loss   | -1.95    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163530   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39740    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6054     |\n","|    total_timesteps | 3925200  |\n","| train/             |          |\n","|    actor_loss      | 10.5     |\n","|    critic_loss     | 34       |\n","|    ent_coef        | 0.00807  |\n","|    ent_coef_loss   | 0.523    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163545   |\n","---------------------------------\n","Eval num_timesteps=3926400, episode_reward=-88.31 +/- 2.88\n","Episode length: 72.20 +/- 0.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 72.2     |\n","|    mean_reward     | -88.3    |\n","| time/              |          |\n","|    total_timesteps | 3926400  |\n","| train/             |          |\n","|    actor_loss      | 10.5     |\n","|    critic_loss     | 16.1     |\n","|    ent_coef        | 0.00801  |\n","|    ent_coef_loss   | -1.42    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39750    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6056     |\n","|    total_timesteps | 3926448  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 3.64     |\n","|    ent_coef        | 0.008    |\n","|    ent_coef_loss   | -1.7     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163597   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39760    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6057     |\n","|    total_timesteps | 3927960  |\n","| train/             |          |\n","|    actor_loss      | 9.69     |\n","|    critic_loss     | 4.78     |\n","|    ent_coef        | 0.00785  |\n","|    ent_coef_loss   | -0.849   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163660   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39770    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6058     |\n","|    total_timesteps | 3928392  |\n","| train/             |          |\n","|    actor_loss      | 10.9     |\n","|    critic_loss     | 7.29     |\n","|    ent_coef        | 0.00775  |\n","|    ent_coef_loss   | 0.425    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163678   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39780    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6058     |\n","|    total_timesteps | 3928536  |\n","| train/             |          |\n","|    actor_loss      | 10.3     |\n","|    critic_loss     | 19.6     |\n","|    ent_coef        | 0.00772  |\n","|    ent_coef_loss   | 0.731    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163684   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39790    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6059     |\n","|    total_timesteps | 3929928  |\n","| train/             |          |\n","|    actor_loss      | 11.2     |\n","|    critic_loss     | 7.16     |\n","|    ent_coef        | 0.00775  |\n","|    ent_coef_loss   | 2.24     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163742   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39800    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6059     |\n","|    total_timesteps | 3930000  |\n","| train/             |          |\n","|    actor_loss      | 10.7     |\n","|    critic_loss     | 15.5     |\n","|    ent_coef        | 0.00776  |\n","|    ent_coef_loss   | 0.839    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163745   |\n","---------------------------------\n","Eval num_timesteps=3931200, episode_reward=-84.83 +/- 2.95\n","Episode length: 103.80 +/- 23.52\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 104      |\n","|    mean_reward     | -84.8    |\n","| time/              |          |\n","|    total_timesteps | 3931200  |\n","| train/             |          |\n","|    actor_loss      | 10.3     |\n","|    critic_loss     | 13.6     |\n","|    ent_coef        | 0.00761  |\n","|    ent_coef_loss   | -0.64    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39810    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6062     |\n","|    total_timesteps | 3931392  |\n","| train/             |          |\n","|    actor_loss      | 10.5     |\n","|    critic_loss     | 4.96     |\n","|    ent_coef        | 0.00761  |\n","|    ent_coef_loss   | 0.233    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163803   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39820    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6062     |\n","|    total_timesteps | 3932232  |\n","| train/             |          |\n","|    actor_loss      | 10.8     |\n","|    critic_loss     | 2.89     |\n","|    ent_coef        | 0.0076   |\n","|    ent_coef_loss   | -0.451   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163838   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39830    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6063     |\n","|    total_timesteps | 3933048  |\n","| train/             |          |\n","|    actor_loss      | 9.79     |\n","|    critic_loss     | 17.8     |\n","|    ent_coef        | 0.00747  |\n","|    ent_coef_loss   | -0.943   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39840    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6063     |\n","|    total_timesteps | 3933312  |\n","| train/             |          |\n","|    actor_loss      | 9.93     |\n","|    critic_loss     | 24.6     |\n","|    ent_coef        | 0.0074   |\n","|    ent_coef_loss   | 0.218    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163883   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39850    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6065     |\n","|    total_timesteps | 3934872  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 2.99     |\n","|    ent_coef        | 0.00739  |\n","|    ent_coef_loss   | 1.86     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163948   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39860    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6065     |\n","|    total_timesteps | 3935256  |\n","| train/             |          |\n","|    actor_loss      | 9.74     |\n","|    critic_loss     | 10.1     |\n","|    ent_coef        | 0.00747  |\n","|    ent_coef_loss   | 0.724    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163964   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39870    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6066     |\n","|    total_timesteps | 3935664  |\n","| train/             |          |\n","|    actor_loss      | 11.1     |\n","|    critic_loss     | 10.9     |\n","|    ent_coef        | 0.00753  |\n","|    ent_coef_loss   | 2.44     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163981   |\n","---------------------------------\n","Eval num_timesteps=3936000, episode_reward=-85.19 +/- 3.92\n","Episode length: 70.60 +/- 2.94\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 70.6     |\n","|    mean_reward     | -85.2    |\n","| time/              |          |\n","|    total_timesteps | 3936000  |\n","| train/             |          |\n","|    actor_loss      | 10.4     |\n","|    critic_loss     | 16.9     |\n","|    ent_coef        | 0.00761  |\n","|    ent_coef_loss   | 2.64     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39880    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6068     |\n","|    total_timesteps | 3936432  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 13.1     |\n","|    ent_coef        | 0.00779  |\n","|    ent_coef_loss   | 1.72     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164013   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39890    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6071     |\n","|    total_timesteps | 3938280  |\n","| train/             |          |\n","|    actor_loss      | 10.3     |\n","|    critic_loss     | 7.92     |\n","|    ent_coef        | 0.00804  |\n","|    ent_coef_loss   | 1.36     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164090   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39900    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6071     |\n","|    total_timesteps | 3938424  |\n","| train/             |          |\n","|    actor_loss      | 10.6     |\n","|    critic_loss     | 8.04     |\n","|    ent_coef        | 0.00809  |\n","|    ent_coef_loss   | 0.531    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164096   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39910    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6073     |\n","|    total_timesteps | 3939264  |\n","| train/             |          |\n","|    actor_loss      | 11.3     |\n","|    critic_loss     | 6.89     |\n","|    ent_coef        | 0.00806  |\n","|    ent_coef_loss   | 0.0378   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164131   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39920    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6074     |\n","|    total_timesteps | 3940392  |\n","| train/             |          |\n","|    actor_loss      | 11.4     |\n","|    critic_loss     | 41.1     |\n","|    ent_coef        | 0.00784  |\n","|    ent_coef_loss   | -1.31    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164178   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39930    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6075     |\n","|    total_timesteps | 3940776  |\n","| train/             |          |\n","|    actor_loss      | 11.3     |\n","|    critic_loss     | 16.6     |\n","|    ent_coef        | 0.00773  |\n","|    ent_coef_loss   | 0.792    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164194   |\n","---------------------------------\n","Eval num_timesteps=3940800, episode_reward=-90.15 +/- 2.76\n","Episode length: 82.60 +/- 7.84\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 82.6     |\n","|    mean_reward     | -90.1    |\n","| time/              |          |\n","|    total_timesteps | 3940800  |\n","| train/             |          |\n","|    actor_loss      | 10.7     |\n","|    critic_loss     | 18.7     |\n","|    ent_coef        | 0.00772  |\n","|    ent_coef_loss   | 0.199    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39940    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6079     |\n","|    total_timesteps | 3942360  |\n","| train/             |          |\n","|    actor_loss      | 9.88     |\n","|    critic_loss     | 14.4     |\n","|    ent_coef        | 0.00769  |\n","|    ent_coef_loss   | 0.354    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164260   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39950    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6079     |\n","|    total_timesteps | 3942480  |\n","| train/             |          |\n","|    actor_loss      | 9.9      |\n","|    critic_loss     | 10.2     |\n","|    ent_coef        | 0.0077   |\n","|    ent_coef_loss   | 1.52     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164265   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39960    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6081     |\n","|    total_timesteps | 3943896  |\n","| train/             |          |\n","|    actor_loss      | 10.4     |\n","|    critic_loss     | 12.9     |\n","|    ent_coef        | 0.00747  |\n","|    ent_coef_loss   | 0.428    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164324   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39970    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6082     |\n","|    total_timesteps | 3944832  |\n","| train/             |          |\n","|    actor_loss      | 8.62     |\n","|    critic_loss     | 4.95     |\n","|    ent_coef        | 0.0073   |\n","|    ent_coef_loss   | -1.28    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164363   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39980    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6083     |\n","|    total_timesteps | 3945528  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 7.59     |\n","|    ent_coef        | 0.00747  |\n","|    ent_coef_loss   | 2.35     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164392   |\n","---------------------------------\n","Eval num_timesteps=3945600, episode_reward=-86.64 +/- 4.68\n","Episode length: 96.00 +/- 2.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 96       |\n","|    mean_reward     | -86.6    |\n","| time/              |          |\n","|    total_timesteps | 3945600  |\n","| train/             |          |\n","|    actor_loss      | 10.4     |\n","|    critic_loss     | 22.9     |\n","|    ent_coef        | 0.00749  |\n","|    ent_coef_loss   | 0.0458   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 39990    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6086     |\n","|    total_timesteps | 3946224  |\n","| train/             |          |\n","|    actor_loss      | 9.67     |\n","|    critic_loss     | 15.8     |\n","|    ent_coef        | 0.00765  |\n","|    ent_coef_loss   | 1.5      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164421   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40000    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6088     |\n","|    total_timesteps | 3947856  |\n","| train/             |          |\n","|    actor_loss      | 8.82     |\n","|    critic_loss     | 15.6     |\n","|    ent_coef        | 0.00762  |\n","|    ent_coef_loss   | -1.47    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164489   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40010    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6090     |\n","|    total_timesteps | 3948936  |\n","| train/             |          |\n","|    actor_loss      | 9.92     |\n","|    critic_loss     | 8.93     |\n","|    ent_coef        | 0.00762  |\n","|    ent_coef_loss   | -0.489   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164534   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40020    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6091     |\n","|    total_timesteps | 3949944  |\n","| train/             |          |\n","|    actor_loss      | 10.7     |\n","|    critic_loss     | 24       |\n","|    ent_coef        | 0.00767  |\n","|    ent_coef_loss   | -2.19    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164576   |\n","---------------------------------\n","Eval num_timesteps=3950400, episode_reward=-91.60 +/- 2.63\n","Episode length: 75.80 +/- 0.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 75.8     |\n","|    mean_reward     | -91.6    |\n","| time/              |          |\n","|    total_timesteps | 3950400  |\n","| train/             |          |\n","|    actor_loss      | 9.27     |\n","|    critic_loss     | 22.4     |\n","|    ent_coef        | 0.00755  |\n","|    ent_coef_loss   | -2.02    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40030    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6095     |\n","|    total_timesteps | 3951672  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 20.4     |\n","|    ent_coef        | 0.00728  |\n","|    ent_coef_loss   | 0.567    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164648   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40040    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6096     |\n","|    total_timesteps | 3952248  |\n","| train/             |          |\n","|    actor_loss      | 9.91     |\n","|    critic_loss     | 17.2     |\n","|    ent_coef        | 0.00725  |\n","|    ent_coef_loss   | -0.215   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40050    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6096     |\n","|    total_timesteps | 3952440  |\n","| train/             |          |\n","|    actor_loss      | 10.7     |\n","|    critic_loss     | 13.4     |\n","|    ent_coef        | 0.00724  |\n","|    ent_coef_loss   | 0.229    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164680   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40060    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6098     |\n","|    total_timesteps | 3954432  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 15.5     |\n","|    ent_coef        | 0.00736  |\n","|    ent_coef_loss   | 0.659    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164763   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40070    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6099     |\n","|    total_timesteps | 3955008  |\n","| train/             |          |\n","|    actor_loss      | 9.52     |\n","|    critic_loss     | 6.3      |\n","|    ent_coef        | 0.00742  |\n","|    ent_coef_loss   | 1.09     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164787   |\n","---------------------------------\n","Eval num_timesteps=3955200, episode_reward=-84.69 +/- 3.91\n","Episode length: 112.20 +/- 50.46\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 112      |\n","|    mean_reward     | -84.7    |\n","| time/              |          |\n","|    total_timesteps | 3955200  |\n","| train/             |          |\n","|    actor_loss      | 10.1     |\n","|    critic_loss     | 7.46     |\n","|    ent_coef        | 0.00744  |\n","|    ent_coef_loss   | 1.44     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40080    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6102     |\n","|    total_timesteps | 3956640  |\n","| train/             |          |\n","|    actor_loss      | 8.83     |\n","|    critic_loss     | 27.6     |\n","|    ent_coef        | 0.00784  |\n","|    ent_coef_loss   | -0.0552  |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164855   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40090    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6102     |\n","|    total_timesteps | 3957624  |\n","| train/             |          |\n","|    actor_loss      | 9.27     |\n","|    critic_loss     | 13.7     |\n","|    ent_coef        | 0.00805  |\n","|    ent_coef_loss   | 2.59     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164896   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40100    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6103     |\n","|    total_timesteps | 3958248  |\n","| train/             |          |\n","|    actor_loss      | 10.6     |\n","|    critic_loss     | 17.7     |\n","|    ent_coef        | 0.00804  |\n","|    ent_coef_loss   | -0.205   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164922   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40110    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6103     |\n","|    total_timesteps | 3958752  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 18.5     |\n","|    ent_coef        | 0.00791  |\n","|    ent_coef_loss   | -0.861   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164943   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40120    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6104     |\n","|    total_timesteps | 3959904  |\n","| train/             |          |\n","|    actor_loss      | 10.4     |\n","|    critic_loss     | 14.7     |\n","|    ent_coef        | 0.00776  |\n","|    ent_coef_loss   | 1.28     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164991   |\n","---------------------------------\n","Eval num_timesteps=3960000, episode_reward=-84.60 +/- 3.25\n","Episode length: 75.00 +/- 4.90\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 75       |\n","|    mean_reward     | -84.6    |\n","| time/              |          |\n","|    total_timesteps | 3960000  |\n","| train/             |          |\n","|    actor_loss      | 12.1     |\n","|    critic_loss     | 20       |\n","|    ent_coef        | 0.00776  |\n","|    ent_coef_loss   | 2.65     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40130    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6106     |\n","|    total_timesteps | 3961296  |\n","| train/             |          |\n","|    actor_loss      | 8.55     |\n","|    critic_loss     | 24.5     |\n","|    ent_coef        | 0.00764  |\n","|    ent_coef_loss   | -2.98    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165049   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40140    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6107     |\n","|    total_timesteps | 3961728  |\n","| train/             |          |\n","|    actor_loss      | 9.68     |\n","|    critic_loss     | 9.17     |\n","|    ent_coef        | 0.00755  |\n","|    ent_coef_loss   | -0.858   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165067   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40150    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6108     |\n","|    total_timesteps | 3962736  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 20.5     |\n","|    ent_coef        | 0.00748  |\n","|    ent_coef_loss   | -0.805   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165109   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40160    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6109     |\n","|    total_timesteps | 3963192  |\n","| train/             |          |\n","|    actor_loss      | 10.7     |\n","|    critic_loss     | 12.5     |\n","|    ent_coef        | 0.00747  |\n","|    ent_coef_loss   | 1.02     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165128   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40170    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6110     |\n","|    total_timesteps | 3963648  |\n","| train/             |          |\n","|    actor_loss      | 11.2     |\n","|    critic_loss     | 8.06     |\n","|    ent_coef        | 0.00747  |\n","|    ent_coef_loss   | 1.35     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165147   |\n","---------------------------------\n","Eval num_timesteps=3964800, episode_reward=-90.02 +/- 5.77\n","Episode length: 95.80 +/- 5.88\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 95.8     |\n","|    mean_reward     | -90      |\n","| time/              |          |\n","|    total_timesteps | 3964800  |\n","| train/             |          |\n","|    actor_loss      | 9.22     |\n","|    critic_loss     | 2.05     |\n","|    ent_coef        | 0.00741  |\n","|    ent_coef_loss   | 0.912    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40180    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6115     |\n","|    total_timesteps | 3965808  |\n","| train/             |          |\n","|    actor_loss      | 10.7     |\n","|    critic_loss     | 8.37     |\n","|    ent_coef        | 0.00745  |\n","|    ent_coef_loss   | 0.667    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165237   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40190    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6115     |\n","|    total_timesteps | 3966168  |\n","| train/             |          |\n","|    actor_loss      | 10.7     |\n","|    critic_loss     | 16       |\n","|    ent_coef        | 0.00744  |\n","|    ent_coef_loss   | 0.972    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165252   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40200    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6116     |\n","|    total_timesteps | 3966480  |\n","| train/             |          |\n","|    actor_loss      | 9.51     |\n","|    critic_loss     | 14.6     |\n","|    ent_coef        | 0.00744  |\n","|    ent_coef_loss   | -1.17    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165265   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40210    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6117     |\n","|    total_timesteps | 3967896  |\n","| train/             |          |\n","|    actor_loss      | 9.31     |\n","|    critic_loss     | 17.7     |\n","|    ent_coef        | 0.0075   |\n","|    ent_coef_loss   | -0.125   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165324   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40220    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6117     |\n","|    total_timesteps | 3968088  |\n","| train/             |          |\n","|    actor_loss      | 9.77     |\n","|    critic_loss     | 17.5     |\n","|    ent_coef        | 0.00751  |\n","|    ent_coef_loss   | 1.65     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165332   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40230    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6119     |\n","|    total_timesteps | 3969504  |\n","| train/             |          |\n","|    actor_loss      | 10.7     |\n","|    critic_loss     | 9.72     |\n","|    ent_coef        | 0.00739  |\n","|    ent_coef_loss   | -1.28    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165391   |\n","---------------------------------\n","Eval num_timesteps=3969600, episode_reward=-92.41 +/- 2.84\n","Episode length: 83.60 +/- 2.94\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 83.6     |\n","|    mean_reward     | -92.4    |\n","| time/              |          |\n","|    total_timesteps | 3969600  |\n","| train/             |          |\n","|    actor_loss      | 11.3     |\n","|    critic_loss     | 13       |\n","|    ent_coef        | 0.00739  |\n","|    ent_coef_loss   | 1.7      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40240    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6121     |\n","|    total_timesteps | 3971232  |\n","| train/             |          |\n","|    actor_loss      | 8.36     |\n","|    critic_loss     | 11.5     |\n","|    ent_coef        | 0.00769  |\n","|    ent_coef_loss   | -2.43    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165463   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40250    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6121     |\n","|    total_timesteps | 3971376  |\n","| train/             |          |\n","|    actor_loss      | 11.5     |\n","|    critic_loss     | 7.35     |\n","|    ent_coef        | 0.00766  |\n","|    ent_coef_loss   | -1.23    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165469   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40260    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6121     |\n","|    total_timesteps | 3971520  |\n","| train/             |          |\n","|    actor_loss      | 11.3     |\n","|    critic_loss     | 12.5     |\n","|    ent_coef        | 0.00763  |\n","|    ent_coef_loss   | 1.95     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165475   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40270    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6123     |\n","|    total_timesteps | 3972912  |\n","| train/             |          |\n","|    actor_loss      | 10.9     |\n","|    critic_loss     | 13.1     |\n","|    ent_coef        | 0.00789  |\n","|    ent_coef_loss   | 0.233    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165533   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40280    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6123     |\n","|    total_timesteps | 3973320  |\n","| train/             |          |\n","|    actor_loss      | 10.9     |\n","|    critic_loss     | 20.9     |\n","|    ent_coef        | 0.00797  |\n","|    ent_coef_loss   | 2.73     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165550   |\n","---------------------------------\n","Eval num_timesteps=3974400, episode_reward=-88.42 +/- 2.76\n","Episode length: 81.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 81       |\n","|    mean_reward     | -88.4    |\n","| time/              |          |\n","|    total_timesteps | 3974400  |\n","| train/             |          |\n","|    actor_loss      | 11.6     |\n","|    critic_loss     | 2.73     |\n","|    ent_coef        | 0.00853  |\n","|    ent_coef_loss   | 1.46     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40290    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6125     |\n","|    total_timesteps | 3974664  |\n","| train/             |          |\n","|    actor_loss      | 9.51     |\n","|    critic_loss     | 4.48     |\n","|    ent_coef        | 0.00863  |\n","|    ent_coef_loss   | 1.12     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165606   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40300    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6126     |\n","|    total_timesteps | 3975816  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 5.54     |\n","|    ent_coef        | 0.00857  |\n","|    ent_coef_loss   | 0.884    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165654   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40310    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6126     |\n","|    total_timesteps | 3976128  |\n","| train/             |          |\n","|    actor_loss      | 9.42     |\n","|    critic_loss     | 8.73     |\n","|    ent_coef        | 0.00853  |\n","|    ent_coef_loss   | -0.75    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165667   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40320    |\n","|    fps             | 649      |\n","|    time_elapsed    | 6128     |\n","|    total_timesteps | 3977448  |\n","| train/             |          |\n","|    actor_loss      | 9.71     |\n","|    critic_loss     | 19.5     |\n","|    ent_coef        | 0.00833  |\n","|    ent_coef_loss   | 1.11     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165722   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40330    |\n","|    fps             | 649      |\n","|    time_elapsed    | 6128     |\n","|    total_timesteps | 3977856  |\n","| train/             |          |\n","|    actor_loss      | 10.1     |\n","|    critic_loss     | 7.11     |\n","|    ent_coef        | 0.00823  |\n","|    ent_coef_loss   | -1.84    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165739   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40340    |\n","|    fps             | 649      |\n","|    time_elapsed    | 6130     |\n","|    total_timesteps | 3978936  |\n","| train/             |          |\n","|    actor_loss      | 10.7     |\n","|    critic_loss     | 4.13     |\n","|    ent_coef        | 0.00803  |\n","|    ent_coef_loss   | -0.196   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165784   |\n","---------------------------------\n","Eval num_timesteps=3979200, episode_reward=-87.15 +/- 3.23\n","Episode length: 75.20 +/- 3.43\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 75.2     |\n","|    mean_reward     | -87.2    |\n","| time/              |          |\n","|    total_timesteps | 3979200  |\n","| train/             |          |\n","|    actor_loss      | 10.9     |\n","|    critic_loss     | 32       |\n","|    ent_coef        | 0.00801  |\n","|    ent_coef_loss   | -0.249   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40350    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6134     |\n","|    total_timesteps | 3980592  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 6.02     |\n","|    ent_coef        | 0.00771  |\n","|    ent_coef_loss   | -0.708   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165853   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40360    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6134     |\n","|    total_timesteps | 3980952  |\n","| train/             |          |\n","|    actor_loss      | 9.98     |\n","|    critic_loss     | 23.6     |\n","|    ent_coef        | 0.00765  |\n","|    ent_coef_loss   | -2.35    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165868   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40370    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6136     |\n","|    total_timesteps | 3982200  |\n","| train/             |          |\n","|    actor_loss      | 10.8     |\n","|    critic_loss     | 12.3     |\n","|    ent_coef        | 0.00757  |\n","|    ent_coef_loss   | 2.44     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165920   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40380    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6137     |\n","|    total_timesteps | 3982512  |\n","| train/             |          |\n","|    actor_loss      | 10.6     |\n","|    critic_loss     | 15.4     |\n","|    ent_coef        | 0.00765  |\n","|    ent_coef_loss   | -1.27    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165933   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40390    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6138     |\n","|    total_timesteps | 3983640  |\n","| train/             |          |\n","|    actor_loss      | 11.5     |\n","|    critic_loss     | 14.7     |\n","|    ent_coef        | 0.00773  |\n","|    ent_coef_loss   | 0.581    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165980   |\n","---------------------------------\n","Eval num_timesteps=3984000, episode_reward=-86.70 +/- 3.54\n","Episode length: 77.20 +/- 1.47\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 77.2     |\n","|    mean_reward     | -86.7    |\n","| time/              |          |\n","|    total_timesteps | 3984000  |\n","| train/             |          |\n","|    actor_loss      | 10.4     |\n","|    critic_loss     | 25.4     |\n","|    ent_coef        | 0.00773  |\n","|    ent_coef_loss   | -2.13    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40400    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6143     |\n","|    total_timesteps | 3985512  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 2.78     |\n","|    ent_coef        | 0.00748  |\n","|    ent_coef_loss   | 1.13     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166058   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40410    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6143     |\n","|    total_timesteps | 3985752  |\n","| train/             |          |\n","|    actor_loss      | 8.72     |\n","|    critic_loss     | 6.68     |\n","|    ent_coef        | 0.00748  |\n","|    ent_coef_loss   | -0.681   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166068   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40420    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6145     |\n","|    total_timesteps | 3986976  |\n","| train/             |          |\n","|    actor_loss      | 10.6     |\n","|    critic_loss     | 7.56     |\n","|    ent_coef        | 0.00746  |\n","|    ent_coef_loss   | -1.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166119   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40430    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6145     |\n","|    total_timesteps | 3987360  |\n","| train/             |          |\n","|    actor_loss      | 9.6      |\n","|    critic_loss     | 22.2     |\n","|    ent_coef        | 0.00739  |\n","|    ent_coef_loss   | -0.0745  |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166135   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40440    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6147     |\n","|    total_timesteps | 3988368  |\n","| train/             |          |\n","|    actor_loss      | 10.4     |\n","|    critic_loss     | 4.18     |\n","|    ent_coef        | 0.00732  |\n","|    ent_coef_loss   | -1.93    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166177   |\n","---------------------------------\n","Eval num_timesteps=3988800, episode_reward=-87.83 +/- 3.38\n","Episode length: 87.60 +/- 4.41\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 87.6     |\n","|    mean_reward     | -87.8    |\n","| time/              |          |\n","|    total_timesteps | 3988800  |\n","| train/             |          |\n","|    actor_loss      | 11       |\n","|    critic_loss     | 8.42     |\n","|    ent_coef        | 0.00719  |\n","|    ent_coef_loss   | -0.85    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40450    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6150     |\n","|    total_timesteps | 3990696  |\n","| train/             |          |\n","|    actor_loss      | 9.6      |\n","|    critic_loss     | 2.73     |\n","|    ent_coef        | 0.00717  |\n","|    ent_coef_loss   | -0.78    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166274   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40460    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6151     |\n","|    total_timesteps | 3991056  |\n","| train/             |          |\n","|    actor_loss      | 10.3     |\n","|    critic_loss     | 34.4     |\n","|    ent_coef        | 0.00723  |\n","|    ent_coef_loss   | 0.308    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166289   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40470    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6152     |\n","|    total_timesteps | 3992160  |\n","| train/             |          |\n","|    actor_loss      | 10.5     |\n","|    critic_loss     | 11       |\n","|    ent_coef        | 0.00763  |\n","|    ent_coef_loss   | 0.742    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166335   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40480    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6153     |\n","|    total_timesteps | 3992880  |\n","| train/             |          |\n","|    actor_loss      | 10.1     |\n","|    critic_loss     | 6.73     |\n","|    ent_coef        | 0.00769  |\n","|    ent_coef_loss   | -0.845   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166365   |\n","---------------------------------\n","Eval num_timesteps=3993600, episode_reward=-86.84 +/- 4.12\n","Episode length: 85.00 +/- 12.25\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 85       |\n","|    mean_reward     | -86.8    |\n","| time/              |          |\n","|    total_timesteps | 3993600  |\n","| train/             |          |\n","|    actor_loss      | 8.7      |\n","|    critic_loss     | 10.6     |\n","|    ent_coef        | 0.00758  |\n","|    ent_coef_loss   | -1.65    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40490    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6158     |\n","|    total_timesteps | 3994944  |\n","| train/             |          |\n","|    actor_loss      | 9.55     |\n","|    critic_loss     | 5.66     |\n","|    ent_coef        | 0.0075   |\n","|    ent_coef_loss   | -0.375   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166451   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40500    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6159     |\n","|    total_timesteps | 3995592  |\n","| train/             |          |\n","|    actor_loss      | 11.3     |\n","|    critic_loss     | 8.49     |\n","|    ent_coef        | 0.00747  |\n","|    ent_coef_loss   | 0.344    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166478   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40510    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6160     |\n","|    total_timesteps | 3996864  |\n","| train/             |          |\n","|    actor_loss      | 10.7     |\n","|    critic_loss     | 10.7     |\n","|    ent_coef        | 0.0076   |\n","|    ent_coef_loss   | -1.14    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166531   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40520    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6161     |\n","|    total_timesteps | 3997320  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 8.36     |\n","|    ent_coef        | 0.00755  |\n","|    ent_coef_loss   | -1.76    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166550   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40530    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6161     |\n","|    total_timesteps | 3997752  |\n","| train/             |          |\n","|    actor_loss      | 10.8     |\n","|    critic_loss     | 7.39     |\n","|    ent_coef        | 0.00754  |\n","|    ent_coef_loss   | -0.314   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166568   |\n","---------------------------------\n","Eval num_timesteps=3998400, episode_reward=-89.10 +/- 3.80\n","Episode length: 79.60 +/- 2.94\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 79.6     |\n","|    mean_reward     | -89.1    |\n","| time/              |          |\n","|    total_timesteps | 3998400  |\n","| train/             |          |\n","|    actor_loss      | 9.62     |\n","|    critic_loss     | 1.57     |\n","|    ent_coef        | 0.00756  |\n","|    ent_coef_loss   | 0.637    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40540    |\n","|    fps             | 648      |\n","|    time_elapsed    | 6163     |\n","|    total_timesteps | 3998664  |\n","| train/             |          |\n","|    actor_loss      | 11.5     |\n","|    critic_loss     | 13.4     |\n","|    ent_coef        | 0.0076   |\n","|    ent_coef_loss   | 2.23     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166606   |\n","---------------------------------\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"s_YTQjcU9ANN"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"mount_file_id":"1tNH0xWOkEmzP-ic_xuZPO9HHOk_62LLx","authorship_tag":"ABX9TyN8h5VYqyaP1dXId0isk+L1"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}