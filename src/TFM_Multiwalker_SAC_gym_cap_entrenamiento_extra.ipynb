{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"d8JAmEUyj9De","executionInfo":{"status":"ok","timestamp":1699520900186,"user_tz":-60,"elapsed":19,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}}},"outputs":[],"source":["# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n","mount='/content/gdrive'\n","drive_root = mount + \"/My Drive/TFM\"\n","\n","try:\n","  from google.colab import drive\n","  IN_COLAB=True\n","except:\n","  IN_COLAB=False"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22981,"status":"ok","timestamp":1699520923151,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-60},"id":"DrEo9QnxkAne","outputId":"9e1459bc-c5d3-4b2b-a5e5-92f9f4f23a9c"},"outputs":[{"output_type":"stream","name":"stdout","text":["We're running Colab\n","Colab: mounting Google drive on  /content/gdrive\n","Mounted at /content/gdrive\n","\n","Colab: making sure  /content/gdrive/My Drive/TFM  exists.\n","\n","Colab: Changing directory to  /content/gdrive/My Drive/TFM\n","/content/gdrive/My Drive/TFM\n","Archivos en el directorio: \n","['Entrenamientos antiguos sin logs', '.ipynb_checkpoints', 'Entrenamientos_log_no_eval', 'PPO_policies', 'DQN_new_pettingzoo_gym_cap.ipynb', 'multi_car_racing', 'policy_log_eval', 'DQN_policies', 'results_rllib', 'MCR_TFM.ipynb', 'multiwalker_ddpg_log_eval', 'multiwalker_sac_log_eval', 'multiwalker_ddpg.zip', 'multiwalker_ppo_log_eval', 'multiwalker_ppo.zip', 'multiwalker_td3_log_eval', 'multiwalker_sac2_log_eval', 'multiwalker_td3_2_log_eval', 'multiwalker_sac3_log_eval', 'multiwalker_sac3.zip', 'multiwalker_ppo_2_log_eval', 'multiwalker_ddpg2_log_eval', 'multiwalker_ppo_2.zip', 'multiwalker_td3_3_log_eval', 'multiwalker_ddpg2_5_log_eval', 'multiwalker_ppo_rew_08_log_eval', 'multiwalker_ppo_rew_08.zip', 'multiwalker_ppo_08_2_log_eval', 'multiwalker_ddpg2_6_log_eval', 'multiwalker_ppo_08_2.zip', 'multiwalker_sac_08_log_eval', 'multiwalker_sac2_08_log_eval', 'multiwalker_ppo_rew_04_log_eval', 'multiwalker_ppo_rew_04.zip', 'multiwalker_ppo_04_2_log_eval', 'multiwalker_ppo_04_2.zip', 'multiwalker_ppo_rew_0_log_eval', 'multiwalker_sac3_08_log_eval', 'multiwalker_ppo_rew_0.zip', 'multiwalker_ppo_0_2_log_eval', 'multiwalker_sac3_08.zip', 'multiwalker_ppo_0_2.zip', 'multiwalker_sac_00_log_eval', 'multiwalker_sac_04_log_eval', 'multiwalker_sac_00.zip', 'multiwalker_sac2_00_log_eval', 'multiwalker_sac2_04_log_eval', 'multiwalker_sac2_04.zip', 'multiwalker_sac3_04_log_eval', 'multiwalker_sac2_00.zip', 'multiwalker_sac3_00_log_eval', 'multiwalker_sac3_04.zip', 'multiwalker_sac3_00.zip', '0prueba_TD3_TFM_Multiwalker_TD3_gym_cap.ipynb', 'TFM_Multiwalker_SAC_recompensas_08_gym_cap.ipynb', 'TFM_Multiwalker_SAC_recompensas_04_gym_cap.ipynb', 'TFM_Multiwalker_SAC_recompensas_00_gym_cap.ipynb', 'TFM_Multiwalker_SAC_gym_cap.ipynb', 'TFM_Multiwalker_PPO_recompensas_gym_cap.ipynb', 'TFM_Multiwalker_DDPG_recompensas_gym_cap.ipynb', 'TFM_Multiwalker_DDPG_PPO_gym_cap.ipynb', 'TFM_Multiwalker_DDPG_gym_cap.ipynb', '=2.13', 'multiwalker_sac_00_log_eval_extra', 'TFM_PPO_2_KAZ_gym_cap.ipynb', 'TFM_PPO_KAZ_gym_cap.ipynb', 'TFM_Multiwalker_SAC_gym_cap_entrenamiento_extra.ipynb']\n"]}],"source":["# Switch to the directory on the Google Drive that you want to use\n","import os\n","if IN_COLAB:\n","  print(\"We're running Colab\")\n","\n","  if IN_COLAB:\n","    # Mount the Google Drive at mount\n","    print(\"Colab: mounting Google drive on \", mount)\n","\n","    drive.mount(mount)\n","\n","    # Create drive_root if it doesn't exist\n","    create_drive_root = True\n","    if create_drive_root:\n","      print(\"\\nColab: making sure \", drive_root, \" exists.\")\n","      os.makedirs(drive_root, exist_ok=True)\n","\n","    # Change to the directory\n","    print(\"\\nColab: Changing directory to \", drive_root)\n","    %cd $drive_root\n","# Verify we're in the correct working directory\n","%pwd\n","print(\"Archivos en el directorio: \")\n","print(os.listdir())"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xAZDg478kEbs","outputId":"683d32cf-736c-4401-ee6d-3f1bfd31aea5","executionInfo":{"status":"ok","timestamp":1699521148890,"user_tz":-60,"elapsed":225756,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gym==0.17.3\n","  Downloading gym-0.17.3.tar.gz (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym==0.17.3) (1.11.3)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.17.3) (1.23.5)\n","Collecting pyglet<=1.5.0,>=1.4.0 (from gym==0.17.3)\n","  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cloudpickle<1.7.0,>=1.2.0 (from gym==0.17.3)\n","  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3) (0.18.3)\n","Building wheels for collected packages: gym\n","  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym: filename=gym-0.17.3-py3-none-any.whl size=1654618 sha256=0aed780d9f2bfbf61e4c2541021e8abc3c33b0d936a7b5e69f1ac3e44067cdef\n","  Stored in directory: /root/.cache/pip/wheels/af/4b/74/fcfc8238472c34d7f96508a63c962ff3ac9485a9a4137afd4e\n","Successfully built gym\n","Installing collected packages: pyglet, cloudpickle, gym\n","  Attempting uninstall: cloudpickle\n","    Found existing installation: cloudpickle 2.2.1\n","    Uninstalling cloudpickle-2.2.1:\n","      Successfully uninstalled cloudpickle-2.2.1\n","  Attempting uninstall: gym\n","    Found existing installation: gym 0.25.2\n","    Uninstalling gym-0.25.2:\n","      Successfully uninstalled gym-0.25.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","bigframes 0.12.0 requires cloudpickle>=2.0.0, but you have cloudpickle 1.6.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed cloudpickle-1.6.0 gym-0.17.3 pyglet-1.5.0\n","Collecting git+https://github.com/Kojoley/atari-py.git\n","  Cloning https://github.com/Kojoley/atari-py.git to /tmp/pip-req-build-gb5d8uc7\n","  Running command git clone --filter=blob:none --quiet https://github.com/Kojoley/atari-py.git /tmp/pip-req-build-gb5d8uc7\n","  Resolved https://github.com/Kojoley/atari-py.git to commit 86a1e05c0a95e9e6233c3a413521fdb34ca8a089\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from atari-py==1.2.2) (1.23.5)\n","Building wheels for collected packages: atari-py\n","  Building wheel for atari-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for atari-py: filename=atari_py-1.2.2-cp310-cp310-linux_x86_64.whl size=4738731 sha256=bc9071a752fdb29fcf6c90cf00e76356c07d36bcbe434821df674b2d64caf519\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-wf81yfc_/wheels/6c/9b/f1/8a80a63a233d6f4eb2e2ee8c7357f4875f21c3ffc7f2613f9f\n","Successfully built atari-py\n","Installing collected packages: atari-py\n","Successfully installed atari-py-1.2.2\n","Collecting keras-rl2==1.0.5\n","  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from keras-rl2==1.0.5) (2.14.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (23.5.26)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (16.0.6)\n","Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.2.0)\n","Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.23.5)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (23.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (4.5.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.34.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.59.2)\n","Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.14.1)\n","Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.14.0)\n","Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.14.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2==1.0.5) (0.41.3)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.5.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.2.2)\n","Installing collected packages: keras-rl2\n","Successfully installed keras-rl2-1.0.5\n","Collecting tensorflow==2.8\n","  Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl (497.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.6.3)\n","Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (23.5.26)\n","Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.9.0)\n","Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.8)\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (16.0.6)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.23.5)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.3.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (4.5.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.14.1)\n","Collecting tensorboard<2.9,>=2.8 (from tensorflow==2.8)\n","  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m114.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tf-estimator-nightly==2.8.0.dev2021122109 (from tensorflow==2.8)\n","  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting keras<2.9,>=2.8.0rc0 (from tensorflow==2.8)\n","  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.34.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.59.2)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.8) (0.41.3)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.17.3)\n","Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.5.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.31.0)\n","Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n","  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n","  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.2.2)\n","Installing collected packages: tf-estimator-nightly, tensorboard-plugin-wit, keras, tensorboard-data-server, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.14.0\n","    Uninstalling keras-2.14.0:\n","      Successfully uninstalled keras-2.14.0\n","  Attempting uninstall: tensorboard-data-server\n","    Found existing installation: tensorboard-data-server 0.7.2\n","    Uninstalling tensorboard-data-server-0.7.2:\n","      Successfully uninstalled tensorboard-data-server-0.7.2\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 1.0.0\n","    Uninstalling google-auth-oauthlib-1.0.0:\n","      Successfully uninstalled google-auth-oauthlib-1.0.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.14.1\n","    Uninstalling tensorboard-2.14.1:\n","      Successfully uninstalled tensorboard-2.14.1\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.14.0\n","    Uninstalling tensorflow-2.14.0:\n","      Successfully uninstalled tensorflow-2.14.0\n","Successfully installed google-auth-oauthlib-0.4.6 keras-2.8.0 keras-preprocessing-1.1.2 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tf-estimator-nightly-2.8.0.dev2021122109\n","Collecting gym-cap\n","  Downloading gym_cap-0.1.4.12-py3-none-any.whl (32 kB)\n","Requirement already satisfied: gym>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from gym-cap) (0.17.3)\n","Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from gym-cap) (1.23.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym>=0.16.0->gym-cap) (1.11.3)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.16.0->gym-cap) (1.5.0)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.16.0->gym-cap) (1.6.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.16.0->gym-cap) (0.18.3)\n","Installing collected packages: gym-cap\n","Successfully installed gym-cap-0.1.4.12\n","Collecting pettingzoo[butterfly]\n","  Downloading pettingzoo-1.24.1-py3-none-any.whl (840 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.8/840.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[butterfly]) (1.23.5)\n","Collecting gymnasium>=0.28.0 (from pettingzoo[butterfly])\n","  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pygame==2.3.0 (from pettingzoo[butterfly])\n","  Downloading pygame-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pymunk==6.2.0 (from pettingzoo[butterfly])\n","  Downloading pymunk-6.2.0.zip (7.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: cffi>1.14.0 in /usr/local/lib/python3.10/dist-packages (from pymunk==6.2.0->pettingzoo[butterfly]) (1.16.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[butterfly]) (1.6.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[butterfly]) (4.5.0)\n","Collecting farama-notifications>=0.0.1 (from gymnasium>=0.28.0->pettingzoo[butterfly])\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>1.14.0->pymunk==6.2.0->pettingzoo[butterfly]) (2.21)\n","Building wheels for collected packages: pymunk\n","  Building wheel for pymunk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pymunk: filename=pymunk-6.2.0-cp310-cp310-linux_x86_64.whl size=801640 sha256=efe43aa787fd5ba9b7b5c354b397e709d1030a2274f1e572bbbf83028377f150\n","  Stored in directory: /root/.cache/pip/wheels/2e/81/a2/f941a9ff417bb4020c37ae218fb7117d12d3fc019ea493d66f\n","Successfully built pymunk\n","Installing collected packages: farama-notifications, pygame, gymnasium, pymunk, pettingzoo\n","  Attempting uninstall: pygame\n","    Found existing installation: pygame 2.5.2\n","    Uninstalling pygame-2.5.2:\n","      Successfully uninstalled pygame-2.5.2\n","Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1 pettingzoo-1.24.1 pygame-2.3.0 pymunk-6.2.0\n"]}],"source":["if IN_COLAB:\n","  %pip install gym==0.17.3\n","  %pip install git+https://github.com/Kojoley/atari-py.git\n","  %pip install keras-rl2==1.0.5\n","  %pip install tensorflow==2.8\n","  %pip install gym-cap\n","  %pip install pettingzoo[butterfly]\n","else:\n","  %pip install gym==0.17.3\n","  %pip install git+https://github.com/Kojoley/atari-py.git\n","  %pip install pyglet==1.5.0\n","  %pip install h5py==3.1.0\n","  %pip install Pillow==9.5.0\n","  %pip install keras-rl2==1.0.5\n","  %pip install Keras==2.2.4\n","  %pip install tensorflow==2.5.3\n","  %pip install torch==2.0.1\n","  %pip install agents==1.4.0"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"G8cw-IX3laE9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699521149354,"user_tz":-60,"elapsed":483,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"794b4602-a27a-44e9-b162-d3159e474d6c"},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'multi_car_racing' already exists and is not an empty directory.\n"]}],"source":["!git clone https://github.com/igilitschenski/multi_car_racing.git\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"IqbMo3gK7vBG","executionInfo":{"status":"ok","timestamp":1699521149354,"user_tz":-60,"elapsed":10,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}}},"outputs":[],"source":["!cd multi_car_racing\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Jekec6f98b3A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699521157822,"user_tz":-60,"elapsed":8475,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"e38c4030-f766-4027-c868-7a91d050fa2d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting swig\n","  Downloading swig-4.1.1.post0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: swig\n","Successfully installed swig-4.1.1.post0\n"]}],"source":["!pip install swig"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"KKxRPBFx85k6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699521214509,"user_tz":-60,"elapsed":56725,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"97b57a16-bfc1-4fa9-fcc6-94e909db8fc3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting box2d\n","  Downloading Box2D-2.3.2.tar.gz (427 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.9/427.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: box2d\n","  Building wheel for box2d (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d: filename=Box2D-2.3.2-cp310-cp310-linux_x86_64.whl size=2391358 sha256=9d142c66f56ec75d04f265a894855567a079a4df4b627f84e115aaf140243793\n","  Stored in directory: /root/.cache/pip/wheels/eb/cb/be/e663f3ce9aba6580611c0febaf7cd3cf7603f87047de2a52f9\n","Successfully built box2d\n","Installing collected packages: box2d\n","Successfully installed box2d-2.3.2\n"]}],"source":["!pip install box2d"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"ijp5V0i09MRF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699521272055,"user_tz":-60,"elapsed":57595,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"18ec7236-4fba-499a-9db3-8ba088ff6d31"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting box2d-py\n","  Downloading box2d-py-2.3.8.tar.gz (374 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/374.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/374.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m368.6/374.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.5/374.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: box2d-py\n","  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-py: filename=box2d_py-2.3.8-cp310-cp310-linux_x86_64.whl size=2373131 sha256=d30af946aaddee1b91da518020eb3892ceee6e24df14bee6191d413a0a33f587\n","  Stored in directory: /root/.cache/pip/wheels/47/01/d2/6a780da77ccb98b1d2facdd520a8d10838a03b590f6f8d50c0\n","Successfully built box2d-py\n","Installing collected packages: box2d-py\n","Successfully installed box2d-py-2.3.8\n"]}],"source":["!pip install box2d-py"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"BwjugqI99g0I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699521298442,"user_tz":-60,"elapsed":26437,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"8d4998d9-d4ec-47d2-935c-a3ab3daac3be"},"outputs":[{"output_type":"stream","name":"stdout","text":["Obtaining file:///content/gdrive/MyDrive/TFM/multi_car_racing\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: box2d-py~=2.3.5 in /usr/local/lib/python3.10/dist-packages (from gym-multi-car-racing==0.0.1) (2.3.8)\n","Collecting shapely~=1.7.0 (from gym-multi-car-racing==0.0.1)\n","  Downloading Shapely-1.7.1.tar.gz (383 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.2/383.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from gym-multi-car-racing==0.0.1) (1.23.5)\n","Requirement already satisfied: gym~=0.17.2 in /usr/local/lib/python3.10/dist-packages (from gym-multi-car-racing==0.0.1) (0.17.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym~=0.17.2->gym-multi-car-racing==0.0.1) (1.11.3)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from gym~=0.17.2->gym-multi-car-racing==0.0.1) (1.5.0)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym~=0.17.2->gym-multi-car-racing==0.0.1) (1.6.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym~=0.17.2->gym-multi-car-racing==0.0.1) (0.18.3)\n","Building wheels for collected packages: shapely\n","  Building wheel for shapely (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for shapely: filename=Shapely-1.7.1-cp310-cp310-linux_x86_64.whl size=997272 sha256=35ac9081878d3d2d495908bd8ce450d5e4d492a54284f473b3b2d2c5c856a650\n","  Stored in directory: /root/.cache/pip/wheels/2e/fa/97/c85f587c35afcaf4a81c481741d36592518d1e50445572f0d4\n","Successfully built shapely\n","Installing collected packages: shapely, gym-multi-car-racing\n","  Attempting uninstall: shapely\n","    Found existing installation: shapely 2.0.2\n","    Uninstalling shapely-2.0.2:\n","      Successfully uninstalled shapely-2.0.2\n","  Running setup.py develop for gym-multi-car-racing\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires fastapi, which is not installed.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed gym-multi-car-racing-0.0.1 shapely-1.7.1\n"]}],"source":["!pip install -e multi_car_racing"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"IrAvXzCW-Z3e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699521307909,"user_tz":-60,"elapsed":9515,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"bd52911c-79f1-4691-cd22-a92c2a309392"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  freeglut3 libegl-dev libgl-dev libgl1-mesa-dev libgles-dev libgles1\n","  libglu1-mesa libglu1-mesa-dev libglvnd-core-dev libglvnd-dev libglx-dev\n","  libice-dev libopengl-dev libsm-dev libxt-dev\n","Suggested packages:\n","  libice-doc libsm-doc libxt-doc\n","The following NEW packages will be installed:\n","  freeglut3 freeglut3-dev libegl-dev libgl-dev libgl1-mesa-dev libgles-dev\n","  libgles1 libglu1-mesa libglu1-mesa-dev libglvnd-core-dev libglvnd-dev\n","  libglx-dev libice-dev libopengl-dev libsm-dev libxt-dev\n","0 upgraded, 16 newly installed, 0 to remove and 19 not upgraded.\n","Need to get 1,261 kB of archives.\n","After this operation, 6,753 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3 amd64 2.8.1-6 [74.0 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglx-dev amd64 1.4.0-1 [14.1 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgl-dev amd64 1.4.0-1 [101 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-core-dev amd64 1.4.0-1 [12.7 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libegl-dev amd64 1.4.0-1 [18.0 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles1 amd64 1.4.0-1 [11.5 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles-dev amd64 1.4.0-1 [49.4 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libopengl-dev amd64 1.4.0-1 [3,400 B]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-dev amd64 1.4.0-1 [3,162 B]\n","Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgl1-mesa-dev amd64 23.0.4-0ubuntu1~22.04.1 [6,510 B]\n","Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n","Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa-dev amd64 9.0.2-1 [231 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 libice-dev amd64 2:1.0.10-1build2 [51.4 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsm-dev amd64 2:1.2.3-1build2 [18.1 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3-dev amd64 2.8.1-6 [126 kB]\n","Fetched 1,261 kB in 1s (1,751 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 16.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package freeglut3:amd64.\n","(Reading database ... 120874 files and directories currently installed.)\n","Preparing to unpack .../00-freeglut3_2.8.1-6_amd64.deb ...\n","Unpacking freeglut3:amd64 (2.8.1-6) ...\n","Selecting previously unselected package libglx-dev:amd64.\n","Preparing to unpack .../01-libglx-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglx-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgl-dev:amd64.\n","Preparing to unpack .../02-libgl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libgl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libglvnd-core-dev:amd64.\n","Preparing to unpack .../03-libglvnd-core-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglvnd-core-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libegl-dev:amd64.\n","Preparing to unpack .../04-libegl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libegl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgles1:amd64.\n","Preparing to unpack .../05-libgles1_1.4.0-1_amd64.deb ...\n","Unpacking libgles1:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgles-dev:amd64.\n","Preparing to unpack .../06-libgles-dev_1.4.0-1_amd64.deb ...\n","Unpacking libgles-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libopengl-dev:amd64.\n","Preparing to unpack .../07-libopengl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libopengl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libglvnd-dev:amd64.\n","Preparing to unpack .../08-libglvnd-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglvnd-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgl1-mesa-dev:amd64.\n","Preparing to unpack .../09-libgl1-mesa-dev_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n","Unpacking libgl1-mesa-dev:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Selecting previously unselected package libglu1-mesa:amd64.\n","Preparing to unpack .../10-libglu1-mesa_9.0.2-1_amd64.deb ...\n","Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n","Selecting previously unselected package libglu1-mesa-dev:amd64.\n","Preparing to unpack .../11-libglu1-mesa-dev_9.0.2-1_amd64.deb ...\n","Unpacking libglu1-mesa-dev:amd64 (9.0.2-1) ...\n","Selecting previously unselected package libice-dev:amd64.\n","Preparing to unpack .../12-libice-dev_2%3a1.0.10-1build2_amd64.deb ...\n","Unpacking libice-dev:amd64 (2:1.0.10-1build2) ...\n","Selecting previously unselected package libsm-dev:amd64.\n","Preparing to unpack .../13-libsm-dev_2%3a1.2.3-1build2_amd64.deb ...\n","Unpacking libsm-dev:amd64 (2:1.2.3-1build2) ...\n","Selecting previously unselected package libxt-dev:amd64.\n","Preparing to unpack .../14-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n","Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n","Selecting previously unselected package freeglut3-dev:amd64.\n","Preparing to unpack .../15-freeglut3-dev_2.8.1-6_amd64.deb ...\n","Unpacking freeglut3-dev:amd64 (2.8.1-6) ...\n","Setting up freeglut3:amd64 (2.8.1-6) ...\n","Setting up libglvnd-core-dev:amd64 (1.4.0-1) ...\n","Setting up libice-dev:amd64 (2:1.0.10-1build2) ...\n","Setting up libsm-dev:amd64 (2:1.2.3-1build2) ...\n","Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n","Setting up libgles1:amd64 (1.4.0-1) ...\n","Setting up libglx-dev:amd64 (1.4.0-1) ...\n","Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n","Setting up libopengl-dev:amd64 (1.4.0-1) ...\n","Setting up libgl-dev:amd64 (1.4.0-1) ...\n","Setting up libegl-dev:amd64 (1.4.0-1) ...\n","Setting up libglu1-mesa-dev:amd64 (9.0.2-1) ...\n","Setting up libgles-dev:amd64 (1.4.0-1) ...\n","Setting up libglvnd-dev:amd64 (1.4.0-1) ...\n","Setting up libgl1-mesa-dev:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Setting up freeglut3-dev:amd64 (2.8.1-6) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n"]}],"source":["!sudo apt-get install freeglut3-dev"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"cgGdQ6n9EERW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699521326174,"user_tz":-60,"elapsed":18319,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"a262297f-a9ef-403a-b260-638716f66460"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n","  xserver-common\n","The following NEW packages will be installed:\n","  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n","  xserver-common xvfb\n","0 upgraded, 9 newly installed, 0 to remove and 19 not upgraded.\n","Need to get 7,814 kB of archives.\n","After this operation, 11.9 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.2 [28.1 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.2 [864 kB]\n","Fetched 7,814 kB in 1s (8,145 kB/s)\n","Selecting previously unselected package libfontenc1:amd64.\n","(Reading database ... 121332 files and directories currently installed.)\n","Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n","Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n","Selecting previously unselected package libxfont2:amd64.\n","Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n","Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n","Selecting previously unselected package libxkbfile1:amd64.\n","Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n","Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n","Selecting previously unselected package x11-xkb-utils.\n","Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n","Unpacking x11-xkb-utils (7.7+5build4) ...\n","Selecting previously unselected package xfonts-encodings.\n","Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n","Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n","Selecting previously unselected package xfonts-utils.\n","Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n","Unpacking xfonts-utils (1:7.7+6build2) ...\n","Selecting previously unselected package xfonts-base.\n","Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n","Unpacking xfonts-base (1:1.0.5) ...\n","Selecting previously unselected package xserver-common.\n","Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.2_all.deb ...\n","Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n","Selecting previously unselected package xvfb.\n","Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.2_amd64.deb ...\n","Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.2) ...\n","Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n","Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n","Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n","Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n","Setting up x11-xkb-utils (7.7+5build4) ...\n","Setting up xfonts-utils (1:7.7+6build2) ...\n","Setting up xfonts-base (1:1.0.5) ...\n","Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n","Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.2) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","Collecting pyvirtualdisplay\n","  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n","Installing collected packages: pyvirtualdisplay\n","Successfully installed pyvirtualdisplay-3.0\n","Collecting piglet\n","  Downloading piglet-1.0.0-py2.py3-none-any.whl (2.2 kB)\n","Collecting piglet-templates (from piglet)\n","  Downloading piglet_templates-1.3.0-py3-none-any.whl (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.5/67.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from piglet-templates->piglet) (3.1.1)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from piglet-templates->piglet) (23.1.0)\n","Requirement already satisfied: astunparse in /usr/local/lib/python3.10/dist-packages (from piglet-templates->piglet) (1.6.3)\n","Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from piglet-templates->piglet) (2.1.3)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse->piglet-templates->piglet) (0.41.3)\n","Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from astunparse->piglet-templates->piglet) (1.16.0)\n","Installing collected packages: piglet-templates, piglet\n","Successfully installed piglet-1.0.0 piglet-templates-1.3.0\n"]}],"source":["!apt install xvfb -y\n","!pip install pyvirtualdisplay\n","!pip install piglet"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"5OaWkBSmhm6R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699521338644,"user_tz":-60,"elapsed":12519,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"5bf90a78-db7a-49d9-f39c-87367ede6f62"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting supersuit\n","  Downloading SuperSuit-3.9.0-py3-none-any.whl (49 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/49.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from supersuit) (1.23.5)\n","Requirement already satisfied: gymnasium>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from supersuit) (0.29.1)\n","Collecting tinyscaler>=1.2.6 (from supersuit)\n","  Downloading tinyscaler-1.2.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (517 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.1/517.1 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (1.6.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (0.0.4)\n","Installing collected packages: tinyscaler, supersuit\n","Successfully installed supersuit-3.9.0 tinyscaler-1.2.7\n","Collecting stable_baselines3\n","  Downloading stable_baselines3-2.1.0-py3-none-any.whl (178 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (0.29.1)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.23.5)\n","Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.1.0+cu118)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.6.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.5.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (3.7.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (0.0.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.13.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2.1.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (4.44.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (23.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable_baselines3) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable_baselines3) (1.3.0)\n","Installing collected packages: stable_baselines3\n","Successfully installed stable_baselines3-2.1.0\n"]}],"source":["!pip install supersuit\n","!pip install stable_baselines3"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"thmOvcHdjKHw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699521348076,"user_tz":-60,"elapsed":9480,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"b3d05852-113a-4f63-b434-a9008782f8a0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting shimmy>=0.2.1\n","  Downloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from shimmy>=0.2.1) (1.23.5)\n","Requirement already satisfied: gymnasium>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from shimmy>=0.2.1) (0.29.1)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy>=0.2.1) (1.6.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy>=0.2.1) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy>=0.2.1) (0.0.4)\n","Installing collected packages: shimmy\n","Successfully installed shimmy-1.3.0\n"]}],"source":["!pip install 'shimmy>=0.2.1'"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"k0iVvep_spQz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699521369660,"user_tz":-60,"elapsed":21629,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"cd7f7837-cf42-4e75-c81f-f3ef693c9eac"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tf-agents-nightly\n","  Downloading tf_agents_nightly-0.19.0.dev20231010-py3-none-any.whl (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.4.0)\n","Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.6.0)\n","Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (0.5.0)\n","Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (0.17.3)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.23.5)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (9.4.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.16.0)\n","Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (3.20.3)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.14.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (4.5.0)\n","Collecting pygame==2.1.3 (from tf-agents-nightly)\n","  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tfp-nightly (from tf-agents-nightly)\n","  Downloading tfp_nightly-0.23.0.dev20231108-py2.py3-none-any.whl (6.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents-nightly) (1.11.3)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents-nightly) (1.5.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tfp-nightly->tf-agents-nightly) (4.4.2)\n","Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tfp-nightly->tf-agents-nightly) (0.5.4)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tfp-nightly->tf-agents-nightly) (0.1.8)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<=0.23.0,>=0.17.0->tf-agents-nightly) (0.18.3)\n","Installing collected packages: tfp-nightly, pygame, tf-agents-nightly\n","  Attempting uninstall: pygame\n","    Found existing installation: pygame 2.3.0\n","    Uninstalling pygame-2.3.0:\n","      Successfully uninstalled pygame-2.3.0\n","Successfully installed pygame-2.1.3 tf-agents-nightly-0.19.0.dev20231010 tfp-nightly-0.23.0.dev20231108\n"]}],"source":["!pip install tf-agents-nightly"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"UlXxViz9tdvH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699522448665,"user_tz":-60,"elapsed":1079053,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"89dea7db-a3d3-450c-be40-8f748b24902a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: tensorflow 2.8.0\n","Uninstalling tensorflow-2.8.0:\n","  Would remove:\n","    /usr/local/bin/estimator_ckpt_converter\n","    /usr/local/bin/import_pb_to_tensorboard\n","    /usr/local/bin/saved_model_cli\n","    /usr/local/bin/tensorboard\n","    /usr/local/bin/tf_upgrade_v2\n","    /usr/local/bin/tflite_convert\n","    /usr/local/bin/toco\n","    /usr/local/bin/toco_from_protos\n","    /usr/local/lib/python3.10/dist-packages/tensorflow-2.8.0.dist-info/*\n","    /usr/local/lib/python3.10/dist-packages/tensorflow/*\n","Proceed (Y/n)? Y\n","  Successfully uninstalled tensorflow-2.8.0\n"]}],"source":["!pip uninstall tensorflow"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"dsWlVQ6MtKLj","executionInfo":{"status":"ok","timestamp":1699522491942,"user_tz":-60,"elapsed":43286,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}}},"outputs":[],"source":["!pip install tensorflow>=2.13"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"wE5AiVtFtZDc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699522496157,"user_tz":-60,"elapsed":4264,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"7e3ef947-68a8-4017-f295-9c29d63db2c4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Name: tensorflow\n","Version: 2.14.0\n","Summary: TensorFlow is an open source machine learning framework for everyone.\n","Home-page: https://www.tensorflow.org/\n","Author: Google Inc.\n","Author-email: packages@tensorflow.org\n","License: Apache 2.0\n","Location: /usr/local/lib/python3.10/dist-packages\n","Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, libclang, ml-dtypes, numpy, opt-einsum, packaging, protobuf, setuptools, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n","Required-by: dopamine-rl, keras-rl2\n"]}],"source":["!pip show tensorflow"]},{"cell_type":"code","source":["!pip install pettingzoo[sisl]"],"metadata":{"id":"PZa1qybXZKSX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699522561312,"user_tz":-60,"elapsed":65165,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"233d6764-ee2b-4039-f3e0-f59432be17c3"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pettingzoo[sisl] in /usr/local/lib/python3.10/dist-packages (1.24.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[sisl]) (1.23.5)\n","Requirement already satisfied: gymnasium>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[sisl]) (0.29.1)\n","Collecting pygame==2.3.0 (from pettingzoo[sisl])\n","  Using cached pygame-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\n","Requirement already satisfied: pymunk==6.2.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[sisl]) (6.2.0)\n","Collecting box2d-py==2.3.5 (from pettingzoo[sisl])\n","  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[sisl]) (1.11.3)\n","Requirement already satisfied: cffi>1.14.0 in /usr/local/lib/python3.10/dist-packages (from pymunk==6.2.0->pettingzoo[sisl]) (1.16.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[sisl]) (1.6.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[sisl]) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[sisl]) (0.0.4)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>1.14.0->pymunk==6.2.0->pettingzoo[sisl]) (2.21)\n","Building wheels for collected packages: box2d-py\n","  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2373129 sha256=419e896f91ddaba9f7733f54640e8a838a2f40d5fe3fc38b4a4182e1f916d342\n","  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n","Successfully built box2d-py\n","Installing collected packages: box2d-py, pygame\n","  Attempting uninstall: box2d-py\n","    Found existing installation: box2d-py 2.3.8\n","    Uninstalling box2d-py-2.3.8:\n","      Successfully uninstalled box2d-py-2.3.8\n","  Attempting uninstall: pygame\n","    Found existing installation: pygame 2.1.3\n","    Uninstalling pygame-2.1.3:\n","      Successfully uninstalled pygame-2.1.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tf-agents-nightly 0.19.0.dev20231010 requires pygame==2.1.3, but you have pygame 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed box2d-py-2.3.5 pygame-2.3.0\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"NFzawQ1QZFwj","executionInfo":{"status":"ok","timestamp":1699522561313,"user_tz":-60,"elapsed":50,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"llJXjwZQTVBe"},"outputs":[],"source":["############################# Código para entrenar Multiwalker ######################"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tnz3fJDA33w8"},"outputs":[],"source":["# from stable_baselines3.dqn import MlpPolicy,CnnPolicy\n","from stable_baselines3 import SAC\n","from pettingzoo.sisl import multiwalker_v9\n","import supersuit as ss\n","from stable_baselines3.common.monitor import Monitor"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":218,"status":"ok","timestamp":1697644598142,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"6-kdhb3CI5VC","outputId":"4db87993-1f4b-464a-f0f0-20ed31aa0c34"},"outputs":[{"output_type":"stream","name":"stdout","text":["drive  gdrive  sample_data\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":395,"status":"ok","timestamp":1697644599460,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"04CbnRTvI9L2","outputId":"347477dc-2a63-4315-a08f-182fdf724421"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/TFM\n"]}],"source":["cd /content/drive/MyDrive/TFM"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":305,"status":"ok","timestamp":1697644601324,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"bUEH8254I_kb","outputId":"406e2b97-e693-46da-9e49-9d56c991f4e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["'=2.13'\t\t\t\t     multiwalker_sac_log_eval\n"," DQN_new_pettingzoo_gym_cap.ipynb    policy_log_eval\n"," DQN_policies\t\t\t     PPO_policies\n","'Entrenamientos antiguos sin logs'   results_rllib\n"," Entrenamientos_log_no_eval\t     TFM_Multiwalker_DDPG_gym_cap.ipynb\n"," MCR_TFM.ipynb\t\t\t     TFM_Multiwalker_SAC_gym_cap.ipynb\n"," multi_car_racing\t\t     TFM_PPO_new_pettingzoo_gym_cap.ipynb\n"," multiwalker_ddpg_log_eval\t     TFM_PPO_pettingzoo_gym_cap.ipynb\n"," multiwalker_ddpg.zip\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2HubK-2G3_vH"},"outputs":[],"source":["env = multiwalker_v9.parallel_env(render_mode=\"rgb_array\",shared_reward=0)\n","\n","env = ss.frame_stack_v1(env, 3)\n","env = ss.pettingzoo_env_to_vec_env_v1(env)\n","env = ss.concat_vec_envs_v1(env, 8, num_cpus=4, base_class='stable_baselines3')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"foI4bTFGbQo1"},"outputs":[],"source":["from stable_baselines3.common.callbacks import EvalCallback\n","eval_callback = EvalCallback(env, best_model_save_path=\"./multiwalker_sac_00_log_eval_extra/\",\n","                             log_path=\"./multiwalker_sac_00_log_eval_extra/\", eval_freq=100,\n","                             deterministic=False, render=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ib9VPIVgec47","outputId":"a8adc43d-194e-4c78-f5aa-8fb3a9979d4a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Logging to multiwalker_sac_00_log_eval_extra/\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n","|    total_timesteps | 3343200  |\n","| train/             |          |\n","|    actor_loss      | -10.9    |\n","|    critic_loss     | 1.89     |\n","|    ent_coef        | 0.00432  |\n","|    ent_coef_loss   | -0.0904  |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 305090   |\n","---------------------------------\n","Eval num_timesteps=3345600, episode_reward=172.77 +/- 4.88\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 173      |\n","| time/              |          |\n","|    total_timesteps | 3345600  |\n","| train/             |          |\n","|    actor_loss      | -9.73    |\n","|    critic_loss     | 1.96     |\n","|    ent_coef        | 0.00428  |\n","|    ent_coef_loss   | 4.4      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 305190   |\n","---------------------------------\n","Eval num_timesteps=3348000, episode_reward=55.50 +/- 106.45\n","Episode length: 342.20 +/- 128.84\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 342      |\n","|    mean_reward     | 55.5     |\n","| time/              |          |\n","|    total_timesteps | 3348000  |\n","| train/             |          |\n","|    actor_loss      | -11.1    |\n","|    critic_loss     | 3.32     |\n","|    ent_coef        | 0.00432  |\n","|    ent_coef_loss   | 3.55     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 305290   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2180     |\n","|    fps             | 290      |\n","|    time_elapsed    | 11510    |\n","|    total_timesteps | 3348576  |\n","| train/             |          |\n","|    actor_loss      | -10.7    |\n","|    critic_loss     | 22.1     |\n","|    ent_coef        | 0.00435  |\n","|    ent_coef_loss   | 0.704    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 305314   |\n","---------------------------------\n","Eval num_timesteps=3350400, episode_reward=164.21 +/- 4.83\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 164      |\n","| time/              |          |\n","|    total_timesteps | 3350400  |\n","| train/             |          |\n","|    actor_loss      | -10.8    |\n","|    critic_loss     | 3.03     |\n","|    ent_coef        | 0.00435  |\n","|    ent_coef_loss   | 3.84     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 305390   |\n","---------------------------------\n","Eval num_timesteps=3352800, episode_reward=177.18 +/- 5.99\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 177      |\n","| time/              |          |\n","|    total_timesteps | 3352800  |\n","| train/             |          |\n","|    actor_loss      | -11.9    |\n","|    critic_loss     | 1.06     |\n","|    ent_coef        | 0.00451  |\n","|    ent_coef_loss   | -3.4     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 305490   |\n","---------------------------------\n","Eval num_timesteps=3355200, episode_reward=66.55 +/- 83.57\n","Episode length: 413.00 +/- 71.04\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 413      |\n","|    mean_reward     | 66.5     |\n","| time/              |          |\n","|    total_timesteps | 3355200  |\n","| train/             |          |\n","|    actor_loss      | -11.2    |\n","|    critic_loss     | 2.37     |\n","|    ent_coef        | 0.00447  |\n","|    ent_coef_loss   | 1.54     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 305590   |\n","---------------------------------\n","Eval num_timesteps=3357600, episode_reward=167.95 +/- 3.83\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 168      |\n","| time/              |          |\n","|    total_timesteps | 3357600  |\n","| train/             |          |\n","|    actor_loss      | -10.7    |\n","|    critic_loss     | 12.4     |\n","|    ent_coef        | 0.00451  |\n","|    ent_coef_loss   | 0.804    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 305690   |\n","---------------------------------\n","Eval num_timesteps=3360000, episode_reward=166.20 +/- 0.63\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 166      |\n","| time/              |          |\n","|    total_timesteps | 3360000  |\n","| train/             |          |\n","|    actor_loss      | -10.6    |\n","|    critic_loss     | 0.851    |\n","|    ent_coef        | 0.00459  |\n","|    ent_coef_loss   | -1.88    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 305790   |\n","---------------------------------\n","Eval num_timesteps=3362400, episode_reward=174.25 +/- 0.71\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 174      |\n","| time/              |          |\n","|    total_timesteps | 3362400  |\n","| train/             |          |\n","|    actor_loss      | -11.8    |\n","|    critic_loss     | 0.964    |\n","|    ent_coef        | 0.00458  |\n","|    ent_coef_loss   | 2.5      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 305890   |\n","---------------------------------\n","Eval num_timesteps=3364800, episode_reward=175.06 +/- 3.85\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 175      |\n","| time/              |          |\n","|    total_timesteps | 3364800  |\n","| train/             |          |\n","|    actor_loss      | -10.9    |\n","|    critic_loss     | 1.79     |\n","|    ent_coef        | 0.00461  |\n","|    ent_coef_loss   | 1.96     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 305990   |\n","---------------------------------\n","Eval num_timesteps=3367200, episode_reward=72.01 +/- 80.34\n","Episode length: 418.00 +/- 100.43\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 418      |\n","|    mean_reward     | 72       |\n","| time/              |          |\n","|    total_timesteps | 3367200  |\n","| train/             |          |\n","|    actor_loss      | -11.5    |\n","|    critic_loss     | 0.639    |\n","|    ent_coef        | 0.00453  |\n","|    ent_coef_loss   | -0.976   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 306090   |\n","---------------------------------\n","Eval num_timesteps=3369600, episode_reward=162.26 +/- 2.50\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 162      |\n","| time/              |          |\n","|    total_timesteps | 3369600  |\n","| train/             |          |\n","|    actor_loss      | -10.8    |\n","|    critic_loss     | 1.51     |\n","|    ent_coef        | 0.00449  |\n","|    ent_coef_loss   | 1.22     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 306190   |\n","---------------------------------\n","Eval num_timesteps=3372000, episode_reward=109.64 +/- 77.96\n","Episode length: 450.00 +/- 61.24\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 450      |\n","|    mean_reward     | 110      |\n","| time/              |          |\n","|    total_timesteps | 3372000  |\n","| train/             |          |\n","|    actor_loss      | -10.9    |\n","|    critic_loss     | 1.11     |\n","|    ent_coef        | 0.00441  |\n","|    ent_coef_loss   | 0.116    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 306290   |\n","---------------------------------\n","Eval num_timesteps=3374400, episode_reward=92.63 +/- 81.23\n","Episode length: 430.80 +/- 84.75\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 431      |\n","|    mean_reward     | 92.6     |\n","| time/              |          |\n","|    total_timesteps | 3374400  |\n","| train/             |          |\n","|    actor_loss      | -10.7    |\n","|    critic_loss     | 2.13     |\n","|    ent_coef        | 0.0044   |\n","|    ent_coef_loss   | 0.584    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 306390   |\n","---------------------------------\n","Eval num_timesteps=3376800, episode_reward=64.65 +/- 72.98\n","Episode length: 407.00 +/- 75.93\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 407      |\n","|    mean_reward     | 64.6     |\n","| time/              |          |\n","|    total_timesteps | 3376800  |\n","| train/             |          |\n","|    actor_loss      | -10.2    |\n","|    critic_loss     | 1.17     |\n","|    ent_coef        | 0.00434  |\n","|    ent_coef_loss   | -0.466   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 306490   |\n","---------------------------------\n","Eval num_timesteps=3379200, episode_reward=148.71 +/- 2.51\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 149      |\n","| time/              |          |\n","|    total_timesteps | 3379200  |\n","| train/             |          |\n","|    actor_loss      | -10.7    |\n","|    critic_loss     | 13.6     |\n","|    ent_coef        | 0.0043   |\n","|    ent_coef_loss   | -0.624   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 306590   |\n","---------------------------------\n","Eval num_timesteps=3381600, episode_reward=145.35 +/- 0.56\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 145      |\n","| time/              |          |\n","|    total_timesteps | 3381600  |\n","| train/             |          |\n","|    actor_loss      | -11.3    |\n","|    critic_loss     | 1.08     |\n","|    ent_coef        | 0.00439  |\n","|    ent_coef_loss   | 6.08     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 306690   |\n","---------------------------------\n","Eval num_timesteps=3384000, episode_reward=147.92 +/- 1.39\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 148      |\n","| time/              |          |\n","|    total_timesteps | 3384000  |\n","| train/             |          |\n","|    actor_loss      | -10.7    |\n","|    critic_loss     | 21.6     |\n","|    ent_coef        | 0.0045   |\n","|    ent_coef_loss   | -0.566   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 306790   |\n","---------------------------------\n","Eval num_timesteps=3386400, episode_reward=116.90 +/- 59.44\n","Episode length: 474.00 +/- 31.84\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 474      |\n","|    mean_reward     | 117      |\n","| time/              |          |\n","|    total_timesteps | 3386400  |\n","| train/             |          |\n","|    actor_loss      | -10.2    |\n","|    critic_loss     | 1.39     |\n","|    ent_coef        | 0.00452  |\n","|    ent_coef_loss   | 0.909    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 306890   |\n","---------------------------------\n","Eval num_timesteps=3388800, episode_reward=139.65 +/- 4.05\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 140      |\n","| time/              |          |\n","|    total_timesteps | 3388800  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 1.81     |\n","|    ent_coef        | 0.00454  |\n","|    ent_coef_loss   | 3.48     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 306990   |\n","---------------------------------\n","Eval num_timesteps=3391200, episode_reward=-37.88 +/- 18.05\n","Episode length: 267.00 +/- 51.44\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 267      |\n","|    mean_reward     | -37.9    |\n","| time/              |          |\n","|    total_timesteps | 3391200  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 0.917    |\n","|    ent_coef        | 0.00465  |\n","|    ent_coef_loss   | 4.73     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 307090   |\n","---------------------------------\n","Eval num_timesteps=3393600, episode_reward=71.47 +/- 62.03\n","Episode length: 485.60 +/- 11.76\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 486      |\n","|    mean_reward     | 71.5     |\n","| time/              |          |\n","|    total_timesteps | 3393600  |\n","| train/             |          |\n","|    actor_loss      | -12      |\n","|    critic_loss     | 1.84     |\n","|    ent_coef        | 0.00468  |\n","|    ent_coef_loss   | -2.35    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 307190   |\n","---------------------------------\n","Eval num_timesteps=3396000, episode_reward=159.71 +/- 6.81\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 160      |\n","| time/              |          |\n","|    total_timesteps | 3396000  |\n","| train/             |          |\n","|    actor_loss      | -10.7    |\n","|    critic_loss     | 0.948    |\n","|    ent_coef        | 0.00479  |\n","|    ent_coef_loss   | 4.24     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 307290   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2190     |\n","|    fps             | 290      |\n","|    time_elapsed    | 11678    |\n","|    total_timesteps | 3397752  |\n","| train/             |          |\n","|    actor_loss      | -10.4    |\n","|    critic_loss     | 0.408    |\n","|    ent_coef        | 0.00492  |\n","|    ent_coef_loss   | 3.49     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 307363   |\n","---------------------------------\n","Eval num_timesteps=3398400, episode_reward=141.30 +/- 4.41\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 141      |\n","| time/              |          |\n","|    total_timesteps | 3398400  |\n","| train/             |          |\n","|    actor_loss      | -10.4    |\n","|    critic_loss     | 1.28     |\n","|    ent_coef        | 0.00494  |\n","|    ent_coef_loss   | 6.02     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 307390   |\n","---------------------------------\n","Eval num_timesteps=3400800, episode_reward=117.91 +/- 55.15\n","Episode length: 496.40 +/- 4.41\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 496      |\n","|    mean_reward     | 118      |\n","| time/              |          |\n","|    total_timesteps | 3400800  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 3.53     |\n","|    ent_coef        | 0.00495  |\n","|    ent_coef_loss   | 1.33     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 307490   |\n","---------------------------------\n","Eval num_timesteps=3403200, episode_reward=55.38 +/- 75.47\n","Episode length: 433.40 +/- 54.38\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 433      |\n","|    mean_reward     | 55.4     |\n","| time/              |          |\n","|    total_timesteps | 3403200  |\n","| train/             |          |\n","|    actor_loss      | -10.2    |\n","|    critic_loss     | 1.21     |\n","|    ent_coef        | 0.0051   |\n","|    ent_coef_loss   | 3.57     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 307590   |\n","---------------------------------\n","Eval num_timesteps=3405600, episode_reward=-26.21 +/- 9.84\n","Episode length: 272.40 +/- 31.35\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 272      |\n","|    mean_reward     | -26.2    |\n","| time/              |          |\n","|    total_timesteps | 3405600  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 3.63     |\n","|    ent_coef        | 0.00517  |\n","|    ent_coef_loss   | -6.23    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 307690   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2200     |\n","|    fps             | 290      |\n","|    time_elapsed    | 11706    |\n","|    total_timesteps | 3406392  |\n","| train/             |          |\n","|    actor_loss      | -10.5    |\n","|    critic_loss     | 1.27     |\n","|    ent_coef        | 0.00513  |\n","|    ent_coef_loss   | 0.153    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 307723   |\n","---------------------------------\n","Eval num_timesteps=3408000, episode_reward=90.94 +/- 61.35\n","Episode length: 457.40 +/- 34.78\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 457      |\n","|    mean_reward     | 90.9     |\n","| time/              |          |\n","|    total_timesteps | 3408000  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 1.05     |\n","|    ent_coef        | 0.00508  |\n","|    ent_coef_loss   | 1.79     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 307790   |\n","---------------------------------\n","Eval num_timesteps=3410400, episode_reward=14.43 +/- 39.28\n","Episode length: 361.80 +/- 104.35\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 362      |\n","|    mean_reward     | 14.4     |\n","| time/              |          |\n","|    total_timesteps | 3410400  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 1.54     |\n","|    ent_coef        | 0.00507  |\n","|    ent_coef_loss   | -2.99    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 307890   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2210     |\n","|    fps             | 291      |\n","|    time_elapsed    | 11724    |\n","|    total_timesteps | 3412272  |\n","| train/             |          |\n","|    actor_loss      | -10.4    |\n","|    critic_loss     | 1.07     |\n","|    ent_coef        | 0.00509  |\n","|    ent_coef_loss   | 0.125    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 307968   |\n","---------------------------------\n","Eval num_timesteps=3412800, episode_reward=177.98 +/- 0.27\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 178      |\n","| time/              |          |\n","|    total_timesteps | 3412800  |\n","| train/             |          |\n","|    actor_loss      | -11.7    |\n","|    critic_loss     | 0.753    |\n","|    ent_coef        | 0.0051   |\n","|    ent_coef_loss   | -2.31    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 307990   |\n","---------------------------------\n","Eval num_timesteps=3415200, episode_reward=64.05 +/- 135.04\n","Episode length: 330.80 +/- 207.23\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 331      |\n","|    mean_reward     | 64       |\n","| time/              |          |\n","|    total_timesteps | 3415200  |\n","| train/             |          |\n","|    actor_loss      | -9.12    |\n","|    critic_loss     | 1.83     |\n","|    ent_coef        | 0.00501  |\n","|    ent_coef_loss   | 1.05     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 308090   |\n","---------------------------------\n","Eval num_timesteps=3417600, episode_reward=166.86 +/- 1.48\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 167      |\n","| time/              |          |\n","|    total_timesteps | 3417600  |\n","| train/             |          |\n","|    actor_loss      | -10.8    |\n","|    critic_loss     | 3.14     |\n","|    ent_coef        | 0.00497  |\n","|    ent_coef_loss   | -2.8     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 308190   |\n","---------------------------------\n","Eval num_timesteps=3420000, episode_reward=103.27 +/- 70.73\n","Episode length: 454.40 +/- 55.85\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 454      |\n","|    mean_reward     | 103      |\n","| time/              |          |\n","|    total_timesteps | 3420000  |\n","| train/             |          |\n","|    actor_loss      | -10.7    |\n","|    critic_loss     | 0.943    |\n","|    ent_coef        | 0.00489  |\n","|    ent_coef_loss   | -0.0399  |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 308290   |\n","---------------------------------\n","Eval num_timesteps=3422400, episode_reward=164.47 +/- 8.31\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 164      |\n","| time/              |          |\n","|    total_timesteps | 3422400  |\n","| train/             |          |\n","|    actor_loss      | -11.4    |\n","|    critic_loss     | 0.625    |\n","|    ent_coef        | 0.00476  |\n","|    ent_coef_loss   | -1.25    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 308390   |\n","---------------------------------\n","Eval num_timesteps=3424800, episode_reward=180.89 +/- 1.64\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 181      |\n","| time/              |          |\n","|    total_timesteps | 3424800  |\n","| train/             |          |\n","|    actor_loss      | -11.2    |\n","|    critic_loss     | 1.31     |\n","|    ent_coef        | 0.00474  |\n","|    ent_coef_loss   | -2.25    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 308490   |\n","---------------------------------\n","Eval num_timesteps=3427200, episode_reward=88.79 +/- 98.44\n","Episode length: 391.20 +/- 133.25\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 391      |\n","|    mean_reward     | 88.8     |\n","| time/              |          |\n","|    total_timesteps | 3427200  |\n","| train/             |          |\n","|    actor_loss      | -10.5    |\n","|    critic_loss     | 4.62     |\n","|    ent_coef        | 0.00478  |\n","|    ent_coef_loss   | -2.63    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 308590   |\n","---------------------------------\n","Eval num_timesteps=3429600, episode_reward=92.89 +/- 105.48\n","Episode length: 389.20 +/- 135.70\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 389      |\n","|    mean_reward     | 92.9     |\n","| time/              |          |\n","|    total_timesteps | 3429600  |\n","| train/             |          |\n","|    actor_loss      | -9.73    |\n","|    critic_loss     | 1.87     |\n","|    ent_coef        | 0.00465  |\n","|    ent_coef_loss   | -0.336   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 308690   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2220     |\n","|    fps             | 290      |\n","|    time_elapsed    | 11792    |\n","|    total_timesteps | 3431280  |\n","| train/             |          |\n","|    actor_loss      | -10.6    |\n","|    critic_loss     | 2.74     |\n","|    ent_coef        | 0.00459  |\n","|    ent_coef_loss   | -0.617   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 308760   |\n","---------------------------------\n","Eval num_timesteps=3432000, episode_reward=178.65 +/- 3.38\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 179      |\n","| time/              |          |\n","|    total_timesteps | 3432000  |\n","| train/             |          |\n","|    actor_loss      | -10      |\n","|    critic_loss     | 2.27     |\n","|    ent_coef        | 0.00457  |\n","|    ent_coef_loss   | 0.288    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 308790   |\n","---------------------------------\n","Eval num_timesteps=3434400, episode_reward=167.83 +/- 1.20\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 168      |\n","| time/              |          |\n","|    total_timesteps | 3434400  |\n","| train/             |          |\n","|    actor_loss      | -10.8    |\n","|    critic_loss     | 2.93     |\n","|    ent_coef        | 0.00453  |\n","|    ent_coef_loss   | 0.844    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 308890   |\n","---------------------------------\n","Eval num_timesteps=3436800, episode_reward=171.82 +/- 6.12\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 172      |\n","| time/              |          |\n","|    total_timesteps | 3436800  |\n","| train/             |          |\n","|    actor_loss      | -11.3    |\n","|    critic_loss     | 2.21     |\n","|    ent_coef        | 0.00461  |\n","|    ent_coef_loss   | 1.16     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 308990   |\n","---------------------------------\n","Eval num_timesteps=3439200, episode_reward=173.46 +/- 2.19\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 173      |\n","| time/              |          |\n","|    total_timesteps | 3439200  |\n","| train/             |          |\n","|    actor_loss      | -10.9    |\n","|    critic_loss     | 1.26     |\n","|    ent_coef        | 0.00451  |\n","|    ent_coef_loss   | -1.15    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 309090   |\n","---------------------------------\n","Eval num_timesteps=3441600, episode_reward=58.73 +/- 94.72\n","Episode length: 360.20 +/- 114.15\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 360      |\n","|    mean_reward     | 58.7     |\n","| time/              |          |\n","|    total_timesteps | 3441600  |\n","| train/             |          |\n","|    actor_loss      | -10.8    |\n","|    critic_loss     | 2.95     |\n","|    ent_coef        | 0.00443  |\n","|    ent_coef_loss   | -2.92    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 309190   |\n","---------------------------------\n","Eval num_timesteps=3444000, episode_reward=-38.62 +/- 6.74\n","Episode length: 231.40 +/- 10.29\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 231      |\n","|    mean_reward     | -38.6    |\n","| time/              |          |\n","|    total_timesteps | 3444000  |\n","| train/             |          |\n","|    actor_loss      | -11.2    |\n","|    critic_loss     | 0.913    |\n","|    ent_coef        | 0.00432  |\n","|    ent_coef_loss   | -3.04    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 309290   |\n","---------------------------------\n","Eval num_timesteps=3446400, episode_reward=184.41 +/- 4.27\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 184      |\n","| time/              |          |\n","|    total_timesteps | 3446400  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 0.742    |\n","|    ent_coef        | 0.00425  |\n","|    ent_coef_loss   | -4.92    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 309390   |\n","---------------------------------\n","New best mean reward!\n","Eval num_timesteps=3448800, episode_reward=79.12 +/- 69.59\n","Episode length: 448.40 +/- 63.20\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 448      |\n","|    mean_reward     | 79.1     |\n","| time/              |          |\n","|    total_timesteps | 3448800  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 1.13     |\n","|    ent_coef        | 0.00416  |\n","|    ent_coef_loss   | -2.04    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 309490   |\n","---------------------------------\n","Eval num_timesteps=3451200, episode_reward=96.17 +/- 83.22\n","Episode length: 429.60 +/- 86.22\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 430      |\n","|    mean_reward     | 96.2     |\n","| time/              |          |\n","|    total_timesteps | 3451200  |\n","| train/             |          |\n","|    actor_loss      | -11.1    |\n","|    critic_loss     | 20.4     |\n","|    ent_coef        | 0.00414  |\n","|    ent_coef_loss   | -2.55    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 309590   |\n","---------------------------------\n","Eval num_timesteps=3453600, episode_reward=169.31 +/- 0.34\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 169      |\n","| time/              |          |\n","|    total_timesteps | 3453600  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 3.31     |\n","|    ent_coef        | 0.00411  |\n","|    ent_coef_loss   | -2.29    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 309690   |\n","---------------------------------\n","Eval num_timesteps=3456000, episode_reward=175.46 +/- 0.41\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 175      |\n","| time/              |          |\n","|    total_timesteps | 3456000  |\n","| train/             |          |\n","|    actor_loss      | -10.6    |\n","|    critic_loss     | 0.963    |\n","|    ent_coef        | 0.00404  |\n","|    ent_coef_loss   | -3.57    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 309790   |\n","---------------------------------\n","Eval num_timesteps=3458400, episode_reward=95.48 +/- 87.46\n","Episode length: 416.80 +/- 101.90\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 417      |\n","|    mean_reward     | 95.5     |\n","| time/              |          |\n","|    total_timesteps | 3458400  |\n","| train/             |          |\n","|    actor_loss      | -11.6    |\n","|    critic_loss     | 0.421    |\n","|    ent_coef        | 0.00406  |\n","|    ent_coef_loss   | 1.55     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 309890   |\n","---------------------------------\n","Eval num_timesteps=3460800, episode_reward=194.01 +/- 1.97\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 194      |\n","| time/              |          |\n","|    total_timesteps | 3460800  |\n","| train/             |          |\n","|    actor_loss      | -10.7    |\n","|    critic_loss     | 0.497    |\n","|    ent_coef        | 0.00408  |\n","|    ent_coef_loss   | 2.84     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 309990   |\n","---------------------------------\n","New best mean reward!\n","Eval num_timesteps=3463200, episode_reward=143.22 +/- 2.63\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 143      |\n","| time/              |          |\n","|    total_timesteps | 3463200  |\n","| train/             |          |\n","|    actor_loss      | -10.8    |\n","|    critic_loss     | 0.749    |\n","|    ent_coef        | 0.00409  |\n","|    ent_coef_loss   | 3.19     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 310090   |\n","---------------------------------\n","Eval num_timesteps=3465600, episode_reward=100.92 +/- 90.92\n","Episode length: 411.20 +/- 108.76\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 411      |\n","|    mean_reward     | 101      |\n","| time/              |          |\n","|    total_timesteps | 3465600  |\n","| train/             |          |\n","|    actor_loss      | -10.5    |\n","|    critic_loss     | 0.383    |\n","|    ent_coef        | 0.00412  |\n","|    ent_coef_loss   | 4.37     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 310190   |\n","---------------------------------\n","Eval num_timesteps=3468000, episode_reward=158.22 +/- 1.17\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 158      |\n","| time/              |          |\n","|    total_timesteps | 3468000  |\n","| train/             |          |\n","|    actor_loss      | -10.5    |\n","|    critic_loss     | 1.45     |\n","|    ent_coef        | 0.00417  |\n","|    ent_coef_loss   | 1.19     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 310290   |\n","---------------------------------\n","Eval num_timesteps=3470400, episode_reward=68.21 +/- 90.49\n","Episode length: 374.60 +/- 102.39\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 375      |\n","|    mean_reward     | 68.2     |\n","| time/              |          |\n","|    total_timesteps | 3470400  |\n","| train/             |          |\n","|    actor_loss      | -10.6    |\n","|    critic_loss     | 0.588    |\n","|    ent_coef        | 0.00428  |\n","|    ent_coef_loss   | 4.03     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 310390   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2230     |\n","|    fps             | 290      |\n","|    time_elapsed    | 11933    |\n","|    total_timesteps | 3472608  |\n","| train/             |          |\n","|    actor_loss      | -11.8    |\n","|    critic_loss     | 1.88     |\n","|    ent_coef        | 0.00444  |\n","|    ent_coef_loss   | 1.03     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 310482   |\n","---------------------------------\n","Eval num_timesteps=3472800, episode_reward=92.85 +/- 78.33\n","Episode length: 425.00 +/- 61.24\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 425      |\n","|    mean_reward     | 92.9     |\n","| time/              |          |\n","|    total_timesteps | 3472800  |\n","| train/             |          |\n","|    actor_loss      | -11.6    |\n","|    critic_loss     | 2.81     |\n","|    ent_coef        | 0.00444  |\n","|    ent_coef_loss   | -1.42    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 310490   |\n","---------------------------------\n","Eval num_timesteps=3475200, episode_reward=-5.30 +/- 12.02\n","Episode length: 322.80 +/- 37.72\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 323      |\n","|    mean_reward     | -5.3     |\n","| time/              |          |\n","|    total_timesteps | 3475200  |\n","| train/             |          |\n","|    actor_loss      | -11.2    |\n","|    critic_loss     | 1.65     |\n","|    ent_coef        | 0.00448  |\n","|    ent_coef_loss   | 0.51     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 310590   |\n","---------------------------------\n","Eval num_timesteps=3477600, episode_reward=78.72 +/- 69.83\n","Episode length: 428.60 +/- 58.30\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 429      |\n","|    mean_reward     | 78.7     |\n","| time/              |          |\n","|    total_timesteps | 3477600  |\n","| train/             |          |\n","|    actor_loss      | -11.5    |\n","|    critic_loss     | 0.863    |\n","|    ent_coef        | 0.0046   |\n","|    ent_coef_loss   | -2.26    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 310690   |\n","---------------------------------\n","Eval num_timesteps=3480000, episode_reward=7.05 +/- 15.34\n","Episode length: 335.40 +/- 38.70\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 335      |\n","|    mean_reward     | 7.05     |\n","| time/              |          |\n","|    total_timesteps | 3480000  |\n","| train/             |          |\n","|    actor_loss      | -9.99    |\n","|    critic_loss     | 1.78     |\n","|    ent_coef        | 0.00461  |\n","|    ent_coef_loss   | 0.713    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 310790   |\n","---------------------------------\n","Eval num_timesteps=3482400, episode_reward=-8.44 +/- 2.98\n","Episode length: 283.80 +/- 5.88\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 284      |\n","|    mean_reward     | -8.44    |\n","| time/              |          |\n","|    total_timesteps | 3482400  |\n","| train/             |          |\n","|    actor_loss      | -10.8    |\n","|    critic_loss     | 0.588    |\n","|    ent_coef        | 0.00467  |\n","|    ent_coef_loss   | -0.435   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 310890   |\n","---------------------------------\n","Eval num_timesteps=3484800, episode_reward=173.93 +/- 4.42\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 174      |\n","| time/              |          |\n","|    total_timesteps | 3484800  |\n","| train/             |          |\n","|    actor_loss      | -10.6    |\n","|    critic_loss     | 0.638    |\n","|    ent_coef        | 0.00474  |\n","|    ent_coef_loss   | 1.87     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 310990   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2240     |\n","|    fps             | 291      |\n","|    time_elapsed    | 11980    |\n","|    total_timesteps | 3487176  |\n","| train/             |          |\n","|    actor_loss      | -10.8    |\n","|    critic_loss     | 3.62     |\n","|    ent_coef        | 0.00475  |\n","|    ent_coef_loss   | -1.5     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 311089   |\n","---------------------------------\n","Eval num_timesteps=3487200, episode_reward=109.08 +/- 96.93\n","Episode length: 411.20 +/- 108.76\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 411      |\n","|    mean_reward     | 109      |\n","| time/              |          |\n","|    total_timesteps | 3487200  |\n","| train/             |          |\n","|    actor_loss      | -10.7    |\n","|    critic_loss     | 0.73     |\n","|    ent_coef        | 0.00475  |\n","|    ent_coef_loss   | -2.29    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 311090   |\n","---------------------------------\n","Eval num_timesteps=3489600, episode_reward=-38.41 +/- 6.01\n","Episode length: 238.00 +/- 22.05\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 238      |\n","|    mean_reward     | -38.4    |\n","| time/              |          |\n","|    total_timesteps | 3489600  |\n","| train/             |          |\n","|    actor_loss      | -11.2    |\n","|    critic_loss     | 1.04     |\n","|    ent_coef        | 0.00474  |\n","|    ent_coef_loss   | -3.3     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 311190   |\n","---------------------------------\n","Eval num_timesteps=3492000, episode_reward=90.59 +/- 115.95\n","Episode length: 375.20 +/- 152.85\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 375      |\n","|    mean_reward     | 90.6     |\n","| time/              |          |\n","|    total_timesteps | 3492000  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 3.76     |\n","|    ent_coef        | 0.00476  |\n","|    ent_coef_loss   | 0.119    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 311290   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2250     |\n","|    fps             | 291      |\n","|    time_elapsed    | 12003    |\n","|    total_timesteps | 3494232  |\n","| train/             |          |\n","|    actor_loss      | -11.3    |\n","|    critic_loss     | 0.705    |\n","|    ent_coef        | 0.00487  |\n","|    ent_coef_loss   | 1.94     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 311383   |\n","---------------------------------\n","Eval num_timesteps=3494400, episode_reward=-2.03 +/- 42.11\n","Episode length: 302.00 +/- 110.23\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 302      |\n","|    mean_reward     | -2.03    |\n","| time/              |          |\n","|    total_timesteps | 3494400  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 1.13     |\n","|    ent_coef        | 0.00488  |\n","|    ent_coef_loss   | 2.46     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 311390   |\n","---------------------------------\n","Eval num_timesteps=3496800, episode_reward=178.30 +/- 0.86\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 178      |\n","| time/              |          |\n","|    total_timesteps | 3496800  |\n","| train/             |          |\n","|    actor_loss      | -11.4    |\n","|    critic_loss     | 1        |\n","|    ent_coef        | 0.0049   |\n","|    ent_coef_loss   | -1.9     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 311490   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2260     |\n","|    fps             | 291      |\n","|    time_elapsed    | 12017    |\n","|    total_timesteps | 3498336  |\n","| train/             |          |\n","|    actor_loss      | -10.2    |\n","|    critic_loss     | 4.08     |\n","|    ent_coef        | 0.0049   |\n","|    ent_coef_loss   | -1.63    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 311554   |\n","---------------------------------\n","Eval num_timesteps=3499200, episode_reward=56.26 +/- 105.01\n","Episode length: 357.20 +/- 116.60\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 357      |\n","|    mean_reward     | 56.3     |\n","| time/              |          |\n","|    total_timesteps | 3499200  |\n","| train/             |          |\n","|    actor_loss      | -11.4    |\n","|    critic_loss     | 0.638    |\n","|    ent_coef        | 0.00487  |\n","|    ent_coef_loss   | -0.315   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 311590   |\n","---------------------------------\n","Eval num_timesteps=3501600, episode_reward=146.66 +/- 0.25\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 147      |\n","| time/              |          |\n","|    total_timesteps | 3501600  |\n","| train/             |          |\n","|    actor_loss      | -11.2    |\n","|    critic_loss     | 7.58     |\n","|    ent_coef        | 0.00487  |\n","|    ent_coef_loss   | 0.739    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 311690   |\n","---------------------------------\n","Eval num_timesteps=3504000, episode_reward=175.99 +/- 1.61\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 176      |\n","| time/              |          |\n","|    total_timesteps | 3504000  |\n","| train/             |          |\n","|    actor_loss      | -10.6    |\n","|    critic_loss     | 1.66     |\n","|    ent_coef        | 0.00485  |\n","|    ent_coef_loss   | 0.525    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 311790   |\n","---------------------------------\n","Eval num_timesteps=3506400, episode_reward=183.55 +/- 0.90\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 184      |\n","| time/              |          |\n","|    total_timesteps | 3506400  |\n","| train/             |          |\n","|    actor_loss      | -10.7    |\n","|    critic_loss     | 1.09     |\n","|    ent_coef        | 0.00478  |\n","|    ent_coef_loss   | -1.76    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 311890   |\n","---------------------------------\n","Eval num_timesteps=3508800, episode_reward=0.80 +/- 41.10\n","Episode length: 331.00 +/- 110.23\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 331      |\n","|    mean_reward     | 0.8      |\n","| time/              |          |\n","|    total_timesteps | 3508800  |\n","| train/             |          |\n","|    actor_loss      | -11.1    |\n","|    critic_loss     | 1.35     |\n","|    ent_coef        | 0.00469  |\n","|    ent_coef_loss   | -1.26    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 311990   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2270     |\n","|    fps             | 291      |\n","|    time_elapsed    | 12059    |\n","|    total_timesteps | 3510696  |\n","| train/             |          |\n","|    actor_loss      | -11.1    |\n","|    critic_loss     | 1.39     |\n","|    ent_coef        | 0.0047   |\n","|    ent_coef_loss   | -1.62    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 312069   |\n","---------------------------------\n","Eval num_timesteps=3511200, episode_reward=103.87 +/- 90.46\n","Episode length: 418.00 +/- 100.43\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 418      |\n","|    mean_reward     | 104      |\n","| time/              |          |\n","|    total_timesteps | 3511200  |\n","| train/             |          |\n","|    actor_loss      | -11.4    |\n","|    critic_loss     | 1.37     |\n","|    ent_coef        | 0.00469  |\n","|    ent_coef_loss   | 0.223    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 312090   |\n","---------------------------------\n","Eval num_timesteps=3513600, episode_reward=1.30 +/- 19.11\n","Episode length: 303.40 +/- 43.60\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 303      |\n","|    mean_reward     | 1.3      |\n","| time/              |          |\n","|    total_timesteps | 3513600  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 1.26     |\n","|    ent_coef        | 0.00461  |\n","|    ent_coef_loss   | 2.21     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 312190   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2280     |\n","|    fps             | 291      |\n","|    time_elapsed    | 12072    |\n","|    total_timesteps | 3514824  |\n","| train/             |          |\n","|    actor_loss      | -10.1    |\n","|    critic_loss     | 1.49     |\n","|    ent_coef        | 0.00458  |\n","|    ent_coef_loss   | -0.372   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 312241   |\n","---------------------------------\n","Eval num_timesteps=3516000, episode_reward=84.86 +/- 114.83\n","Episode length: 382.40 +/- 144.03\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 382      |\n","|    mean_reward     | 84.9     |\n","| time/              |          |\n","|    total_timesteps | 3516000  |\n","| train/             |          |\n","|    actor_loss      | -11.2    |\n","|    critic_loss     | 2.14     |\n","|    ent_coef        | 0.00455  |\n","|    ent_coef_loss   | -0.404   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 312290   |\n","---------------------------------\n","Eval num_timesteps=3518400, episode_reward=-38.43 +/- 7.94\n","Episode length: 255.00 +/- 4.90\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 255      |\n","|    mean_reward     | -38.4    |\n","| time/              |          |\n","|    total_timesteps | 3518400  |\n","| train/             |          |\n","|    actor_loss      | -9.94    |\n","|    critic_loss     | 2.14     |\n","|    ent_coef        | 0.00461  |\n","|    ent_coef_loss   | 5.82     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 312390   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2290     |\n","|    fps             | 291      |\n","|    time_elapsed    | 12088    |\n","|    total_timesteps | 3520512  |\n","| train/             |          |\n","|    actor_loss      | -10.8    |\n","|    critic_loss     | 0.471    |\n","|    ent_coef        | 0.00463  |\n","|    ent_coef_loss   | -0.778   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 312478   |\n","---------------------------------\n","Eval num_timesteps=3520800, episode_reward=85.27 +/- 56.42\n","Episode length: 481.40 +/- 15.19\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 481      |\n","|    mean_reward     | 85.3     |\n","| time/              |          |\n","|    total_timesteps | 3520800  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 1.1      |\n","|    ent_coef        | 0.00463  |\n","|    ent_coef_loss   | -0.538   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 312490   |\n","---------------------------------\n","Eval num_timesteps=3523200, episode_reward=60.51 +/- 126.74\n","Episode length: 344.00 +/- 191.06\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 344      |\n","|    mean_reward     | 60.5     |\n","| time/              |          |\n","|    total_timesteps | 3523200  |\n","| train/             |          |\n","|    actor_loss      | -10.1    |\n","|    critic_loss     | 1.19     |\n","|    ent_coef        | 0.00463  |\n","|    ent_coef_loss   | 0.387    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 312590   |\n","---------------------------------\n","Eval num_timesteps=3525600, episode_reward=40.74 +/- 101.84\n","Episode length: 348.80 +/- 123.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 349      |\n","|    mean_reward     | 40.7     |\n","| time/              |          |\n","|    total_timesteps | 3525600  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 0.865    |\n","|    ent_coef        | 0.00459  |\n","|    ent_coef_loss   | -3       |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 312690   |\n","---------------------------------\n","Eval num_timesteps=3528000, episode_reward=152.85 +/- 1.74\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 153      |\n","| time/              |          |\n","|    total_timesteps | 3528000  |\n","| train/             |          |\n","|    actor_loss      | -11.1    |\n","|    critic_loss     | 0.975    |\n","|    ent_coef        | 0.00451  |\n","|    ent_coef_loss   | 3.36     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 312790   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2300     |\n","|    fps             | 291      |\n","|    time_elapsed    | 12121    |\n","|    total_timesteps | 3529176  |\n","| train/             |          |\n","|    actor_loss      | -11.4    |\n","|    critic_loss     | 1        |\n","|    ent_coef        | 0.00457  |\n","|    ent_coef_loss   | 0.86     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 312839   |\n","---------------------------------\n","Eval num_timesteps=3530400, episode_reward=51.87 +/- 74.96\n","Episode length: 408.20 +/- 74.95\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 408      |\n","|    mean_reward     | 51.9     |\n","| time/              |          |\n","|    total_timesteps | 3530400  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 12.3     |\n","|    ent_coef        | 0.00457  |\n","|    ent_coef_loss   | 1.31     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 312890   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2310     |\n","|    fps             | 291      |\n","|    time_elapsed    | 12130    |\n","|    total_timesteps | 3532536  |\n","| train/             |          |\n","|    actor_loss      | -10.9    |\n","|    critic_loss     | 3.42     |\n","|    ent_coef        | 0.00464  |\n","|    ent_coef_loss   | 0.285    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 312979   |\n","---------------------------------\n","Eval num_timesteps=3532800, episode_reward=160.55 +/- 3.44\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 161      |\n","| time/              |          |\n","|    total_timesteps | 3532800  |\n","| train/             |          |\n","|    actor_loss      | -10.7    |\n","|    critic_loss     | 41.2     |\n","|    ent_coef        | 0.00465  |\n","|    ent_coef_loss   | 2.89     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 312990   |\n","---------------------------------\n","Eval num_timesteps=3535200, episode_reward=63.31 +/- 70.37\n","Episode length: 419.60 +/- 65.65\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 420      |\n","|    mean_reward     | 63.3     |\n","| time/              |          |\n","|    total_timesteps | 3535200  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 2.42     |\n","|    ent_coef        | 0.00464  |\n","|    ent_coef_loss   | -2.83    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 313090   |\n","---------------------------------\n","Eval num_timesteps=3537600, episode_reward=44.50 +/- 79.22\n","Episode length: 393.20 +/- 87.20\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 393      |\n","|    mean_reward     | 44.5     |\n","| time/              |          |\n","|    total_timesteps | 3537600  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 1.57     |\n","|    ent_coef        | 0.00462  |\n","|    ent_coef_loss   | 0.773    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 313190   |\n","---------------------------------\n","Eval num_timesteps=3540000, episode_reward=78.08 +/- 69.79\n","Episode length: 453.80 +/- 37.72\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 454      |\n","|    mean_reward     | 78.1     |\n","| time/              |          |\n","|    total_timesteps | 3540000  |\n","| train/             |          |\n","|    actor_loss      | -10.1    |\n","|    critic_loss     | 0.62     |\n","|    ent_coef        | 0.00463  |\n","|    ent_coef_loss   | 1.18     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 313290   |\n","---------------------------------\n","Eval num_timesteps=3542400, episode_reward=95.01 +/- 60.30\n","Episode length: 471.20 +/- 35.27\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 471      |\n","|    mean_reward     | 95       |\n","| time/              |          |\n","|    total_timesteps | 3542400  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 0.872    |\n","|    ent_coef        | 0.00474  |\n","|    ent_coef_loss   | 0.185    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 313390   |\n","---------------------------------\n","Eval num_timesteps=3544800, episode_reward=62.69 +/- 98.38\n","Episode length: 386.40 +/- 139.13\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 386      |\n","|    mean_reward     | 62.7     |\n","| time/              |          |\n","|    total_timesteps | 3544800  |\n","| train/             |          |\n","|    actor_loss      | -9.54    |\n","|    critic_loss     | 1.99     |\n","|    ent_coef        | 0.00472  |\n","|    ent_coef_loss   | 5.46     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 313490   |\n","---------------------------------\n","Eval num_timesteps=3547200, episode_reward=148.01 +/- 4.22\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 148      |\n","| time/              |          |\n","|    total_timesteps | 3547200  |\n","| train/             |          |\n","|    actor_loss      | -10.1    |\n","|    critic_loss     | 1.22     |\n","|    ent_coef        | 0.00479  |\n","|    ent_coef_loss   | -0.526   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 313590   |\n","---------------------------------\n","Eval num_timesteps=3549600, episode_reward=-73.70 +/- 5.32\n","Episode length: 186.40 +/- 11.76\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 186      |\n","|    mean_reward     | -73.7    |\n","| time/              |          |\n","|    total_timesteps | 3549600  |\n","| train/             |          |\n","|    actor_loss      | -11.1    |\n","|    critic_loss     | 1.26     |\n","|    ent_coef        | 0.00466  |\n","|    ent_coef_loss   | -3.14    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 313690   |\n","---------------------------------\n","Eval num_timesteps=3552000, episode_reward=34.40 +/- 111.75\n","Episode length: 328.40 +/- 140.11\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 328      |\n","|    mean_reward     | 34.4     |\n","| time/              |          |\n","|    total_timesteps | 3552000  |\n","| train/             |          |\n","|    actor_loss      | -11.1    |\n","|    critic_loss     | 0.819    |\n","|    ent_coef        | 0.00459  |\n","|    ent_coef_loss   | -3.28    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 313790   |\n","---------------------------------\n","Eval num_timesteps=3554400, episode_reward=-49.06 +/- 12.78\n","Episode length: 252.40 +/- 33.80\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 252      |\n","|    mean_reward     | -49.1    |\n","| time/              |          |\n","|    total_timesteps | 3554400  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 0.831    |\n","|    ent_coef        | 0.00451  |\n","|    ent_coef_loss   | 0.817    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 313890   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2320     |\n","|    fps             | 291      |\n","|    time_elapsed    | 12209    |\n","|    total_timesteps | 3556584  |\n","| train/             |          |\n","|    actor_loss      | -11.3    |\n","|    critic_loss     | 1.73     |\n","|    ent_coef        | 0.00458  |\n","|    ent_coef_loss   | 2.94     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 313981   |\n","---------------------------------\n","Eval num_timesteps=3556800, episode_reward=162.46 +/- 2.79\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 162      |\n","| time/              |          |\n","|    total_timesteps | 3556800  |\n","| train/             |          |\n","|    actor_loss      | -11.5    |\n","|    critic_loss     | 0.434    |\n","|    ent_coef        | 0.00459  |\n","|    ent_coef_loss   | -0.676   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 313990   |\n","---------------------------------\n","Eval num_timesteps=3559200, episode_reward=-43.68 +/- 10.02\n","Episode length: 265.00 +/- 26.94\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 265      |\n","|    mean_reward     | -43.7    |\n","| time/              |          |\n","|    total_timesteps | 3559200  |\n","| train/             |          |\n","|    actor_loss      | -10.9    |\n","|    critic_loss     | 1.29     |\n","|    ent_coef        | 0.0046   |\n","|    ent_coef_loss   | 1.19     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 314090   |\n","---------------------------------\n","Eval num_timesteps=3561600, episode_reward=-5.78 +/- 6.10\n","Episode length: 310.20 +/- 10.78\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 310      |\n","|    mean_reward     | -5.78    |\n","| time/              |          |\n","|    total_timesteps | 3561600  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 0.828    |\n","|    ent_coef        | 0.00461  |\n","|    ent_coef_loss   | 0.363    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 314190   |\n","---------------------------------\n","Eval num_timesteps=3564000, episode_reward=-43.25 +/- 15.17\n","Episode length: 243.20 +/- 42.62\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 243      |\n","|    mean_reward     | -43.2    |\n","| time/              |          |\n","|    total_timesteps | 3564000  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 0.774    |\n","|    ent_coef        | 0.00454  |\n","|    ent_coef_loss   | -1.82    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 314290   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2330     |\n","|    fps             | 291      |\n","|    time_elapsed    | 12234    |\n","|    total_timesteps | 3565704  |\n","| train/             |          |\n","|    actor_loss      | -11.1    |\n","|    critic_loss     | 0.873    |\n","|    ent_coef        | 0.0045   |\n","|    ent_coef_loss   | -3.83    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 314361   |\n","---------------------------------\n","Eval num_timesteps=3566400, episode_reward=167.00 +/- 0.71\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 167      |\n","| time/              |          |\n","|    total_timesteps | 3566400  |\n","| train/             |          |\n","|    actor_loss      | -11.3    |\n","|    critic_loss     | 1.48     |\n","|    ent_coef        | 0.00448  |\n","|    ent_coef_loss   | -0.9     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 314390   |\n","---------------------------------\n","Eval num_timesteps=3568800, episode_reward=35.46 +/- 86.39\n","Episode length: 406.40 +/- 76.42\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 406      |\n","|    mean_reward     | 35.5     |\n","| time/              |          |\n","|    total_timesteps | 3568800  |\n","| train/             |          |\n","|    actor_loss      | -11.2    |\n","|    critic_loss     | 1.93     |\n","|    ent_coef        | 0.00463  |\n","|    ent_coef_loss   | 5.85     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 314490   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2340     |\n","|    fps             | 291      |\n","|    time_elapsed    | 12252    |\n","|    total_timesteps | 3571176  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 1.39     |\n","|    ent_coef        | 0.00484  |\n","|    ent_coef_loss   | 3.39     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 314589   |\n","---------------------------------\n","Eval num_timesteps=3571200, episode_reward=141.57 +/- 4.93\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 142      |\n","| time/              |          |\n","|    total_timesteps | 3571200  |\n","| train/             |          |\n","|    actor_loss      | -10.8    |\n","|    critic_loss     | 0.962    |\n","|    ent_coef        | 0.00484  |\n","|    ent_coef_loss   | 4.68     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 314590   |\n","---------------------------------\n","Eval num_timesteps=3573600, episode_reward=180.97 +/- 2.95\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 181      |\n","| time/              |          |\n","|    total_timesteps | 3573600  |\n","| train/             |          |\n","|    actor_loss      | -9.97    |\n","|    critic_loss     | 10.4     |\n","|    ent_coef        | 0.00497  |\n","|    ent_coef_loss   | 0.482    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 314690   |\n","---------------------------------\n","Eval num_timesteps=3576000, episode_reward=141.46 +/- 5.23\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 141      |\n","| time/              |          |\n","|    total_timesteps | 3576000  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 1.13     |\n","|    ent_coef        | 0.00504  |\n","|    ent_coef_loss   | 3.7      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 314790   |\n","---------------------------------\n","Eval num_timesteps=3578400, episode_reward=83.59 +/- 99.50\n","Episode length: 400.00 +/- 122.47\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 400      |\n","|    mean_reward     | 83.6     |\n","| time/              |          |\n","|    total_timesteps | 3578400  |\n","| train/             |          |\n","|    actor_loss      | -11.7    |\n","|    critic_loss     | 1.72     |\n","|    ent_coef        | 0.00514  |\n","|    ent_coef_loss   | 1.44     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 314890   |\n","---------------------------------\n","Eval num_timesteps=3580800, episode_reward=148.21 +/- 1.76\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 148      |\n","| time/              |          |\n","|    total_timesteps | 3580800  |\n","| train/             |          |\n","|    actor_loss      | -11.7    |\n","|    critic_loss     | 0.981    |\n","|    ent_coef        | 0.00527  |\n","|    ent_coef_loss   | 1.01     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 314990   |\n","---------------------------------\n","Eval num_timesteps=3583200, episode_reward=72.10 +/- 100.42\n","Episode length: 390.80 +/- 133.74\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 391      |\n","|    mean_reward     | 72.1     |\n","| time/              |          |\n","|    total_timesteps | 3583200  |\n","| train/             |          |\n","|    actor_loss      | -11.1    |\n","|    critic_loss     | 2.35     |\n","|    ent_coef        | 0.00525  |\n","|    ent_coef_loss   | -5.03    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 315090   |\n","---------------------------------\n","Eval num_timesteps=3585600, episode_reward=19.31 +/- 96.59\n","Episode length: 341.00 +/- 129.82\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 341      |\n","|    mean_reward     | 19.3     |\n","| time/              |          |\n","|    total_timesteps | 3585600  |\n","| train/             |          |\n","|    actor_loss      | -11.4    |\n","|    critic_loss     | 1.11     |\n","|    ent_coef        | 0.0053   |\n","|    ent_coef_loss   | -0.247   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 315190   |\n","---------------------------------\n","Eval num_timesteps=3588000, episode_reward=54.91 +/- 104.70\n","Episode length: 376.00 +/- 151.87\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 376      |\n","|    mean_reward     | 54.9     |\n","| time/              |          |\n","|    total_timesteps | 3588000  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 1.25     |\n","|    ent_coef        | 0.00539  |\n","|    ent_coef_loss   | 1.88     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 315290   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2350     |\n","|    fps             | 291      |\n","|    time_elapsed    | 12318    |\n","|    total_timesteps | 3590376  |\n","| train/             |          |\n","|    actor_loss      | -9.98    |\n","|    critic_loss     | 4.3      |\n","|    ent_coef        | 0.00532  |\n","|    ent_coef_loss   | 0.492    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 315389   |\n","---------------------------------\n","Eval num_timesteps=3590400, episode_reward=46.99 +/- 120.16\n","Episode length: 338.40 +/- 197.92\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 338      |\n","|    mean_reward     | 47       |\n","| time/              |          |\n","|    total_timesteps | 3590400  |\n","| train/             |          |\n","|    actor_loss      | -11.8    |\n","|    critic_loss     | 0.614    |\n","|    ent_coef        | 0.00532  |\n","|    ent_coef_loss   | -3.34    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 315390   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2360     |\n","|    fps             | 291      |\n","|    time_elapsed    | 12329    |\n","|    total_timesteps | 3592608  |\n","| train/             |          |\n","|    actor_loss      | -11.3    |\n","|    critic_loss     | 1.44     |\n","|    ent_coef        | 0.00523  |\n","|    ent_coef_loss   | -2.31    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 315482   |\n","---------------------------------\n","Eval num_timesteps=3592800, episode_reward=-0.39 +/- 111.99\n","Episode length: 276.20 +/- 182.73\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 276      |\n","|    mean_reward     | -0.391   |\n","| time/              |          |\n","|    total_timesteps | 3592800  |\n","| train/             |          |\n","|    actor_loss      | -11.6    |\n","|    critic_loss     | 0.948    |\n","|    ent_coef        | 0.00522  |\n","|    ent_coef_loss   | -1.79    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 315490   |\n","---------------------------------\n","Eval num_timesteps=3595200, episode_reward=82.37 +/- 68.63\n","Episode length: 455.60 +/- 54.38\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 456      |\n","|    mean_reward     | 82.4     |\n","| time/              |          |\n","|    total_timesteps | 3595200  |\n","| train/             |          |\n","|    actor_loss      | -11.3    |\n","|    critic_loss     | 1.11     |\n","|    ent_coef        | 0.00519  |\n","|    ent_coef_loss   | 0.342    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 315590   |\n","---------------------------------\n","Eval num_timesteps=3597600, episode_reward=-60.98 +/- 2.21\n","Episode length: 219.40 +/- 2.94\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 219      |\n","|    mean_reward     | -61      |\n","| time/              |          |\n","|    total_timesteps | 3597600  |\n","| train/             |          |\n","|    actor_loss      | -11.6    |\n","|    critic_loss     | 0.36     |\n","|    ent_coef        | 0.00518  |\n","|    ent_coef_loss   | -0.85    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 315690   |\n","---------------------------------\n","Eval num_timesteps=3600000, episode_reward=167.74 +/- 5.96\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 168      |\n","| time/              |          |\n","|    total_timesteps | 3600000  |\n","| train/             |          |\n","|    actor_loss      | -10.1    |\n","|    critic_loss     | 0.519    |\n","|    ent_coef        | 0.00529  |\n","|    ent_coef_loss   | -3.62    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 315790   |\n","---------------------------------\n","Eval num_timesteps=3602400, episode_reward=174.15 +/- 0.61\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 174      |\n","| time/              |          |\n","|    total_timesteps | 3602400  |\n","| train/             |          |\n","|    actor_loss      | -11.4    |\n","|    critic_loss     | 2.16     |\n","|    ent_coef        | 0.00527  |\n","|    ent_coef_loss   | -1.58    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 315890   |\n","---------------------------------\n","Eval num_timesteps=3604800, episode_reward=68.31 +/- 73.45\n","Episode length: 427.40 +/- 59.28\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 427      |\n","|    mean_reward     | 68.3     |\n","| time/              |          |\n","|    total_timesteps | 3604800  |\n","| train/             |          |\n","|    actor_loss      | -11.9    |\n","|    critic_loss     | 0.715    |\n","|    ent_coef        | 0.00521  |\n","|    ent_coef_loss   | -4.13    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 315990   |\n","---------------------------------\n","Eval num_timesteps=3607200, episode_reward=-28.09 +/- 20.08\n","Episode length: 305.40 +/- 55.85\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 305      |\n","|    mean_reward     | -28.1    |\n","| time/              |          |\n","|    total_timesteps | 3607200  |\n","| train/             |          |\n","|    actor_loss      | -10.4    |\n","|    critic_loss     | 45.8     |\n","|    ent_coef        | 0.00514  |\n","|    ent_coef_loss   | 0.2      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 316090   |\n","---------------------------------\n","Eval num_timesteps=3609600, episode_reward=100.11 +/- 94.42\n","Episode length: 420.80 +/- 97.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 421      |\n","|    mean_reward     | 100      |\n","| time/              |          |\n","|    total_timesteps | 3609600  |\n","| train/             |          |\n","|    actor_loss      | -11.4    |\n","|    critic_loss     | 0.849    |\n","|    ent_coef        | 0.00513  |\n","|    ent_coef_loss   | -1.04    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 316190   |\n","---------------------------------\n","Eval num_timesteps=3612000, episode_reward=156.94 +/- 1.11\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 157      |\n","| time/              |          |\n","|    total_timesteps | 3612000  |\n","| train/             |          |\n","|    actor_loss      | -11.6    |\n","|    critic_loss     | 1.11     |\n","|    ent_coef        | 0.00504  |\n","|    ent_coef_loss   | -4.15    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 316290   |\n","---------------------------------\n","Eval num_timesteps=3614400, episode_reward=63.57 +/- 98.89\n","Episode length: 387.20 +/- 138.15\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 387      |\n","|    mean_reward     | 63.6     |\n","| time/              |          |\n","|    total_timesteps | 3614400  |\n","| train/             |          |\n","|    actor_loss      | -10.7    |\n","|    critic_loss     | 1.13     |\n","|    ent_coef        | 0.00496  |\n","|    ent_coef_loss   | -0.191   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 316390   |\n","---------------------------------\n","Eval num_timesteps=3616800, episode_reward=162.36 +/- 2.93\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 162      |\n","| time/              |          |\n","|    total_timesteps | 3616800  |\n","| train/             |          |\n","|    actor_loss      | -10.2    |\n","|    critic_loss     | 1.17     |\n","|    ent_coef        | 0.00492  |\n","|    ent_coef_loss   | 1.33     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 316490   |\n","---------------------------------\n","Eval num_timesteps=3619200, episode_reward=72.21 +/- 96.27\n","Episode length: 410.40 +/- 109.74\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 410      |\n","|    mean_reward     | 72.2     |\n","| time/              |          |\n","|    total_timesteps | 3619200  |\n","| train/             |          |\n","|    actor_loss      | -10.6    |\n","|    critic_loss     | 1.12     |\n","|    ent_coef        | 0.00509  |\n","|    ent_coef_loss   | 0.412    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 316590   |\n","---------------------------------\n","Eval num_timesteps=3621600, episode_reward=144.24 +/- 1.25\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 144      |\n","| time/              |          |\n","|    total_timesteps | 3621600  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 24.7     |\n","|    ent_coef        | 0.00505  |\n","|    ent_coef_loss   | 2.06     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 316690   |\n","---------------------------------\n","Eval num_timesteps=3624000, episode_reward=57.16 +/- 73.65\n","Episode length: 414.80 +/- 69.57\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 415      |\n","|    mean_reward     | 57.2     |\n","| time/              |          |\n","|    total_timesteps | 3624000  |\n","| train/             |          |\n","|    actor_loss      | -10.8    |\n","|    critic_loss     | 1.13     |\n","|    ent_coef        | 0.00508  |\n","|    ent_coef_loss   | 0.361    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 316790   |\n","---------------------------------\n","Eval num_timesteps=3626400, episode_reward=69.08 +/- 68.75\n","Episode length: 444.20 +/- 45.56\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 444      |\n","|    mean_reward     | 69.1     |\n","| time/              |          |\n","|    total_timesteps | 3626400  |\n","| train/             |          |\n","|    actor_loss      | -9.75    |\n","|    critic_loss     | 0.95     |\n","|    ent_coef        | 0.0051   |\n","|    ent_coef_loss   | -1.43    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 316890   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2370     |\n","|    fps             | 291      |\n","|    time_elapsed    | 12449    |\n","|    total_timesteps | 3628440  |\n","| train/             |          |\n","|    actor_loss      | -10.4    |\n","|    critic_loss     | 1.42     |\n","|    ent_coef        | 0.00508  |\n","|    ent_coef_loss   | 2.75     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 316975   |\n","---------------------------------\n","Eval num_timesteps=3628800, episode_reward=139.13 +/- 0.23\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 139      |\n","| time/              |          |\n","|    total_timesteps | 3628800  |\n","| train/             |          |\n","|    actor_loss      | -11.7    |\n","|    critic_loss     | 0.821    |\n","|    ent_coef        | 0.00509  |\n","|    ent_coef_loss   | 0.056    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 316990   |\n","---------------------------------\n","Eval num_timesteps=3631200, episode_reward=64.47 +/- 57.75\n","Episode length: 465.80 +/- 27.92\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 466      |\n","|    mean_reward     | 64.5     |\n","| time/              |          |\n","|    total_timesteps | 3631200  |\n","| train/             |          |\n","|    actor_loss      | -10.5    |\n","|    critic_loss     | 12.6     |\n","|    ent_coef        | 0.00519  |\n","|    ent_coef_loss   | -1.32    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 317090   |\n","---------------------------------\n","Eval num_timesteps=3633600, episode_reward=149.85 +/- 0.95\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 150      |\n","| time/              |          |\n","|    total_timesteps | 3633600  |\n","| train/             |          |\n","|    actor_loss      | -10.5    |\n","|    critic_loss     | 0.704    |\n","|    ent_coef        | 0.00516  |\n","|    ent_coef_loss   | -2.32    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 317190   |\n","---------------------------------\n","Eval num_timesteps=3636000, episode_reward=79.53 +/- 81.75\n","Episode length: 424.80 +/- 92.10\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 425      |\n","|    mean_reward     | 79.5     |\n","| time/              |          |\n","|    total_timesteps | 3636000  |\n","| train/             |          |\n","|    actor_loss      | -11.1    |\n","|    critic_loss     | 1        |\n","|    ent_coef        | 0.00512  |\n","|    ent_coef_loss   | 2.4      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 317290   |\n","---------------------------------\n","Eval num_timesteps=3638400, episode_reward=126.12 +/- 3.27\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 126      |\n","| time/              |          |\n","|    total_timesteps | 3638400  |\n","| train/             |          |\n","|    actor_loss      | -11.4    |\n","|    critic_loss     | 3.05     |\n","|    ent_coef        | 0.00524  |\n","|    ent_coef_loss   | -0.541   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 317390   |\n","---------------------------------\n","Eval num_timesteps=3640800, episode_reward=160.28 +/- 4.34\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 160      |\n","| time/              |          |\n","|    total_timesteps | 3640800  |\n","| train/             |          |\n","|    actor_loss      | -11.1    |\n","|    critic_loss     | 1        |\n","|    ent_coef        | 0.00527  |\n","|    ent_coef_loss   | 1.61     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 317490   |\n","---------------------------------\n","Eval num_timesteps=3643200, episode_reward=156.61 +/- 0.39\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 157      |\n","| time/              |          |\n","|    total_timesteps | 3643200  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 0.805    |\n","|    ent_coef        | 0.00537  |\n","|    ent_coef_loss   | -2.54    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 317590   |\n","---------------------------------\n","Eval num_timesteps=3645600, episode_reward=-83.68 +/- 9.87\n","Episode length: 144.20 +/- 27.92\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 144      |\n","|    mean_reward     | -83.7    |\n","| time/              |          |\n","|    total_timesteps | 3645600  |\n","| train/             |          |\n","|    actor_loss      | -10.9    |\n","|    critic_loss     | 1.47     |\n","|    ent_coef        | 0.00528  |\n","|    ent_coef_loss   | -2.9     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 317690   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2380     |\n","|    fps             | 291      |\n","|    time_elapsed    | 12512    |\n","|    total_timesteps | 3646248  |\n","| train/             |          |\n","|    actor_loss      | -10.1    |\n","|    critic_loss     | 1.82     |\n","|    ent_coef        | 0.00524  |\n","|    ent_coef_loss   | -1.71    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 317717   |\n","---------------------------------\n","Eval num_timesteps=3648000, episode_reward=108.10 +/- 78.65\n","Episode length: 443.60 +/- 69.08\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 444      |\n","|    mean_reward     | 108      |\n","| time/              |          |\n","|    total_timesteps | 3648000  |\n","| train/             |          |\n","|    actor_loss      | -10.6    |\n","|    critic_loss     | 3.62     |\n","|    ent_coef        | 0.00519  |\n","|    ent_coef_loss   | -2.07    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 317790   |\n","---------------------------------\n","Eval num_timesteps=3650400, episode_reward=164.67 +/- 4.75\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 165      |\n","| time/              |          |\n","|    total_timesteps | 3650400  |\n","| train/             |          |\n","|    actor_loss      | -10.7    |\n","|    critic_loss     | 0.755    |\n","|    ent_coef        | 0.00525  |\n","|    ent_coef_loss   | -0.777   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 317890   |\n","---------------------------------\n","Eval num_timesteps=3652800, episode_reward=145.14 +/- 9.51\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 145      |\n","| time/              |          |\n","|    total_timesteps | 3652800  |\n","| train/             |          |\n","|    actor_loss      | -10.8    |\n","|    critic_loss     | 2.44     |\n","|    ent_coef        | 0.00527  |\n","|    ent_coef_loss   | -1.71    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 317990   |\n","---------------------------------\n","Eval num_timesteps=3655200, episode_reward=48.70 +/- 125.45\n","Episode length: 330.00 +/- 208.21\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 330      |\n","|    mean_reward     | 48.7     |\n","| time/              |          |\n","|    total_timesteps | 3655200  |\n","| train/             |          |\n","|    actor_loss      | -11.2    |\n","|    critic_loss     | 2.47     |\n","|    ent_coef        | 0.00536  |\n","|    ent_coef_loss   | 1.99     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 318090   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2390     |\n","|    fps             | 291      |\n","|    time_elapsed    | 12545    |\n","|    total_timesteps | 3657000  |\n","| train/             |          |\n","|    actor_loss      | -10.5    |\n","|    critic_loss     | 1.43     |\n","|    ent_coef        | 0.00545  |\n","|    ent_coef_loss   | 2.32     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 318165   |\n","---------------------------------\n","Eval num_timesteps=3657600, episode_reward=49.56 +/- 85.66\n","Episode length: 369.80 +/- 106.31\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 370      |\n","|    mean_reward     | 49.6     |\n","| time/              |          |\n","|    total_timesteps | 3657600  |\n","| train/             |          |\n","|    actor_loss      | -10.5    |\n","|    critic_loss     | 2.12     |\n","|    ent_coef        | 0.00546  |\n","|    ent_coef_loss   | -2.82    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 318190   |\n","---------------------------------\n","Eval num_timesteps=3660000, episode_reward=166.21 +/- 4.23\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 166      |\n","| time/              |          |\n","|    total_timesteps | 3660000  |\n","| train/             |          |\n","|    actor_loss      | -11.6    |\n","|    critic_loss     | 5.65     |\n","|    ent_coef        | 0.00544  |\n","|    ent_coef_loss   | 0.834    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 318290   |\n","---------------------------------\n","Eval num_timesteps=3662400, episode_reward=169.20 +/- 1.00\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 169      |\n","| time/              |          |\n","|    total_timesteps | 3662400  |\n","| train/             |          |\n","|    actor_loss      | -10.9    |\n","|    critic_loss     | 3.03     |\n","|    ent_coef        | 0.00546  |\n","|    ent_coef_loss   | 1.63     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 318390   |\n","---------------------------------\n","Eval num_timesteps=3664800, episode_reward=170.08 +/- 0.31\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 170      |\n","| time/              |          |\n","|    total_timesteps | 3664800  |\n","| train/             |          |\n","|    actor_loss      | -9.58    |\n","|    critic_loss     | 3.17     |\n","|    ent_coef        | 0.00559  |\n","|    ent_coef_loss   | 3.57     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 318490   |\n","---------------------------------\n","Eval num_timesteps=3667200, episode_reward=146.84 +/- 9.24\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 147      |\n","| time/              |          |\n","|    total_timesteps | 3667200  |\n","| train/             |          |\n","|    actor_loss      | -9.73    |\n","|    critic_loss     | 1.98     |\n","|    ent_coef        | 0.00565  |\n","|    ent_coef_loss   | 0.855    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 318590   |\n","---------------------------------\n","Eval num_timesteps=3669600, episode_reward=173.16 +/- 3.04\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 173      |\n","| time/              |          |\n","|    total_timesteps | 3669600  |\n","| train/             |          |\n","|    actor_loss      | -10.7    |\n","|    critic_loss     | 2.11     |\n","|    ent_coef        | 0.00564  |\n","|    ent_coef_loss   | 2.84     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 318690   |\n","---------------------------------\n","Eval num_timesteps=3672000, episode_reward=27.49 +/- 75.60\n","Episode length: 409.40 +/- 73.97\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 409      |\n","|    mean_reward     | 27.5     |\n","| time/              |          |\n","|    total_timesteps | 3672000  |\n","| train/             |          |\n","|    actor_loss      | -11.4    |\n","|    critic_loss     | 0.982    |\n","|    ent_coef        | 0.00569  |\n","|    ent_coef_loss   | 3.16     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 318790   |\n","---------------------------------\n","Eval num_timesteps=3674400, episode_reward=67.23 +/- 80.71\n","Episode length: 431.60 +/- 83.77\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 432      |\n","|    mean_reward     | 67.2     |\n","| time/              |          |\n","|    total_timesteps | 3674400  |\n","| train/             |          |\n","|    actor_loss      | -10.2    |\n","|    critic_loss     | 1.5      |\n","|    ent_coef        | 0.00575  |\n","|    ent_coef_loss   | 0.73     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 318890   |\n","---------------------------------\n","Eval num_timesteps=3676800, episode_reward=149.83 +/- 0.38\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 150      |\n","| time/              |          |\n","|    total_timesteps | 3676800  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 6.68     |\n","|    ent_coef        | 0.00575  |\n","|    ent_coef_loss   | 1.29     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 318990   |\n","---------------------------------\n","Eval num_timesteps=3679200, episode_reward=40.97 +/- 104.46\n","Episode length: 332.60 +/- 136.68\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 333      |\n","|    mean_reward     | 41       |\n","| time/              |          |\n","|    total_timesteps | 3679200  |\n","| train/             |          |\n","|    actor_loss      | -11.2    |\n","|    critic_loss     | 1.04     |\n","|    ent_coef        | 0.00574  |\n","|    ent_coef_loss   | -1.33    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 319090   |\n","---------------------------------\n","Eval num_timesteps=3681600, episode_reward=-18.64 +/- 40.49\n","Episode length: 290.00 +/- 100.43\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 290      |\n","|    mean_reward     | -18.6    |\n","| time/              |          |\n","|    total_timesteps | 3681600  |\n","| train/             |          |\n","|    actor_loss      | -11.5    |\n","|    critic_loss     | 1.25     |\n","|    ent_coef        | 0.00562  |\n","|    ent_coef_loss   | -1.9     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 319190   |\n","---------------------------------\n","Eval num_timesteps=3684000, episode_reward=84.98 +/- 101.25\n","Episode length: 406.80 +/- 114.15\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 407      |\n","|    mean_reward     | 85       |\n","| time/              |          |\n","|    total_timesteps | 3684000  |\n","| train/             |          |\n","|    actor_loss      | -10.5    |\n","|    critic_loss     | 1.36     |\n","|    ent_coef        | 0.00559  |\n","|    ent_coef_loss   | 1.77     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 319290   |\n","---------------------------------\n","Eval num_timesteps=3686400, episode_reward=157.04 +/- 2.62\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 157      |\n","| time/              |          |\n","|    total_timesteps | 3686400  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 0.809    |\n","|    ent_coef        | 0.0055   |\n","|    ent_coef_loss   | -0.399   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 319390   |\n","---------------------------------\n","Eval num_timesteps=3688800, episode_reward=70.55 +/- 100.28\n","Episode length: 384.00 +/- 142.07\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 384      |\n","|    mean_reward     | 70.5     |\n","| time/              |          |\n","|    total_timesteps | 3688800  |\n","| train/             |          |\n","|    actor_loss      | -10.5    |\n","|    critic_loss     | 2.28     |\n","|    ent_coef        | 0.00549  |\n","|    ent_coef_loss   | 0.719    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 319490   |\n","---------------------------------\n","Eval num_timesteps=3691200, episode_reward=-40.34 +/- 22.22\n","Episode length: 254.20 +/- 82.30\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 254      |\n","|    mean_reward     | -40.3    |\n","| time/              |          |\n","|    total_timesteps | 3691200  |\n","| train/             |          |\n","|    actor_loss      | -10.4    |\n","|    critic_loss     | 3.35     |\n","|    ent_coef        | 0.00541  |\n","|    ent_coef_loss   | -2.25    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 319590   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2400     |\n","|    fps             | 291      |\n","|    time_elapsed    | 12667    |\n","|    total_timesteps | 3691392  |\n","| train/             |          |\n","|    actor_loss      | -10.6    |\n","|    critic_loss     | 1.19     |\n","|    ent_coef        | 0.00539  |\n","|    ent_coef_loss   | -0.128   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 319598   |\n","---------------------------------\n","Eval num_timesteps=3693600, episode_reward=160.35 +/- 1.52\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 160      |\n","| time/              |          |\n","|    total_timesteps | 3693600  |\n","| train/             |          |\n","|    actor_loss      | -10.1    |\n","|    critic_loss     | 1        |\n","|    ent_coef        | 0.00537  |\n","|    ent_coef_loss   | 0.0887   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 319690   |\n","---------------------------------\n","Eval num_timesteps=3696000, episode_reward=174.84 +/- 3.65\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 175      |\n","| time/              |          |\n","|    total_timesteps | 3696000  |\n","| train/             |          |\n","|    actor_loss      | -10.9    |\n","|    critic_loss     | 1.41     |\n","|    ent_coef        | 0.00547  |\n","|    ent_coef_loss   | 0.426    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 319790   |\n","---------------------------------\n","Eval num_timesteps=3698400, episode_reward=82.49 +/- 93.13\n","Episode length: 408.40 +/- 112.19\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 408      |\n","|    mean_reward     | 82.5     |\n","| time/              |          |\n","|    total_timesteps | 3698400  |\n","| train/             |          |\n","|    actor_loss      | -10      |\n","|    critic_loss     | 1.81     |\n","|    ent_coef        | 0.00545  |\n","|    ent_coef_loss   | -0.0965  |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 319890   |\n","---------------------------------\n","Eval num_timesteps=3700800, episode_reward=-52.58 +/- 21.87\n","Episode length: 235.00 +/- 68.59\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 235      |\n","|    mean_reward     | -52.6    |\n","| time/              |          |\n","|    total_timesteps | 3700800  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 1.12     |\n","|    ent_coef        | 0.0054   |\n","|    ent_coef_loss   | -0.895   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 319990   |\n","---------------------------------\n","Eval num_timesteps=3703200, episode_reward=60.42 +/- 86.65\n","Episode length: 388.40 +/- 91.12\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 388      |\n","|    mean_reward     | 60.4     |\n","| time/              |          |\n","|    total_timesteps | 3703200  |\n","| train/             |          |\n","|    actor_loss      | -10.4    |\n","|    critic_loss     | 1.41     |\n","|    ent_coef        | 0.00542  |\n","|    ent_coef_loss   | 4        |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 320090   |\n","---------------------------------\n","Eval num_timesteps=3705600, episode_reward=-62.76 +/- 3.74\n","Episode length: 213.00 +/- 4.90\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 213      |\n","|    mean_reward     | -62.8    |\n","| time/              |          |\n","|    total_timesteps | 3705600  |\n","| train/             |          |\n","|    actor_loss      | -9.45    |\n","|    critic_loss     | 46.2     |\n","|    ent_coef        | 0.00548  |\n","|    ent_coef_loss   | 2.46     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 320190   |\n","---------------------------------\n","Eval num_timesteps=3708000, episode_reward=64.91 +/- 94.62\n","Episode length: 402.00 +/- 120.02\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 402      |\n","|    mean_reward     | 64.9     |\n","| time/              |          |\n","|    total_timesteps | 3708000  |\n","| train/             |          |\n","|    actor_loss      | -10.5    |\n","|    critic_loss     | 1.2      |\n","|    ent_coef        | 0.00562  |\n","|    ent_coef_loss   | -0.62    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 320290   |\n","---------------------------------\n","Eval num_timesteps=3710400, episode_reward=14.87 +/- 33.90\n","Episode length: 389.60 +/- 83.77\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 390      |\n","|    mean_reward     | 14.9     |\n","| time/              |          |\n","|    total_timesteps | 3710400  |\n","| train/             |          |\n","|    actor_loss      | -10.5    |\n","|    critic_loss     | 2.08     |\n","|    ent_coef        | 0.00554  |\n","|    ent_coef_loss   | -0.427   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 320390   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2410     |\n","|    fps             | 291      |\n","|    time_elapsed    | 12729    |\n","|    total_timesteps | 3711408  |\n","| train/             |          |\n","|    actor_loss      | -10.5    |\n","|    critic_loss     | 0.589    |\n","|    ent_coef        | 0.00556  |\n","|    ent_coef_loss   | 0.764    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 320432   |\n","---------------------------------\n","Eval num_timesteps=3712800, episode_reward=-32.97 +/- 10.78\n","Episode length: 262.80 +/- 27.92\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 263      |\n","|    mean_reward     | -33      |\n","| time/              |          |\n","|    total_timesteps | 3712800  |\n","| train/             |          |\n","|    actor_loss      | -10.6    |\n","|    critic_loss     | 1.52     |\n","|    ent_coef        | 0.00558  |\n","|    ent_coef_loss   | -3.1     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 320490   |\n","---------------------------------\n","Eval num_timesteps=3715200, episode_reward=-36.52 +/- 17.59\n","Episode length: 299.20 +/- 48.01\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 299      |\n","|    mean_reward     | -36.5    |\n","| time/              |          |\n","|    total_timesteps | 3715200  |\n","| train/             |          |\n","|    actor_loss      | -10.2    |\n","|    critic_loss     | 1.2      |\n","|    ent_coef        | 0.00551  |\n","|    ent_coef_loss   | 0.878    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 320590   |\n","---------------------------------\n","Eval num_timesteps=3717600, episode_reward=156.66 +/- 0.54\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 157      |\n","| time/              |          |\n","|    total_timesteps | 3717600  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 18.6     |\n","|    ent_coef        | 0.00554  |\n","|    ent_coef_loss   | -1.66    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 320690   |\n","---------------------------------\n","Eval num_timesteps=3720000, episode_reward=75.83 +/- 74.43\n","Episode length: 446.00 +/- 66.14\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 446      |\n","|    mean_reward     | 75.8     |\n","| time/              |          |\n","|    total_timesteps | 3720000  |\n","| train/             |          |\n","|    actor_loss      | -10.5    |\n","|    critic_loss     | 1.06     |\n","|    ent_coef        | 0.00553  |\n","|    ent_coef_loss   | -0.0631  |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 320790   |\n","---------------------------------\n","Eval num_timesteps=3722400, episode_reward=75.21 +/- 86.44\n","Episode length: 411.60 +/- 108.27\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 412      |\n","|    mean_reward     | 75.2     |\n","| time/              |          |\n","|    total_timesteps | 3722400  |\n","| train/             |          |\n","|    actor_loss      | -10.8    |\n","|    critic_loss     | 1.42     |\n","|    ent_coef        | 0.0055   |\n","|    ent_coef_loss   | -0.99    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 320890   |\n","---------------------------------\n","Eval num_timesteps=3724800, episode_reward=50.23 +/- 88.53\n","Episode length: 373.40 +/- 103.37\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 373      |\n","|    mean_reward     | 50.2     |\n","| time/              |          |\n","|    total_timesteps | 3724800  |\n","| train/             |          |\n","|    actor_loss      | -9.26    |\n","|    critic_loss     | 0.86     |\n","|    ent_coef        | 0.00555  |\n","|    ent_coef_loss   | 0.92     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 320990   |\n","---------------------------------\n","Eval num_timesteps=3727200, episode_reward=76.49 +/- 107.96\n","Episode length: 380.00 +/- 146.97\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 380      |\n","|    mean_reward     | 76.5     |\n","| time/              |          |\n","|    total_timesteps | 3727200  |\n","| train/             |          |\n","|    actor_loss      | -10.9    |\n","|    critic_loss     | 2.29     |\n","|    ent_coef        | 0.00561  |\n","|    ent_coef_loss   | -3.11    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 321090   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2420     |\n","|    fps             | 291      |\n","|    time_elapsed    | 12784    |\n","|    total_timesteps | 3729144  |\n","| train/             |          |\n","|    actor_loss      | -10.7    |\n","|    critic_loss     | 1.6      |\n","|    ent_coef        | 0.00558  |\n","|    ent_coef_loss   | -1.58    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 321171   |\n","---------------------------------\n","Eval num_timesteps=3729600, episode_reward=146.54 +/- 1.03\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 147      |\n","| time/              |          |\n","|    total_timesteps | 3729600  |\n","| train/             |          |\n","|    actor_loss      | -10.7    |\n","|    critic_loss     | 1.03     |\n","|    ent_coef        | 0.00558  |\n","|    ent_coef_loss   | -1.7     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 321190   |\n","---------------------------------\n","Eval num_timesteps=3732000, episode_reward=-25.83 +/- 35.30\n","Episode length: 250.20 +/- 89.16\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 250      |\n","|    mean_reward     | -25.8    |\n","| time/              |          |\n","|    total_timesteps | 3732000  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 1.83     |\n","|    ent_coef        | 0.00549  |\n","|    ent_coef_loss   | -1.31    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 321290   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2430     |\n","|    fps             | 291      |\n","|    time_elapsed    | 12799    |\n","|    total_timesteps | 3732432  |\n","| train/             |          |\n","|    actor_loss      | -10.1    |\n","|    critic_loss     | 1.95     |\n","|    ent_coef        | 0.00549  |\n","|    ent_coef_loss   | 3.96     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 321308   |\n","---------------------------------\n","Eval num_timesteps=3734400, episode_reward=150.48 +/- 0.57\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 150      |\n","| time/              |          |\n","|    total_timesteps | 3734400  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 1.87     |\n","|    ent_coef        | 0.00552  |\n","|    ent_coef_loss   | -1.64    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 321390   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2440     |\n","|    fps             | 291      |\n","|    time_elapsed    | 12808    |\n","|    total_timesteps | 3736752  |\n","| train/             |          |\n","|    actor_loss      | -10.7    |\n","|    critic_loss     | 2.43     |\n","|    ent_coef        | 0.00558  |\n","|    ent_coef_loss   | 3.09     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 321488   |\n","---------------------------------\n","Eval num_timesteps=3736800, episode_reward=160.61 +/- 2.21\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 161      |\n","| time/              |          |\n","|    total_timesteps | 3736800  |\n","| train/             |          |\n","|    actor_loss      | -9.48    |\n","|    critic_loss     | 3.59     |\n","|    ent_coef        | 0.00558  |\n","|    ent_coef_loss   | 4.01     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 321490   |\n","---------------------------------\n","Eval num_timesteps=3739200, episode_reward=88.52 +/- 67.73\n","Episode length: 467.60 +/- 39.68\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 468      |\n","|    mean_reward     | 88.5     |\n","| time/              |          |\n","|    total_timesteps | 3739200  |\n","| train/             |          |\n","|    actor_loss      | -10.2    |\n","|    critic_loss     | 1.96     |\n","|    ent_coef        | 0.00583  |\n","|    ent_coef_loss   | 4.39     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 321590   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2450     |\n","|    fps             | 291      |\n","|    time_elapsed    | 12825    |\n","|    total_timesteps | 3741576  |\n","| train/             |          |\n","|    actor_loss      | -11.4    |\n","|    critic_loss     | 1.12     |\n","|    ent_coef        | 0.00613  |\n","|    ent_coef_loss   | 2.01     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 321689   |\n","---------------------------------\n","Eval num_timesteps=3741600, episode_reward=-97.20 +/- 5.03\n","Episode length: 113.60 +/- 0.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 114      |\n","|    mean_reward     | -97.2    |\n","| time/              |          |\n","|    total_timesteps | 3741600  |\n","| train/             |          |\n","|    actor_loss      | -9.54    |\n","|    critic_loss     | 2.36     |\n","|    ent_coef        | 0.00613  |\n","|    ent_coef_loss   | 4.79     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 321690   |\n","---------------------------------\n","Eval num_timesteps=3744000, episode_reward=-8.05 +/- 119.62\n","Episode length: 253.40 +/- 201.35\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 253      |\n","|    mean_reward     | -8.05    |\n","| time/              |          |\n","|    total_timesteps | 3744000  |\n","| train/             |          |\n","|    actor_loss      | -10.2    |\n","|    critic_loss     | 0.925    |\n","|    ent_coef        | 0.00638  |\n","|    ent_coef_loss   | 8        |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 321790   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2460     |\n","|    fps             | 291      |\n","|    time_elapsed    | 12838    |\n","|    total_timesteps | 3744504  |\n","| train/             |          |\n","|    actor_loss      | -10      |\n","|    critic_loss     | 1.74     |\n","|    ent_coef        | 0.00644  |\n","|    ent_coef_loss   | 4.42     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 321811   |\n","---------------------------------\n","Eval num_timesteps=3746400, episode_reward=162.11 +/- 3.63\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 162      |\n","| time/              |          |\n","|    total_timesteps | 3746400  |\n","| train/             |          |\n","|    actor_loss      | -7.9     |\n","|    critic_loss     | 2.3      |\n","|    ent_coef        | 0.00645  |\n","|    ent_coef_loss   | 2.49     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 321890   |\n","---------------------------------\n","Eval num_timesteps=3748800, episode_reward=137.39 +/- 0.77\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 137      |\n","| time/              |          |\n","|    total_timesteps | 3748800  |\n","| train/             |          |\n","|    actor_loss      | -10.5    |\n","|    critic_loss     | 0.933    |\n","|    ent_coef        | 0.00641  |\n","|    ent_coef_loss   | 0.98     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 321990   |\n","---------------------------------\n","Eval num_timesteps=3751200, episode_reward=98.79 +/- 93.39\n","Episode length: 410.00 +/- 110.23\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 410      |\n","|    mean_reward     | 98.8     |\n","| time/              |          |\n","|    total_timesteps | 3751200  |\n","| train/             |          |\n","|    actor_loss      | -10.7    |\n","|    critic_loss     | 0.868    |\n","|    ent_coef        | 0.00631  |\n","|    ent_coef_loss   | -5.09    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 322090   |\n","---------------------------------\n","Eval num_timesteps=3753600, episode_reward=143.59 +/- 6.99\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 144      |\n","| time/              |          |\n","|    total_timesteps | 3753600  |\n","| train/             |          |\n","|    actor_loss      | -10.8    |\n","|    critic_loss     | 26.8     |\n","|    ent_coef        | 0.00614  |\n","|    ent_coef_loss   | -1.7     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 322190   |\n","---------------------------------\n","Eval num_timesteps=3756000, episode_reward=66.17 +/- 89.27\n","Episode length: 392.00 +/- 132.27\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 392      |\n","|    mean_reward     | 66.2     |\n","| time/              |          |\n","|    total_timesteps | 3756000  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 0.734    |\n","|    ent_coef        | 0.00599  |\n","|    ent_coef_loss   | -3.92    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 322290   |\n","---------------------------------\n","Eval num_timesteps=3758400, episode_reward=162.72 +/- 2.00\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 163      |\n","| time/              |          |\n","|    total_timesteps | 3758400  |\n","| train/             |          |\n","|    actor_loss      | -9.54    |\n","|    critic_loss     | 3.79     |\n","|    ent_coef        | 0.00589  |\n","|    ent_coef_loss   | 3.44     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 322390   |\n","---------------------------------\n","Eval num_timesteps=3760800, episode_reward=3.39 +/- 58.85\n","Episode length: 374.60 +/- 147.46\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 375      |\n","|    mean_reward     | 3.39     |\n","| time/              |          |\n","|    total_timesteps | 3760800  |\n","| train/             |          |\n","|    actor_loss      | -9.37    |\n","|    critic_loss     | 2.32     |\n","|    ent_coef        | 0.00584  |\n","|    ent_coef_loss   | 2.18     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 322490   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2470     |\n","|    fps             | 291      |\n","|    time_elapsed    | 12896    |\n","|    total_timesteps | 3760920  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 1.83     |\n","|    ent_coef        | 0.00584  |\n","|    ent_coef_loss   | 4.77     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 322495   |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    episodes        | 2480    |\n","|    fps             | 291     |\n","|    time_elapsed    | 12896   |\n","|    total_timesteps | 3760920 |\n","--------------------------------\n","Eval num_timesteps=3763200, episode_reward=-42.29 +/- 13.47\n","Episode length: 243.20 +/- 21.07\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 243      |\n","|    mean_reward     | -42.3    |\n","| time/              |          |\n","|    total_timesteps | 3763200  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 2.08     |\n","|    ent_coef        | 0.00593  |\n","|    ent_coef_loss   | -1.97    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 322590   |\n","---------------------------------\n","Eval num_timesteps=3765600, episode_reward=164.66 +/- 3.11\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 165      |\n","| time/              |          |\n","|    total_timesteps | 3765600  |\n","| train/             |          |\n","|    actor_loss      | -10.4    |\n","|    critic_loss     | 1.38     |\n","|    ent_coef        | 0.00586  |\n","|    ent_coef_loss   | 6.19     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 322690   |\n","---------------------------------\n","Eval num_timesteps=3768000, episode_reward=34.93 +/- 102.03\n","Episode length: 373.60 +/- 154.81\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 374      |\n","|    mean_reward     | 34.9     |\n","| time/              |          |\n","|    total_timesteps | 3768000  |\n","| train/             |          |\n","|    actor_loss      | -8.94    |\n","|    critic_loss     | 2.97     |\n","|    ent_coef        | 0.00585  |\n","|    ent_coef_loss   | 0.79     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 322790   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2490     |\n","|    fps             | 291      |\n","|    time_elapsed    | 12920    |\n","|    total_timesteps | 3769368  |\n","| train/             |          |\n","|    actor_loss      | -11.3    |\n","|    critic_loss     | 1.46     |\n","|    ent_coef        | 0.00583  |\n","|    ent_coef_loss   | 0.195    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 322847   |\n","---------------------------------\n","Eval num_timesteps=3770400, episode_reward=-67.78 +/- 21.01\n","Episode length: 189.80 +/- 54.87\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 190      |\n","|    mean_reward     | -67.8    |\n","| time/              |          |\n","|    total_timesteps | 3770400  |\n","| train/             |          |\n","|    actor_loss      | -9.83    |\n","|    critic_loss     | 2.49     |\n","|    ent_coef        | 0.00592  |\n","|    ent_coef_loss   | 4.16     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 322890   |\n","---------------------------------\n","Eval num_timesteps=3772800, episode_reward=167.00 +/- 3.31\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 167      |\n","| time/              |          |\n","|    total_timesteps | 3772800  |\n","| train/             |          |\n","|    actor_loss      | -11.5    |\n","|    critic_loss     | 0.702    |\n","|    ent_coef        | 0.00604  |\n","|    ent_coef_loss   | 2.84     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 322990   |\n","---------------------------------\n","Eval num_timesteps=3775200, episode_reward=160.34 +/- 5.21\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 160      |\n","| time/              |          |\n","|    total_timesteps | 3775200  |\n","| train/             |          |\n","|    actor_loss      | -11.1    |\n","|    critic_loss     | 0.739    |\n","|    ent_coef        | 0.0062   |\n","|    ent_coef_loss   | 0.626    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 323090   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2500     |\n","|    fps             | 291      |\n","|    time_elapsed    | 12943    |\n","|    total_timesteps | 3777240  |\n","| train/             |          |\n","|    actor_loss      | -9.73    |\n","|    critic_loss     | 14.3     |\n","|    ent_coef        | 0.00615  |\n","|    ent_coef_loss   | -0.367   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 323175   |\n","---------------------------------\n","Eval num_timesteps=3777600, episode_reward=140.84 +/- 2.39\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 141      |\n","| time/              |          |\n","|    total_timesteps | 3777600  |\n","| train/             |          |\n","|    actor_loss      | -10.9    |\n","|    critic_loss     | 2.25     |\n","|    ent_coef        | 0.00614  |\n","|    ent_coef_loss   | -0.0978  |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 323190   |\n","---------------------------------\n","Eval num_timesteps=3780000, episode_reward=148.31 +/- 2.70\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 148      |\n","| time/              |          |\n","|    total_timesteps | 3780000  |\n","| train/             |          |\n","|    actor_loss      | -11.1    |\n","|    critic_loss     | 2.01     |\n","|    ent_coef        | 0.0062   |\n","|    ent_coef_loss   | -2.25    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 323290   |\n","---------------------------------\n","Eval num_timesteps=3782400, episode_reward=77.76 +/- 91.51\n","Episode length: 402.40 +/- 119.54\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 402      |\n","|    mean_reward     | 77.8     |\n","| time/              |          |\n","|    total_timesteps | 3782400  |\n","| train/             |          |\n","|    actor_loss      | -11.1    |\n","|    critic_loss     | 1.97     |\n","|    ent_coef        | 0.00613  |\n","|    ent_coef_loss   | 0.148    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 323390   |\n","---------------------------------\n","Eval num_timesteps=3784800, episode_reward=34.97 +/- 77.35\n","Episode length: 386.00 +/- 93.08\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 386      |\n","|    mean_reward     | 35       |\n","| time/              |          |\n","|    total_timesteps | 3784800  |\n","| train/             |          |\n","|    actor_loss      | -10.5    |\n","|    critic_loss     | 1.37     |\n","|    ent_coef        | 0.00611  |\n","|    ent_coef_loss   | -0.112   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 323490   |\n","---------------------------------\n","Eval num_timesteps=3787200, episode_reward=-7.32 +/- 35.04\n","Episode length: 305.40 +/- 75.44\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 305      |\n","|    mean_reward     | -7.32    |\n","| time/              |          |\n","|    total_timesteps | 3787200  |\n","| train/             |          |\n","|    actor_loss      | -10.6    |\n","|    critic_loss     | 0.631    |\n","|    ent_coef        | 0.00608  |\n","|    ent_coef_loss   | -2.1     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 323590   |\n","---------------------------------\n","Eval num_timesteps=3789600, episode_reward=163.84 +/- 0.44\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 164      |\n","| time/              |          |\n","|    total_timesteps | 3789600  |\n","| train/             |          |\n","|    actor_loss      | -10.7    |\n","|    critic_loss     | 1.53     |\n","|    ent_coef        | 0.00596  |\n","|    ent_coef_loss   | -2.15    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 323690   |\n","---------------------------------\n","Eval num_timesteps=3792000, episode_reward=162.55 +/- 0.28\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 163      |\n","| time/              |          |\n","|    total_timesteps | 3792000  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 1.19     |\n","|    ent_coef        | 0.00595  |\n","|    ent_coef_loss   | -1.15    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 323790   |\n","---------------------------------\n","Eval num_timesteps=3794400, episode_reward=85.00 +/- 81.45\n","Episode length: 420.40 +/- 97.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 420      |\n","|    mean_reward     | 85       |\n","| time/              |          |\n","|    total_timesteps | 3794400  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 1.99     |\n","|    ent_coef        | 0.00594  |\n","|    ent_coef_loss   | 0.145    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 323890   |\n","---------------------------------\n","Eval num_timesteps=3796800, episode_reward=178.48 +/- 0.46\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 178      |\n","| time/              |          |\n","|    total_timesteps | 3796800  |\n","| train/             |          |\n","|    actor_loss      | -10.6    |\n","|    critic_loss     | 1.17     |\n","|    ent_coef        | 0.0059   |\n","|    ent_coef_loss   | -1.66    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 323990   |\n","---------------------------------\n","Eval num_timesteps=3799200, episode_reward=160.88 +/- 1.67\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 161      |\n","| time/              |          |\n","|    total_timesteps | 3799200  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 1.85     |\n","|    ent_coef        | 0.00584  |\n","|    ent_coef_loss   | -2.73    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 324090   |\n","---------------------------------\n","Eval num_timesteps=3801600, episode_reward=162.18 +/- 0.90\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 162      |\n","| time/              |          |\n","|    total_timesteps | 3801600  |\n","| train/             |          |\n","|    actor_loss      | -9.66    |\n","|    critic_loss     | 0.861    |\n","|    ent_coef        | 0.00572  |\n","|    ent_coef_loss   | -1.82    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 324190   |\n","---------------------------------\n","Eval num_timesteps=3804000, episode_reward=159.96 +/- 12.06\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 160      |\n","| time/              |          |\n","|    total_timesteps | 3804000  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 1.18     |\n","|    ent_coef        | 0.0056   |\n","|    ent_coef_loss   | 0.601    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 324290   |\n","---------------------------------\n","Eval num_timesteps=3806400, episode_reward=150.93 +/- 0.73\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 151      |\n","| time/              |          |\n","|    total_timesteps | 3806400  |\n","| train/             |          |\n","|    actor_loss      | -10.1    |\n","|    critic_loss     | 3.49     |\n","|    ent_coef        | 0.00557  |\n","|    ent_coef_loss   | 0.256    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 324390   |\n","---------------------------------\n","Eval num_timesteps=3808800, episode_reward=90.57 +/- 113.80\n","Episode length: 378.40 +/- 148.93\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 378      |\n","|    mean_reward     | 90.6     |\n","| time/              |          |\n","|    total_timesteps | 3808800  |\n","| train/             |          |\n","|    actor_loss      | -9.54    |\n","|    critic_loss     | 1.08     |\n","|    ent_coef        | 0.00561  |\n","|    ent_coef_loss   | 3.33     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 324490   |\n","---------------------------------\n","Eval num_timesteps=3811200, episode_reward=37.91 +/- 106.35\n","Episode length: 321.20 +/- 145.99\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 321      |\n","|    mean_reward     | 37.9     |\n","| time/              |          |\n","|    total_timesteps | 3811200  |\n","| train/             |          |\n","|    actor_loss      | -9.99    |\n","|    critic_loss     | 0.745    |\n","|    ent_coef        | 0.00558  |\n","|    ent_coef_loss   | -0.775   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 324590   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2510     |\n","|    fps             | 291      |\n","|    time_elapsed    | 13067    |\n","|    total_timesteps | 3813072  |\n","| train/             |          |\n","|    actor_loss      | -9.74    |\n","|    critic_loss     | 1.96     |\n","|    ent_coef        | 0.00559  |\n","|    ent_coef_loss   | 5.17     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 324668   |\n","---------------------------------\n","Eval num_timesteps=3813600, episode_reward=146.33 +/- 0.75\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 146      |\n","| time/              |          |\n","|    total_timesteps | 3813600  |\n","| train/             |          |\n","|    actor_loss      | -10.1    |\n","|    critic_loss     | 0.907    |\n","|    ent_coef        | 0.00564  |\n","|    ent_coef_loss   | 2.47     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 324690   |\n","---------------------------------\n","Eval num_timesteps=3816000, episode_reward=65.49 +/- 109.27\n","Episode length: 369.20 +/- 160.20\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 369      |\n","|    mean_reward     | 65.5     |\n","| time/              |          |\n","|    total_timesteps | 3816000  |\n","| train/             |          |\n","|    actor_loss      | -10.6    |\n","|    critic_loss     | 2.49     |\n","|    ent_coef        | 0.00576  |\n","|    ent_coef_loss   | -1.18    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 324790   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2520     |\n","|    fps             | 291      |\n","|    time_elapsed    | 13085    |\n","|    total_timesteps | 3818232  |\n","| train/             |          |\n","|    actor_loss      | -10.8    |\n","|    critic_loss     | 21.8     |\n","|    ent_coef        | 0.00579  |\n","|    ent_coef_loss   | -2.18    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 324883   |\n","---------------------------------\n","Eval num_timesteps=3818400, episode_reward=159.46 +/- 2.20\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 159      |\n","| time/              |          |\n","|    total_timesteps | 3818400  |\n","| train/             |          |\n","|    actor_loss      | -10.5    |\n","|    critic_loss     | 2.22     |\n","|    ent_coef        | 0.00579  |\n","|    ent_coef_loss   | 0.678    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 324890   |\n","---------------------------------\n","Eval num_timesteps=3820800, episode_reward=158.04 +/- 1.81\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 158      |\n","| time/              |          |\n","|    total_timesteps | 3820800  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 1.84     |\n","|    ent_coef        | 0.00583  |\n","|    ent_coef_loss   | -1.29    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 324990   |\n","---------------------------------\n","Eval num_timesteps=3823200, episode_reward=78.51 +/- 58.71\n","Episode length: 480.80 +/- 15.68\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 481      |\n","|    mean_reward     | 78.5     |\n","| time/              |          |\n","|    total_timesteps | 3823200  |\n","| train/             |          |\n","|    actor_loss      | -9.88    |\n","|    critic_loss     | 41.9     |\n","|    ent_coef        | 0.006    |\n","|    ent_coef_loss   | 0.187    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 325090   |\n","---------------------------------\n","Eval num_timesteps=3825600, episode_reward=-25.37 +/- 14.00\n","Episode length: 301.20 +/- 32.82\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 301      |\n","|    mean_reward     | -25.4    |\n","| time/              |          |\n","|    total_timesteps | 3825600  |\n","| train/             |          |\n","|    actor_loss      | -9.52    |\n","|    critic_loss     | 1.48     |\n","|    ent_coef        | 0.00612  |\n","|    ent_coef_loss   | 3.05     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 325190   |\n","---------------------------------\n","Eval num_timesteps=3828000, episode_reward=70.82 +/- 77.42\n","Episode length: 427.60 +/- 88.67\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 428      |\n","|    mean_reward     | 70.8     |\n","| time/              |          |\n","|    total_timesteps | 3828000  |\n","| train/             |          |\n","|    actor_loss      | -10.6    |\n","|    critic_loss     | 2.33     |\n","|    ent_coef        | 0.00628  |\n","|    ent_coef_loss   | 3.92     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 325290   |\n","---------------------------------\n","Eval num_timesteps=3830400, episode_reward=161.97 +/- 3.10\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 162      |\n","| time/              |          |\n","|    total_timesteps | 3830400  |\n","| train/             |          |\n","|    actor_loss      | -10.8    |\n","|    critic_loss     | 0.892    |\n","|    ent_coef        | 0.00643  |\n","|    ent_coef_loss   | 0.0063   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 325390   |\n","---------------------------------\n","Eval num_timesteps=3832800, episode_reward=23.00 +/- 105.73\n","Episode length: 317.00 +/- 149.42\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 317      |\n","|    mean_reward     | 23       |\n","| time/              |          |\n","|    total_timesteps | 3832800  |\n","| train/             |          |\n","|    actor_loss      | -9.88    |\n","|    critic_loss     | 2.32     |\n","|    ent_coef        | 0.00668  |\n","|    ent_coef_loss   | 1.74     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 325490   |\n","---------------------------------\n","Eval num_timesteps=3835200, episode_reward=146.19 +/- 0.72\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 146      |\n","| time/              |          |\n","|    total_timesteps | 3835200  |\n","| train/             |          |\n","|    actor_loss      | -10.5    |\n","|    critic_loss     | 1.08     |\n","|    ent_coef        | 0.00684  |\n","|    ent_coef_loss   | 4.26     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 325590   |\n","---------------------------------\n","Eval num_timesteps=3837600, episode_reward=149.60 +/- 1.27\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 150      |\n","| time/              |          |\n","|    total_timesteps | 3837600  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 0.605    |\n","|    ent_coef        | 0.00698  |\n","|    ent_coef_loss   | -0.803   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 325690   |\n","---------------------------------\n","Eval num_timesteps=3840000, episode_reward=123.59 +/- 2.22\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 124      |\n","| time/              |          |\n","|    total_timesteps | 3840000  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 0.64     |\n","|    ent_coef        | 0.00708  |\n","|    ent_coef_loss   | 6.2      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 325790   |\n","---------------------------------\n","Eval num_timesteps=3842400, episode_reward=76.99 +/- 102.06\n","Episode length: 398.40 +/- 124.43\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 398      |\n","|    mean_reward     | 77       |\n","| time/              |          |\n","|    total_timesteps | 3842400  |\n","| train/             |          |\n","|    actor_loss      | -10.5    |\n","|    critic_loss     | 1.11     |\n","|    ent_coef        | 0.00732  |\n","|    ent_coef_loss   | 3.34     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 325890   |\n","---------------------------------\n","Eval num_timesteps=3844800, episode_reward=151.49 +/- 0.46\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 151      |\n","| time/              |          |\n","|    total_timesteps | 3844800  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 0.847    |\n","|    ent_coef        | 0.0073   |\n","|    ent_coef_loss   | -2.41    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 325990   |\n","---------------------------------\n","Eval num_timesteps=3847200, episode_reward=97.76 +/- 66.23\n","Episode length: 479.60 +/- 24.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 480      |\n","|    mean_reward     | 97.8     |\n","| time/              |          |\n","|    total_timesteps | 3847200  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 1.2      |\n","|    ent_coef        | 0.00721  |\n","|    ent_coef_loss   | -1.03    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 326090   |\n","---------------------------------\n","Eval num_timesteps=3849600, episode_reward=19.17 +/- 106.45\n","Episode length: 314.00 +/- 151.87\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 314      |\n","|    mean_reward     | 19.2     |\n","| time/              |          |\n","|    total_timesteps | 3849600  |\n","| train/             |          |\n","|    actor_loss      | -10.8    |\n","|    critic_loss     | 1.35     |\n","|    ent_coef        | 0.00725  |\n","|    ent_coef_loss   | 2.56     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 326190   |\n","---------------------------------\n","Eval num_timesteps=3852000, episode_reward=-65.84 +/- 31.67\n","Episode length: 217.00 +/- 75.93\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 217      |\n","|    mean_reward     | -65.8    |\n","| time/              |          |\n","|    total_timesteps | 3852000  |\n","| train/             |          |\n","|    actor_loss      | -10.1    |\n","|    critic_loss     | 0.913    |\n","|    ent_coef        | 0.00731  |\n","|    ent_coef_loss   | 1.52     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 326290   |\n","---------------------------------\n","Eval num_timesteps=3854400, episode_reward=151.28 +/- 3.05\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 151      |\n","| time/              |          |\n","|    total_timesteps | 3854400  |\n","| train/             |          |\n","|    actor_loss      | -9.81    |\n","|    critic_loss     | 1.5      |\n","|    ent_coef        | 0.00725  |\n","|    ent_coef_loss   | -4.07    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 326390   |\n","---------------------------------\n","Eval num_timesteps=3856800, episode_reward=46.93 +/- 86.25\n","Episode length: 373.40 +/- 103.37\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 373      |\n","|    mean_reward     | 46.9     |\n","| time/              |          |\n","|    total_timesteps | 3856800  |\n","| train/             |          |\n","|    actor_loss      | -10.2    |\n","|    critic_loss     | 1.14     |\n","|    ent_coef        | 0.00695  |\n","|    ent_coef_loss   | -5.53    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 326490   |\n","---------------------------------\n","Eval num_timesteps=3859200, episode_reward=143.30 +/- 2.00\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 143      |\n","| time/              |          |\n","|    total_timesteps | 3859200  |\n","| train/             |          |\n","|    actor_loss      | -10.2    |\n","|    critic_loss     | 2.84     |\n","|    ent_coef        | 0.00681  |\n","|    ent_coef_loss   | 2.84     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 326590   |\n","---------------------------------\n","Eval num_timesteps=3861600, episode_reward=67.44 +/- 63.03\n","Episode length: 472.00 +/- 34.29\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 472      |\n","|    mean_reward     | 67.4     |\n","| time/              |          |\n","|    total_timesteps | 3861600  |\n","| train/             |          |\n","|    actor_loss      | -9.24    |\n","|    critic_loss     | 2.58     |\n","|    ent_coef        | 0.00687  |\n","|    ent_coef_loss   | 2.98     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 326690   |\n","---------------------------------\n","Eval num_timesteps=3864000, episode_reward=141.18 +/- 2.46\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 141      |\n","| time/              |          |\n","|    total_timesteps | 3864000  |\n","| train/             |          |\n","|    actor_loss      | -10.2    |\n","|    critic_loss     | 0.825    |\n","|    ent_coef        | 0.007    |\n","|    ent_coef_loss   | -0.446   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 326790   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2530     |\n","|    fps             | 291      |\n","|    time_elapsed    | 13249    |\n","|    total_timesteps | 3866184  |\n","| train/             |          |\n","|    actor_loss      | -9.92    |\n","|    critic_loss     | 3.81     |\n","|    ent_coef        | 0.00682  |\n","|    ent_coef_loss   | 3.54     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 326881   |\n","---------------------------------\n","Eval num_timesteps=3866400, episode_reward=46.73 +/- 115.72\n","Episode length: 347.60 +/- 186.65\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 348      |\n","|    mean_reward     | 46.7     |\n","| time/              |          |\n","|    total_timesteps | 3866400  |\n","| train/             |          |\n","|    actor_loss      | -9.77    |\n","|    critic_loss     | 1.41     |\n","|    ent_coef        | 0.00683  |\n","|    ent_coef_loss   | 2.82     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 326890   |\n","---------------------------------\n","Eval num_timesteps=3868800, episode_reward=150.68 +/- 6.15\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 151      |\n","| time/              |          |\n","|    total_timesteps | 3868800  |\n","| train/             |          |\n","|    actor_loss      | -10.1    |\n","|    critic_loss     | 2.85     |\n","|    ent_coef        | 0.00674  |\n","|    ent_coef_loss   | -2.96    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 326990   |\n","---------------------------------\n","Eval num_timesteps=3871200, episode_reward=81.04 +/- 58.82\n","Episode length: 476.60 +/- 19.11\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 477      |\n","|    mean_reward     | 81       |\n","| time/              |          |\n","|    total_timesteps | 3871200  |\n","| train/             |          |\n","|    actor_loss      | -10.1    |\n","|    critic_loss     | 1.17     |\n","|    ent_coef        | 0.00663  |\n","|    ent_coef_loss   | -1.1     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 327090   |\n","---------------------------------\n","Eval num_timesteps=3873600, episode_reward=53.31 +/- 79.15\n","Episode length: 398.60 +/- 82.79\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 399      |\n","|    mean_reward     | 53.3     |\n","| time/              |          |\n","|    total_timesteps | 3873600  |\n","| train/             |          |\n","|    actor_loss      | -9.54    |\n","|    critic_loss     | 3.36     |\n","|    ent_coef        | 0.00662  |\n","|    ent_coef_loss   | -0.554   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 327190   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2540     |\n","|    fps             | 291      |\n","|    time_elapsed    | 13283    |\n","|    total_timesteps | 3875856  |\n","| train/             |          |\n","|    actor_loss      | -10.7    |\n","|    critic_loss     | 0.98     |\n","|    ent_coef        | 0.0067   |\n","|    ent_coef_loss   | -0.346   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 327284   |\n","---------------------------------\n","Eval num_timesteps=3876000, episode_reward=69.84 +/- 55.75\n","Episode length: 461.60 +/- 31.35\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 462      |\n","|    mean_reward     | 69.8     |\n","| time/              |          |\n","|    total_timesteps | 3876000  |\n","| train/             |          |\n","|    actor_loss      | -10.4    |\n","|    critic_loss     | 1.26     |\n","|    ent_coef        | 0.0067   |\n","|    ent_coef_loss   | 1.64     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 327290   |\n","---------------------------------\n","Eval num_timesteps=3878400, episode_reward=152.36 +/- 1.38\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 152      |\n","| time/              |          |\n","|    total_timesteps | 3878400  |\n","| train/             |          |\n","|    actor_loss      | -10.1    |\n","|    critic_loss     | 0.645    |\n","|    ent_coef        | 0.00694  |\n","|    ent_coef_loss   | 6.03     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 327390   |\n","---------------------------------\n","Eval num_timesteps=3880800, episode_reward=137.84 +/- 3.03\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 138      |\n","| time/              |          |\n","|    total_timesteps | 3880800  |\n","| train/             |          |\n","|    actor_loss      | -8.98    |\n","|    critic_loss     | 5.92     |\n","|    ent_coef        | 0.00714  |\n","|    ent_coef_loss   | 3.66     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 327490   |\n","---------------------------------\n","Eval num_timesteps=3883200, episode_reward=147.49 +/- 1.17\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 147      |\n","| time/              |          |\n","|    total_timesteps | 3883200  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 2.48     |\n","|    ent_coef        | 0.00713  |\n","|    ent_coef_loss   | 0.378    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 327590   |\n","---------------------------------\n","Eval num_timesteps=3885600, episode_reward=149.43 +/- 3.14\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 149      |\n","| time/              |          |\n","|    total_timesteps | 3885600  |\n","| train/             |          |\n","|    actor_loss      | -9.65    |\n","|    critic_loss     | 1.08     |\n","|    ent_coef        | 0.00726  |\n","|    ent_coef_loss   | 2.67     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 327690   |\n","---------------------------------\n","Eval num_timesteps=3888000, episode_reward=40.28 +/- 88.57\n","Episode length: 368.60 +/- 107.29\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 369      |\n","|    mean_reward     | 40.3     |\n","| time/              |          |\n","|    total_timesteps | 3888000  |\n","| train/             |          |\n","|    actor_loss      | -10.2    |\n","|    critic_loss     | 1.01     |\n","|    ent_coef        | 0.00727  |\n","|    ent_coef_loss   | 0.481    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 327790   |\n","---------------------------------\n","Eval num_timesteps=3890400, episode_reward=-26.28 +/- 8.60\n","Episode length: 267.60 +/- 10.29\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 268      |\n","|    mean_reward     | -26.3    |\n","| time/              |          |\n","|    total_timesteps | 3890400  |\n","| train/             |          |\n","|    actor_loss      | -9.79    |\n","|    critic_loss     | 0.716    |\n","|    ent_coef        | 0.00723  |\n","|    ent_coef_loss   | -1.58    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 327890   |\n","---------------------------------\n","Eval num_timesteps=3892800, episode_reward=146.85 +/- 3.64\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 147      |\n","| time/              |          |\n","|    total_timesteps | 3892800  |\n","| train/             |          |\n","|    actor_loss      | -10.5    |\n","|    critic_loss     | 1.21     |\n","|    ent_coef        | 0.00721  |\n","|    ent_coef_loss   | 1.57     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 327990   |\n","---------------------------------\n","Eval num_timesteps=3895200, episode_reward=151.58 +/- 0.36\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 152      |\n","| time/              |          |\n","|    total_timesteps | 3895200  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 1.11     |\n","|    ent_coef        | 0.00722  |\n","|    ent_coef_loss   | -2.71    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 328090   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2550     |\n","|    fps             | 291      |\n","|    time_elapsed    | 13355    |\n","|    total_timesteps | 3897480  |\n","| train/             |          |\n","|    actor_loss      | -9.46    |\n","|    critic_loss     | 2.7      |\n","|    ent_coef        | 0.00725  |\n","|    ent_coef_loss   | 1.83     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 328185   |\n","---------------------------------\n","Eval num_timesteps=3897600, episode_reward=156.86 +/- 3.01\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 157      |\n","| time/              |          |\n","|    total_timesteps | 3897600  |\n","| train/             |          |\n","|    actor_loss      | -8.64    |\n","|    critic_loss     | 47.8     |\n","|    ent_coef        | 0.00725  |\n","|    ent_coef_loss   | 2.85     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 328190   |\n","---------------------------------\n","Eval num_timesteps=3900000, episode_reward=79.66 +/- 107.25\n","Episode length: 383.60 +/- 142.56\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 384      |\n","|    mean_reward     | 79.7     |\n","| time/              |          |\n","|    total_timesteps | 3900000  |\n","| train/             |          |\n","|    actor_loss      | -10.6    |\n","|    critic_loss     | 2.33     |\n","|    ent_coef        | 0.00739  |\n","|    ent_coef_loss   | -1.36    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 328290   |\n","---------------------------------\n","Eval num_timesteps=3902400, episode_reward=162.37 +/- 0.55\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 162      |\n","| time/              |          |\n","|    total_timesteps | 3902400  |\n","| train/             |          |\n","|    actor_loss      | -9.53    |\n","|    critic_loss     | 1.61     |\n","|    ent_coef        | 0.00734  |\n","|    ent_coef_loss   | -1.76    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 328390   |\n","---------------------------------\n","Eval num_timesteps=3904800, episode_reward=136.08 +/- 5.27\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 136      |\n","| time/              |          |\n","|    total_timesteps | 3904800  |\n","| train/             |          |\n","|    actor_loss      | -10.6    |\n","|    critic_loss     | 0.882    |\n","|    ent_coef        | 0.00722  |\n","|    ent_coef_loss   | -0.794   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 328490   |\n","---------------------------------\n","Eval num_timesteps=3907200, episode_reward=163.31 +/- 3.70\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 163      |\n","| time/              |          |\n","|    total_timesteps | 3907200  |\n","| train/             |          |\n","|    actor_loss      | -10.5    |\n","|    critic_loss     | 1.21     |\n","|    ent_coef        | 0.00727  |\n","|    ent_coef_loss   | -0.46    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 328590   |\n","---------------------------------\n","Eval num_timesteps=3909600, episode_reward=155.12 +/- 5.20\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 155      |\n","| time/              |          |\n","|    total_timesteps | 3909600  |\n","| train/             |          |\n","|    actor_loss      | -10.8    |\n","|    critic_loss     | 0.821    |\n","|    ent_coef        | 0.00717  |\n","|    ent_coef_loss   | 0.266    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 328690   |\n","---------------------------------\n","Eval num_timesteps=3912000, episode_reward=56.97 +/- 111.73\n","Episode length: 354.80 +/- 177.83\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 355      |\n","|    mean_reward     | 57       |\n","| time/              |          |\n","|    total_timesteps | 3912000  |\n","| train/             |          |\n","|    actor_loss      | -9.75    |\n","|    critic_loss     | 1.18     |\n","|    ent_coef        | 0.00726  |\n","|    ent_coef_loss   | 2.93     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 328790   |\n","---------------------------------\n","Eval num_timesteps=3914400, episode_reward=174.42 +/- 0.40\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 174      |\n","| time/              |          |\n","|    total_timesteps | 3914400  |\n","| train/             |          |\n","|    actor_loss      | -8.55    |\n","|    critic_loss     | 1.49     |\n","|    ent_coef        | 0.0074   |\n","|    ent_coef_loss   | 1.78     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 328890   |\n","---------------------------------\n","Eval num_timesteps=3916800, episode_reward=171.80 +/- 1.95\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 172      |\n","| time/              |          |\n","|    total_timesteps | 3916800  |\n","| train/             |          |\n","|    actor_loss      | -9.36    |\n","|    critic_loss     | 1.32     |\n","|    ent_coef        | 0.0074   |\n","|    ent_coef_loss   | 0.141    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 328990   |\n","---------------------------------\n","Eval num_timesteps=3919200, episode_reward=67.39 +/- 72.66\n","Episode length: 426.80 +/- 59.77\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 427      |\n","|    mean_reward     | 67.4     |\n","| time/              |          |\n","|    total_timesteps | 3919200  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 1.88     |\n","|    ent_coef        | 0.00737  |\n","|    ent_coef_loss   | -2.91    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 329090   |\n","---------------------------------\n","Eval num_timesteps=3921600, episode_reward=165.04 +/- 1.35\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 165      |\n","| time/              |          |\n","|    total_timesteps | 3921600  |\n","| train/             |          |\n","|    actor_loss      | -10.5    |\n","|    critic_loss     | 5.03     |\n","|    ent_coef        | 0.00735  |\n","|    ent_coef_loss   | -0.716   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 329190   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2560     |\n","|    fps             | 291      |\n","|    time_elapsed    | 13447    |\n","|    total_timesteps | 3922416  |\n","| train/             |          |\n","|    actor_loss      | -10.4    |\n","|    critic_loss     | 2.34     |\n","|    ent_coef        | 0.00735  |\n","|    ent_coef_loss   | 0.937    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 329224   |\n","---------------------------------\n","Eval num_timesteps=3924000, episode_reward=56.01 +/- 134.68\n","Episode length: 330.40 +/- 207.72\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 330      |\n","|    mean_reward     | 56       |\n","| time/              |          |\n","|    total_timesteps | 3924000  |\n","| train/             |          |\n","|    actor_loss      | -10      |\n","|    critic_loss     | 0.745    |\n","|    ent_coef        | 0.00744  |\n","|    ent_coef_loss   | 0.545    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 329290   |\n","---------------------------------\n","Eval num_timesteps=3926400, episode_reward=159.54 +/- 0.76\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 160      |\n","| time/              |          |\n","|    total_timesteps | 3926400  |\n","| train/             |          |\n","|    actor_loss      | -10      |\n","|    critic_loss     | 2.59     |\n","|    ent_coef        | 0.00729  |\n","|    ent_coef_loss   | -2.22    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 329390   |\n","---------------------------------\n","Eval num_timesteps=3928800, episode_reward=153.02 +/- 1.94\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 153      |\n","| time/              |          |\n","|    total_timesteps | 3928800  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 1.14     |\n","|    ent_coef        | 0.00734  |\n","|    ent_coef_loss   | -1.62    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 329490   |\n","---------------------------------\n","Eval num_timesteps=3931200, episode_reward=60.21 +/- 72.06\n","Episode length: 464.60 +/- 28.90\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 465      |\n","|    mean_reward     | 60.2     |\n","| time/              |          |\n","|    total_timesteps | 3931200  |\n","| train/             |          |\n","|    actor_loss      | -10.6    |\n","|    critic_loss     | 1.45     |\n","|    ent_coef        | 0.00743  |\n","|    ent_coef_loss   | 0.61     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 329590   |\n","---------------------------------\n","Eval num_timesteps=3933600, episode_reward=147.48 +/- 8.01\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 147      |\n","| time/              |          |\n","|    total_timesteps | 3933600  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 0.88     |\n","|    ent_coef        | 0.00727  |\n","|    ent_coef_loss   | 3.61     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 329690   |\n","---------------------------------\n","Eval num_timesteps=3936000, episode_reward=28.58 +/- 96.72\n","Episode length: 351.20 +/- 121.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 351      |\n","|    mean_reward     | 28.6     |\n","| time/              |          |\n","|    total_timesteps | 3936000  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 1.22     |\n","|    ent_coef        | 0.00737  |\n","|    ent_coef_loss   | 1.81     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 329790   |\n","---------------------------------\n","Eval num_timesteps=3938400, episode_reward=151.14 +/- 1.85\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 151      |\n","| time/              |          |\n","|    total_timesteps | 3938400  |\n","| train/             |          |\n","|    actor_loss      | -10.4    |\n","|    critic_loss     | 1.72     |\n","|    ent_coef        | 0.00746  |\n","|    ent_coef_loss   | 0.749    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 329890   |\n","---------------------------------\n","Eval num_timesteps=3940800, episode_reward=144.57 +/- 0.26\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 145      |\n","| time/              |          |\n","|    total_timesteps | 3940800  |\n","| train/             |          |\n","|    actor_loss      | -10      |\n","|    critic_loss     | 1.51     |\n","|    ent_coef        | 0.00746  |\n","|    ent_coef_loss   | -2.38    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 329990   |\n","---------------------------------\n","Eval num_timesteps=3943200, episode_reward=10.40 +/- 117.72\n","Episode length: 291.20 +/- 170.48\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 291      |\n","|    mean_reward     | 10.4     |\n","| time/              |          |\n","|    total_timesteps | 3943200  |\n","| train/             |          |\n","|    actor_loss      | -10.7    |\n","|    critic_loss     | 1.53     |\n","|    ent_coef        | 0.00732  |\n","|    ent_coef_loss   | 0.572    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 330090   |\n","---------------------------------\n","Eval num_timesteps=3945600, episode_reward=163.55 +/- 0.42\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 164      |\n","| time/              |          |\n","|    total_timesteps | 3945600  |\n","| train/             |          |\n","|    actor_loss      | -9.22    |\n","|    critic_loss     | 2.98     |\n","|    ent_coef        | 0.00735  |\n","|    ent_coef_loss   | -2.05    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 330190   |\n","---------------------------------\n","Eval num_timesteps=3948000, episode_reward=67.76 +/- 106.28\n","Episode length: 381.20 +/- 145.50\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 381      |\n","|    mean_reward     | 67.8     |\n","| time/              |          |\n","|    total_timesteps | 3948000  |\n","| train/             |          |\n","|    actor_loss      | -9.31    |\n","|    critic_loss     | 1.62     |\n","|    ent_coef        | 0.00716  |\n","|    ent_coef_loss   | 0.446    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 330290   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2570     |\n","|    fps             | 291      |\n","|    time_elapsed    | 13539    |\n","|    total_timesteps | 3949272  |\n","| train/             |          |\n","|    actor_loss      | -10.2    |\n","|    critic_loss     | 0.834    |\n","|    ent_coef        | 0.00716  |\n","|    ent_coef_loss   | -1.89    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 330343   |\n","---------------------------------\n","Eval num_timesteps=3950400, episode_reward=165.23 +/- 6.11\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 165      |\n","| time/              |          |\n","|    total_timesteps | 3950400  |\n","| train/             |          |\n","|    actor_loss      | -10.2    |\n","|    critic_loss     | 0.88     |\n","|    ent_coef        | 0.00708  |\n","|    ent_coef_loss   | -0.865   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 330390   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2580     |\n","|    fps             | 291      |\n","|    time_elapsed    | 13549    |\n","|    total_timesteps | 3952536  |\n","| train/             |          |\n","|    actor_loss      | -10.4    |\n","|    critic_loss     | 53       |\n","|    ent_coef        | 0.00695  |\n","|    ent_coef_loss   | -1.02    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 330479   |\n","---------------------------------\n","Eval num_timesteps=3952800, episode_reward=164.18 +/- 1.95\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 164      |\n","| time/              |          |\n","|    total_timesteps | 3952800  |\n","| train/             |          |\n","|    actor_loss      | -9.71    |\n","|    critic_loss     | 3.85     |\n","|    ent_coef        | 0.00695  |\n","|    ent_coef_loss   | -1.35    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 330490   |\n","---------------------------------\n","Eval num_timesteps=3955200, episode_reward=155.68 +/- 5.22\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 156      |\n","| time/              |          |\n","|    total_timesteps | 3955200  |\n","| train/             |          |\n","|    actor_loss      | -10      |\n","|    critic_loss     | 0.781    |\n","|    ent_coef        | 0.00694  |\n","|    ent_coef_loss   | 1.3      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 330590   |\n","---------------------------------\n","Eval num_timesteps=3957600, episode_reward=-60.54 +/- 24.02\n","Episode length: 249.00 +/- 44.09\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 249      |\n","|    mean_reward     | -60.5    |\n","| time/              |          |\n","|    total_timesteps | 3957600  |\n","| train/             |          |\n","|    actor_loss      | -9.68    |\n","|    critic_loss     | 2.29     |\n","|    ent_coef        | 0.00693  |\n","|    ent_coef_loss   | -0.392   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 330690   |\n","---------------------------------\n","Eval num_timesteps=3960000, episode_reward=-77.81 +/- 7.14\n","Episode length: 179.00 +/- 19.60\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 179      |\n","|    mean_reward     | -77.8    |\n","| time/              |          |\n","|    total_timesteps | 3960000  |\n","| train/             |          |\n","|    actor_loss      | -9.17    |\n","|    critic_loss     | 1.11     |\n","|    ent_coef        | 0.00684  |\n","|    ent_coef_loss   | -1.59    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 330790   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2590     |\n","|    fps             | 291      |\n","|    time_elapsed    | 13576    |\n","|    total_timesteps | 3962160  |\n","| train/             |          |\n","|    actor_loss      | -9.88    |\n","|    critic_loss     | 3.29     |\n","|    ent_coef        | 0.00683  |\n","|    ent_coef_loss   | -2.22    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 330880   |\n","---------------------------------\n","Eval num_timesteps=3962400, episode_reward=83.45 +/- 98.12\n","Episode length: 404.40 +/- 117.09\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 404      |\n","|    mean_reward     | 83.5     |\n","| time/              |          |\n","|    total_timesteps | 3962400  |\n","| train/             |          |\n","|    actor_loss      | -8.3     |\n","|    critic_loss     | 42.9     |\n","|    ent_coef        | 0.00681  |\n","|    ent_coef_loss   | -1.9     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 330890   |\n","---------------------------------\n","Eval num_timesteps=3964800, episode_reward=146.05 +/- 2.16\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 146      |\n","| time/              |          |\n","|    total_timesteps | 3964800  |\n","| train/             |          |\n","|    actor_loss      | -8.49    |\n","|    critic_loss     | 16.8     |\n","|    ent_coef        | 0.00681  |\n","|    ent_coef_loss   | 3.13     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 330990   |\n","---------------------------------\n","Eval num_timesteps=3967200, episode_reward=57.56 +/- 70.69\n","Episode length: 423.20 +/- 62.71\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 423      |\n","|    mean_reward     | 57.6     |\n","| time/              |          |\n","|    total_timesteps | 3967200  |\n","| train/             |          |\n","|    actor_loss      | -9.21    |\n","|    critic_loss     | 1.66     |\n","|    ent_coef        | 0.00709  |\n","|    ent_coef_loss   | 0.664    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 331090   |\n","---------------------------------\n","Eval num_timesteps=3969600, episode_reward=-41.41 +/- 8.20\n","Episode length: 280.20 +/- 11.27\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 280      |\n","|    mean_reward     | -41.4    |\n","| time/              |          |\n","|    total_timesteps | 3969600  |\n","| train/             |          |\n","|    actor_loss      | -8.46    |\n","|    critic_loss     | 2.35     |\n","|    ent_coef        | 0.00709  |\n","|    ent_coef_loss   | -0.623   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 331190   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2600     |\n","|    fps             | 291      |\n","|    time_elapsed    | 13606    |\n","|    total_timesteps | 3971568  |\n","| train/             |          |\n","|    actor_loss      | -9.38    |\n","|    critic_loss     | 3.1      |\n","|    ent_coef        | 0.00704  |\n","|    ent_coef_loss   | -3.54    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 331272   |\n","---------------------------------\n","Eval num_timesteps=3972000, episode_reward=158.87 +/- 3.92\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 159      |\n","| time/              |          |\n","|    total_timesteps | 3972000  |\n","| train/             |          |\n","|    actor_loss      | -10.6    |\n","|    critic_loss     | 1.14     |\n","|    ent_coef        | 0.007    |\n","|    ent_coef_loss   | 0.682    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 331290   |\n","---------------------------------\n","Eval num_timesteps=3974400, episode_reward=-52.16 +/- 5.03\n","Episode length: 211.00 +/- 9.80\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 211      |\n","|    mean_reward     | -52.2    |\n","| time/              |          |\n","|    total_timesteps | 3974400  |\n","| train/             |          |\n","|    actor_loss      | -9.63    |\n","|    critic_loss     | 2.17     |\n","|    ent_coef        | 0.00712  |\n","|    ent_coef_loss   | -1.65    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 331390   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2610     |\n","|    fps             | 291      |\n","|    time_elapsed    | 13619    |\n","|    total_timesteps | 3975912  |\n","| train/             |          |\n","|    actor_loss      | -9.14    |\n","|    critic_loss     | 1.41     |\n","|    ent_coef        | 0.00716  |\n","|    ent_coef_loss   | 3.78     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 331453   |\n","---------------------------------\n","Eval num_timesteps=3976800, episode_reward=162.57 +/- 3.58\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 163      |\n","| time/              |          |\n","|    total_timesteps | 3976800  |\n","| train/             |          |\n","|    actor_loss      | -9.11    |\n","|    critic_loss     | 2.94     |\n","|    ent_coef        | 0.00722  |\n","|    ent_coef_loss   | -1.09    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 331490   |\n","---------------------------------\n","Eval num_timesteps=3979200, episode_reward=51.78 +/- 92.45\n","Episode length: 395.60 +/- 85.24\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 396      |\n","|    mean_reward     | 51.8     |\n","| time/              |          |\n","|    total_timesteps | 3979200  |\n","| train/             |          |\n","|    actor_loss      | -9.44    |\n","|    critic_loss     | 1.5      |\n","|    ent_coef        | 0.00711  |\n","|    ent_coef_loss   | 2.35     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 331590   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2620     |\n","|    fps             | 291      |\n","|    time_elapsed    | 13638    |\n","|    total_timesteps | 3981480  |\n","| train/             |          |\n","|    actor_loss      | -10.5    |\n","|    critic_loss     | 1.06     |\n","|    ent_coef        | 0.00693  |\n","|    ent_coef_loss   | 2.06     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 331685   |\n","---------------------------------\n","Eval num_timesteps=3981600, episode_reward=165.99 +/- 1.48\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 166      |\n","| time/              |          |\n","|    total_timesteps | 3981600  |\n","| train/             |          |\n","|    actor_loss      | -10.2    |\n","|    critic_loss     | 3.74     |\n","|    ent_coef        | 0.00693  |\n","|    ent_coef_loss   | 0.0528   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 331690   |\n","---------------------------------\n","Eval num_timesteps=3984000, episode_reward=171.87 +/- 1.18\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 172      |\n","| time/              |          |\n","|    total_timesteps | 3984000  |\n","| train/             |          |\n","|    actor_loss      | -9.14    |\n","|    critic_loss     | 1.69     |\n","|    ent_coef        | 0.00695  |\n","|    ent_coef_loss   | 2.25     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 331790   |\n","---------------------------------\n","Eval num_timesteps=3986400, episode_reward=83.96 +/- 100.77\n","Episode length: 394.40 +/- 129.33\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 394      |\n","|    mean_reward     | 84       |\n","| time/              |          |\n","|    total_timesteps | 3986400  |\n","| train/             |          |\n","|    actor_loss      | -9.46    |\n","|    critic_loss     | 1.29     |\n","|    ent_coef        | 0.00694  |\n","|    ent_coef_loss   | 1.66     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 331890   |\n","---------------------------------\n","Eval num_timesteps=3988800, episode_reward=84.87 +/- 97.53\n","Episode length: 421.60 +/- 96.02\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 422      |\n","|    mean_reward     | 84.9     |\n","| time/              |          |\n","|    total_timesteps | 3988800  |\n","| train/             |          |\n","|    actor_loss      | -9.82    |\n","|    critic_loss     | 2.6      |\n","|    ent_coef        | 0.007    |\n","|    ent_coef_loss   | -3.41    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 331990   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2630     |\n","|    fps             | 291      |\n","|    time_elapsed    | 13669    |\n","|    total_timesteps | 3988872  |\n","| train/             |          |\n","|    actor_loss      | -10.6    |\n","|    critic_loss     | 10.5     |\n","|    ent_coef        | 0.00699  |\n","|    ent_coef_loss   | -0.437   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 331993   |\n","---------------------------------\n","Eval num_timesteps=3991200, episode_reward=151.49 +/- 2.19\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 151      |\n","| time/              |          |\n","|    total_timesteps | 3991200  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 3.66     |\n","|    ent_coef        | 0.00706  |\n","|    ent_coef_loss   | -0.678   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 332090   |\n","---------------------------------\n","Eval num_timesteps=3993600, episode_reward=142.85 +/- 5.63\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 143      |\n","| time/              |          |\n","|    total_timesteps | 3993600  |\n","| train/             |          |\n","|    actor_loss      | -9.12    |\n","|    critic_loss     | 72.6     |\n","|    ent_coef        | 0.00718  |\n","|    ent_coef_loss   | -3.3     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 332190   |\n","---------------------------------\n","Eval num_timesteps=3996000, episode_reward=146.56 +/- 6.24\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 147      |\n","| time/              |          |\n","|    total_timesteps | 3996000  |\n","| train/             |          |\n","|    actor_loss      | -10.1    |\n","|    critic_loss     | 0.784    |\n","|    ent_coef        | 0.00725  |\n","|    ent_coef_loss   | -0.867   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 332290   |\n","---------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n"]},{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-22-73e3d1bad3ec>\", line 12, in <cell line: 12>\n","    model.learn(total_timesteps=4000000, log_interval=10,callback=eval_callback)\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/sac/sac.py\", line 307, in learn\n","    return super().learn(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 312, in learn\n","    rollout = self.collect_rollouts(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 552, in collect_rollouts\n","    if callback.on_step() is False:\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 104, in on_step\n","    return self._on_step()\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 471, in _on_step\n","    np.savez(\n","  File \"<__array_function__ internals>\", line 180, in savez\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 615, in savez\n","    _savez(file, args, kwds, False)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 712, in _savez\n","    zipf = zipfile_factory(file, mode=\"w\", compression=compression)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 103, in zipfile_factory\n","    return zipfile.ZipFile(file, *args, **kwargs)\n","  File \"/usr/lib/python3.10/zipfile.py\", line 1251, in __init__\n","    self.fp = io.open(file, filemode)\n","OSError: [Errno 107] Transport endpoint is not connected: './multiwalker_sac_00_log_eval_extra/evaluations.npz'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'OSError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-22-73e3d1bad3ec>\", line 12, in <cell line: 12>\n","    model.learn(total_timesteps=4000000, log_interval=10,callback=eval_callback)\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/sac/sac.py\", line 307, in learn\n","    return super().learn(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 312, in learn\n","    rollout = self.collect_rollouts(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 552, in collect_rollouts\n","    if callback.on_step() is False:\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 104, in on_step\n","    return self._on_step()\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 471, in _on_step\n","    np.savez(\n","  File \"<__array_function__ internals>\", line 180, in savez\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 615, in savez\n","    _savez(file, args, kwds, False)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 712, in _savez\n","    zipf = zipfile_factory(file, mode=\"w\", compression=compression)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 103, in zipfile_factory\n","    return zipfile.ZipFile(file, *args, **kwargs)\n","  File \"/usr/lib/python3.10/zipfile.py\", line 1251, in __init__\n","    self.fp = io.open(file, filemode)\n","OSError: [Errno 107] Transport endpoint is not connected: './multiwalker_sac_00_log_eval_extra/evaluations.npz'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'OSError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n","    if (await self.run_code(code, result,  async_=asy)):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n","    self.showtraceback(running_compiled_code=True)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n","    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-22-73e3d1bad3ec>\", line 12, in <cell line: 12>\n","    model.learn(total_timesteps=4000000, log_interval=10,callback=eval_callback)\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/sac/sac.py\", line 307, in learn\n","    return super().learn(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 312, in learn\n","    rollout = self.collect_rollouts(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 552, in collect_rollouts\n","    if callback.on_step() is False:\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 104, in on_step\n","    return self._on_step()\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 471, in _on_step\n","    np.savez(\n","  File \"<__array_function__ internals>\", line 180, in savez\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 615, in savez\n","    _savez(file, args, kwds, False)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 712, in _savez\n","    zipf = zipfile_factory(file, mode=\"w\", compression=compression)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 103, in zipfile_factory\n","    return zipfile.ZipFile(file, *args, **kwargs)\n","  File \"/usr/lib/python3.10/zipfile.py\", line 1251, in __init__\n","    self.fp = io.open(file, filemode)\n","OSError: [Errno 107] Transport endpoint is not connected: './multiwalker_sac_00_log_eval_extra/evaluations.npz'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'OSError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n","    if (await self.run_code(code, result,  async_=asy)):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n","    self.showtraceback(running_compiled_code=True)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n","    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n","    return runner(coro)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n","    coro.send(None)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n","    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3492, in run_ast_nodes\n","    self.showtraceback()\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1142, in structured_traceback\n","    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n"]}],"source":["from stable_baselines3.common.logger import configure\n","\n","tmp_path = \"multiwalker_sac_00_log_eval_extra/\"\n","# set up logger\n","new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n","\n","model = SAC.load(\"./multiwalker_sac_00_log_eval/best_model\")\n","model.set_env(env)\n","model.set_parameters(\"./multiwalker_sac_00_log_eval/best_model\")\n","\n","model.set_logger(new_logger)\n","model.learn(total_timesteps=4000000, log_interval=10,callback=eval_callback)\n","model.save(\"multiwalker_sac_00_extra\")"]},{"cell_type":"code","source":[],"metadata":{"id":"uhKI5u22V1Ci"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":19,"metadata":{"id":"Tx-Vh7NQV1Tt","executionInfo":{"status":"ok","timestamp":1699522576917,"user_tz":-60,"elapsed":15162,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}}},"outputs":[],"source":["# from stable_baselines3.dqn import MlpPolicy,CnnPolicy\n","from stable_baselines3 import SAC\n","from pettingzoo.sisl import multiwalker_v9\n","import supersuit as ss\n","from stable_baselines3.common.monitor import Monitor"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"97T4M8XqV1Tv","executionInfo":{"status":"ok","timestamp":1699522576918,"user_tz":-60,"elapsed":57,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}}},"outputs":[],"source":["env = multiwalker_v9.parallel_env(render_mode=\"rgb_array\",shared_reward=0)\n","\n","env = ss.frame_stack_v1(env, 3)\n","env = ss.pettingzoo_env_to_vec_env_v1(env)\n","env = ss.concat_vec_envs_v1(env, 8, num_cpus=4, base_class='stable_baselines3')"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"zLD_Rb42V1Tv","executionInfo":{"status":"ok","timestamp":1699522576919,"user_tz":-60,"elapsed":55,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}}},"outputs":[],"source":["from stable_baselines3.common.callbacks import EvalCallback\n","eval_callback = EvalCallback(env, best_model_save_path=\"./multiwalker_sac_00_log_eval_extra2/\",\n","                             log_path=\"./multiwalker_sac_00_log_eval_extra2/\", eval_freq=100,\n","                             deterministic=False, render=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"86ff471b-2d3f-4a9a-edf7-d92345dc7dd6","id":"Z-kMGfeLV1Tw"},"outputs":[{"output_type":"stream","name":"stdout","text":["Logging to multiwalker_sac_00_log_eval_extra2/\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n","| time/              |          |\n","|    total_timesteps | 2834400  |\n","| train/             |          |\n","|    actor_loss      | -8.89    |\n","|    critic_loss     | 1.21     |\n","|    ent_coef        | 0.00728  |\n","|    ent_coef_loss   | -1.77    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 428085   |\n","---------------------------------\n","Eval num_timesteps=2836800, episode_reward=94.79 +/- 92.15\n","Episode length: 411.60 +/- 108.27\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 412      |\n","|    mean_reward     | 94.8     |\n","| time/              |          |\n","|    total_timesteps | 2836800  |\n","| train/             |          |\n","|    actor_loss      | -9.68    |\n","|    critic_loss     | 1.91     |\n","|    ent_coef        | 0.00726  |\n","|    ent_coef_loss   | 1.66     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 428185   |\n","---------------------------------\n","Eval num_timesteps=2839200, episode_reward=173.33 +/- 0.41\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 173      |\n","| time/              |          |\n","|    total_timesteps | 2839200  |\n","| train/             |          |\n","|    actor_loss      | -8.63    |\n","|    critic_loss     | 1.55     |\n","|    ent_coef        | 0.00726  |\n","|    ent_coef_loss   | 1.57     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 428285   |\n","---------------------------------\n","Eval num_timesteps=2841600, episode_reward=172.53 +/- 1.81\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 173      |\n","| time/              |          |\n","|    total_timesteps | 2841600  |\n","| train/             |          |\n","|    actor_loss      | -8.88    |\n","|    critic_loss     | 8.49     |\n","|    ent_coef        | 0.00722  |\n","|    ent_coef_loss   | -0.345   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 428385   |\n","---------------------------------\n","Eval num_timesteps=2844000, episode_reward=170.43 +/- 1.15\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 170      |\n","| time/              |          |\n","|    total_timesteps | 2844000  |\n","| train/             |          |\n","|    actor_loss      | -9.15    |\n","|    critic_loss     | 1.97     |\n","|    ent_coef        | 0.00721  |\n","|    ent_coef_loss   | -0.145   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 428485   |\n","---------------------------------\n","Eval num_timesteps=2846400, episode_reward=179.33 +/- 0.79\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 179      |\n","| time/              |          |\n","|    total_timesteps | 2846400  |\n","| train/             |          |\n","|    actor_loss      | -8.33    |\n","|    critic_loss     | 18.9     |\n","|    ent_coef        | 0.00721  |\n","|    ent_coef_loss   | -0.659   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 428585   |\n","---------------------------------\n","Eval num_timesteps=2848800, episode_reward=163.66 +/- 3.70\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 164      |\n","| time/              |          |\n","|    total_timesteps | 2848800  |\n","| train/             |          |\n","|    actor_loss      | -9.17    |\n","|    critic_loss     | 2.09     |\n","|    ent_coef        | 0.00726  |\n","|    ent_coef_loss   | 0.164    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 428685   |\n","---------------------------------\n","Eval num_timesteps=2851200, episode_reward=173.39 +/- 0.47\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 173      |\n","| time/              |          |\n","|    total_timesteps | 2851200  |\n","| train/             |          |\n","|    actor_loss      | -9.43    |\n","|    critic_loss     | 0.965    |\n","|    ent_coef        | 0.00745  |\n","|    ent_coef_loss   | -2.22    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 428785   |\n","---------------------------------\n","Eval num_timesteps=2853600, episode_reward=170.87 +/- 2.60\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 171      |\n","| time/              |          |\n","|    total_timesteps | 2853600  |\n","| train/             |          |\n","|    actor_loss      | -8.52    |\n","|    critic_loss     | 2.8      |\n","|    ent_coef        | 0.0073   |\n","|    ent_coef_loss   | -0.256   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 428885   |\n","---------------------------------\n","Eval num_timesteps=2856000, episode_reward=113.55 +/- 84.54\n","Episode length: 439.60 +/- 73.97\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 440      |\n","|    mean_reward     | 114      |\n","| time/              |          |\n","|    total_timesteps | 2856000  |\n","| train/             |          |\n","|    actor_loss      | -9.5     |\n","|    critic_loss     | 1.97     |\n","|    ent_coef        | 0.0071   |\n","|    ent_coef_loss   | -2.39    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 428985   |\n","---------------------------------\n","Eval num_timesteps=2858400, episode_reward=98.08 +/- 84.10\n","Episode length: 440.40 +/- 72.99\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 440      |\n","|    mean_reward     | 98.1     |\n","| time/              |          |\n","|    total_timesteps | 2858400  |\n","| train/             |          |\n","|    actor_loss      | -9.77    |\n","|    critic_loss     | 1.46     |\n","|    ent_coef        | 0.0069   |\n","|    ent_coef_loss   | -1.65    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 429085   |\n","---------------------------------\n","Eval num_timesteps=2860800, episode_reward=171.81 +/- 0.37\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 172      |\n","| time/              |          |\n","|    total_timesteps | 2860800  |\n","| train/             |          |\n","|    actor_loss      | -9.91    |\n","|    critic_loss     | 1.84     |\n","|    ent_coef        | 0.00687  |\n","|    ent_coef_loss   | -3.78    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 429185   |\n","---------------------------------\n","Eval num_timesteps=2863200, episode_reward=172.11 +/- 0.20\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 172      |\n","| time/              |          |\n","|    total_timesteps | 2863200  |\n","| train/             |          |\n","|    actor_loss      | -10.1    |\n","|    critic_loss     | 0.837    |\n","|    ent_coef        | 0.00664  |\n","|    ent_coef_loss   | -2.92    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 429285   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 1990     |\n","|    fps             | 277      |\n","|    time_elapsed    | 10307    |\n","|    total_timesteps | 2865144  |\n","| train/             |          |\n","|    actor_loss      | -9.14    |\n","|    critic_loss     | 0.959    |\n","|    ent_coef        | 0.00651  |\n","|    ent_coef_loss   | 0.906    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 429366   |\n","---------------------------------\n","Eval num_timesteps=2865600, episode_reward=148.35 +/- 2.79\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 148      |\n","| time/              |          |\n","|    total_timesteps | 2865600  |\n","| train/             |          |\n","|    actor_loss      | -9.29    |\n","|    critic_loss     | 1.77     |\n","|    ent_coef        | 0.00652  |\n","|    ent_coef_loss   | 2.29     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 429385   |\n","---------------------------------\n","Eval num_timesteps=2868000, episode_reward=169.87 +/- 5.84\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 170      |\n","| time/              |          |\n","|    total_timesteps | 2868000  |\n","| train/             |          |\n","|    actor_loss      | -9.5     |\n","|    critic_loss     | 2.58     |\n","|    ent_coef        | 0.00657  |\n","|    ent_coef_loss   | 0.891    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 429485   |\n","---------------------------------\n","Eval num_timesteps=2870400, episode_reward=160.40 +/- 0.91\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 160      |\n","| time/              |          |\n","|    total_timesteps | 2870400  |\n","| train/             |          |\n","|    actor_loss      | -9.6     |\n","|    critic_loss     | 1.44     |\n","|    ent_coef        | 0.00649  |\n","|    ent_coef_loss   | -2.03    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 429585   |\n","---------------------------------\n","Eval num_timesteps=2872800, episode_reward=162.57 +/- 2.99\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 163      |\n","| time/              |          |\n","|    total_timesteps | 2872800  |\n","| train/             |          |\n","|    actor_loss      | -8.85    |\n","|    critic_loss     | 13.1     |\n","|    ent_coef        | 0.00632  |\n","|    ent_coef_loss   | -0.676   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 429685   |\n","---------------------------------\n","Eval num_timesteps=2875200, episode_reward=166.86 +/- 1.40\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 167      |\n","| time/              |          |\n","|    total_timesteps | 2875200  |\n","| train/             |          |\n","|    actor_loss      | -10.1    |\n","|    critic_loss     | 1.22     |\n","|    ent_coef        | 0.00622  |\n","|    ent_coef_loss   | -2.1     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 429785   |\n","---------------------------------\n","Eval num_timesteps=2877600, episode_reward=166.34 +/- 2.32\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 166      |\n","| time/              |          |\n","|    total_timesteps | 2877600  |\n","| train/             |          |\n","|    actor_loss      | -9.34    |\n","|    critic_loss     | 1.13     |\n","|    ent_coef        | 0.00619  |\n","|    ent_coef_loss   | 3.13     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 429885   |\n","---------------------------------\n","Eval num_timesteps=2880000, episode_reward=167.35 +/- 2.80\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 167      |\n","| time/              |          |\n","|    total_timesteps | 2880000  |\n","| train/             |          |\n","|    actor_loss      | -9.36    |\n","|    critic_loss     | 2.15     |\n","|    ent_coef        | 0.00613  |\n","|    ent_coef_loss   | -0.699   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 429985   |\n","---------------------------------\n","Eval num_timesteps=2882400, episode_reward=166.20 +/- 1.62\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 166      |\n","| time/              |          |\n","|    total_timesteps | 2882400  |\n","| train/             |          |\n","|    actor_loss      | -9.42    |\n","|    critic_loss     | 1.01     |\n","|    ent_coef        | 0.00604  |\n","|    ent_coef_loss   | 0.39     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 430085   |\n","---------------------------------\n","Eval num_timesteps=2884800, episode_reward=170.97 +/- 0.71\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 171      |\n","| time/              |          |\n","|    total_timesteps | 2884800  |\n","| train/             |          |\n","|    actor_loss      | -10      |\n","|    critic_loss     | 1.9      |\n","|    ent_coef        | 0.00607  |\n","|    ent_coef_loss   | -1.62    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 430185   |\n","---------------------------------\n","Eval num_timesteps=2887200, episode_reward=164.05 +/- 0.12\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 164      |\n","| time/              |          |\n","|    total_timesteps | 2887200  |\n","| train/             |          |\n","|    actor_loss      | -10.6    |\n","|    critic_loss     | 0.815    |\n","|    ent_coef        | 0.00595  |\n","|    ent_coef_loss   | -4.05    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 430285   |\n","---------------------------------\n","Eval num_timesteps=2889600, episode_reward=167.87 +/- 1.99\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 168      |\n","| time/              |          |\n","|    total_timesteps | 2889600  |\n","| train/             |          |\n","|    actor_loss      | -8.63    |\n","|    critic_loss     | 1.35     |\n","|    ent_coef        | 0.00592  |\n","|    ent_coef_loss   | -0.648   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 430385   |\n","---------------------------------\n","Eval num_timesteps=2892000, episode_reward=81.89 +/- 96.40\n","Episode length: 398.80 +/- 123.94\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 399      |\n","|    mean_reward     | 81.9     |\n","| time/              |          |\n","|    total_timesteps | 2892000  |\n","| train/             |          |\n","|    actor_loss      | -9.82    |\n","|    critic_loss     | 0.776    |\n","|    ent_coef        | 0.00589  |\n","|    ent_coef_loss   | -0.519   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 430485   |\n","---------------------------------\n","Eval num_timesteps=2894400, episode_reward=103.78 +/- 89.87\n","Episode length: 432.80 +/- 82.30\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 433      |\n","|    mean_reward     | 104      |\n","| time/              |          |\n","|    total_timesteps | 2894400  |\n","| train/             |          |\n","|    actor_loss      | -10.4    |\n","|    critic_loss     | 2.21     |\n","|    ent_coef        | 0.00578  |\n","|    ent_coef_loss   | -2.04    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 430585   |\n","---------------------------------\n","Eval num_timesteps=2896800, episode_reward=181.52 +/- 7.47\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 182      |\n","| time/              |          |\n","|    total_timesteps | 2896800  |\n","| train/             |          |\n","|    actor_loss      | -9.41    |\n","|    critic_loss     | 7.4      |\n","|    ent_coef        | 0.00568  |\n","|    ent_coef_loss   | 0.573    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 430685   |\n","---------------------------------\n","Eval num_timesteps=2899200, episode_reward=192.18 +/- 3.14\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 192      |\n","| time/              |          |\n","|    total_timesteps | 2899200  |\n","| train/             |          |\n","|    actor_loss      | -9.55    |\n","|    critic_loss     | 1.44     |\n","|    ent_coef        | 0.00569  |\n","|    ent_coef_loss   | -1.78    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 430785   |\n","---------------------------------\n","Eval num_timesteps=2901600, episode_reward=19.82 +/- 128.80\n","Episode length: 258.80 +/- 196.94\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 259      |\n","|    mean_reward     | 19.8     |\n","| time/              |          |\n","|    total_timesteps | 2901600  |\n","| train/             |          |\n","|    actor_loss      | -10.1    |\n","|    critic_loss     | 1.03     |\n","|    ent_coef        | 0.00559  |\n","|    ent_coef_loss   | 3.69     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 430885   |\n","---------------------------------\n","Eval num_timesteps=2904000, episode_reward=173.58 +/- 2.77\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 174      |\n","| time/              |          |\n","|    total_timesteps | 2904000  |\n","| train/             |          |\n","|    actor_loss      | -10      |\n","|    critic_loss     | 30.4     |\n","|    ent_coef        | 0.00575  |\n","|    ent_coef_loss   | -1.02    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 430985   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2000     |\n","|    fps             | 277      |\n","|    time_elapsed    | 10455    |\n","|    total_timesteps | 2904744  |\n","| train/             |          |\n","|    actor_loss      | -9.63    |\n","|    critic_loss     | 13.7     |\n","|    ent_coef        | 0.00576  |\n","|    ent_coef_loss   | 1.94     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 431016   |\n","---------------------------------\n","Eval num_timesteps=2906400, episode_reward=52.15 +/- 106.37\n","Episode length: 344.00 +/- 127.37\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 344      |\n","|    mean_reward     | 52.1     |\n","| time/              |          |\n","|    total_timesteps | 2906400  |\n","| train/             |          |\n","|    actor_loss      | -9.62    |\n","|    critic_loss     | 0.846    |\n","|    ent_coef        | 0.00571  |\n","|    ent_coef_loss   | -0.0933  |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 431085   |\n","---------------------------------\n","Eval num_timesteps=2908800, episode_reward=33.64 +/- 11.11\n","Episode length: 401.80 +/- 38.21\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 402      |\n","|    mean_reward     | 33.6     |\n","| time/              |          |\n","|    total_timesteps | 2908800  |\n","| train/             |          |\n","|    actor_loss      | -10.5    |\n","|    critic_loss     | 0.938    |\n","|    ent_coef        | 0.00564  |\n","|    ent_coef_loss   | -2.09    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 431185   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2010     |\n","|    fps             | 277      |\n","|    time_elapsed    | 10472    |\n","|    total_timesteps | 2908848  |\n","| train/             |          |\n","|    actor_loss      | -10.4    |\n","|    critic_loss     | 0.956    |\n","|    ent_coef        | 0.00564  |\n","|    ent_coef_loss   | -1.51    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 431187   |\n","---------------------------------\n","Eval num_timesteps=2911200, episode_reward=181.25 +/- 2.68\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 181      |\n","| time/              |          |\n","|    total_timesteps | 2911200  |\n","| train/             |          |\n","|    actor_loss      | -10.8    |\n","|    critic_loss     | 1.92     |\n","|    ent_coef        | 0.00554  |\n","|    ent_coef_loss   | -3.27    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 431285   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2020     |\n","|    fps             | 277      |\n","|    time_elapsed    | 10482    |\n","|    total_timesteps | 2913528  |\n","| train/             |          |\n","|    actor_loss      | -9.08    |\n","|    critic_loss     | 1.61     |\n","|    ent_coef        | 0.00549  |\n","|    ent_coef_loss   | 5.09     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 431382   |\n","---------------------------------\n","Eval num_timesteps=2913600, episode_reward=44.95 +/- 105.72\n","Episode length: 329.00 +/- 139.62\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 329      |\n","|    mean_reward     | 45       |\n","| time/              |          |\n","|    total_timesteps | 2913600  |\n","| train/             |          |\n","|    actor_loss      | -11.1    |\n","|    critic_loss     | 0.242    |\n","|    ent_coef        | 0.00549  |\n","|    ent_coef_loss   | 1.58     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 431385   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2030     |\n","|    fps             | 277      |\n","|    time_elapsed    | 10493    |\n","|    total_timesteps | 2915904  |\n","| train/             |          |\n","|    actor_loss      | -11.1    |\n","|    critic_loss     | 0.68     |\n","|    ent_coef        | 0.00575  |\n","|    ent_coef_loss   | 0.754    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 431481   |\n","---------------------------------\n","Eval num_timesteps=2916000, episode_reward=177.76 +/- 2.15\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 178      |\n","| time/              |          |\n","|    total_timesteps | 2916000  |\n","| train/             |          |\n","|    actor_loss      | -10      |\n","|    critic_loss     | 1.29     |\n","|    ent_coef        | 0.00576  |\n","|    ent_coef_loss   | 5.91     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 431485   |\n","---------------------------------\n","Eval num_timesteps=2918400, episode_reward=110.56 +/- 81.07\n","Episode length: 445.20 +/- 67.12\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 445      |\n","|    mean_reward     | 111      |\n","| time/              |          |\n","|    total_timesteps | 2918400  |\n","| train/             |          |\n","|    actor_loss      | -10.2    |\n","|    critic_loss     | 48.5     |\n","|    ent_coef        | 0.00589  |\n","|    ent_coef_loss   | -1.75    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 431585   |\n","---------------------------------\n","Eval num_timesteps=2920800, episode_reward=169.55 +/- 4.90\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 170      |\n","| time/              |          |\n","|    total_timesteps | 2920800  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 2.39     |\n","|    ent_coef        | 0.00612  |\n","|    ent_coef_loss   | 1.07     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 431685   |\n","---------------------------------\n","Eval num_timesteps=2923200, episode_reward=39.11 +/- 110.16\n","Episode length: 324.20 +/- 143.54\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 324      |\n","|    mean_reward     | 39.1     |\n","| time/              |          |\n","|    total_timesteps | 2923200  |\n","| train/             |          |\n","|    actor_loss      | -9.55    |\n","|    critic_loss     | 15.9     |\n","|    ent_coef        | 0.006    |\n","|    ent_coef_loss   | -0.356   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 431785   |\n","---------------------------------\n","Eval num_timesteps=2925600, episode_reward=146.87 +/- 1.56\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 147      |\n","| time/              |          |\n","|    total_timesteps | 2925600  |\n","| train/             |          |\n","|    actor_loss      | -10      |\n","|    critic_loss     | 0.917    |\n","|    ent_coef        | 0.00593  |\n","|    ent_coef_loss   | 0.226    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 431885   |\n","---------------------------------\n","Eval num_timesteps=2928000, episode_reward=156.74 +/- 8.10\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 157      |\n","| time/              |          |\n","|    total_timesteps | 2928000  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 0.67     |\n","|    ent_coef        | 0.00589  |\n","|    ent_coef_loss   | -3.04    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 431985   |\n","---------------------------------\n","Eval num_timesteps=2930400, episode_reward=145.62 +/- 7.60\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 146      |\n","| time/              |          |\n","|    total_timesteps | 2930400  |\n","| train/             |          |\n","|    actor_loss      | -9.78    |\n","|    critic_loss     | 1.87     |\n","|    ent_coef        | 0.0058   |\n","|    ent_coef_loss   | 0.585    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 432085   |\n","---------------------------------\n","Eval num_timesteps=2932800, episode_reward=161.88 +/- 1.76\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 162      |\n","| time/              |          |\n","|    total_timesteps | 2932800  |\n","| train/             |          |\n","|    actor_loss      | -8.8     |\n","|    critic_loss     | 49.3     |\n","|    ent_coef        | 0.00571  |\n","|    ent_coef_loss   | -0.861   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 432185   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2040     |\n","|    fps             | 277      |\n","|    time_elapsed    | 10562    |\n","|    total_timesteps | 2934768  |\n","| train/             |          |\n","|    actor_loss      | -10.5    |\n","|    critic_loss     | 1.1      |\n","|    ent_coef        | 0.00573  |\n","|    ent_coef_loss   | -0.681   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 432267   |\n","---------------------------------\n","Eval num_timesteps=2935200, episode_reward=144.20 +/- 5.97\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 144      |\n","| time/              |          |\n","|    total_timesteps | 2935200  |\n","| train/             |          |\n","|    actor_loss      | -10.4    |\n","|    critic_loss     | 1.37     |\n","|    ent_coef        | 0.00572  |\n","|    ent_coef_loss   | -2.87    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 432285   |\n","---------------------------------\n","Eval num_timesteps=2937600, episode_reward=22.44 +/- 83.54\n","Episode length: 344.60 +/- 126.88\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 345      |\n","|    mean_reward     | 22.4     |\n","| time/              |          |\n","|    total_timesteps | 2937600  |\n","| train/             |          |\n","|    actor_loss      | -10.8    |\n","|    critic_loss     | 1.53     |\n","|    ent_coef        | 0.0056   |\n","|    ent_coef_loss   | 1.65     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 432385   |\n","---------------------------------\n","Eval num_timesteps=2940000, episode_reward=32.88 +/- 84.76\n","Episode length: 367.40 +/- 108.27\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 367      |\n","|    mean_reward     | 32.9     |\n","| time/              |          |\n","|    total_timesteps | 2940000  |\n","| train/             |          |\n","|    actor_loss      | -10.8    |\n","|    critic_loss     | 1.33     |\n","|    ent_coef        | 0.00555  |\n","|    ent_coef_loss   | -3       |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 432485   |\n","---------------------------------\n","Eval num_timesteps=2942400, episode_reward=-7.41 +/- 38.87\n","Episode length: 382.40 +/- 121.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 382      |\n","|    mean_reward     | -7.41    |\n","| time/              |          |\n","|    total_timesteps | 2942400  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 35.7     |\n","|    ent_coef        | 0.0055   |\n","|    ent_coef_loss   | -1.49    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 432585   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2050     |\n","|    fps             | 277      |\n","|    time_elapsed    | 10596    |\n","|    total_timesteps | 2942832  |\n","| train/             |          |\n","|    actor_loss      | -10.8    |\n","|    critic_loss     | 1.36     |\n","|    ent_coef        | 0.00548  |\n","|    ent_coef_loss   | -1.1     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 432603   |\n","---------------------------------\n","Eval num_timesteps=2944800, episode_reward=169.09 +/- 0.63\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 169      |\n","| time/              |          |\n","|    total_timesteps | 2944800  |\n","| train/             |          |\n","|    actor_loss      | -10.9    |\n","|    critic_loss     | 0.685    |\n","|    ent_coef        | 0.0054   |\n","|    ent_coef_loss   | -0.439   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 432685   |\n","---------------------------------\n","Eval num_timesteps=2947200, episode_reward=177.58 +/- 2.03\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 178      |\n","| time/              |          |\n","|    total_timesteps | 2947200  |\n","| train/             |          |\n","|    actor_loss      | -10.7    |\n","|    critic_loss     | 0.959    |\n","|    ent_coef        | 0.0054   |\n","|    ent_coef_loss   | 0.995    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 432785   |\n","---------------------------------\n","Eval num_timesteps=2949600, episode_reward=181.79 +/- 2.24\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 182      |\n","| time/              |          |\n","|    total_timesteps | 2949600  |\n","| train/             |          |\n","|    actor_loss      | -10.2    |\n","|    critic_loss     | 1.3      |\n","|    ent_coef        | 0.00546  |\n","|    ent_coef_loss   | -1.86    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 432885   |\n","---------------------------------\n","Eval num_timesteps=2952000, episode_reward=70.03 +/- 116.38\n","Episode length: 356.80 +/- 175.38\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 357      |\n","|    mean_reward     | 70       |\n","| time/              |          |\n","|    total_timesteps | 2952000  |\n","| train/             |          |\n","|    actor_loss      | -10.1    |\n","|    critic_loss     | 1.01     |\n","|    ent_coef        | 0.00547  |\n","|    ent_coef_loss   | 2.35     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 432985   |\n","---------------------------------\n","Eval num_timesteps=2954400, episode_reward=173.25 +/- 10.04\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 173      |\n","| time/              |          |\n","|    total_timesteps | 2954400  |\n","| train/             |          |\n","|    actor_loss      | -9.3     |\n","|    critic_loss     | 3.05     |\n","|    ent_coef        | 0.00545  |\n","|    ent_coef_loss   | 4.05     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 433085   |\n","---------------------------------\n","Eval num_timesteps=2956800, episode_reward=148.07 +/- 1.56\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 148      |\n","| time/              |          |\n","|    total_timesteps | 2956800  |\n","| train/             |          |\n","|    actor_loss      | -9.6     |\n","|    critic_loss     | 0.675    |\n","|    ent_coef        | 0.00541  |\n","|    ent_coef_loss   | 1.08     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 433185   |\n","---------------------------------\n","Eval num_timesteps=2959200, episode_reward=169.23 +/- 3.04\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 169      |\n","| time/              |          |\n","|    total_timesteps | 2959200  |\n","| train/             |          |\n","|    actor_loss      | -9.21    |\n","|    critic_loss     | 1.88     |\n","|    ent_coef        | 0.00538  |\n","|    ent_coef_loss   | 2.67     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 433285   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2060     |\n","|    fps             | 277      |\n","|    time_elapsed    | 10659    |\n","|    total_timesteps | 2960376  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 1.8      |\n","|    ent_coef        | 0.00543  |\n","|    ent_coef_loss   | -0.295   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 433334   |\n","---------------------------------\n","Eval num_timesteps=2961600, episode_reward=168.59 +/- 2.52\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 169      |\n","| time/              |          |\n","|    total_timesteps | 2961600  |\n","| train/             |          |\n","|    actor_loss      | -10.2    |\n","|    critic_loss     | 1.57     |\n","|    ent_coef        | 0.00543  |\n","|    ent_coef_loss   | 2.02     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 433385   |\n","---------------------------------\n","Eval num_timesteps=2964000, episode_reward=-43.09 +/- 5.41\n","Episode length: 224.00 +/- 14.70\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 224      |\n","|    mean_reward     | -43.1    |\n","| time/              |          |\n","|    total_timesteps | 2964000  |\n","| train/             |          |\n","|    actor_loss      | -10.6    |\n","|    critic_loss     | 1.21     |\n","|    ent_coef        | 0.00542  |\n","|    ent_coef_loss   | -3.49    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 433485   |\n","---------------------------------\n","Eval num_timesteps=2966400, episode_reward=91.91 +/- 88.06\n","Episode length: 427.60 +/- 88.67\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 428      |\n","|    mean_reward     | 91.9     |\n","| time/              |          |\n","|    total_timesteps | 2966400  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 1.34     |\n","|    ent_coef        | 0.00532  |\n","|    ent_coef_loss   | 1.12     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 433585   |\n","---------------------------------\n","Eval num_timesteps=2968800, episode_reward=99.82 +/- 66.75\n","Episode length: 467.20 +/- 40.17\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 467      |\n","|    mean_reward     | 99.8     |\n","| time/              |          |\n","|    total_timesteps | 2968800  |\n","| train/             |          |\n","|    actor_loss      | -9.96    |\n","|    critic_loss     | 1.01     |\n","|    ent_coef        | 0.00532  |\n","|    ent_coef_loss   | -1.88    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 433685   |\n","---------------------------------\n","Eval num_timesteps=2971200, episode_reward=150.37 +/- 1.83\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 150      |\n","| time/              |          |\n","|    total_timesteps | 2971200  |\n","| train/             |          |\n","|    actor_loss      | -11.3    |\n","|    critic_loss     | 0.322    |\n","|    ent_coef        | 0.00528  |\n","|    ent_coef_loss   | 0.324    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 433785   |\n","---------------------------------\n","Eval num_timesteps=2973600, episode_reward=176.50 +/- 0.73\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 176      |\n","| time/              |          |\n","|    total_timesteps | 2973600  |\n","| train/             |          |\n","|    actor_loss      | -10.2    |\n","|    critic_loss     | 1.43     |\n","|    ent_coef        | 0.00537  |\n","|    ent_coef_loss   | -2.91    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 433885   |\n","---------------------------------\n","Eval num_timesteps=2976000, episode_reward=168.52 +/- 4.39\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 169      |\n","| time/              |          |\n","|    total_timesteps | 2976000  |\n","| train/             |          |\n","|    actor_loss      | -10.6    |\n","|    critic_loss     | 1.47     |\n","|    ent_coef        | 0.00531  |\n","|    ent_coef_loss   | 0.519    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 433985   |\n","---------------------------------\n","Eval num_timesteps=2978400, episode_reward=176.78 +/- 7.41\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 177      |\n","| time/              |          |\n","|    total_timesteps | 2978400  |\n","| train/             |          |\n","|    actor_loss      | -11.1    |\n","|    critic_loss     | 1.04     |\n","|    ent_coef        | 0.00541  |\n","|    ent_coef_loss   | -0.174   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 434085   |\n","---------------------------------\n","Eval num_timesteps=2980800, episode_reward=179.68 +/- 1.43\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 180      |\n","| time/              |          |\n","|    total_timesteps | 2980800  |\n","| train/             |          |\n","|    actor_loss      | -10.8    |\n","|    critic_loss     | 0.712    |\n","|    ent_coef        | 0.00547  |\n","|    ent_coef_loss   | 2.56     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 434185   |\n","---------------------------------\n","Eval num_timesteps=2983200, episode_reward=161.09 +/- 5.93\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 161      |\n","| time/              |          |\n","|    total_timesteps | 2983200  |\n","| train/             |          |\n","|    actor_loss      | -10.6    |\n","|    critic_loss     | 2.56     |\n","|    ent_coef        | 0.00545  |\n","|    ent_coef_loss   | -1.76    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 434285   |\n","---------------------------------\n","Eval num_timesteps=2985600, episode_reward=16.61 +/- 135.27\n","Episode length: 272.60 +/- 185.67\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 273      |\n","|    mean_reward     | 16.6     |\n","| time/              |          |\n","|    total_timesteps | 2985600  |\n","| train/             |          |\n","|    actor_loss      | -9.97    |\n","|    critic_loss     | 1.39     |\n","|    ent_coef        | 0.00546  |\n","|    ent_coef_loss   | -0.133   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 434385   |\n","---------------------------------\n","Eval num_timesteps=2988000, episode_reward=184.18 +/- 0.14\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 184      |\n","| time/              |          |\n","|    total_timesteps | 2988000  |\n","| train/             |          |\n","|    actor_loss      | -10.8    |\n","|    critic_loss     | 0.479    |\n","|    ent_coef        | 0.00547  |\n","|    ent_coef_loss   | -1.12    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 434485   |\n","---------------------------------\n","Eval num_timesteps=2990400, episode_reward=172.89 +/- 2.97\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 173      |\n","| time/              |          |\n","|    total_timesteps | 2990400  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 1.02     |\n","|    ent_coef        | 0.0054   |\n","|    ent_coef_loss   | 3.46     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 434585   |\n","---------------------------------\n","Eval num_timesteps=2992800, episode_reward=174.68 +/- 0.68\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 175      |\n","| time/              |          |\n","|    total_timesteps | 2992800  |\n","| train/             |          |\n","|    actor_loss      | -10.7    |\n","|    critic_loss     | 2.72     |\n","|    ent_coef        | 0.00532  |\n","|    ent_coef_loss   | -3.09    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 434685   |\n","---------------------------------\n","Eval num_timesteps=2995200, episode_reward=-52.10 +/- 10.72\n","Episode length: 192.20 +/- 25.47\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 192      |\n","|    mean_reward     | -52.1    |\n","| time/              |          |\n","|    total_timesteps | 2995200  |\n","| train/             |          |\n","|    actor_loss      | -10      |\n","|    critic_loss     | 4.89     |\n","|    ent_coef        | 0.00534  |\n","|    ent_coef_loss   | 0.0242   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 434785   |\n","---------------------------------\n","Eval num_timesteps=2997600, episode_reward=176.53 +/- 0.24\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 177      |\n","| time/              |          |\n","|    total_timesteps | 2997600  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 7.31     |\n","|    ent_coef        | 0.0053   |\n","|    ent_coef_loss   | 1.14     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 434885   |\n","---------------------------------\n","Eval num_timesteps=3000000, episode_reward=179.75 +/- 0.80\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 180      |\n","| time/              |          |\n","|    total_timesteps | 3000000  |\n","| train/             |          |\n","|    actor_loss      | -10.6    |\n","|    critic_loss     | 7.26     |\n","|    ent_coef        | 0.00529  |\n","|    ent_coef_loss   | -1.45    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 434985   |\n","---------------------------------\n","Eval num_timesteps=3002400, episode_reward=179.21 +/- 2.02\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 179      |\n","| time/              |          |\n","|    total_timesteps | 3002400  |\n","| train/             |          |\n","|    actor_loss      | -10.4    |\n","|    critic_loss     | 1.03     |\n","|    ent_coef        | 0.00541  |\n","|    ent_coef_loss   | 0.44     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 435085   |\n","---------------------------------\n","Eval num_timesteps=3004800, episode_reward=52.05 +/- 93.38\n","Episode length: 359.60 +/- 114.64\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 360      |\n","|    mean_reward     | 52.1     |\n","| time/              |          |\n","|    total_timesteps | 3004800  |\n","| train/             |          |\n","|    actor_loss      | -11.1    |\n","|    critic_loss     | 1.98     |\n","|    ent_coef        | 0.00535  |\n","|    ent_coef_loss   | -3       |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 435185   |\n","---------------------------------\n","Eval num_timesteps=3007200, episode_reward=185.68 +/- 1.90\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 186      |\n","| time/              |          |\n","|    total_timesteps | 3007200  |\n","| train/             |          |\n","|    actor_loss      | -10.7    |\n","|    critic_loss     | 0.791    |\n","|    ent_coef        | 0.00529  |\n","|    ent_coef_loss   | -0.797   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 435285   |\n","---------------------------------\n","Eval num_timesteps=3009600, episode_reward=177.54 +/- 0.90\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 178      |\n","| time/              |          |\n","|    total_timesteps | 3009600  |\n","| train/             |          |\n","|    actor_loss      | -11.3    |\n","|    critic_loss     | 0.729    |\n","|    ent_coef        | 0.00537  |\n","|    ent_coef_loss   | -1.34    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 435385   |\n","---------------------------------\n","Eval num_timesteps=3012000, episode_reward=149.19 +/- 2.31\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 149      |\n","| time/              |          |\n","|    total_timesteps | 3012000  |\n","| train/             |          |\n","|    actor_loss      | -10.8    |\n","|    critic_loss     | 0.796    |\n","|    ent_coef        | 0.00544  |\n","|    ent_coef_loss   | -2.16    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 435485   |\n","---------------------------------\n","Eval num_timesteps=3014400, episode_reward=-12.42 +/- 32.63\n","Episode length: 316.40 +/- 102.39\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 316      |\n","|    mean_reward     | -12.4    |\n","| time/              |          |\n","|    total_timesteps | 3014400  |\n","| train/             |          |\n","|    actor_loss      | -10.2    |\n","|    critic_loss     | 1.22     |\n","|    ent_coef        | 0.00528  |\n","|    ent_coef_loss   | -0.866   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 435585   |\n","---------------------------------\n","Eval num_timesteps=3016800, episode_reward=-43.86 +/- 42.85\n","Episode length: 213.80 +/- 89.16\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 214      |\n","|    mean_reward     | -43.9    |\n","| time/              |          |\n","|    total_timesteps | 3016800  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 1.95     |\n","|    ent_coef        | 0.0052   |\n","|    ent_coef_loss   | -1.29    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 435685   |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    episodes        | 2070    |\n","|    fps             | 277     |\n","|    time_elapsed    | 10860   |\n","|    total_timesteps | 3016800 |\n","--------------------------------\n","--------------------------------\n","| time/              |         |\n","|    episodes        | 2080    |\n","|    fps             | 277     |\n","|    time_elapsed    | 10860   |\n","|    total_timesteps | 3016800 |\n","--------------------------------\n","Eval num_timesteps=3019200, episode_reward=170.35 +/- 2.85\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 170      |\n","| time/              |          |\n","|    total_timesteps | 3019200  |\n","| train/             |          |\n","|    actor_loss      | -10.7    |\n","|    critic_loss     | 0.809    |\n","|    ent_coef        | 0.0052   |\n","|    ent_coef_loss   | 1.36     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 435785   |\n","---------------------------------\n","Eval num_timesteps=3021600, episode_reward=169.35 +/- 1.17\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 169      |\n","| time/              |          |\n","|    total_timesteps | 3021600  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 1.48     |\n","|    ent_coef        | 0.00517  |\n","|    ent_coef_loss   | -1.53    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 435885   |\n","---------------------------------\n","Eval num_timesteps=3024000, episode_reward=167.70 +/- 2.38\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 168      |\n","| time/              |          |\n","|    total_timesteps | 3024000  |\n","| train/             |          |\n","|    actor_loss      | -10.4    |\n","|    critic_loss     | 1.85     |\n","|    ent_coef        | 0.00522  |\n","|    ent_coef_loss   | 4.55     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 435985   |\n","---------------------------------\n","Eval num_timesteps=3026400, episode_reward=159.67 +/- 1.37\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 160      |\n","| time/              |          |\n","|    total_timesteps | 3026400  |\n","| train/             |          |\n","|    actor_loss      | -9.61    |\n","|    critic_loss     | 1.44     |\n","|    ent_coef        | 0.00531  |\n","|    ent_coef_loss   | 1.18     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 436085   |\n","---------------------------------\n","Eval num_timesteps=3028800, episode_reward=175.44 +/- 1.11\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 175      |\n","| time/              |          |\n","|    total_timesteps | 3028800  |\n","| train/             |          |\n","|    actor_loss      | -8.47    |\n","|    critic_loss     | 2.13     |\n","|    ent_coef        | 0.00538  |\n","|    ent_coef_loss   | 0.878    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 436185   |\n","---------------------------------\n","Eval num_timesteps=3031200, episode_reward=100.92 +/- 54.00\n","Episode length: 480.80 +/- 15.68\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 481      |\n","|    mean_reward     | 101      |\n","| time/              |          |\n","|    total_timesteps | 3031200  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 0.988    |\n","|    ent_coef        | 0.00549  |\n","|    ent_coef_loss   | 1.59     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 436285   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2090     |\n","|    fps             | 277      |\n","|    time_elapsed    | 10915    |\n","|    total_timesteps | 3031680  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 1.89     |\n","|    ent_coef        | 0.00551  |\n","|    ent_coef_loss   | 0.334    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 436305   |\n","---------------------------------\n","Eval num_timesteps=3033600, episode_reward=102.38 +/- 90.29\n","Episode length: 417.20 +/- 101.41\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 417      |\n","|    mean_reward     | 102      |\n","| time/              |          |\n","|    total_timesteps | 3033600  |\n","| train/             |          |\n","|    actor_loss      | -9.75    |\n","|    critic_loss     | 4.31     |\n","|    ent_coef        | 0.00555  |\n","|    ent_coef_loss   | 2.67     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 436385   |\n","---------------------------------\n","Eval num_timesteps=3036000, episode_reward=170.87 +/- 0.98\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 171      |\n","| time/              |          |\n","|    total_timesteps | 3036000  |\n","| train/             |          |\n","|    actor_loss      | -9.01    |\n","|    critic_loss     | 1.5      |\n","|    ent_coef        | 0.00576  |\n","|    ent_coef_loss   | 3.34     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 436485   |\n","---------------------------------\n","Eval num_timesteps=3038400, episode_reward=32.91 +/- 108.60\n","Episode length: 309.80 +/- 155.30\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 310      |\n","|    mean_reward     | 32.9     |\n","| time/              |          |\n","|    total_timesteps | 3038400  |\n","| train/             |          |\n","|    actor_loss      | -10.6    |\n","|    critic_loss     | 1.45     |\n","|    ent_coef        | 0.00581  |\n","|    ent_coef_loss   | 2.93     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 436585   |\n","---------------------------------\n","Eval num_timesteps=3040800, episode_reward=46.18 +/- 103.04\n","Episode length: 333.80 +/- 135.70\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 334      |\n","|    mean_reward     | 46.2     |\n","| time/              |          |\n","|    total_timesteps | 3040800  |\n","| train/             |          |\n","|    actor_loss      | -10.2    |\n","|    critic_loss     | 4.6      |\n","|    ent_coef        | 0.0058   |\n","|    ent_coef_loss   | 4.07     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 436685   |\n","---------------------------------\n","Eval num_timesteps=3043200, episode_reward=98.44 +/- 87.85\n","Episode length: 430.40 +/- 85.24\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 430      |\n","|    mean_reward     | 98.4     |\n","| time/              |          |\n","|    total_timesteps | 3043200  |\n","| train/             |          |\n","|    actor_loss      | -9.43    |\n","|    critic_loss     | 23.4     |\n","|    ent_coef        | 0.0058   |\n","|    ent_coef_loss   | 2.44     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 436785   |\n","---------------------------------\n","Eval num_timesteps=3045600, episode_reward=176.04 +/- 2.70\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 176      |\n","| time/              |          |\n","|    total_timesteps | 3045600  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 3.18     |\n","|    ent_coef        | 0.00584  |\n","|    ent_coef_loss   | 2.3      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 436885   |\n","---------------------------------\n","Eval num_timesteps=3048000, episode_reward=178.27 +/- 5.47\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 178      |\n","| time/              |          |\n","|    total_timesteps | 3048000  |\n","| train/             |          |\n","|    actor_loss      | -10.2    |\n","|    critic_loss     | 2.07     |\n","|    ent_coef        | 0.00592  |\n","|    ent_coef_loss   | 2.73     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 436985   |\n","---------------------------------\n","Eval num_timesteps=3050400, episode_reward=90.07 +/- 78.94\n","Episode length: 419.00 +/- 66.14\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 419      |\n","|    mean_reward     | 90.1     |\n","| time/              |          |\n","|    total_timesteps | 3050400  |\n","| train/             |          |\n","|    actor_loss      | -10.4    |\n","|    critic_loss     | 2.55     |\n","|    ent_coef        | 0.00584  |\n","|    ent_coef_loss   | -3.35    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 437085   |\n","---------------------------------\n","Eval num_timesteps=3052800, episode_reward=78.48 +/- 82.40\n","Episode length: 401.60 +/- 80.34\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 402      |\n","|    mean_reward     | 78.5     |\n","| time/              |          |\n","|    total_timesteps | 3052800  |\n","| train/             |          |\n","|    actor_loss      | -9.86    |\n","|    critic_loss     | 1.52     |\n","|    ent_coef        | 0.00562  |\n","|    ent_coef_loss   | 0.604    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 437185   |\n","---------------------------------\n","Eval num_timesteps=3055200, episode_reward=171.92 +/- 3.77\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 172      |\n","| time/              |          |\n","|    total_timesteps | 3055200  |\n","| train/             |          |\n","|    actor_loss      | -10.7    |\n","|    critic_loss     | 0.826    |\n","|    ent_coef        | 0.00563  |\n","|    ent_coef_loss   | -2.78    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 437285   |\n","---------------------------------\n","Eval num_timesteps=3057600, episode_reward=168.98 +/- 2.54\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 169      |\n","| time/              |          |\n","|    total_timesteps | 3057600  |\n","| train/             |          |\n","|    actor_loss      | -10.5    |\n","|    critic_loss     | 1.14     |\n","|    ent_coef        | 0.00554  |\n","|    ent_coef_loss   | -2.07    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 437385   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2100     |\n","|    fps             | 277      |\n","|    time_elapsed    | 11013    |\n","|    total_timesteps | 3058872  |\n","| train/             |          |\n","|    actor_loss      | -10.9    |\n","|    critic_loss     | 0.711    |\n","|    ent_coef        | 0.00551  |\n","|    ent_coef_loss   | 2.45     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 437438   |\n","---------------------------------\n","Eval num_timesteps=3060000, episode_reward=101.32 +/- 84.31\n","Episode length: 422.40 +/- 95.04\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 422      |\n","|    mean_reward     | 101      |\n","| time/              |          |\n","|    total_timesteps | 3060000  |\n","| train/             |          |\n","|    actor_loss      | -11.1    |\n","|    critic_loss     | 0.55     |\n","|    ent_coef        | 0.00555  |\n","|    ent_coef_loss   | -0.889   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 437485   |\n","---------------------------------\n","Eval num_timesteps=3062400, episode_reward=16.48 +/- 139.61\n","Episode length: 237.80 +/- 214.09\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 238      |\n","|    mean_reward     | 16.5     |\n","| time/              |          |\n","|    total_timesteps | 3062400  |\n","| train/             |          |\n","|    actor_loss      | -11.4    |\n","|    critic_loss     | 0.422    |\n","|    ent_coef        | 0.00554  |\n","|    ent_coef_loss   | -3.09    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 437585   |\n","---------------------------------\n","Eval num_timesteps=3064800, episode_reward=3.67 +/- 44.01\n","Episode length: 337.80 +/- 114.15\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 338      |\n","|    mean_reward     | 3.67     |\n","| time/              |          |\n","|    total_timesteps | 3064800  |\n","| train/             |          |\n","|    actor_loss      | -10.4    |\n","|    critic_loss     | 0.601    |\n","|    ent_coef        | 0.00547  |\n","|    ent_coef_loss   | 4.69     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 437685   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2110     |\n","|    fps             | 277      |\n","|    time_elapsed    | 11040    |\n","|    total_timesteps | 3066456  |\n","| train/             |          |\n","|    actor_loss      | -11.3    |\n","|    critic_loss     | 1        |\n","|    ent_coef        | 0.00553  |\n","|    ent_coef_loss   | 0.156    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 437754   |\n","---------------------------------\n","Eval num_timesteps=3067200, episode_reward=86.94 +/- 120.91\n","Episode length: 374.40 +/- 153.83\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 374      |\n","|    mean_reward     | 86.9     |\n","| time/              |          |\n","|    total_timesteps | 3067200  |\n","| train/             |          |\n","|    actor_loss      | -11.1    |\n","|    critic_loss     | 0.902    |\n","|    ent_coef        | 0.00553  |\n","|    ent_coef_loss   | -1.26    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 437785   |\n","---------------------------------\n","Eval num_timesteps=3069600, episode_reward=116.53 +/- 56.81\n","Episode length: 496.40 +/- 2.94\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 496      |\n","|    mean_reward     | 117      |\n","| time/              |          |\n","|    total_timesteps | 3069600  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 2.25     |\n","|    ent_coef        | 0.00544  |\n","|    ent_coef_loss   | -2.86    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 437885   |\n","---------------------------------\n","Eval num_timesteps=3072000, episode_reward=94.61 +/- 104.07\n","Episode length: 391.60 +/- 132.76\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 392      |\n","|    mean_reward     | 94.6     |\n","| time/              |          |\n","|    total_timesteps | 3072000  |\n","| train/             |          |\n","|    actor_loss      | -11.4    |\n","|    critic_loss     | 0.67     |\n","|    ent_coef        | 0.00538  |\n","|    ent_coef_loss   | -0.797   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 437985   |\n","---------------------------------\n","Eval num_timesteps=3074400, episode_reward=43.17 +/- 108.62\n","Episode length: 332.00 +/- 137.17\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 332      |\n","|    mean_reward     | 43.2     |\n","| time/              |          |\n","|    total_timesteps | 3074400  |\n","| train/             |          |\n","|    actor_loss      | -10.7    |\n","|    critic_loss     | 1.95     |\n","|    ent_coef        | 0.00541  |\n","|    ent_coef_loss   | 3.71     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 438085   |\n","---------------------------------\n","Eval num_timesteps=3076800, episode_reward=-87.01 +/- 7.69\n","Episode length: 100.40 +/- 4.41\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 100      |\n","|    mean_reward     | -87      |\n","| time/              |          |\n","|    total_timesteps | 3076800  |\n","| train/             |          |\n","|    actor_loss      | -11.2    |\n","|    critic_loss     | 1.86     |\n","|    ent_coef        | 0.00529  |\n","|    ent_coef_loss   | -2.37    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 438185   |\n","---------------------------------\n","Eval num_timesteps=3079200, episode_reward=166.04 +/- 1.33\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 166      |\n","| time/              |          |\n","|    total_timesteps | 3079200  |\n","| train/             |          |\n","|    actor_loss      | -11.1    |\n","|    critic_loss     | 1.68     |\n","|    ent_coef        | 0.00527  |\n","|    ent_coef_loss   | 0.339    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 438285   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2120     |\n","|    fps             | 277      |\n","|    time_elapsed    | 11089    |\n","|    total_timesteps | 3080880  |\n","| train/             |          |\n","|    actor_loss      | -10.2    |\n","|    critic_loss     | 2.01     |\n","|    ent_coef        | 0.00529  |\n","|    ent_coef_loss   | 1.67     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 438355   |\n","---------------------------------\n","Eval num_timesteps=3081600, episode_reward=163.01 +/- 0.85\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 163      |\n","| time/              |          |\n","|    total_timesteps | 3081600  |\n","| train/             |          |\n","|    actor_loss      | -10.5    |\n","|    critic_loss     | 3.5      |\n","|    ent_coef        | 0.00528  |\n","|    ent_coef_loss   | 0.113    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 438385   |\n","---------------------------------\n","Eval num_timesteps=3084000, episode_reward=-11.13 +/- 8.64\n","Episode length: 347.20 +/- 15.68\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 347      |\n","|    mean_reward     | -11.1    |\n","| time/              |          |\n","|    total_timesteps | 3084000  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 10.5     |\n","|    ent_coef        | 0.00535  |\n","|    ent_coef_loss   | -1.52    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 438485   |\n","---------------------------------\n","Eval num_timesteps=3086400, episode_reward=39.79 +/- 79.50\n","Episode length: 381.80 +/- 96.51\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 382      |\n","|    mean_reward     | 39.8     |\n","| time/              |          |\n","|    total_timesteps | 3086400  |\n","| train/             |          |\n","|    actor_loss      | -11.9    |\n","|    critic_loss     | 0.395    |\n","|    ent_coef        | 0.00524  |\n","|    ent_coef_loss   | -2.91    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 438585   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2130     |\n","|    fps             | 277      |\n","|    time_elapsed    | 11114    |\n","|    total_timesteps | 3088608  |\n","| train/             |          |\n","|    actor_loss      | -10.9    |\n","|    critic_loss     | 2.43     |\n","|    ent_coef        | 0.00525  |\n","|    ent_coef_loss   | 1.21     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 438677   |\n","---------------------------------\n","Eval num_timesteps=3088800, episode_reward=-14.66 +/- 29.81\n","Episode length: 359.80 +/- 52.91\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 360      |\n","|    mean_reward     | -14.7    |\n","| time/              |          |\n","|    total_timesteps | 3088800  |\n","| train/             |          |\n","|    actor_loss      | -11.2    |\n","|    critic_loss     | 1.09     |\n","|    ent_coef        | 0.00525  |\n","|    ent_coef_loss   | -2.76    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 438685   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2140     |\n","|    fps             | 277      |\n","|    time_elapsed    | 11120    |\n","|    total_timesteps | 3090168  |\n","| train/             |          |\n","|    actor_loss      | -9.63    |\n","|    critic_loss     | 1.48     |\n","|    ent_coef        | 0.00525  |\n","|    ent_coef_loss   | 0.437    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 438742   |\n","---------------------------------\n","Eval num_timesteps=3091200, episode_reward=160.44 +/- 0.28\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 160      |\n","| time/              |          |\n","|    total_timesteps | 3091200  |\n","| train/             |          |\n","|    actor_loss      | -11.2    |\n","|    critic_loss     | 0.736    |\n","|    ent_coef        | 0.00521  |\n","|    ent_coef_loss   | -3.87    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 438785   |\n","---------------------------------\n","Eval num_timesteps=3093600, episode_reward=67.82 +/- 128.56\n","Episode length: 346.00 +/- 188.61\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 346      |\n","|    mean_reward     | 67.8     |\n","| time/              |          |\n","|    total_timesteps | 3093600  |\n","| train/             |          |\n","|    actor_loss      | -9.23    |\n","|    critic_loss     | 1.51     |\n","|    ent_coef        | 0.00515  |\n","|    ent_coef_loss   | -1       |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 438885   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2150     |\n","|    fps             | 277      |\n","|    time_elapsed    | 11140    |\n","|    total_timesteps | 3095952  |\n","| train/             |          |\n","|    actor_loss      | -11.4    |\n","|    critic_loss     | 17.4     |\n","|    ent_coef        | 0.00505  |\n","|    ent_coef_loss   | -1.72    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 438983   |\n","---------------------------------\n","Eval num_timesteps=3096000, episode_reward=53.98 +/- 114.64\n","Episode length: 353.20 +/- 179.79\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 353      |\n","|    mean_reward     | 54       |\n","| time/              |          |\n","|    total_timesteps | 3096000  |\n","| train/             |          |\n","|    actor_loss      | -9.88    |\n","|    critic_loss     | 1.19     |\n","|    ent_coef        | 0.00505  |\n","|    ent_coef_loss   | -0.371   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 438985   |\n","---------------------------------\n","Eval num_timesteps=3098400, episode_reward=170.89 +/- 0.57\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 171      |\n","| time/              |          |\n","|    total_timesteps | 3098400  |\n","| train/             |          |\n","|    actor_loss      | -11.1    |\n","|    critic_loss     | 1.26     |\n","|    ent_coef        | 0.00505  |\n","|    ent_coef_loss   | 4.54     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 439085   |\n","---------------------------------\n","Eval num_timesteps=3100800, episode_reward=176.18 +/- 0.33\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 176      |\n","| time/              |          |\n","|    total_timesteps | 3100800  |\n","| train/             |          |\n","|    actor_loss      | -10.7    |\n","|    critic_loss     | 2.08     |\n","|    ent_coef        | 0.00511  |\n","|    ent_coef_loss   | 1.14     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 439185   |\n","---------------------------------\n","Eval num_timesteps=3103200, episode_reward=162.31 +/- 0.52\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 162      |\n","| time/              |          |\n","|    total_timesteps | 3103200  |\n","| train/             |          |\n","|    actor_loss      | -10.4    |\n","|    critic_loss     | 37.3     |\n","|    ent_coef        | 0.00507  |\n","|    ent_coef_loss   | 1.33     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 439285   |\n","---------------------------------\n","Eval num_timesteps=3105600, episode_reward=154.19 +/- 7.96\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 154      |\n","| time/              |          |\n","|    total_timesteps | 3105600  |\n","| train/             |          |\n","|    actor_loss      | -11.5    |\n","|    critic_loss     | 6.18     |\n","|    ent_coef        | 0.00506  |\n","|    ent_coef_loss   | -2.6     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 439385   |\n","---------------------------------\n","Eval num_timesteps=3108000, episode_reward=-66.21 +/- 5.40\n","Episode length: 191.60 +/- 9.31\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 192      |\n","|    mean_reward     | -66.2    |\n","| time/              |          |\n","|    total_timesteps | 3108000  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 0.938    |\n","|    ent_coef        | 0.00506  |\n","|    ent_coef_loss   | 0.499    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 439485   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2160     |\n","|    fps             | 277      |\n","|    time_elapsed    | 11189    |\n","|    total_timesteps | 3109128  |\n","| train/             |          |\n","|    actor_loss      | -11.8    |\n","|    critic_loss     | 12.9     |\n","|    ent_coef        | 0.0051   |\n","|    ent_coef_loss   | 2.03     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 439532   |\n","---------------------------------\n","Eval num_timesteps=3110400, episode_reward=173.98 +/- 0.50\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 174      |\n","| time/              |          |\n","|    total_timesteps | 3110400  |\n","| train/             |          |\n","|    actor_loss      | -10.6    |\n","|    critic_loss     | 2.92     |\n","|    ent_coef        | 0.00513  |\n","|    ent_coef_loss   | 0.246    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 439585   |\n","---------------------------------\n","Eval num_timesteps=3112800, episode_reward=150.28 +/- 2.95\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 150      |\n","| time/              |          |\n","|    total_timesteps | 3112800  |\n","| train/             |          |\n","|    actor_loss      | -10.5    |\n","|    critic_loss     | 1.86     |\n","|    ent_coef        | 0.00521  |\n","|    ent_coef_loss   | 3.57     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 439685   |\n","---------------------------------\n","Eval num_timesteps=3115200, episode_reward=159.32 +/- 0.16\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 159      |\n","| time/              |          |\n","|    total_timesteps | 3115200  |\n","| train/             |          |\n","|    actor_loss      | -10.5    |\n","|    critic_loss     | 1.32     |\n","|    ent_coef        | 0.00532  |\n","|    ent_coef_loss   | 0.602    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 439785   |\n","---------------------------------\n","Eval num_timesteps=3117600, episode_reward=87.80 +/- 65.90\n","Episode length: 450.00 +/- 61.24\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 450      |\n","|    mean_reward     | 87.8     |\n","| time/              |          |\n","|    total_timesteps | 3117600  |\n","| train/             |          |\n","|    actor_loss      | -11.5    |\n","|    critic_loss     | 0.76     |\n","|    ent_coef        | 0.00541  |\n","|    ent_coef_loss   | -1.25    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 439885   |\n","---------------------------------\n","Eval num_timesteps=3120000, episode_reward=159.61 +/- 6.45\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 160      |\n","| time/              |          |\n","|    total_timesteps | 3120000  |\n","| train/             |          |\n","|    actor_loss      | -11.3    |\n","|    critic_loss     | 2.15     |\n","|    ent_coef        | 0.00548  |\n","|    ent_coef_loss   | 1.33     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 439985   |\n","---------------------------------\n","Eval num_timesteps=3122400, episode_reward=168.24 +/- 5.04\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 168      |\n","| time/              |          |\n","|    total_timesteps | 3122400  |\n","| train/             |          |\n","|    actor_loss      | -10.4    |\n","|    critic_loss     | 0.931    |\n","|    ent_coef        | 0.00563  |\n","|    ent_coef_loss   | 3.44     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 440085   |\n","---------------------------------\n","Eval num_timesteps=3124800, episode_reward=7.89 +/- 40.66\n","Episode length: 390.00 +/- 112.68\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 390      |\n","|    mean_reward     | 7.89     |\n","| time/              |          |\n","|    total_timesteps | 3124800  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 1.63     |\n","|    ent_coef        | 0.00573  |\n","|    ent_coef_loss   | -0.245   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 440185   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2170     |\n","|    fps             | 277      |\n","|    time_elapsed    | 11249    |\n","|    total_timesteps | 3125232  |\n","| train/             |          |\n","|    actor_loss      | -11.2    |\n","|    critic_loss     | 3.17     |\n","|    ent_coef        | 0.00573  |\n","|    ent_coef_loss   | 1.12     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 440203   |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    episodes        | 2180    |\n","|    fps             | 277     |\n","|    time_elapsed    | 11249   |\n","|    total_timesteps | 3125232 |\n","--------------------------------\n","Eval num_timesteps=3127200, episode_reward=162.63 +/- 0.13\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 163      |\n","| time/              |          |\n","|    total_timesteps | 3127200  |\n","| train/             |          |\n","|    actor_loss      | -10.6    |\n","|    critic_loss     | 16.8     |\n","|    ent_coef        | 0.00573  |\n","|    ent_coef_loss   | -1       |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 440285   |\n","---------------------------------\n","Eval num_timesteps=3129600, episode_reward=156.58 +/- 0.33\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 157      |\n","| time/              |          |\n","|    total_timesteps | 3129600  |\n","| train/             |          |\n","|    actor_loss      | -10.9    |\n","|    critic_loss     | 0.843    |\n","|    ent_coef        | 0.00573  |\n","|    ent_coef_loss   | 1.37     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 440385   |\n","---------------------------------\n","Eval num_timesteps=3132000, episode_reward=173.21 +/- 2.57\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 173      |\n","| time/              |          |\n","|    total_timesteps | 3132000  |\n","| train/             |          |\n","|    actor_loss      | -11.6    |\n","|    critic_loss     | 0.818    |\n","|    ent_coef        | 0.00573  |\n","|    ent_coef_loss   | 1.89     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 440485   |\n","---------------------------------\n","Eval num_timesteps=3134400, episode_reward=155.25 +/- 2.31\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 155      |\n","| time/              |          |\n","|    total_timesteps | 3134400  |\n","| train/             |          |\n","|    actor_loss      | -10.9    |\n","|    critic_loss     | 0.629    |\n","|    ent_coef        | 0.00573  |\n","|    ent_coef_loss   | -0.0915  |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 440585   |\n","---------------------------------\n","Eval num_timesteps=3136800, episode_reward=95.92 +/- 96.63\n","Episode length: 418.80 +/- 99.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 419      |\n","|    mean_reward     | 95.9     |\n","| time/              |          |\n","|    total_timesteps | 3136800  |\n","| train/             |          |\n","|    actor_loss      | -11.5    |\n","|    critic_loss     | 33       |\n","|    ent_coef        | 0.00574  |\n","|    ent_coef_loss   | 0.614    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 440685   |\n","---------------------------------\n","Eval num_timesteps=3139200, episode_reward=118.50 +/- 61.34\n","Episode length: 484.00 +/- 19.60\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 484      |\n","|    mean_reward     | 118      |\n","| time/              |          |\n","|    total_timesteps | 3139200  |\n","| train/             |          |\n","|    actor_loss      | -11.6    |\n","|    critic_loss     | 1.44     |\n","|    ent_coef        | 0.00575  |\n","|    ent_coef_loss   | -1.09    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 440785   |\n","---------------------------------\n","Eval num_timesteps=3141600, episode_reward=148.61 +/- 1.93\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 149      |\n","| time/              |          |\n","|    total_timesteps | 3141600  |\n","| train/             |          |\n","|    actor_loss      | -11.3    |\n","|    critic_loss     | 1.58     |\n","|    ent_coef        | 0.00579  |\n","|    ent_coef_loss   | -1.51    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 440885   |\n","---------------------------------\n","Eval num_timesteps=3144000, episode_reward=72.58 +/- 79.08\n","Episode length: 411.80 +/- 72.01\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 412      |\n","|    mean_reward     | 72.6     |\n","| time/              |          |\n","|    total_timesteps | 3144000  |\n","| train/             |          |\n","|    actor_loss      | -11.9    |\n","|    critic_loss     | 0.717    |\n","|    ent_coef        | 0.00562  |\n","|    ent_coef_loss   | -2.5     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 440985   |\n","---------------------------------\n","Eval num_timesteps=3146400, episode_reward=110.27 +/- 56.65\n","Episode length: 487.20 +/- 15.68\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 487      |\n","|    mean_reward     | 110      |\n","| time/              |          |\n","|    total_timesteps | 3146400  |\n","| train/             |          |\n","|    actor_loss      | -10.8    |\n","|    critic_loss     | 2.98     |\n","|    ent_coef        | 0.00557  |\n","|    ent_coef_loss   | -1.28    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 441085   |\n","---------------------------------\n","Eval num_timesteps=3148800, episode_reward=159.90 +/- 0.76\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 160      |\n","| time/              |          |\n","|    total_timesteps | 3148800  |\n","| train/             |          |\n","|    actor_loss      | -10.2    |\n","|    critic_loss     | 7.26     |\n","|    ent_coef        | 0.00553  |\n","|    ent_coef_loss   | 2.84     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 441185   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2190     |\n","|    fps             | 277      |\n","|    time_elapsed    | 11343    |\n","|    total_timesteps | 3150792  |\n","| train/             |          |\n","|    actor_loss      | -11.6    |\n","|    critic_loss     | 1.39     |\n","|    ent_coef        | 0.00567  |\n","|    ent_coef_loss   | 2.96     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 441268   |\n","---------------------------------\n","Eval num_timesteps=3151200, episode_reward=139.16 +/- 4.43\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 139      |\n","| time/              |          |\n","|    total_timesteps | 3151200  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 1.29     |\n","|    ent_coef        | 0.00569  |\n","|    ent_coef_loss   | 3.15     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 441285   |\n","---------------------------------\n","Eval num_timesteps=3153600, episode_reward=159.48 +/- 0.64\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 159      |\n","| time/              |          |\n","|    total_timesteps | 3153600  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 2.45     |\n","|    ent_coef        | 0.0057   |\n","|    ent_coef_loss   | 0.775    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 441385   |\n","---------------------------------\n","Eval num_timesteps=3156000, episode_reward=82.89 +/- 75.00\n","Episode length: 442.00 +/- 71.04\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 442      |\n","|    mean_reward     | 82.9     |\n","| time/              |          |\n","|    total_timesteps | 3156000  |\n","| train/             |          |\n","|    actor_loss      | -11.6    |\n","|    critic_loss     | 1.58     |\n","|    ent_coef        | 0.00596  |\n","|    ent_coef_loss   | 1.77     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 441485   |\n","---------------------------------\n","Eval num_timesteps=3158400, episode_reward=154.90 +/- 0.16\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 155      |\n","| time/              |          |\n","|    total_timesteps | 3158400  |\n","| train/             |          |\n","|    actor_loss      | -10.4    |\n","|    critic_loss     | 1.24     |\n","|    ent_coef        | 0.00601  |\n","|    ent_coef_loss   | 4.19     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 441585   |\n","---------------------------------\n","Eval num_timesteps=3160800, episode_reward=114.69 +/- 91.08\n","Episode length: 425.60 +/- 91.12\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 426      |\n","|    mean_reward     | 115      |\n","| time/              |          |\n","|    total_timesteps | 3160800  |\n","| train/             |          |\n","|    actor_loss      | -10.8    |\n","|    critic_loss     | 1.95     |\n","|    ent_coef        | 0.00604  |\n","|    ent_coef_loss   | 0.487    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 441685   |\n","---------------------------------\n","Eval num_timesteps=3163200, episode_reward=176.17 +/- 1.94\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 176      |\n","| time/              |          |\n","|    total_timesteps | 3163200  |\n","| train/             |          |\n","|    actor_loss      | -11.7    |\n","|    critic_loss     | 2.55     |\n","|    ent_coef        | 0.00613  |\n","|    ent_coef_loss   | -1.16    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 441785   |\n","---------------------------------\n","Eval num_timesteps=3165600, episode_reward=69.90 +/- 116.96\n","Episode length: 362.00 +/- 169.01\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 362      |\n","|    mean_reward     | 69.9     |\n","| time/              |          |\n","|    total_timesteps | 3165600  |\n","| train/             |          |\n","|    actor_loss      | -10.4    |\n","|    critic_loss     | 1.46     |\n","|    ent_coef        | 0.00618  |\n","|    ent_coef_loss   | 0.975    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 441885   |\n","---------------------------------\n","Eval num_timesteps=3168000, episode_reward=174.13 +/- 2.94\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 174      |\n","| time/              |          |\n","|    total_timesteps | 3168000  |\n","| train/             |          |\n","|    actor_loss      | -11.3    |\n","|    critic_loss     | 1.87     |\n","|    ent_coef        | 0.00609  |\n","|    ent_coef_loss   | 0.498    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 441985   |\n","---------------------------------\n","Eval num_timesteps=3170400, episode_reward=169.24 +/- 6.09\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 169      |\n","| time/              |          |\n","|    total_timesteps | 3170400  |\n","| train/             |          |\n","|    actor_loss      | -10.8    |\n","|    critic_loss     | 2.32     |\n","|    ent_coef        | 0.00612  |\n","|    ent_coef_loss   | -0.353   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 442085   |\n","---------------------------------\n","Eval num_timesteps=3172800, episode_reward=173.83 +/- 0.86\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 174      |\n","| time/              |          |\n","|    total_timesteps | 3172800  |\n","| train/             |          |\n","|    actor_loss      | -11.8    |\n","|    critic_loss     | 0.757    |\n","|    ent_coef        | 0.00619  |\n","|    ent_coef_loss   | 0.675    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 442185   |\n","---------------------------------\n","Eval num_timesteps=3175200, episode_reward=170.56 +/- 0.27\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 171      |\n","| time/              |          |\n","|    total_timesteps | 3175200  |\n","| train/             |          |\n","|    actor_loss      | -10.5    |\n","|    critic_loss     | 6.37     |\n","|    ent_coef        | 0.00625  |\n","|    ent_coef_loss   | 0.894    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 442285   |\n","---------------------------------\n","Eval num_timesteps=3177600, episode_reward=170.29 +/- 3.04\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 170      |\n","| time/              |          |\n","|    total_timesteps | 3177600  |\n","| train/             |          |\n","|    actor_loss      | -10.9    |\n","|    critic_loss     | 1.27     |\n","|    ent_coef        | 0.00633  |\n","|    ent_coef_loss   | 0.347    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 442385   |\n","---------------------------------\n","Eval num_timesteps=3180000, episode_reward=175.65 +/- 1.21\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 176      |\n","| time/              |          |\n","|    total_timesteps | 3180000  |\n","| train/             |          |\n","|    actor_loss      | -11.2    |\n","|    critic_loss     | 1.51     |\n","|    ent_coef        | 0.0062   |\n","|    ent_coef_loss   | -0.452   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 442485   |\n","---------------------------------\n","Eval num_timesteps=3182400, episode_reward=163.94 +/- 0.52\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 164      |\n","| time/              |          |\n","|    total_timesteps | 3182400  |\n","| train/             |          |\n","|    actor_loss      | -11.2    |\n","|    critic_loss     | 0.634    |\n","|    ent_coef        | 0.00618  |\n","|    ent_coef_loss   | -1.09    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 442585   |\n","---------------------------------\n","Eval num_timesteps=3184800, episode_reward=168.60 +/- 0.12\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 169      |\n","| time/              |          |\n","|    total_timesteps | 3184800  |\n","| train/             |          |\n","|    actor_loss      | -11.2    |\n","|    critic_loss     | 3.52     |\n","|    ent_coef        | 0.00623  |\n","|    ent_coef_loss   | 3.08     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 442685   |\n","---------------------------------\n","Eval num_timesteps=3187200, episode_reward=165.29 +/- 3.30\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 165      |\n","| time/              |          |\n","|    total_timesteps | 3187200  |\n","| train/             |          |\n","|    actor_loss      | -8.77    |\n","|    critic_loss     | 3.23     |\n","|    ent_coef        | 0.00626  |\n","|    ent_coef_loss   | 1.37     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 442785   |\n","---------------------------------\n","Eval num_timesteps=3189600, episode_reward=172.67 +/- 0.90\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 173      |\n","| time/              |          |\n","|    total_timesteps | 3189600  |\n","| train/             |          |\n","|    actor_loss      | -9.79    |\n","|    critic_loss     | 2.53     |\n","|    ent_coef        | 0.00637  |\n","|    ent_coef_loss   | 5.35     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 442885   |\n","---------------------------------\n","Eval num_timesteps=3192000, episode_reward=175.20 +/- 3.57\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 175      |\n","| time/              |          |\n","|    total_timesteps | 3192000  |\n","| train/             |          |\n","|    actor_loss      | -12.2    |\n","|    critic_loss     | 0.895    |\n","|    ent_coef        | 0.00635  |\n","|    ent_coef_loss   | -2.64    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 442985   |\n","---------------------------------\n","Eval num_timesteps=3194400, episode_reward=164.73 +/- 3.65\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 165      |\n","| time/              |          |\n","|    total_timesteps | 3194400  |\n","| train/             |          |\n","|    actor_loss      | -11.1    |\n","|    critic_loss     | 1.75     |\n","|    ent_coef        | 0.00635  |\n","|    ent_coef_loss   | 0.885    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 443085   |\n","---------------------------------\n","Eval num_timesteps=3196800, episode_reward=168.63 +/- 0.77\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 169      |\n","| time/              |          |\n","|    total_timesteps | 3196800  |\n","| train/             |          |\n","|    actor_loss      | -11.6    |\n","|    critic_loss     | 16.1     |\n","|    ent_coef        | 0.00642  |\n","|    ent_coef_loss   | -2.61    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 443185   |\n","---------------------------------\n","Eval num_timesteps=3199200, episode_reward=-61.00 +/- 10.15\n","Episode length: 168.40 +/- 14.21\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 168      |\n","|    mean_reward     | -61      |\n","| time/              |          |\n","|    total_timesteps | 3199200  |\n","| train/             |          |\n","|    actor_loss      | -11.9    |\n","|    critic_loss     | 1.16     |\n","|    ent_coef        | 0.00636  |\n","|    ent_coef_loss   | -0.566   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 443285   |\n","---------------------------------\n","Eval num_timesteps=3201600, episode_reward=157.05 +/- 0.38\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 157      |\n","| time/              |          |\n","|    total_timesteps | 3201600  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 2.77     |\n","|    ent_coef        | 0.00638  |\n","|    ent_coef_loss   | -1.08    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 443385   |\n","---------------------------------\n","Eval num_timesteps=3204000, episode_reward=87.18 +/- 89.87\n","Episode length: 416.40 +/- 102.39\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 416      |\n","|    mean_reward     | 87.2     |\n","| time/              |          |\n","|    total_timesteps | 3204000  |\n","| train/             |          |\n","|    actor_loss      | -10.7    |\n","|    critic_loss     | 6.16     |\n","|    ent_coef        | 0.0063   |\n","|    ent_coef_loss   | 0.172    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 443485   |\n","---------------------------------\n","Eval num_timesteps=3206400, episode_reward=158.70 +/- 0.39\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 159      |\n","| time/              |          |\n","|    total_timesteps | 3206400  |\n","| train/             |          |\n","|    actor_loss      | -12      |\n","|    critic_loss     | 0.805    |\n","|    ent_coef        | 0.00633  |\n","|    ent_coef_loss   | 3.12     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 443585   |\n","---------------------------------\n","Eval num_timesteps=3208800, episode_reward=163.42 +/- 0.84\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 163      |\n","| time/              |          |\n","|    total_timesteps | 3208800  |\n","| train/             |          |\n","|    actor_loss      | -10.4    |\n","|    critic_loss     | 2.73     |\n","|    ent_coef        | 0.00638  |\n","|    ent_coef_loss   | 0.813    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 443685   |\n","---------------------------------\n","Eval num_timesteps=3211200, episode_reward=36.40 +/- 115.59\n","Episode length: 299.60 +/- 163.63\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 300      |\n","|    mean_reward     | 36.4     |\n","| time/              |          |\n","|    total_timesteps | 3211200  |\n","| train/             |          |\n","|    actor_loss      | -10.5    |\n","|    critic_loss     | 2.26     |\n","|    ent_coef        | 0.00634  |\n","|    ent_coef_loss   | 1.26     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 443785   |\n","---------------------------------\n","Eval num_timesteps=3213600, episode_reward=175.39 +/- 0.22\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 175      |\n","| time/              |          |\n","|    total_timesteps | 3213600  |\n","| train/             |          |\n","|    actor_loss      | -11.6    |\n","|    critic_loss     | 1.06     |\n","|    ent_coef        | 0.00628  |\n","|    ent_coef_loss   | 3.89     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 443885   |\n","---------------------------------\n","Eval num_timesteps=3216000, episode_reward=156.10 +/- 2.69\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 156      |\n","| time/              |          |\n","|    total_timesteps | 3216000  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 1.07     |\n","|    ent_coef        | 0.00635  |\n","|    ent_coef_loss   | 0.456    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 443985   |\n","---------------------------------\n","Eval num_timesteps=3218400, episode_reward=165.51 +/- 2.32\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 166      |\n","| time/              |          |\n","|    total_timesteps | 3218400  |\n","| train/             |          |\n","|    actor_loss      | -11.3    |\n","|    critic_loss     | 0.831    |\n","|    ent_coef        | 0.00634  |\n","|    ent_coef_loss   | -0.623   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 444085   |\n","---------------------------------\n","Eval num_timesteps=3220800, episode_reward=165.67 +/- 5.55\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 166      |\n","| time/              |          |\n","|    total_timesteps | 3220800  |\n","| train/             |          |\n","|    actor_loss      | -9.99    |\n","|    critic_loss     | 1.39     |\n","|    ent_coef        | 0.00624  |\n","|    ent_coef_loss   | -0.365   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 444185   |\n","---------------------------------\n","Eval num_timesteps=3223200, episode_reward=144.53 +/- 4.77\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 145      |\n","| time/              |          |\n","|    total_timesteps | 3223200  |\n","| train/             |          |\n","|    actor_loss      | -11.1    |\n","|    critic_loss     | 10.5     |\n","|    ent_coef        | 0.00617  |\n","|    ent_coef_loss   | -1.97    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 444285   |\n","---------------------------------\n","Eval num_timesteps=3225600, episode_reward=151.32 +/- 1.82\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 151      |\n","| time/              |          |\n","|    total_timesteps | 3225600  |\n","| train/             |          |\n","|    actor_loss      | -11.6    |\n","|    critic_loss     | 1.63     |\n","|    ent_coef        | 0.00605  |\n","|    ent_coef_loss   | -0.883   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 444385   |\n","---------------------------------\n","Eval num_timesteps=3228000, episode_reward=154.70 +/- 2.73\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 155      |\n","| time/              |          |\n","|    total_timesteps | 3228000  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 1.43     |\n","|    ent_coef        | 0.00608  |\n","|    ent_coef_loss   | 0.814    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 444485   |\n","---------------------------------\n","Eval num_timesteps=3230400, episode_reward=156.40 +/- 2.88\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 156      |\n","| time/              |          |\n","|    total_timesteps | 3230400  |\n","| train/             |          |\n","|    actor_loss      | -10.7    |\n","|    critic_loss     | 2.02     |\n","|    ent_coef        | 0.00613  |\n","|    ent_coef_loss   | -0.0614  |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 444585   |\n","---------------------------------\n","Eval num_timesteps=3232800, episode_reward=165.10 +/- 3.32\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 165      |\n","| time/              |          |\n","|    total_timesteps | 3232800  |\n","| train/             |          |\n","|    actor_loss      | -11.4    |\n","|    critic_loss     | 1.97     |\n","|    ent_coef        | 0.00607  |\n","|    ent_coef_loss   | 2.32     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 444685   |\n","---------------------------------\n","Eval num_timesteps=3235200, episode_reward=166.76 +/- 2.81\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 167      |\n","| time/              |          |\n","|    total_timesteps | 3235200  |\n","| train/             |          |\n","|    actor_loss      | -12.2    |\n","|    critic_loss     | 1.28     |\n","|    ent_coef        | 0.00612  |\n","|    ent_coef_loss   | -2.82    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 444785   |\n","---------------------------------\n","Eval num_timesteps=3237600, episode_reward=108.76 +/- 49.59\n","Episode length: 491.60 +/- 6.86\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 492      |\n","|    mean_reward     | 109      |\n","| time/              |          |\n","|    total_timesteps | 3237600  |\n","| train/             |          |\n","|    actor_loss      | -10.6    |\n","|    critic_loss     | 1.91     |\n","|    ent_coef        | 0.00609  |\n","|    ent_coef_loss   | 2.3      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 444885   |\n","---------------------------------\n","Eval num_timesteps=3240000, episode_reward=171.28 +/- 2.93\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 171      |\n","| time/              |          |\n","|    total_timesteps | 3240000  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 2.97     |\n","|    ent_coef        | 0.00616  |\n","|    ent_coef_loss   | -0.193   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 444985   |\n","---------------------------------\n","Eval num_timesteps=3242400, episode_reward=174.86 +/- 0.74\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 175      |\n","| time/              |          |\n","|    total_timesteps | 3242400  |\n","| train/             |          |\n","|    actor_loss      | -11.3    |\n","|    critic_loss     | 3.29     |\n","|    ent_coef        | 0.00618  |\n","|    ent_coef_loss   | 0.886    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 445085   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2200     |\n","|    fps             | 277      |\n","|    time_elapsed    | 11696    |\n","|    total_timesteps | 3243696  |\n","| train/             |          |\n","|    actor_loss      | -12      |\n","|    critic_loss     | 1.45     |\n","|    ent_coef        | 0.0063   |\n","|    ent_coef_loss   | 0.559    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 445139   |\n","---------------------------------\n","Eval num_timesteps=3244800, episode_reward=152.48 +/- 0.45\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 152      |\n","| time/              |          |\n","|    total_timesteps | 3244800  |\n","| train/             |          |\n","|    actor_loss      | -11.3    |\n","|    critic_loss     | 1.91     |\n","|    ent_coef        | 0.00639  |\n","|    ent_coef_loss   | -0.299   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 445185   |\n","---------------------------------\n","Eval num_timesteps=3247200, episode_reward=-57.30 +/- 4.85\n","Episode length: 198.00 +/- 19.60\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 198      |\n","|    mean_reward     | -57.3    |\n","| time/              |          |\n","|    total_timesteps | 3247200  |\n","| train/             |          |\n","|    actor_loss      | -10.8    |\n","|    critic_loss     | 1.2      |\n","|    ent_coef        | 0.00646  |\n","|    ent_coef_loss   | 10.7     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 445285   |\n","---------------------------------\n","Eval num_timesteps=3249600, episode_reward=143.18 +/- 3.52\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 143      |\n","| time/              |          |\n","|    total_timesteps | 3249600  |\n","| train/             |          |\n","|    actor_loss      | -11.1    |\n","|    critic_loss     | 1.75     |\n","|    ent_coef        | 0.00677  |\n","|    ent_coef_loss   | -2.11    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 445385   |\n","---------------------------------\n","Eval num_timesteps=3252000, episode_reward=142.54 +/- 2.19\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 143      |\n","| time/              |          |\n","|    total_timesteps | 3252000  |\n","| train/             |          |\n","|    actor_loss      | -10.3    |\n","|    critic_loss     | 1.58     |\n","|    ent_coef        | 0.00649  |\n","|    ent_coef_loss   | -2.33    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 445485   |\n","---------------------------------\n","Eval num_timesteps=3254400, episode_reward=69.56 +/- 81.13\n","Episode length: 401.00 +/- 80.83\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 401      |\n","|    mean_reward     | 69.6     |\n","| time/              |          |\n","|    total_timesteps | 3254400  |\n","| train/             |          |\n","|    actor_loss      | -11.2    |\n","|    critic_loss     | 1.68     |\n","|    ent_coef        | 0.00638  |\n","|    ent_coef_loss   | -0.889   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 445585   |\n","---------------------------------\n","Eval num_timesteps=3256800, episode_reward=156.43 +/- 0.59\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 156      |\n","| time/              |          |\n","|    total_timesteps | 3256800  |\n","| train/             |          |\n","|    actor_loss      | -11.9    |\n","|    critic_loss     | 1        |\n","|    ent_coef        | 0.00635  |\n","|    ent_coef_loss   | 2.29     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 445685   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2210     |\n","|    fps             | 277      |\n","|    time_elapsed    | 11751    |\n","|    total_timesteps | 3258768  |\n","| train/             |          |\n","|    actor_loss      | -11.4    |\n","|    critic_loss     | 2.17     |\n","|    ent_coef        | 0.00636  |\n","|    ent_coef_loss   | 1.96     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 445767   |\n","---------------------------------\n","Eval num_timesteps=3259200, episode_reward=173.50 +/- 2.35\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 173      |\n","| time/              |          |\n","|    total_timesteps | 3259200  |\n","| train/             |          |\n","|    actor_loss      | -11.5    |\n","|    critic_loss     | 1.19     |\n","|    ent_coef        | 0.0064   |\n","|    ent_coef_loss   | 7.61     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 445785   |\n","---------------------------------\n","Eval num_timesteps=3261600, episode_reward=-2.63 +/- 12.83\n","Episode length: 338.60 +/- 49.48\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 339      |\n","|    mean_reward     | -2.63    |\n","| time/              |          |\n","|    total_timesteps | 3261600  |\n","| train/             |          |\n","|    actor_loss      | -11.5    |\n","|    critic_loss     | 0.484    |\n","|    ent_coef        | 0.00647  |\n","|    ent_coef_loss   | -1.24    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 445885   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2220     |\n","|    fps             | 277      |\n","|    time_elapsed    | 11765    |\n","|    total_timesteps | 3262920  |\n","| train/             |          |\n","|    actor_loss      | -9.95    |\n","|    critic_loss     | 1.66     |\n","|    ent_coef        | 0.00643  |\n","|    ent_coef_loss   | 1.66     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 445940   |\n","---------------------------------\n","Eval num_timesteps=3264000, episode_reward=155.57 +/- 0.43\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 156      |\n","| time/              |          |\n","|    total_timesteps | 3264000  |\n","| train/             |          |\n","|    actor_loss      | -11.5    |\n","|    critic_loss     | 2.45     |\n","|    ent_coef        | 0.00644  |\n","|    ent_coef_loss   | -1.08    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 445985   |\n","---------------------------------\n","Eval num_timesteps=3266400, episode_reward=155.16 +/- 3.95\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 155      |\n","| time/              |          |\n","|    total_timesteps | 3266400  |\n","| train/             |          |\n","|    actor_loss      | -11.3    |\n","|    critic_loss     | 1.49     |\n","|    ent_coef        | 0.00642  |\n","|    ent_coef_loss   | -0.851   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 446085   |\n","---------------------------------\n","Eval num_timesteps=3268800, episode_reward=88.91 +/- 87.46\n","Episode length: 426.80 +/- 89.65\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 427      |\n","|    mean_reward     | 88.9     |\n","| time/              |          |\n","|    total_timesteps | 3268800  |\n","| train/             |          |\n","|    actor_loss      | -10.8    |\n","|    critic_loss     | 1.21     |\n","|    ent_coef        | 0.0065   |\n","|    ent_coef_loss   | 2.99     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 446185   |\n","---------------------------------\n","Eval num_timesteps=3271200, episode_reward=160.06 +/- 2.76\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 160      |\n","| time/              |          |\n","|    total_timesteps | 3271200  |\n","| train/             |          |\n","|    actor_loss      | -11.4    |\n","|    critic_loss     | 1.17     |\n","|    ent_coef        | 0.0066   |\n","|    ent_coef_loss   | 2.75     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 446285   |\n","---------------------------------\n","Eval num_timesteps=3273600, episode_reward=155.64 +/- 1.52\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 156      |\n","| time/              |          |\n","|    total_timesteps | 3273600  |\n","| train/             |          |\n","|    actor_loss      | -11.2    |\n","|    critic_loss     | 1.63     |\n","|    ent_coef        | 0.00661  |\n","|    ent_coef_loss   | 1.79     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 446385   |\n","---------------------------------\n","Eval num_timesteps=3276000, episode_reward=145.27 +/- 3.30\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 145      |\n","| time/              |          |\n","|    total_timesteps | 3276000  |\n","| train/             |          |\n","|    actor_loss      | -11.9    |\n","|    critic_loss     | 1.16     |\n","|    ent_coef        | 0.0067   |\n","|    ent_coef_loss   | -1.21    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 446485   |\n","---------------------------------\n","Eval num_timesteps=3278400, episode_reward=151.66 +/- 8.28\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 152      |\n","| time/              |          |\n","|    total_timesteps | 3278400  |\n","| train/             |          |\n","|    actor_loss      | -10.7    |\n","|    critic_loss     | 3.91     |\n","|    ent_coef        | 0.00663  |\n","|    ent_coef_loss   | 1.42     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 446585   |\n","---------------------------------\n","Eval num_timesteps=3280800, episode_reward=155.37 +/- 0.18\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 155      |\n","| time/              |          |\n","|    total_timesteps | 3280800  |\n","| train/             |          |\n","|    actor_loss      | -10.9    |\n","|    critic_loss     | 1.92     |\n","|    ent_coef        | 0.00658  |\n","|    ent_coef_loss   | 1.44     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 446685   |\n","---------------------------------\n","Eval num_timesteps=3283200, episode_reward=116.34 +/- 55.54\n","Episode length: 485.60 +/- 17.64\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 486      |\n","|    mean_reward     | 116      |\n","| time/              |          |\n","|    total_timesteps | 3283200  |\n","| train/             |          |\n","|    actor_loss      | -11.4    |\n","|    critic_loss     | 1.12     |\n","|    ent_coef        | 0.00655  |\n","|    ent_coef_loss   | 1.58     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 446785   |\n","---------------------------------\n","Eval num_timesteps=3285600, episode_reward=31.95 +/- 125.48\n","Episode length: 285.80 +/- 174.89\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 286      |\n","|    mean_reward     | 31.9     |\n","| time/              |          |\n","|    total_timesteps | 3285600  |\n","| train/             |          |\n","|    actor_loss      | -10.1    |\n","|    critic_loss     | 19.7     |\n","|    ent_coef        | 0.0065   |\n","|    ent_coef_loss   | -0.695   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 446885   |\n","---------------------------------\n","Eval num_timesteps=3288000, episode_reward=144.36 +/- 7.50\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 144      |\n","| time/              |          |\n","|    total_timesteps | 3288000  |\n","| train/             |          |\n","|    actor_loss      | -11.7    |\n","|    critic_loss     | 0.726    |\n","|    ent_coef        | 0.00657  |\n","|    ent_coef_loss   | -0.948   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 446985   |\n","---------------------------------\n","Eval num_timesteps=3290400, episode_reward=154.56 +/- 3.89\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 155      |\n","| time/              |          |\n","|    total_timesteps | 3290400  |\n","| train/             |          |\n","|    actor_loss      | -12.1    |\n","|    critic_loss     | 0.575    |\n","|    ent_coef        | 0.00657  |\n","|    ent_coef_loss   | -3.31    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 447085   |\n","---------------------------------\n","Eval num_timesteps=3292800, episode_reward=100.48 +/- 67.35\n","Episode length: 464.40 +/- 43.60\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 464      |\n","|    mean_reward     | 100      |\n","| time/              |          |\n","|    total_timesteps | 3292800  |\n","| train/             |          |\n","|    actor_loss      | -11.4    |\n","|    critic_loss     | 1.8      |\n","|    ent_coef        | 0.00654  |\n","|    ent_coef_loss   | -0.126   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 447185   |\n","---------------------------------\n","Eval num_timesteps=3295200, episode_reward=167.77 +/- 5.07\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 168      |\n","| time/              |          |\n","|    total_timesteps | 3295200  |\n","| train/             |          |\n","|    actor_loss      | -11.3    |\n","|    critic_loss     | 2.32     |\n","|    ent_coef        | 0.00649  |\n","|    ent_coef_loss   | -2.15    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 447285   |\n","---------------------------------\n","Eval num_timesteps=3297600, episode_reward=163.20 +/- 4.38\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 163      |\n","| time/              |          |\n","|    total_timesteps | 3297600  |\n","| train/             |          |\n","|    actor_loss      | -10.5    |\n","|    critic_loss     | 3.56     |\n","|    ent_coef        | 0.00642  |\n","|    ent_coef_loss   | 3.56     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 447385   |\n","---------------------------------\n","Eval num_timesteps=3300000, episode_reward=162.89 +/- 0.35\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 163      |\n","| time/              |          |\n","|    total_timesteps | 3300000  |\n","| train/             |          |\n","|    actor_loss      | -12      |\n","|    critic_loss     | 1.22     |\n","|    ent_coef        | 0.00657  |\n","|    ent_coef_loss   | 1.4      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 447485   |\n","---------------------------------\n","Eval num_timesteps=3302400, episode_reward=157.47 +/- 0.22\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 157      |\n","| time/              |          |\n","|    total_timesteps | 3302400  |\n","| train/             |          |\n","|    actor_loss      | -11.8    |\n","|    critic_loss     | 58.9     |\n","|    ent_coef        | 0.00671  |\n","|    ent_coef_loss   | 2.07     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 447585   |\n","---------------------------------\n","Eval num_timesteps=3304800, episode_reward=163.35 +/- 3.11\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 163      |\n","| time/              |          |\n","|    total_timesteps | 3304800  |\n","| train/             |          |\n","|    actor_loss      | -10.9    |\n","|    critic_loss     | 53.5     |\n","|    ent_coef        | 0.00676  |\n","|    ent_coef_loss   | -0.668   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 447685   |\n","---------------------------------\n","Eval num_timesteps=3307200, episode_reward=156.75 +/- 3.46\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 157      |\n","| time/              |          |\n","|    total_timesteps | 3307200  |\n","| train/             |          |\n","|    actor_loss      | -11.8    |\n","|    critic_loss     | 0.5      |\n","|    ent_coef        | 0.00668  |\n","|    ent_coef_loss   | -0.37    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 447785   |\n","---------------------------------\n","Eval num_timesteps=3309600, episode_reward=155.75 +/- 3.23\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 156      |\n","| time/              |          |\n","|    total_timesteps | 3309600  |\n","| train/             |          |\n","|    actor_loss      | -11.3    |\n","|    critic_loss     | 1.21     |\n","|    ent_coef        | 0.00653  |\n","|    ent_coef_loss   | -3.05    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 447885   |\n","---------------------------------\n","Eval num_timesteps=3312000, episode_reward=161.01 +/- 2.79\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 161      |\n","| time/              |          |\n","|    total_timesteps | 3312000  |\n","| train/             |          |\n","|    actor_loss      | -11.7    |\n","|    critic_loss     | 2.43     |\n","|    ent_coef        | 0.00637  |\n","|    ent_coef_loss   | -3.1     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 447985   |\n","---------------------------------\n","Eval num_timesteps=3314400, episode_reward=148.10 +/- 3.30\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 148      |\n","| time/              |          |\n","|    total_timesteps | 3314400  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 2.58     |\n","|    ent_coef        | 0.00628  |\n","|    ent_coef_loss   | 4.7      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 448085   |\n","---------------------------------\n","Eval num_timesteps=3316800, episode_reward=88.35 +/- 69.18\n","Episode length: 454.40 +/- 37.23\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 454      |\n","|    mean_reward     | 88.3     |\n","| time/              |          |\n","|    total_timesteps | 3316800  |\n","| train/             |          |\n","|    actor_loss      | -11.8    |\n","|    critic_loss     | 31.1     |\n","|    ent_coef        | 0.0063   |\n","|    ent_coef_loss   | -1.71    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 448185   |\n","---------------------------------\n","Eval num_timesteps=3319200, episode_reward=166.99 +/- 2.47\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 167      |\n","| time/              |          |\n","|    total_timesteps | 3319200  |\n","| train/             |          |\n","|    actor_loss      | -10.8    |\n","|    critic_loss     | 1.24     |\n","|    ent_coef        | 0.00621  |\n","|    ent_coef_loss   | 0.281    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 448285   |\n","---------------------------------\n","Eval num_timesteps=3321600, episode_reward=160.22 +/- 6.86\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 160      |\n","| time/              |          |\n","|    total_timesteps | 3321600  |\n","| train/             |          |\n","|    actor_loss      | -12.1    |\n","|    critic_loss     | 1.31     |\n","|    ent_coef        | 0.00602  |\n","|    ent_coef_loss   | -3.56    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 448385   |\n","---------------------------------\n","Eval num_timesteps=3324000, episode_reward=-28.17 +/- 18.15\n","Episode length: 260.40 +/- 46.05\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 260      |\n","|    mean_reward     | -28.2    |\n","| time/              |          |\n","|    total_timesteps | 3324000  |\n","| train/             |          |\n","|    actor_loss      | -10.9    |\n","|    critic_loss     | 1.3      |\n","|    ent_coef        | 0.00591  |\n","|    ent_coef_loss   | 0.949    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 448485   |\n","---------------------------------\n","Eval num_timesteps=3326400, episode_reward=43.16 +/- 99.30\n","Episode length: 328.40 +/- 140.11\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 328      |\n","|    mean_reward     | 43.2     |\n","| time/              |          |\n","|    total_timesteps | 3326400  |\n","| train/             |          |\n","|    actor_loss      | -12.3    |\n","|    critic_loss     | 1.22     |\n","|    ent_coef        | 0.00593  |\n","|    ent_coef_loss   | -1.21    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 448585   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2230     |\n","|    fps             | 276      |\n","|    time_elapsed    | 12017    |\n","|    total_timesteps | 3328368  |\n","| train/             |          |\n","|    actor_loss      | -11.6    |\n","|    critic_loss     | 1.4      |\n","|    ent_coef        | 0.00596  |\n","|    ent_coef_loss   | 0.744    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 448667   |\n","---------------------------------\n","Eval num_timesteps=3328800, episode_reward=164.05 +/- 1.35\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 164      |\n","| time/              |          |\n","|    total_timesteps | 3328800  |\n","| train/             |          |\n","|    actor_loss      | -10.6    |\n","|    critic_loss     | 1.95     |\n","|    ent_coef        | 0.00599  |\n","|    ent_coef_loss   | 0.452    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 448685   |\n","---------------------------------\n","Eval num_timesteps=3331200, episode_reward=13.71 +/- 27.03\n","Episode length: 409.00 +/- 97.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 409      |\n","|    mean_reward     | 13.7     |\n","| time/              |          |\n","|    total_timesteps | 3331200  |\n","| train/             |          |\n","|    actor_loss      | -10.4    |\n","|    critic_loss     | 1.19     |\n","|    ent_coef        | 0.00613  |\n","|    ent_coef_loss   | -0.566   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 448785   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2240     |\n","|    fps             | 276      |\n","|    time_elapsed    | 12035    |\n","|    total_timesteps | 3331464  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 1.22     |\n","|    ent_coef        | 0.00614  |\n","|    ent_coef_loss   | 1.61     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 448796   |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    episodes        | 2250    |\n","|    fps             | 276     |\n","|    time_elapsed    | 12035   |\n","|    total_timesteps | 3331464 |\n","--------------------------------\n","Eval num_timesteps=3333600, episode_reward=151.48 +/- 1.05\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 151      |\n","| time/              |          |\n","|    total_timesteps | 3333600  |\n","| train/             |          |\n","|    actor_loss      | -11.5    |\n","|    critic_loss     | 1.83     |\n","|    ent_coef        | 0.00618  |\n","|    ent_coef_loss   | 2.1      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 448885   |\n","---------------------------------\n","Eval num_timesteps=3336000, episode_reward=-23.67 +/- 9.16\n","Episode length: 285.80 +/- 8.33\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 286      |\n","|    mean_reward     | -23.7    |\n","| time/              |          |\n","|    total_timesteps | 3336000  |\n","| train/             |          |\n","|    actor_loss      | -9.58    |\n","|    critic_loss     | 1.91     |\n","|    ent_coef        | 0.00617  |\n","|    ent_coef_loss   | 1.34     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 448985   |\n","---------------------------------\n","Eval num_timesteps=3338400, episode_reward=79.72 +/- 81.56\n","Episode length: 412.80 +/- 106.80\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 413      |\n","|    mean_reward     | 79.7     |\n","| time/              |          |\n","|    total_timesteps | 3338400  |\n","| train/             |          |\n","|    actor_loss      | -11.4    |\n","|    critic_loss     | 2.11     |\n","|    ent_coef        | 0.00628  |\n","|    ent_coef_loss   | 2.58     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 449085   |\n","---------------------------------\n","Eval num_timesteps=3340800, episode_reward=72.44 +/- 102.98\n","Episode length: 380.40 +/- 146.48\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 380      |\n","|    mean_reward     | 72.4     |\n","| time/              |          |\n","|    total_timesteps | 3340800  |\n","| train/             |          |\n","|    actor_loss      | -11.9    |\n","|    critic_loss     | 0.647    |\n","|    ent_coef        | 0.00644  |\n","|    ent_coef_loss   | 1.51     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 449185   |\n","---------------------------------\n","Eval num_timesteps=3343200, episode_reward=151.92 +/- 1.15\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 152      |\n","| time/              |          |\n","|    total_timesteps | 3343200  |\n","| train/             |          |\n","|    actor_loss      | -10.8    |\n","|    critic_loss     | 0.767    |\n","|    ent_coef        | 0.00645  |\n","|    ent_coef_loss   | -1.5     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 449285   |\n","---------------------------------\n","Eval num_timesteps=3345600, episode_reward=158.15 +/- 5.56\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 158      |\n","| time/              |          |\n","|    total_timesteps | 3345600  |\n","| train/             |          |\n","|    actor_loss      | -11.6    |\n","|    critic_loss     | 0.855    |\n","|    ent_coef        | 0.00653  |\n","|    ent_coef_loss   | 0.559    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 449385   |\n","---------------------------------\n","Eval num_timesteps=3348000, episode_reward=106.76 +/- 59.12\n","Episode length: 489.20 +/- 13.23\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 489      |\n","|    mean_reward     | 107      |\n","| time/              |          |\n","|    total_timesteps | 3348000  |\n","| train/             |          |\n","|    actor_loss      | -12.1    |\n","|    critic_loss     | 0.977    |\n","|    ent_coef        | 0.00662  |\n","|    ent_coef_loss   | 0.71     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 449485   |\n","---------------------------------\n","Eval num_timesteps=3350400, episode_reward=101.91 +/- 73.51\n","Episode length: 444.00 +/- 68.59\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 444      |\n","|    mean_reward     | 102      |\n","| time/              |          |\n","|    total_timesteps | 3350400  |\n","| train/             |          |\n","|    actor_loss      | -10.6    |\n","|    critic_loss     | 1.37     |\n","|    ent_coef        | 0.00653  |\n","|    ent_coef_loss   | 0.793    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 449585   |\n","---------------------------------\n","Eval num_timesteps=3352800, episode_reward=149.40 +/- 5.52\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 149      |\n","| time/              |          |\n","|    total_timesteps | 3352800  |\n","| train/             |          |\n","|    actor_loss      | -11.8    |\n","|    critic_loss     | 0.564    |\n","|    ent_coef        | 0.00656  |\n","|    ent_coef_loss   | -0.836   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 449685   |\n","---------------------------------\n","Eval num_timesteps=3355200, episode_reward=149.38 +/- 0.57\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 149      |\n","| time/              |          |\n","|    total_timesteps | 3355200  |\n","| train/             |          |\n","|    actor_loss      | -11.9    |\n","|    critic_loss     | 4.15     |\n","|    ent_coef        | 0.00659  |\n","|    ent_coef_loss   | -3.92    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 449785   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2260     |\n","|    fps             | 276      |\n","|    time_elapsed    | 12127    |\n","|    total_timesteps | 3357312  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 2.02     |\n","|    ent_coef        | 0.00653  |\n","|    ent_coef_loss   | -0.674   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 449873   |\n","---------------------------------\n","Eval num_timesteps=3357600, episode_reward=24.30 +/- 102.94\n","Episode length: 307.40 +/- 157.26\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 307      |\n","|    mean_reward     | 24.3     |\n","| time/              |          |\n","|    total_timesteps | 3357600  |\n","| train/             |          |\n","|    actor_loss      | -11.6    |\n","|    critic_loss     | 1.15     |\n","|    ent_coef        | 0.00652  |\n","|    ent_coef_loss   | -1.8     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 449885   |\n","---------------------------------\n","Eval num_timesteps=3360000, episode_reward=-5.79 +/- 20.20\n","Episode length: 321.80 +/- 45.07\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 322      |\n","|    mean_reward     | -5.79    |\n","| time/              |          |\n","|    total_timesteps | 3360000  |\n","| train/             |          |\n","|    actor_loss      | -11.2    |\n","|    critic_loss     | 2.14     |\n","|    ent_coef        | 0.00642  |\n","|    ent_coef_loss   | -3.61    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 449985   |\n","---------------------------------\n","Eval num_timesteps=3362400, episode_reward=35.47 +/- 30.38\n","Episode length: 418.80 +/- 79.85\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 419      |\n","|    mean_reward     | 35.5     |\n","| time/              |          |\n","|    total_timesteps | 3362400  |\n","| train/             |          |\n","|    actor_loss      | -11.4    |\n","|    critic_loss     | 1.45     |\n","|    ent_coef        | 0.00624  |\n","|    ent_coef_loss   | -1.78    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 450085   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2270     |\n","|    fps             | 276      |\n","|    time_elapsed    | 12154    |\n","|    total_timesteps | 3362784  |\n","| train/             |          |\n","|    actor_loss      | -10.6    |\n","|    critic_loss     | 1.44     |\n","|    ent_coef        | 0.00622  |\n","|    ent_coef_loss   | -0.0916  |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 450101   |\n","---------------------------------\n","Eval num_timesteps=3364800, episode_reward=160.97 +/- 2.19\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 161      |\n","| time/              |          |\n","|    total_timesteps | 3364800  |\n","| train/             |          |\n","|    actor_loss      | -11.7    |\n","|    critic_loss     | 1.59     |\n","|    ent_coef        | 0.00619  |\n","|    ent_coef_loss   | -1.08    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 450185   |\n","---------------------------------\n","Eval num_timesteps=3367200, episode_reward=1.25 +/- 37.52\n","Episode length: 370.60 +/- 80.34\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 371      |\n","|    mean_reward     | 1.25     |\n","| time/              |          |\n","|    total_timesteps | 3367200  |\n","| train/             |          |\n","|    actor_loss      | -11.4    |\n","|    critic_loss     | 0.849    |\n","|    ent_coef        | 0.00637  |\n","|    ent_coef_loss   | 3.2      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 450285   |\n","---------------------------------\n","Eval num_timesteps=3369600, episode_reward=60.07 +/- 101.33\n","Episode length: 383.20 +/- 143.05\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 383      |\n","|    mean_reward     | 60.1     |\n","| time/              |          |\n","|    total_timesteps | 3369600  |\n","| train/             |          |\n","|    actor_loss      | -12      |\n","|    critic_loss     | 1.84     |\n","|    ent_coef        | 0.00648  |\n","|    ent_coef_loss   | -3.62    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 450385   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2280     |\n","|    fps             | 276      |\n","|    time_elapsed    | 12182    |\n","|    total_timesteps | 3369720  |\n","| train/             |          |\n","|    actor_loss      | -11.1    |\n","|    critic_loss     | 1.05     |\n","|    ent_coef        | 0.00647  |\n","|    ent_coef_loss   | -0.0645  |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 450390   |\n","---------------------------------\n","Eval num_timesteps=3372000, episode_reward=160.21 +/- 0.50\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 160      |\n","| time/              |          |\n","|    total_timesteps | 3372000  |\n","| train/             |          |\n","|    actor_loss      | -11.7    |\n","|    critic_loss     | 3.22     |\n","|    ent_coef        | 0.00637  |\n","|    ent_coef_loss   | -2.2     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 450485   |\n","---------------------------------\n","Eval num_timesteps=3374400, episode_reward=157.65 +/- 3.65\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 158      |\n","| time/              |          |\n","|    total_timesteps | 3374400  |\n","| train/             |          |\n","|    actor_loss      | -11.5    |\n","|    critic_loss     | 51.5     |\n","|    ent_coef        | 0.00625  |\n","|    ent_coef_loss   | 1.81     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 450585   |\n","---------------------------------\n","Eval num_timesteps=3376800, episode_reward=62.73 +/- 95.78\n","Episode length: 371.00 +/- 105.33\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 371      |\n","|    mean_reward     | 62.7     |\n","| time/              |          |\n","|    total_timesteps | 3376800  |\n","| train/             |          |\n","|    actor_loss      | -12.4    |\n","|    critic_loss     | 0.477    |\n","|    ent_coef        | 0.0063   |\n","|    ent_coef_loss   | -1.94    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 450685   |\n","---------------------------------\n","Eval num_timesteps=3379200, episode_reward=-30.93 +/- 6.66\n","Episode length: 282.40 +/- 15.19\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 282      |\n","|    mean_reward     | -30.9    |\n","| time/              |          |\n","|    total_timesteps | 3379200  |\n","| train/             |          |\n","|    actor_loss      | -10.8    |\n","|    critic_loss     | 3.58     |\n","|    ent_coef        | 0.00622  |\n","|    ent_coef_loss   | -1.18    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 450785   |\n","---------------------------------\n","Eval num_timesteps=3381600, episode_reward=165.91 +/- 0.71\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 166      |\n","| time/              |          |\n","|    total_timesteps | 3381600  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 1.31     |\n","|    ent_coef        | 0.00615  |\n","|    ent_coef_loss   | -3.8     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 450885   |\n","---------------------------------\n","Eval num_timesteps=3384000, episode_reward=169.44 +/- 1.26\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 169      |\n","| time/              |          |\n","|    total_timesteps | 3384000  |\n","| train/             |          |\n","|    actor_loss      | -11.3    |\n","|    critic_loss     | 1.5      |\n","|    ent_coef        | 0.00613  |\n","|    ent_coef_loss   | -2.65    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 450985   |\n","---------------------------------\n","Eval num_timesteps=3386400, episode_reward=104.38 +/- 75.14\n","Episode length: 455.20 +/- 54.87\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 455      |\n","|    mean_reward     | 104      |\n","| time/              |          |\n","|    total_timesteps | 3386400  |\n","| train/             |          |\n","|    actor_loss      | -11.7    |\n","|    critic_loss     | 0.93     |\n","|    ent_coef        | 0.00614  |\n","|    ent_coef_loss   | 0.554    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 451085   |\n","---------------------------------\n","Eval num_timesteps=3388800, episode_reward=99.94 +/- 58.11\n","Episode length: 482.00 +/- 22.05\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 482      |\n","|    mean_reward     | 99.9     |\n","| time/              |          |\n","|    total_timesteps | 3388800  |\n","| train/             |          |\n","|    actor_loss      | -11.8    |\n","|    critic_loss     | 0.826    |\n","|    ent_coef        | 0.00619  |\n","|    ent_coef_loss   | -0.865   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 451185   |\n","---------------------------------\n","Eval num_timesteps=3391200, episode_reward=62.81 +/- 102.78\n","Episode length: 377.20 +/- 150.40\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 377      |\n","|    mean_reward     | 62.8     |\n","| time/              |          |\n","|    total_timesteps | 3391200  |\n","| train/             |          |\n","|    actor_loss      | -11.8    |\n","|    critic_loss     | 0.981    |\n","|    ent_coef        | 0.0063   |\n","|    ent_coef_loss   | -1.07    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 451285   |\n","---------------------------------\n","Eval num_timesteps=3393600, episode_reward=169.56 +/- 2.25\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 170      |\n","| time/              |          |\n","|    total_timesteps | 3393600  |\n","| train/             |          |\n","|    actor_loss      | -11.5    |\n","|    critic_loss     | 1.39     |\n","|    ent_coef        | 0.00641  |\n","|    ent_coef_loss   | 0.838    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 451385   |\n","---------------------------------\n","Eval num_timesteps=3396000, episode_reward=170.81 +/- 1.73\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 171      |\n","| time/              |          |\n","|    total_timesteps | 3396000  |\n","| train/             |          |\n","|    actor_loss      | -9.92    |\n","|    critic_loss     | 1.8      |\n","|    ent_coef        | 0.00654  |\n","|    ent_coef_loss   | 1.23     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 451485   |\n","---------------------------------\n","Eval num_timesteps=3398400, episode_reward=95.23 +/- 58.05\n","Episode length: 495.80 +/- 3.43\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 496      |\n","|    mean_reward     | 95.2     |\n","| time/              |          |\n","|    total_timesteps | 3398400  |\n","| train/             |          |\n","|    actor_loss      | -11.1    |\n","|    critic_loss     | 1.28     |\n","|    ent_coef        | 0.00657  |\n","|    ent_coef_loss   | -3.37    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 451585   |\n","---------------------------------\n","Eval num_timesteps=3400800, episode_reward=87.55 +/- 96.30\n","Episode length: 410.40 +/- 109.74\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 410      |\n","|    mean_reward     | 87.5     |\n","| time/              |          |\n","|    total_timesteps | 3400800  |\n","| train/             |          |\n","|    actor_loss      | -10.8    |\n","|    critic_loss     | 1.05     |\n","|    ent_coef        | 0.00639  |\n","|    ent_coef_loss   | 1.31     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 451685   |\n","---------------------------------\n","Eval num_timesteps=3403200, episode_reward=108.73 +/- 66.48\n","Episode length: 468.40 +/- 38.70\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 468      |\n","|    mean_reward     | 109      |\n","| time/              |          |\n","|    total_timesteps | 3403200  |\n","| train/             |          |\n","|    actor_loss      | -11.8    |\n","|    critic_loss     | 2.02     |\n","|    ent_coef        | 0.00642  |\n","|    ent_coef_loss   | 0.153    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 451785   |\n","---------------------------------\n","Eval num_timesteps=3405600, episode_reward=100.48 +/- 84.21\n","Episode length: 439.20 +/- 74.46\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 439      |\n","|    mean_reward     | 100      |\n","| time/              |          |\n","|    total_timesteps | 3405600  |\n","| train/             |          |\n","|    actor_loss      | -10.5    |\n","|    critic_loss     | 2.55     |\n","|    ent_coef        | 0.0064   |\n","|    ent_coef_loss   | -0.473   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 451885   |\n","---------------------------------\n","Eval num_timesteps=3408000, episode_reward=155.62 +/- 1.50\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 156      |\n","| time/              |          |\n","|    total_timesteps | 3408000  |\n","| train/             |          |\n","|    actor_loss      | -10.7    |\n","|    critic_loss     | 1.16     |\n","|    ent_coef        | 0.00649  |\n","|    ent_coef_loss   | 2.15     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 451985   |\n","---------------------------------\n","Eval num_timesteps=3410400, episode_reward=77.59 +/- 89.30\n","Episode length: 421.60 +/- 96.02\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 422      |\n","|    mean_reward     | 77.6     |\n","| time/              |          |\n","|    total_timesteps | 3410400  |\n","| train/             |          |\n","|    actor_loss      | -9.42    |\n","|    critic_loss     | 3.14     |\n","|    ent_coef        | 0.00657  |\n","|    ent_coef_loss   | 1.11     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 452085   |\n","---------------------------------\n","Eval num_timesteps=3412800, episode_reward=75.91 +/- 105.05\n","Episode length: 373.20 +/- 155.30\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 373      |\n","|    mean_reward     | 75.9     |\n","| time/              |          |\n","|    total_timesteps | 3412800  |\n","| train/             |          |\n","|    actor_loss      | -11.3    |\n","|    critic_loss     | 1.05     |\n","|    ent_coef        | 0.00652  |\n","|    ent_coef_loss   | -2.45    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 452185   |\n","---------------------------------\n","Eval num_timesteps=3415200, episode_reward=7.08 +/- 25.71\n","Episode length: 343.60 +/- 72.99\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 344      |\n","|    mean_reward     | 7.08     |\n","| time/              |          |\n","|    total_timesteps | 3415200  |\n","| train/             |          |\n","|    actor_loss      | -10.9    |\n","|    critic_loss     | 4.09     |\n","|    ent_coef        | 0.00647  |\n","|    ent_coef_loss   | -0.673   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 452285   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2290     |\n","|    fps             | 276      |\n","|    time_elapsed    | 12358    |\n","|    total_timesteps | 3416808  |\n","| train/             |          |\n","|    actor_loss      | -11.5    |\n","|    critic_loss     | 0.906    |\n","|    ent_coef        | 0.00645  |\n","|    ent_coef_loss   | 0.145    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 452352   |\n","---------------------------------\n","Eval num_timesteps=3417600, episode_reward=107.20 +/- 61.45\n","Episode length: 472.00 +/- 34.29\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 472      |\n","|    mean_reward     | 107      |\n","| time/              |          |\n","|    total_timesteps | 3417600  |\n","| train/             |          |\n","|    actor_loss      | -11.2    |\n","|    critic_loss     | 2.59     |\n","|    ent_coef        | 0.00642  |\n","|    ent_coef_loss   | 0.714    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 452385   |\n","---------------------------------\n","Eval num_timesteps=3420000, episode_reward=162.86 +/- 4.06\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 163      |\n","| time/              |          |\n","|    total_timesteps | 3420000  |\n","| train/             |          |\n","|    actor_loss      | -12.1    |\n","|    critic_loss     | 1.85     |\n","|    ent_coef        | 0.00642  |\n","|    ent_coef_loss   | 0.839    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 452485   |\n","---------------------------------\n","Eval num_timesteps=3422400, episode_reward=116.95 +/- 52.83\n","Episode length: 499.20 +/- 0.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 499      |\n","|    mean_reward     | 117      |\n","| time/              |          |\n","|    total_timesteps | 3422400  |\n","| train/             |          |\n","|    actor_loss      | -11.9    |\n","|    critic_loss     | 0.772    |\n","|    ent_coef        | 0.00645  |\n","|    ent_coef_loss   | -0.253   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 452585   |\n","---------------------------------\n","Eval num_timesteps=3424800, episode_reward=160.82 +/- 1.42\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 161      |\n","| time/              |          |\n","|    total_timesteps | 3424800  |\n","| train/             |          |\n","|    actor_loss      | -10.8    |\n","|    critic_loss     | 1.35     |\n","|    ent_coef        | 0.00662  |\n","|    ent_coef_loss   | 1.56     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 452685   |\n","---------------------------------\n","Eval num_timesteps=3427200, episode_reward=-58.57 +/- 29.59\n","Episode length: 216.20 +/- 74.95\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 216      |\n","|    mean_reward     | -58.6    |\n","| time/              |          |\n","|    total_timesteps | 3427200  |\n","| train/             |          |\n","|    actor_loss      | -11.4    |\n","|    critic_loss     | 1.1      |\n","|    ent_coef        | 0.0067   |\n","|    ent_coef_loss   | 2.88     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 452785   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2300     |\n","|    fps             | 276      |\n","|    time_elapsed    | 12402    |\n","|    total_timesteps | 3428280  |\n","| train/             |          |\n","|    actor_loss      | -11      |\n","|    critic_loss     | 2.35     |\n","|    ent_coef        | 0.00681  |\n","|    ent_coef_loss   | 4.66     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 452830   |\n","---------------------------------\n","Eval num_timesteps=3429600, episode_reward=79.56 +/- 83.81\n","Episode length: 402.00 +/- 120.02\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 402      |\n","|    mean_reward     | 79.6     |\n","| time/              |          |\n","|    total_timesteps | 3429600  |\n","| train/             |          |\n","|    actor_loss      | -11.5    |\n","|    critic_loss     | 1.73     |\n","|    ent_coef        | 0.0068   |\n","|    ent_coef_loss   | -0.205   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 452885   |\n","---------------------------------\n","Eval num_timesteps=3432000, episode_reward=74.28 +/- 85.36\n","Episode length: 403.20 +/- 118.56\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 403      |\n","|    mean_reward     | 74.3     |\n","| time/              |          |\n","|    total_timesteps | 3432000  |\n","| train/             |          |\n","|    actor_loss      | -11.4    |\n","|    critic_loss     | 1.77     |\n","|    ent_coef        | 0.00683  |\n","|    ent_coef_loss   | -1.09    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 452985   |\n","---------------------------------\n","Eval num_timesteps=3434400, episode_reward=153.58 +/- 1.14\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 154      |\n","| time/              |          |\n","|    total_timesteps | 3434400  |\n","| train/             |          |\n","|    actor_loss      | -9.98    |\n","|    critic_loss     | 2.98     |\n","|    ent_coef        | 0.00672  |\n","|    ent_coef_loss   | 4.27     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 453085   |\n","---------------------------------\n","Eval num_timesteps=3436800, episode_reward=158.69 +/- 5.30\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 159      |\n","| time/              |          |\n","|    total_timesteps | 3436800  |\n","| train/             |          |\n","|    actor_loss      | -12.3    |\n","|    critic_loss     | 0.885    |\n","|    ent_coef        | 0.00673  |\n","|    ent_coef_loss   | -2.31    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 453185   |\n","---------------------------------\n","Eval num_timesteps=3439200, episode_reward=153.29 +/- 2.92\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 153      |\n","| time/              |          |\n","|    total_timesteps | 3439200  |\n","| train/             |          |\n","|    actor_loss      | -11.3    |\n","|    critic_loss     | 6.94     |\n","|    ent_coef        | 0.00656  |\n","|    ent_coef_loss   | -2.2     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 453285   |\n","---------------------------------\n","Eval num_timesteps=3441600, episode_reward=84.44 +/- 80.85\n","Episode length: 426.00 +/- 90.63\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 426      |\n","|    mean_reward     | 84.4     |\n","| time/              |          |\n","|    total_timesteps | 3441600  |\n","| train/             |          |\n","|    actor_loss      | -10.5    |\n","|    critic_loss     | 4.05     |\n","|    ent_coef        | 0.00643  |\n","|    ent_coef_loss   | 2.95     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 453385   |\n","---------------------------------\n","Eval num_timesteps=3444000, episode_reward=94.88 +/- 92.61\n","Episode length: 415.60 +/- 103.37\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 416      |\n","|    mean_reward     | 94.9     |\n","| time/              |          |\n","|    total_timesteps | 3444000  |\n","| train/             |          |\n","|    actor_loss      | -11.8    |\n","|    critic_loss     | 4.36     |\n","|    ent_coef        | 0.00637  |\n","|    ent_coef_loss   | -1.59    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 453485   |\n","---------------------------------\n","Eval num_timesteps=3446400, episode_reward=139.56 +/- 1.74\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 140      |\n","| time/              |          |\n","|    total_timesteps | 3446400  |\n","| train/             |          |\n","|    actor_loss      | -11.4    |\n","|    critic_loss     | 1.18     |\n","|    ent_coef        | 0.00637  |\n","|    ent_coef_loss   | 2.21     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 453585   |\n","---------------------------------\n","Eval num_timesteps=3448800, episode_reward=32.25 +/- 80.72\n","Episode length: 404.00 +/- 78.38\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 404      |\n","|    mean_reward     | 32.3     |\n","| time/              |          |\n","|    total_timesteps | 3448800  |\n","| train/             |          |\n","|    actor_loss      | -12.4    |\n","|    critic_loss     | 0.964    |\n","|    ent_coef        | 0.00637  |\n","|    ent_coef_loss   | -2.23    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 453685   |\n","---------------------------------\n","Eval num_timesteps=3451200, episode_reward=140.74 +/- 6.33\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 141      |\n","| time/              |          |\n","|    total_timesteps | 3451200  |\n","| train/             |          |\n","|    actor_loss      | -10      |\n","|    critic_loss     | 2.15     |\n","|    ent_coef        | 0.00618  |\n","|    ent_coef_loss   | 0.874    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 453785   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2310     |\n","|    fps             | 276      |\n","|    time_elapsed    | 12496    |\n","|    total_timesteps | 3452568  |\n","| train/             |          |\n","|    actor_loss      | -11.2    |\n","|    critic_loss     | 0.976    |\n","|    ent_coef        | 0.00615  |\n","|    ent_coef_loss   | -0.591   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 453842   |\n","---------------------------------\n","Eval num_timesteps=3453600, episode_reward=-32.69 +/- 17.00\n","Episode length: 302.00 +/- 41.64\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 302      |\n","|    mean_reward     | -32.7    |\n","| time/              |          |\n","|    total_timesteps | 3453600  |\n","| train/             |          |\n","|    actor_loss      | -11.3    |\n","|    critic_loss     | 1.39     |\n","|    ent_coef        | 0.00615  |\n","|    ent_coef_loss   | 1.83     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 453885   |\n","---------------------------------\n","Eval num_timesteps=3456000, episode_reward=96.48 +/- 82.47\n","Episode length: 427.20 +/- 89.16\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 427      |\n","|    mean_reward     | 96.5     |\n","| time/              |          |\n","|    total_timesteps | 3456000  |\n","| train/             |          |\n","|    actor_loss      | -11.5    |\n","|    critic_loss     | 1.56     |\n","|    ent_coef        | 0.00611  |\n","|    ent_coef_loss   | -0.185   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 453985   |\n","---------------------------------\n","Eval num_timesteps=3458400, episode_reward=92.52 +/- 97.18\n","Episode length: 398.80 +/- 123.94\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 399      |\n","|    mean_reward     | 92.5     |\n","| time/              |          |\n","|    total_timesteps | 3458400  |\n","| train/             |          |\n","|    actor_loss      | -11.9    |\n","|    critic_loss     | 0.889    |\n","|    ent_coef        | 0.00606  |\n","|    ent_coef_loss   | -3.01    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 454085   |\n","---------------------------------\n","Eval num_timesteps=3460800, episode_reward=167.94 +/- 9.51\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 168      |\n","| time/              |          |\n","|    total_timesteps | 3460800  |\n","| train/             |          |\n","|    actor_loss      | -11.4    |\n","|    critic_loss     | 1.42     |\n","|    ent_coef        | 0.00608  |\n","|    ent_coef_loss   | -1.26    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 454185   |\n","---------------------------------\n","Eval num_timesteps=3463200, episode_reward=163.36 +/- 1.45\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 163      |\n","| time/              |          |\n","|    total_timesteps | 3463200  |\n","| train/             |          |\n","|    actor_loss      | -12.1    |\n","|    critic_loss     | 0.739    |\n","|    ent_coef        | 0.00606  |\n","|    ent_coef_loss   | -1.4     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 454285   |\n","---------------------------------\n","Eval num_timesteps=3465600, episode_reward=164.22 +/- 0.12\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 164      |\n","| time/              |          |\n","|    total_timesteps | 3465600  |\n","| train/             |          |\n","|    actor_loss      | -10      |\n","|    critic_loss     | 2.07     |\n","|    ent_coef        | 0.00601  |\n","|    ent_coef_loss   | 0.879    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 454385   |\n","---------------------------------\n","Eval num_timesteps=3468000, episode_reward=153.85 +/- 2.42\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 154      |\n","| time/              |          |\n","|    total_timesteps | 3468000  |\n","| train/             |          |\n","|    actor_loss      | -11.8    |\n","|    critic_loss     | 0.782    |\n","|    ent_coef        | 0.00598  |\n","|    ent_coef_loss   | -2.88    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 454485   |\n","---------------------------------\n","Eval num_timesteps=3470400, episode_reward=46.11 +/- 99.57\n","Episode length: 339.20 +/- 131.29\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 339      |\n","|    mean_reward     | 46.1     |\n","| time/              |          |\n","|    total_timesteps | 3470400  |\n","| train/             |          |\n","|    actor_loss      | -11.8    |\n","|    critic_loss     | 1.03     |\n","|    ent_coef        | 0.00588  |\n","|    ent_coef_loss   | -1.62    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 454585   |\n","---------------------------------\n","Eval num_timesteps=3472800, episode_reward=91.11 +/- 87.41\n","Episode length: 426.40 +/- 90.14\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 426      |\n","|    mean_reward     | 91.1     |\n","| time/              |          |\n","|    total_timesteps | 3472800  |\n","| train/             |          |\n","|    actor_loss      | -10.9    |\n","|    critic_loss     | 1.53     |\n","|    ent_coef        | 0.00579  |\n","|    ent_coef_loss   | 0.0403   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 454685   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2320     |\n","|    fps             | 276      |\n","|    time_elapsed    | 12577    |\n","|    total_timesteps | 3473664  |\n","| train/             |          |\n","|    actor_loss      | -11.8    |\n","|    critic_loss     | 1.65     |\n","|    ent_coef        | 0.00581  |\n","|    ent_coef_loss   | 0.657    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 454721   |\n","---------------------------------\n","Eval num_timesteps=3475200, episode_reward=169.03 +/- 1.22\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 169      |\n","| time/              |          |\n","|    total_timesteps | 3475200  |\n","| train/             |          |\n","|    actor_loss      | -12.1    |\n","|    critic_loss     | 0.951    |\n","|    ent_coef        | 0.00578  |\n","|    ent_coef_loss   | -1.25    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 454785   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 2330     |\n","|    fps             | 276      |\n","|    time_elapsed    | 12587    |\n","|    total_timesteps | 3476064  |\n","| train/             |          |\n","|    actor_loss      | -11.8    |\n","|    critic_loss     | 0.92     |\n","|    ent_coef        | 0.00574  |\n","|    ent_coef_loss   | 1.22     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 454821   |\n","---------------------------------\n","Eval num_timesteps=3477600, episode_reward=77.75 +/- 69.97\n","Episode length: 434.60 +/- 53.40\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 435      |\n","|    mean_reward     | 77.7     |\n","| time/              |          |\n","|    total_timesteps | 3477600  |\n","| train/             |          |\n","|    actor_loss      | -11.2    |\n","|    critic_loss     | 12.4     |\n","|    ent_coef        | 0.00574  |\n","|    ent_coef_loss   | 3.68     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 454885   |\n","---------------------------------\n","Eval num_timesteps=3480000, episode_reward=170.96 +/- 2.04\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 171      |\n","| time/              |          |\n","|    total_timesteps | 3480000  |\n","| train/             |          |\n","|    actor_loss      | -11.7    |\n","|    critic_loss     | 0.859    |\n","|    ent_coef        | 0.00561  |\n","|    ent_coef_loss   | -0.876   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 454985   |\n","---------------------------------\n","Eval num_timesteps=3482400, episode_reward=80.75 +/- 67.50\n","Episode length: 443.60 +/- 46.05\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 444      |\n","|    mean_reward     | 80.8     |\n","| time/              |          |\n","|    total_timesteps | 3482400  |\n","| train/             |          |\n","|    actor_loss      | -10.6    |\n","|    critic_loss     | 1.59     |\n","|    ent_coef        | 0.0056   |\n","|    ent_coef_loss   | 2.48     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 455085   |\n","---------------------------------\n","Eval num_timesteps=3484800, episode_reward=73.74 +/- 88.80\n","Episode length: 430.80 +/- 84.75\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 431      |\n","|    mean_reward     | 73.7     |\n","| time/              |          |\n","|    total_timesteps | 3484800  |\n","| train/             |          |\n","|    actor_loss      | -11.8    |\n","|    critic_loss     | 1.55     |\n","|    ent_coef        | 0.00565  |\n","|    ent_coef_loss   | 0.891    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 455185   |\n","---------------------------------\n","Eval num_timesteps=3487200, episode_reward=28.41 +/- 104.61\n","Episode length: 319.40 +/- 147.46\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 319      |\n","|    mean_reward     | 28.4     |\n","| time/              |          |\n","|    total_timesteps | 3487200  |\n","| train/             |          |\n","|    actor_loss      | -12      |\n","|    critic_loss     | 1.93     |\n","|    ent_coef        | 0.00557  |\n","|    ent_coef_loss   | -0.132   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 455285   |\n","---------------------------------\n","Eval num_timesteps=3489600, episode_reward=177.95 +/- 3.57\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 178      |\n","| time/              |          |\n","|    total_timesteps | 3489600  |\n","| train/             |          |\n","|    actor_loss      | -10.9    |\n","|    critic_loss     | 1.37     |\n","|    ent_coef        | 0.0054   |\n","|    ent_coef_loss   | 1.54     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 455385   |\n","---------------------------------\n","Eval num_timesteps=3492000, episode_reward=165.96 +/- 0.27\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 166      |\n","| time/              |          |\n","|    total_timesteps | 3492000  |\n","| train/             |          |\n","|    actor_loss      | -11.5    |\n","|    critic_loss     | 1.63     |\n","|    ent_coef        | 0.00535  |\n","|    ent_coef_loss   | 0.989    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 455485   |\n","---------------------------------\n","Eval num_timesteps=3494400, episode_reward=-74.91 +/- 20.29\n","Episode length: 195.80 +/- 35.27\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 196      |\n","|    mean_reward     | -74.9    |\n","| time/              |          |\n","|    total_timesteps | 3494400  |\n","| train/             |          |\n","|    actor_loss      | -11.4    |\n","|    critic_loss     | 2.12     |\n","|    ent_coef        | 0.0054   |\n","|    ent_coef_loss   | -0.0859  |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 455585   |\n","---------------------------------\n","Eval num_timesteps=3496800, episode_reward=86.58 +/- 84.41\n","Episode length: 427.60 +/- 88.67\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 428      |\n","|    mean_reward     | 86.6     |\n","| time/              |          |\n","|    total_timesteps | 3496800  |\n","| train/             |          |\n","|    actor_loss      | -11.5    |\n","|    critic_loss     | 6.62     |\n","|    ent_coef        | 0.00541  |\n","|    ent_coef_loss   | -1.61    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 455685   |\n","---------------------------------\n","Eval num_timesteps=3499200, episode_reward=161.28 +/- 1.54\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 161      |\n","| time/              |          |\n","|    total_timesteps | 3499200  |\n","| train/             |          |\n","|    actor_loss      | -10.6    |\n","|    critic_loss     | 48.2     |\n","|    ent_coef        | 0.00549  |\n","|    ent_coef_loss   | 4.05     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 455785   |\n","---------------------------------\n","Eval num_timesteps=3501600, episode_reward=-8.06 +/- 7.87\n","Episode length: 353.80 +/- 5.88\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 354      |\n","|    mean_reward     | -8.06    |\n","| time/              |          |\n","|    total_timesteps | 3501600  |\n","| train/             |          |\n","|    actor_loss      | -11.6    |\n","|    critic_loss     | 1.39     |\n","|    ent_coef        | 0.00568  |\n","|    ent_coef_loss   | 6.83     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 455885   |\n","---------------------------------\n","Eval num_timesteps=3504000, episode_reward=148.53 +/- 8.85\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 149      |\n","| time/              |          |\n","|    total_timesteps | 3504000  |\n","| train/             |          |\n","|    actor_loss      | -11.1    |\n","|    critic_loss     | 2.87     |\n","|    ent_coef        | 0.00586  |\n","|    ent_coef_loss   | 2.3      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 455985   |\n","---------------------------------\n","Eval num_timesteps=3506400, episode_reward=164.84 +/- 0.63\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 165      |\n","| time/              |          |\n","|    total_timesteps | 3506400  |\n","| train/             |          |\n","|    actor_loss      | -12.6    |\n","|    critic_loss     | 0.691    |\n","|    ent_coef        | 0.00598  |\n","|    ent_coef_loss   | -2.18    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 456085   |\n","---------------------------------\n","Eval num_timesteps=3508800, episode_reward=72.77 +/- 94.76\n","Episode length: 386.00 +/- 139.62\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 386      |\n","|    mean_reward     | 72.8     |\n","| time/              |          |\n","|    total_timesteps | 3508800  |\n","| train/             |          |\n","|    actor_loss      | -11.3    |\n","|    critic_loss     | 1.97     |\n","|    ent_coef        | 0.00591  |\n","|    ent_coef_loss   | -0.225   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 456185   |\n","---------------------------------\n","Eval num_timesteps=3511200, episode_reward=93.47 +/- 73.72\n","Episode length: 458.80 +/- 50.46\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 459      |\n","|    mean_reward     | 93.5     |\n","| time/              |          |\n","|    total_timesteps | 3511200  |\n","| train/             |          |\n","|    actor_loss      | -11.7    |\n","|    critic_loss     | 2.07     |\n","|    ent_coef        | 0.00588  |\n","|    ent_coef_loss   | -0.913   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 456285   |\n","---------------------------------\n","Eval num_timesteps=3513600, episode_reward=160.25 +/- 0.37\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 160      |\n","| time/              |          |\n","|    total_timesteps | 3513600  |\n","| train/             |          |\n","|    actor_loss      | -10.4    |\n","|    critic_loss     | 2.04     |\n","|    ent_coef        | 0.00582  |\n","|    ent_coef_loss   | 2.45     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 456385   |\n","---------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n"]},{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-22-49e808331ec5>\", line 12, in <cell line: 12>\n","    model.learn(total_timesteps=4000000, log_interval=10,callback=eval_callback)\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/sac/sac.py\", line 307, in learn\n","    return super().learn(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 312, in learn\n","    rollout = self.collect_rollouts(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 552, in collect_rollouts\n","    if callback.on_step() is False:\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 104, in on_step\n","    return self._on_step()\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 471, in _on_step\n","    np.savez(\n","  File \"<__array_function__ internals>\", line 180, in savez\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 615, in savez\n","    _savez(file, args, kwds, False)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 712, in _savez\n","    zipf = zipfile_factory(file, mode=\"w\", compression=compression)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 103, in zipfile_factory\n","    return zipfile.ZipFile(file, *args, **kwargs)\n","  File \"/usr/lib/python3.10/zipfile.py\", line 1251, in __init__\n","    self.fp = io.open(file, filemode)\n","OSError: [Errno 107] Transport endpoint is not connected: './multiwalker_sac_00_log_eval_extra2/evaluations.npz'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'OSError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-22-49e808331ec5>\", line 12, in <cell line: 12>\n","    model.learn(total_timesteps=4000000, log_interval=10,callback=eval_callback)\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/sac/sac.py\", line 307, in learn\n","    return super().learn(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 312, in learn\n","    rollout = self.collect_rollouts(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 552, in collect_rollouts\n","    if callback.on_step() is False:\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 104, in on_step\n","    return self._on_step()\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 471, in _on_step\n","    np.savez(\n","  File \"<__array_function__ internals>\", line 180, in savez\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 615, in savez\n","    _savez(file, args, kwds, False)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 712, in _savez\n","    zipf = zipfile_factory(file, mode=\"w\", compression=compression)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 103, in zipfile_factory\n","    return zipfile.ZipFile(file, *args, **kwargs)\n","  File \"/usr/lib/python3.10/zipfile.py\", line 1251, in __init__\n","    self.fp = io.open(file, filemode)\n","OSError: [Errno 107] Transport endpoint is not connected: './multiwalker_sac_00_log_eval_extra2/evaluations.npz'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'OSError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n","    if (await self.run_code(code, result,  async_=asy)):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n","    self.showtraceback(running_compiled_code=True)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n","    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-22-49e808331ec5>\", line 12, in <cell line: 12>\n","    model.learn(total_timesteps=4000000, log_interval=10,callback=eval_callback)\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/sac/sac.py\", line 307, in learn\n","    return super().learn(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 312, in learn\n","    rollout = self.collect_rollouts(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 552, in collect_rollouts\n","    if callback.on_step() is False:\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 104, in on_step\n","    return self._on_step()\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 471, in _on_step\n","    np.savez(\n","  File \"<__array_function__ internals>\", line 180, in savez\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 615, in savez\n","    _savez(file, args, kwds, False)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 712, in _savez\n","    zipf = zipfile_factory(file, mode=\"w\", compression=compression)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 103, in zipfile_factory\n","    return zipfile.ZipFile(file, *args, **kwargs)\n","  File \"/usr/lib/python3.10/zipfile.py\", line 1251, in __init__\n","    self.fp = io.open(file, filemode)\n","OSError: [Errno 107] Transport endpoint is not connected: './multiwalker_sac_00_log_eval_extra2/evaluations.npz'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'OSError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n","    if (await self.run_code(code, result,  async_=asy)):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n","    self.showtraceback(running_compiled_code=True)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n","    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n","    return runner(coro)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n","    coro.send(None)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n","    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3492, in run_ast_nodes\n","    self.showtraceback()\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1142, in structured_traceback\n","    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n"]}],"source":["from stable_baselines3.common.logger import configure\n","\n","tmp_path = \"multiwalker_sac_00_log_eval_extra2/\"\n","# set up logger\n","new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n","\n","model = SAC.load(\"./multiwalker_sac_00_log_eval_extra/best_model\")\n","model.set_env(env)\n","model.set_parameters(\"./multiwalker_sac_00_log_eval_extra/best_model\")\n","\n","model.set_logger(new_logger)\n","model.learn(total_timesteps=4000000, log_interval=10,callback=eval_callback)\n","model.save(\"multiwalker_sac_00_extra2\")"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"mount_file_id":"1tNH0xWOkEmzP-ic_xuZPO9HHOk_62LLx","authorship_tag":"ABX9TyP2kCTt7xbO+ia2IYBx3Wfs"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}