{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"d8JAmEUyj9De"},"outputs":[],"source":["# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n","mount='/content/gdrive'\n","drive_root = mount + \"/My Drive/TFM\"\n","\n","try:\n","  from google.colab import drive\n","  IN_COLAB=True\n","except:\n","  IN_COLAB=False"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21352,"status":"ok","timestamp":1698306044930,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"DrEo9QnxkAne","outputId":"49cd05a0-1ac5-4ec2-d466-d9a23962c697"},"outputs":[{"output_type":"stream","name":"stdout","text":["We're running Colab\n","Colab: mounting Google drive on  /content/gdrive\n","Mounted at /content/gdrive\n","\n","Colab: making sure  /content/gdrive/My Drive/TFM  exists.\n","\n","Colab: Changing directory to  /content/gdrive/My Drive/TFM\n","/content/gdrive/My Drive/TFM\n","Archivos en el directorio: \n","['Entrenamientos antiguos sin logs', '.ipynb_checkpoints', 'Entrenamientos_log_no_eval', 'PPO_policies', 'TFM_PPO_pettingzoo_gym_cap.ipynb', 'TFM_PPO_new_pettingzoo_gym_cap.ipynb', 'DQN_new_pettingzoo_gym_cap.ipynb', 'multi_car_racing', 'policy_log_eval', 'DQN_policies', 'results_rllib', 'MCR_TFM.ipynb', 'multiwalker_ddpg_log_eval', 'multiwalker_sac_log_eval', 'multiwalker_ddpg.zip', 'multiwalker_ppo_log_eval', 'multiwalker_ppo.zip', 'multiwalker_td3_log_eval', 'TFM_Multiwalker_DDPG_PPO_gym_cap.ipynb', 'multiwalker_sac2_log_eval', '=2.13', 'multiwalker_td3_2_log_eval', 'multiwalker_sac3_log_eval', 'multiwalker_sac3.zip', 'TFM_Multiwalker_TD3_gym_cap.ipynb', 'TFM_Multiwalker_SAC_gym_cap.ipynb']\n"]}],"source":["# Switch to the directory on the Google Drive that you want to use\n","import os\n","if IN_COLAB:\n","  print(\"We're running Colab\")\n","\n","  if IN_COLAB:\n","    # Mount the Google Drive at mount\n","    print(\"Colab: mounting Google drive on \", mount)\n","\n","    drive.mount(mount)\n","\n","    # Create drive_root if it doesn't exist\n","    create_drive_root = True\n","    if create_drive_root:\n","      print(\"\\nColab: making sure \", drive_root, \" exists.\")\n","      os.makedirs(drive_root, exist_ok=True)\n","\n","    # Change to the directory\n","    print(\"\\nColab: Changing directory to \", drive_root)\n","    %cd $drive_root\n","# Verify we're in the correct working directory\n","%pwd\n","print(\"Archivos en el directorio: \")\n","print(os.listdir())"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":246061,"status":"ok","timestamp":1698306290974,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"xAZDg478kEbs","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9a7b8431-a582-41ad-8933-bc7ff512877f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gym==0.17.3\n","  Downloading gym-0.17.3.tar.gz (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym==0.17.3) (1.11.3)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.17.3) (1.23.5)\n","Collecting pyglet<=1.5.0,>=1.4.0 (from gym==0.17.3)\n","  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cloudpickle<1.7.0,>=1.2.0 (from gym==0.17.3)\n","  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3) (0.18.3)\n","Building wheels for collected packages: gym\n","  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym: filename=gym-0.17.3-py3-none-any.whl size=1654617 sha256=738abf1af85e6aaf858618e6a9caf1f0002147a1b74a0d1b62d8ca7d040aef07\n","  Stored in directory: /root/.cache/pip/wheels/af/4b/74/fcfc8238472c34d7f96508a63c962ff3ac9485a9a4137afd4e\n","Successfully built gym\n","Installing collected packages: pyglet, cloudpickle, gym\n","  Attempting uninstall: cloudpickle\n","    Found existing installation: cloudpickle 2.2.1\n","    Uninstalling cloudpickle-2.2.1:\n","      Successfully uninstalled cloudpickle-2.2.1\n","  Attempting uninstall: gym\n","    Found existing installation: gym 0.25.2\n","    Uninstalling gym-0.25.2:\n","      Successfully uninstalled gym-0.25.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","bigframes 0.10.0 requires cloudpickle>=2.0.0, but you have cloudpickle 1.6.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed cloudpickle-1.6.0 gym-0.17.3 pyglet-1.5.0\n","Collecting git+https://github.com/Kojoley/atari-py.git\n","  Cloning https://github.com/Kojoley/atari-py.git to /tmp/pip-req-build-0bow76uc\n","  Running command git clone --filter=blob:none --quiet https://github.com/Kojoley/atari-py.git /tmp/pip-req-build-0bow76uc\n","  Resolved https://github.com/Kojoley/atari-py.git to commit 86a1e05c0a95e9e6233c3a413521fdb34ca8a089\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from atari-py==1.2.2) (1.23.5)\n","Building wheels for collected packages: atari-py\n","  Building wheel for atari-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for atari-py: filename=atari_py-1.2.2-cp310-cp310-linux_x86_64.whl size=4738731 sha256=c5483f358b4d0fddab868d84b9b2b4d11298fcd8600d4d778a6907ea115e2e84\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-x5kvcwp4/wheels/6c/9b/f1/8a80a63a233d6f4eb2e2ee8c7357f4875f21c3ffc7f2613f9f\n","Successfully built atari-py\n","Installing collected packages: atari-py\n","Successfully installed atari-py-1.2.2\n","Collecting keras-rl2==1.0.5\n","  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from keras-rl2==1.0.5) (2.14.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (23.5.26)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (16.0.6)\n","Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.2.0)\n","Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.23.5)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (23.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (4.5.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.34.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.59.0)\n","Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.14.1)\n","Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.14.0)\n","Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.14.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2==1.0.5) (0.41.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (0.7.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.0.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (5.3.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.2.2)\n","Installing collected packages: keras-rl2\n","Successfully installed keras-rl2-1.0.5\n","Collecting tensorflow==2.8\n","  Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl (497.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.6.3)\n","Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (23.5.26)\n","Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.9.0)\n","Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.8)\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (16.0.6)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.23.5)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.3.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (4.5.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.14.1)\n","Collecting tensorboard<2.9,>=2.8 (from tensorflow==2.8)\n","  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tf-estimator-nightly==2.8.0.dev2021122109 (from tensorflow==2.8)\n","  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting keras<2.9,>=2.8.0rc0 (from tensorflow==2.8)\n","  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.34.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.59.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.8) (0.41.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.17.3)\n","Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.31.0)\n","Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n","  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m98.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n","  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.0.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (5.3.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.2.2)\n","Installing collected packages: tf-estimator-nightly, tensorboard-plugin-wit, keras, tensorboard-data-server, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.14.0\n","    Uninstalling keras-2.14.0:\n","      Successfully uninstalled keras-2.14.0\n","  Attempting uninstall: tensorboard-data-server\n","    Found existing installation: tensorboard-data-server 0.7.1\n","    Uninstalling tensorboard-data-server-0.7.1:\n","      Successfully uninstalled tensorboard-data-server-0.7.1\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 1.0.0\n","    Uninstalling google-auth-oauthlib-1.0.0:\n","      Successfully uninstalled google-auth-oauthlib-1.0.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.14.1\n","    Uninstalling tensorboard-2.14.1:\n","      Successfully uninstalled tensorboard-2.14.1\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.14.0\n","    Uninstalling tensorflow-2.14.0:\n","      Successfully uninstalled tensorflow-2.14.0\n","Successfully installed google-auth-oauthlib-0.4.6 keras-2.8.0 keras-preprocessing-1.1.2 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tf-estimator-nightly-2.8.0.dev2021122109\n","Collecting gym-cap\n","  Downloading gym_cap-0.1.4.12-py3-none-any.whl (32 kB)\n","Requirement already satisfied: gym>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from gym-cap) (0.17.3)\n","Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from gym-cap) (1.23.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym>=0.16.0->gym-cap) (1.11.3)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.16.0->gym-cap) (1.5.0)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.16.0->gym-cap) (1.6.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.16.0->gym-cap) (0.18.3)\n","Installing collected packages: gym-cap\n","Successfully installed gym-cap-0.1.4.12\n","Collecting pettingzoo[butterfly]\n","  Downloading pettingzoo-1.24.1-py3-none-any.whl (840 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.8/840.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[butterfly]) (1.23.5)\n","Collecting gymnasium>=0.28.0 (from pettingzoo[butterfly])\n","  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pygame==2.3.0 (from pettingzoo[butterfly])\n","  Downloading pygame-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pymunk==6.2.0 (from pettingzoo[butterfly])\n","  Downloading pymunk-6.2.0.zip (7.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: cffi>1.14.0 in /usr/local/lib/python3.10/dist-packages (from pymunk==6.2.0->pettingzoo[butterfly]) (1.16.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[butterfly]) (1.6.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[butterfly]) (4.5.0)\n","Collecting farama-notifications>=0.0.1 (from gymnasium>=0.28.0->pettingzoo[butterfly])\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>1.14.0->pymunk==6.2.0->pettingzoo[butterfly]) (2.21)\n","Building wheels for collected packages: pymunk\n","  Building wheel for pymunk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pymunk: filename=pymunk-6.2.0-cp310-cp310-linux_x86_64.whl size=801634 sha256=f37a6aa3cf1e3ebc537e50a2f9be220ef54aa8c5fa05633a77beb9086aeef01b\n","  Stored in directory: /root/.cache/pip/wheels/2e/81/a2/f941a9ff417bb4020c37ae218fb7117d12d3fc019ea493d66f\n","Successfully built pymunk\n","Installing collected packages: farama-notifications, pygame, gymnasium, pymunk, pettingzoo\n","  Attempting uninstall: pygame\n","    Found existing installation: pygame 2.5.2\n","    Uninstalling pygame-2.5.2:\n","      Successfully uninstalled pygame-2.5.2\n","Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1 pettingzoo-1.24.1 pygame-2.3.0 pymunk-6.2.0\n"]}],"source":["if IN_COLAB:\n","  %pip install gym==0.17.3\n","  %pip install git+https://github.com/Kojoley/atari-py.git\n","  %pip install keras-rl2==1.0.5\n","  %pip install tensorflow==2.8\n","  %pip install gym-cap\n","  %pip install pettingzoo[butterfly]\n","else:\n","  %pip install gym==0.17.3\n","  %pip install git+https://github.com/Kojoley/atari-py.git\n","  %pip install pyglet==1.5.0\n","  %pip install h5py==3.1.0\n","  %pip install Pillow==9.5.0\n","  %pip install keras-rl2==1.0.5\n","  %pip install Keras==2.2.4\n","  %pip install tensorflow==2.5.3\n","  %pip install torch==2.0.1\n","  %pip install agents==1.4.0"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":390,"status":"ok","timestamp":1698306291334,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"G8cw-IX3laE9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fc891f52-a00b-47cd-e091-adbc36aeac94"},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'multi_car_racing' already exists and is not an empty directory.\n"]}],"source":["!git clone https://github.com/igilitschenski/multi_car_racing.git\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IqbMo3gK7vBG"},"outputs":[],"source":["!cd multi_car_racing\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9995,"status":"ok","timestamp":1698306301324,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"Jekec6f98b3A","colab":{"base_uri":"https://localhost:8080/"},"outputId":"994e9999-5367-4799-a1dc-d2eeb808a43c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting swig\n","  Downloading swig-4.1.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: swig\n","Successfully installed swig-4.1.1\n"]}],"source":["!pip install swig"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":72346,"status":"ok","timestamp":1698306373625,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"KKxRPBFx85k6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6d29c785-3b10-46a0-e073-a6d9194abfd1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting box2d\n","  Downloading Box2D-2.3.2.tar.gz (427 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/427.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/427.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.9/427.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: box2d\n","  Building wheel for box2d (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d: filename=Box2D-2.3.2-cp310-cp310-linux_x86_64.whl size=2391311 sha256=39bfa926f29f6b54ea0df24dfb3fe7f2a635f2164102390ddd793e5ddb921e14\n","  Stored in directory: /root/.cache/pip/wheels/eb/cb/be/e663f3ce9aba6580611c0febaf7cd3cf7603f87047de2a52f9\n","Successfully built box2d\n","Installing collected packages: box2d\n","Successfully installed box2d-2.3.2\n"]}],"source":["!pip install box2d"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":69060,"status":"ok","timestamp":1698306442665,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"ijp5V0i09MRF","colab":{"base_uri":"https://localhost:8080/"},"outputId":"44197644-7b2f-4bed-aa72-aa34bab0fe90"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting box2d-py\n","  Downloading box2d-py-2.3.8.tar.gz (374 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.5/374.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: box2d-py\n","  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-py: filename=box2d_py-2.3.8-cp310-cp310-linux_x86_64.whl size=2373075 sha256=c3b7abf616976c998c34a2b0c9d340251208230078715112aa5aac3fd050e261\n","  Stored in directory: /root/.cache/pip/wheels/47/01/d2/6a780da77ccb98b1d2facdd520a8d10838a03b590f6f8d50c0\n","Successfully built box2d-py\n","Installing collected packages: box2d-py\n","Successfully installed box2d-py-2.3.8\n"]}],"source":["!pip install box2d-py"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":32636,"status":"ok","timestamp":1698306475242,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"BwjugqI99g0I","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5c80ffa6-e6f3-4762-b292-713e5232cf80"},"outputs":[{"output_type":"stream","name":"stdout","text":["Obtaining file:///content/gdrive/MyDrive/TFM/multi_car_racing\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: box2d-py~=2.3.5 in /usr/local/lib/python3.10/dist-packages (from gym-multi-car-racing==0.0.1) (2.3.8)\n","Collecting shapely~=1.7.0 (from gym-multi-car-racing==0.0.1)\n","  Downloading Shapely-1.7.1.tar.gz (383 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.2/383.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from gym-multi-car-racing==0.0.1) (1.23.5)\n","Requirement already satisfied: gym~=0.17.2 in /usr/local/lib/python3.10/dist-packages (from gym-multi-car-racing==0.0.1) (0.17.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym~=0.17.2->gym-multi-car-racing==0.0.1) (1.11.3)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from gym~=0.17.2->gym-multi-car-racing==0.0.1) (1.5.0)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym~=0.17.2->gym-multi-car-racing==0.0.1) (1.6.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym~=0.17.2->gym-multi-car-racing==0.0.1) (0.18.3)\n","Building wheels for collected packages: shapely\n","  Building wheel for shapely (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for shapely: filename=Shapely-1.7.1-cp310-cp310-linux_x86_64.whl size=997163 sha256=0bdac8589ce7dcfcadda7d47d3fc3b23d763fb22a7eec964a54c2d09ac3198fa\n","  Stored in directory: /root/.cache/pip/wheels/2e/fa/97/c85f587c35afcaf4a81c481741d36592518d1e50445572f0d4\n","Successfully built shapely\n","Installing collected packages: shapely, gym-multi-car-racing\n","  Attempting uninstall: shapely\n","    Found existing installation: shapely 2.0.2\n","    Uninstalling shapely-2.0.2:\n","      Successfully uninstalled shapely-2.0.2\n","  Running setup.py develop for gym-multi-car-racing\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires fastapi, which is not installed.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed gym-multi-car-racing-0.0.1 shapely-1.7.1\n"]}],"source":["!pip install -e multi_car_racing"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":12268,"status":"ok","timestamp":1698306487453,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"IrAvXzCW-Z3e","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ac289496-814b-4a37-dc94-1a02de09353f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  freeglut3 libegl-dev libgl-dev libgl1-mesa-dev libgles-dev libgles1\n","  libglu1-mesa libglu1-mesa-dev libglvnd-core-dev libglvnd-dev libglx-dev\n","  libice-dev libopengl-dev libsm-dev libxt-dev\n","Suggested packages:\n","  libice-doc libsm-doc libxt-doc\n","The following NEW packages will be installed:\n","  freeglut3 freeglut3-dev libegl-dev libgl-dev libgl1-mesa-dev libgles-dev\n","  libgles1 libglu1-mesa libglu1-mesa-dev libglvnd-core-dev libglvnd-dev\n","  libglx-dev libice-dev libopengl-dev libsm-dev libxt-dev\n","0 upgraded, 16 newly installed, 0 to remove and 18 not upgraded.\n","Need to get 1,261 kB of archives.\n","After this operation, 6,753 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3 amd64 2.8.1-6 [74.0 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglx-dev amd64 1.4.0-1 [14.1 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgl-dev amd64 1.4.0-1 [101 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-core-dev amd64 1.4.0-1 [12.7 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libegl-dev amd64 1.4.0-1 [18.0 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles1 amd64 1.4.0-1 [11.5 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles-dev amd64 1.4.0-1 [49.4 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libopengl-dev amd64 1.4.0-1 [3,400 B]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-dev amd64 1.4.0-1 [3,162 B]\n","Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgl1-mesa-dev amd64 23.0.4-0ubuntu1~22.04.1 [6,510 B]\n","Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n","Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa-dev amd64 9.0.2-1 [231 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 libice-dev amd64 2:1.0.10-1build2 [51.4 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsm-dev amd64 2:1.2.3-1build2 [18.1 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3-dev amd64 2.8.1-6 [126 kB]\n","Fetched 1,261 kB in 2s (611 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 16.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package freeglut3:amd64.\n","(Reading database ... 120874 files and directories currently installed.)\n","Preparing to unpack .../00-freeglut3_2.8.1-6_amd64.deb ...\n","Unpacking freeglut3:amd64 (2.8.1-6) ...\n","Selecting previously unselected package libglx-dev:amd64.\n","Preparing to unpack .../01-libglx-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglx-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgl-dev:amd64.\n","Preparing to unpack .../02-libgl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libgl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libglvnd-core-dev:amd64.\n","Preparing to unpack .../03-libglvnd-core-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglvnd-core-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libegl-dev:amd64.\n","Preparing to unpack .../04-libegl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libegl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgles1:amd64.\n","Preparing to unpack .../05-libgles1_1.4.0-1_amd64.deb ...\n","Unpacking libgles1:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgles-dev:amd64.\n","Preparing to unpack .../06-libgles-dev_1.4.0-1_amd64.deb ...\n","Unpacking libgles-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libopengl-dev:amd64.\n","Preparing to unpack .../07-libopengl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libopengl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libglvnd-dev:amd64.\n","Preparing to unpack .../08-libglvnd-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglvnd-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgl1-mesa-dev:amd64.\n","Preparing to unpack .../09-libgl1-mesa-dev_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n","Unpacking libgl1-mesa-dev:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Selecting previously unselected package libglu1-mesa:amd64.\n","Preparing to unpack .../10-libglu1-mesa_9.0.2-1_amd64.deb ...\n","Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n","Selecting previously unselected package libglu1-mesa-dev:amd64.\n","Preparing to unpack .../11-libglu1-mesa-dev_9.0.2-1_amd64.deb ...\n","Unpacking libglu1-mesa-dev:amd64 (9.0.2-1) ...\n","Selecting previously unselected package libice-dev:amd64.\n","Preparing to unpack .../12-libice-dev_2%3a1.0.10-1build2_amd64.deb ...\n","Unpacking libice-dev:amd64 (2:1.0.10-1build2) ...\n","Selecting previously unselected package libsm-dev:amd64.\n","Preparing to unpack .../13-libsm-dev_2%3a1.2.3-1build2_amd64.deb ...\n","Unpacking libsm-dev:amd64 (2:1.2.3-1build2) ...\n","Selecting previously unselected package libxt-dev:amd64.\n","Preparing to unpack .../14-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n","Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n","Selecting previously unselected package freeglut3-dev:amd64.\n","Preparing to unpack .../15-freeglut3-dev_2.8.1-6_amd64.deb ...\n","Unpacking freeglut3-dev:amd64 (2.8.1-6) ...\n","Setting up freeglut3:amd64 (2.8.1-6) ...\n","Setting up libglvnd-core-dev:amd64 (1.4.0-1) ...\n","Setting up libice-dev:amd64 (2:1.0.10-1build2) ...\n","Setting up libsm-dev:amd64 (2:1.2.3-1build2) ...\n","Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n","Setting up libgles1:amd64 (1.4.0-1) ...\n","Setting up libglx-dev:amd64 (1.4.0-1) ...\n","Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n","Setting up libopengl-dev:amd64 (1.4.0-1) ...\n","Setting up libgl-dev:amd64 (1.4.0-1) ...\n","Setting up libegl-dev:amd64 (1.4.0-1) ...\n","Setting up libglu1-mesa-dev:amd64 (9.0.2-1) ...\n","Setting up libgles-dev:amd64 (1.4.0-1) ...\n","Setting up libglvnd-dev:amd64 (1.4.0-1) ...\n","Setting up libgl1-mesa-dev:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Setting up freeglut3-dev:amd64 (2.8.1-6) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n"]}],"source":["!sudo apt-get install freeglut3-dev"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":27629,"status":"ok","timestamp":1698306515023,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"cgGdQ6n9EERW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"aeb09f5a-b66d-43f1-f92c-071cb1157c45"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n","  xserver-common\n","The following NEW packages will be installed:\n","  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n","  xserver-common xvfb\n","0 upgraded, 9 newly installed, 0 to remove and 18 not upgraded.\n","Need to get 7,812 kB of archives.\n","After this operation, 11.9 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.1 [28.0 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.1 [863 kB]\n","Fetched 7,812 kB in 1s (5,352 kB/s)\n","Selecting previously unselected package libfontenc1:amd64.\n","(Reading database ... 121332 files and directories currently installed.)\n","Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n","Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n","Selecting previously unselected package libxfont2:amd64.\n","Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n","Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n","Selecting previously unselected package libxkbfile1:amd64.\n","Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n","Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n","Selecting previously unselected package x11-xkb-utils.\n","Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n","Unpacking x11-xkb-utils (7.7+5build4) ...\n","Selecting previously unselected package xfonts-encodings.\n","Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n","Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n","Selecting previously unselected package xfonts-utils.\n","Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n","Unpacking xfonts-utils (1:7.7+6build2) ...\n","Selecting previously unselected package xfonts-base.\n","Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n","Unpacking xfonts-base (1:1.0.5) ...\n","Selecting previously unselected package xserver-common.\n","Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.1_all.deb ...\n","Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.1) ...\n","Selecting previously unselected package xvfb.\n","Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.1_amd64.deb ...\n","Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.1) ...\n","Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n","Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n","Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n","Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n","Setting up x11-xkb-utils (7.7+5build4) ...\n","Setting up xfonts-utils (1:7.7+6build2) ...\n","Setting up xfonts-base (1:1.0.5) ...\n","Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.1) ...\n","Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.1) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","Collecting pyvirtualdisplay\n","  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n","Installing collected packages: pyvirtualdisplay\n","Successfully installed pyvirtualdisplay-3.0\n","Collecting piglet\n","  Downloading piglet-1.0.0-py2.py3-none-any.whl (2.2 kB)\n","Collecting piglet-templates (from piglet)\n","  Downloading piglet_templates-1.3.0-py3-none-any.whl (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.5/67.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from piglet-templates->piglet) (3.1.1)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from piglet-templates->piglet) (23.1.0)\n","Requirement already satisfied: astunparse in /usr/local/lib/python3.10/dist-packages (from piglet-templates->piglet) (1.6.3)\n","Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from piglet-templates->piglet) (2.1.3)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse->piglet-templates->piglet) (0.41.2)\n","Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from astunparse->piglet-templates->piglet) (1.16.0)\n","Installing collected packages: piglet-templates, piglet\n","Successfully installed piglet-1.0.0 piglet-templates-1.3.0\n"]}],"source":["!apt install xvfb -y\n","!pip install pyvirtualdisplay\n","!pip install piglet"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":21288,"status":"ok","timestamp":1698306536256,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"5OaWkBSmhm6R","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fb496d00-573f-406a-d537-c37806c2f3fb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting supersuit\n","  Downloading SuperSuit-3.9.0-py3-none-any.whl (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from supersuit) (1.23.5)\n","Requirement already satisfied: gymnasium>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from supersuit) (0.29.1)\n","Collecting tinyscaler>=1.2.6 (from supersuit)\n","  Downloading tinyscaler-1.2.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (517 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.1/517.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (1.6.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (0.0.4)\n","Installing collected packages: tinyscaler, supersuit\n","Successfully installed supersuit-3.9.0 tinyscaler-1.2.7\n","Collecting stable_baselines3\n","  Downloading stable_baselines3-2.1.0-py3-none-any.whl (178 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (0.29.1)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.23.5)\n","Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.1.0+cu118)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.6.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.5.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (3.7.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (0.0.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.12.4)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2.1.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.1.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (4.43.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (23.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable_baselines3) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable_baselines3) (1.3.0)\n","Installing collected packages: stable_baselines3\n","Successfully installed stable_baselines3-2.1.0\n"]}],"source":["!pip install supersuit\n","!pip install stable_baselines3"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6303,"status":"ok","timestamp":1698306542502,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"thmOvcHdjKHw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9964e0c8-7078-45d4-bac9-eb2c544e4ad3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting shimmy>=0.2.1\n","  Downloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from shimmy>=0.2.1) (1.23.5)\n","Requirement already satisfied: gymnasium>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from shimmy>=0.2.1) (0.29.1)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy>=0.2.1) (1.6.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy>=0.2.1) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy>=0.2.1) (0.0.4)\n","Installing collected packages: shimmy\n","Successfully installed shimmy-1.3.0\n"]}],"source":["!pip install 'shimmy>=0.2.1'"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":28985,"status":"ok","timestamp":1698306571440,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"k0iVvep_spQz","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0b397e81-440b-4121-f111-d15c6272c97c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tf-agents-nightly\n","  Downloading tf_agents_nightly-0.19.0.dev20231010-py3-none-any.whl (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.4.0)\n","Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.6.0)\n","Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (0.5.0)\n","Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (0.17.3)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.23.5)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (9.4.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.16.0)\n","Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (3.20.3)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.14.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (4.5.0)\n","Collecting pygame==2.1.3 (from tf-agents-nightly)\n","  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tfp-nightly (from tf-agents-nightly)\n","  Downloading tfp_nightly-0.23.0.dev20231025-py2.py3-none-any.whl (6.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents-nightly) (1.11.3)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents-nightly) (1.5.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tfp-nightly->tf-agents-nightly) (4.4.2)\n","Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tfp-nightly->tf-agents-nightly) (0.5.4)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tfp-nightly->tf-agents-nightly) (0.1.8)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<=0.23.0,>=0.17.0->tf-agents-nightly) (0.18.3)\n","Installing collected packages: tfp-nightly, pygame, tf-agents-nightly\n","  Attempting uninstall: pygame\n","    Found existing installation: pygame 2.3.0\n","    Uninstalling pygame-2.3.0:\n","      Successfully uninstalled pygame-2.3.0\n","Successfully installed pygame-2.1.3 tf-agents-nightly-0.19.0.dev20231010 tfp-nightly-0.23.0.dev20231025\n"]}],"source":["!pip install tf-agents-nightly"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":151714,"status":"ok","timestamp":1698306723094,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"UlXxViz9tdvH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fd8b263f-5d39-48fe-dd7a-84f0213627c1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: tensorflow 2.8.0\n","Uninstalling tensorflow-2.8.0:\n","  Would remove:\n","    /usr/local/bin/estimator_ckpt_converter\n","    /usr/local/bin/import_pb_to_tensorboard\n","    /usr/local/bin/saved_model_cli\n","    /usr/local/bin/tensorboard\n","    /usr/local/bin/tf_upgrade_v2\n","    /usr/local/bin/tflite_convert\n","    /usr/local/bin/toco\n","    /usr/local/bin/toco_from_protos\n","    /usr/local/lib/python3.10/dist-packages/tensorflow-2.8.0.dist-info/*\n","    /usr/local/lib/python3.10/dist-packages/tensorflow/*\n","Proceed (Y/n)? Y\n","  Successfully uninstalled tensorflow-2.8.0\n"]}],"source":["!pip uninstall tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dsWlVQ6MtKLj"},"outputs":[],"source":["!pip install tensorflow>=2.13"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10254,"status":"ok","timestamp":1698306780122,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"wE5AiVtFtZDc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"893768a7-d029-43e1-eee6-f6ad3382d2cf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Name: tensorflow\n","Version: 2.14.0\n","Summary: TensorFlow is an open source machine learning framework for everyone.\n","Home-page: https://www.tensorflow.org/\n","Author: Google Inc.\n","Author-email: packages@tensorflow.org\n","License: Apache 2.0\n","Location: /usr/local/lib/python3.10/dist-packages\n","Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, libclang, ml-dtypes, numpy, opt-einsum, packaging, protobuf, setuptools, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n","Required-by: dopamine-rl, keras-rl2\n"]}],"source":["!pip show tensorflow"]},{"cell_type":"code","source":["!pip install pettingzoo[sisl]"],"metadata":{"id":"PZa1qybXZKSX","executionInfo":{"status":"ok","timestamp":1698306858769,"user_tz":-120,"elapsed":78655,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"98e6df17-b8dc-489d-edc7-8bb6ff9dbaa0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pettingzoo[sisl] in /usr/local/lib/python3.10/dist-packages (1.24.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[sisl]) (1.23.5)\n","Requirement already satisfied: gymnasium>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[sisl]) (0.29.1)\n","Collecting pygame==2.3.0 (from pettingzoo[sisl])\n","  Using cached pygame-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\n","Requirement already satisfied: pymunk==6.2.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[sisl]) (6.2.0)\n","Collecting box2d-py==2.3.5 (from pettingzoo[sisl])\n","  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[sisl]) (1.11.3)\n","Requirement already satisfied: cffi>1.14.0 in /usr/local/lib/python3.10/dist-packages (from pymunk==6.2.0->pettingzoo[sisl]) (1.16.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[sisl]) (1.6.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[sisl]) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[sisl]) (0.0.4)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>1.14.0->pymunk==6.2.0->pettingzoo[sisl]) (2.21)\n","Building wheels for collected packages: box2d-py\n","  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2373072 sha256=d1c0413347479f64c80ea3f78f99fd8b070b485c2087d093046b21d7ebf9dfba\n","  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n","Successfully built box2d-py\n","Installing collected packages: box2d-py, pygame\n","  Attempting uninstall: box2d-py\n","    Found existing installation: box2d-py 2.3.8\n","    Uninstalling box2d-py-2.3.8:\n","      Successfully uninstalled box2d-py-2.3.8\n","  Attempting uninstall: pygame\n","    Found existing installation: pygame 2.1.3\n","    Uninstalling pygame-2.1.3:\n","      Successfully uninstalled pygame-2.1.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tf-agents-nightly 0.19.0.dev20231010 requires pygame==2.1.3, but you have pygame 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed box2d-py-2.3.5 pygame-2.3.0\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"NFzawQ1QZFwj"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JpA_YhKzCeC2"},"outputs":[],"source":["############################# Código para entrenar Multiwalker ######################"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tnz3fJDA33w8"},"outputs":[],"source":["from stable_baselines3 import DDPG\n","# from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n","from pettingzoo.sisl import multiwalker_v9\n","import supersuit as ss\n","from stable_baselines3.common.monitor import Monitor"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":556,"status":"ok","timestamp":1697649056527,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"6-kdhb3CI5VC","outputId":"c6003468-c14d-4ee8-e786-b1447befe97a"},"outputs":[{"output_type":"stream","name":"stdout","text":["ls: cannot open directory '.': Transport endpoint is not connected\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":319,"status":"ok","timestamp":1696848224082,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"04CbnRTvI9L2","outputId":"7e3b8b24-a1bb-49f2-e4da-e8287d01683e"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/TFM\n"]}],"source":["cd /content/drive/MyDrive/TFM"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1696848225257,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"bUEH8254I_kb","outputId":"00564d52-42b1-49fd-b614-1e041a847e34"},"outputs":[{"name":"stdout","output_type":"stream","text":["'=2.13'\t\t\t\t     policy3_log_eval\n"," Atari_TFM.ipynb\t\t     policy_eval.zip\n","'Entrenamientos antiguos sin logs'   policy_log_eval\n"," Entrenamientos_log_no_eval\t     policy_new2_log_eval\n"," MCR_TFM.ipynb\t\t\t     policy_new_log_eval\n"," multi_car_racing\t\t     TFM_new_pettingzoo_gym_cap.ipynb\n"," og_multi_car_racing\t\t     TFM_pettingzoo_gym_cap.ipynb\n"," policy2_log_eval\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2HubK-2G3_vH"},"outputs":[],"source":["env = multiwalker_v9.parallel_env(render_mode=\"rgb_array\")\n","\n","# #---------------------------------\n","# log_dir = \"./policy_log\"\n","# os.makedirs(log_dir, exist_ok=True)\n","# env = Monitor(env, log_dir)\n","# #---------------------------------\n","# env = ss.color_reduction_v0(env, mode='B')\n","# env = ss.black_death_v3(env)\n","# env = ss.resize_v1(env, x_size=84, y_size=84)\n","env = ss.frame_stack_v1(env, 3)\n","env = ss.pettingzoo_env_to_vec_env_v1(env)\n","env = ss.concat_vec_envs_v1(env, 8, num_cpus=4, base_class='stable_baselines3')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"foI4bTFGbQo1"},"outputs":[],"source":["from stable_baselines3.common.callbacks import EvalCallback\n","eval_callback = EvalCallback(env, best_model_save_path=\"./multiwalker_ddpg_log_eval/\",\n","                             log_path=\"./multiwalker_ddpg_log_eval/\", eval_freq=100,\n","                             deterministic=False, render=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ib9VPIVgec47","outputId":"060c8d2b-e2d6-449c-d826-baf0dfc246be","executionInfo":{"status":"ok","timestamp":1697647956634,"user_tz":-120,"elapsed":3888951,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n","---------------------------------\n","Eval num_timesteps=271992, episode_reward=-107.28 +/- 2.86\n","Episode length: 77.40 +/- 0.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 77.4     |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 271992   |\n","| train/             |          |\n","|    actor_loss      | 69.5     |\n","|    critic_loss     | 6.01     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 271872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3610     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2113     |\n","|    total_timesteps | 272328   |\n","| train/             |          |\n","|    actor_loss      | 69.2     |\n","|    critic_loss     | 5.97     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 272208   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3620     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2118     |\n","|    total_timesteps | 273096   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 9.12     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 272976   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3630     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2125     |\n","|    total_timesteps | 274176   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 5.42     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 274056   |\n","---------------------------------\n","Eval num_timesteps=274392, episode_reward=-100.31 +/- 1.52\n","Episode length: 76.60 +/- 4.41\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 76.6     |\n","|    mean_reward     | -100     |\n","| time/              |          |\n","|    total_timesteps | 274392   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 8.58     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 274272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3640     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2136     |\n","|    total_timesteps | 275328   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 7.14     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 275208   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3650     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2142     |\n","|    total_timesteps | 276240   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 8.53     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 276120   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3660     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2143     |\n","|    total_timesteps | 276408   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 8.7      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 276288   |\n","---------------------------------\n","Eval num_timesteps=276792, episode_reward=-105.42 +/- 1.43\n","Episode length: 72.80 +/- 35.27\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 72.8     |\n","|    mean_reward     | -105     |\n","| time/              |          |\n","|    total_timesteps | 276792   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 8.35     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 276672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3670     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2156     |\n","|    total_timesteps | 277848   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 8.67     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 277728   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3680     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2160     |\n","|    total_timesteps | 278496   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 8.67     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 278376   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3690     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2162     |\n","|    total_timesteps | 278808   |\n","| train/             |          |\n","|    actor_loss      | 67.8     |\n","|    critic_loss     | 10.8     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 278688   |\n","---------------------------------\n","Eval num_timesteps=279192, episode_reward=-107.94 +/- 0.22\n","Episode length: 45.80 +/- 1.47\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 45.8     |\n","|    mean_reward     | -108     |\n","| time/              |          |\n","|    total_timesteps | 279192   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 12.8     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 279072   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3700     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2172     |\n","|    total_timesteps | 279984   |\n","| train/             |          |\n","|    actor_loss      | 67.8     |\n","|    critic_loss     | 9.45     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 279864   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3710     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2179     |\n","|    total_timesteps | 280896   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 6.38     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 280776   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3720     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2180     |\n","|    total_timesteps | 281184   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 8.08     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 281064   |\n","---------------------------------\n","Eval num_timesteps=281592, episode_reward=-105.28 +/- 0.58\n","Episode length: 81.00 +/- 4.90\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 81       |\n","|    mean_reward     | -105     |\n","| time/              |          |\n","|    total_timesteps | 281592   |\n","| train/             |          |\n","|    actor_loss      | 68       |\n","|    critic_loss     | 10.8     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 281472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3730     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2192     |\n","|    total_timesteps | 282672   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 8.24     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 282552   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3740     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2199     |\n","|    total_timesteps | 283632   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 8.85     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 283512   |\n","---------------------------------\n","Eval num_timesteps=283992, episode_reward=-103.25 +/- 3.37\n","Episode length: 86.40 +/- 2.94\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 86.4     |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 283992   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 8.64     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 283872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3750     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2209     |\n","|    total_timesteps | 285024   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 8.67     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 284904   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3760     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2218     |\n","|    total_timesteps | 286080   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 7.12     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 285960   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3770     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2219     |\n","|    total_timesteps | 286224   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 7.44     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 286104   |\n","---------------------------------\n","Eval num_timesteps=286392, episode_reward=-106.22 +/- 0.54\n","Episode length: 69.00 +/- 19.60\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 69       |\n","|    mean_reward     | -106     |\n","| time/              |          |\n","|    total_timesteps | 286392   |\n","| train/             |          |\n","|    actor_loss      | 67.8     |\n","|    critic_loss     | 7.53     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 286272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3780     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2222     |\n","|    total_timesteps | 286536   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 9.18     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 286416   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3790     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2229     |\n","|    total_timesteps | 287640   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 7.67     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 287520   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3800     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2233     |\n","|    total_timesteps | 288144   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 7.94     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 288024   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3810     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2238     |\n","|    total_timesteps | 288648   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 13.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 288528   |\n","---------------------------------\n","Eval num_timesteps=288792, episode_reward=-106.55 +/- 3.64\n","Episode length: 62.20 +/- 18.62\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 62.2     |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 288792   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 9.33     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 288672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3820     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2240     |\n","|    total_timesteps | 288888   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 8.4      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 288768   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3830     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2246     |\n","|    total_timesteps | 289896   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 9.42     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 289776   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3840     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2255     |\n","|    total_timesteps | 290928   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 7.45     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 290808   |\n","---------------------------------\n","Eval num_timesteps=291192, episode_reward=-103.48 +/- 3.65\n","Episode length: 61.60 +/- 21.56\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 61.6     |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 291192   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 9.97     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 291072   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3850     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2259     |\n","|    total_timesteps | 291264   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 11       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 291144   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3860     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2262     |\n","|    total_timesteps | 291840   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 8        |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 291720   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3870     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2265     |\n","|    total_timesteps | 292344   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 6.37     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 292224   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3880     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2274     |\n","|    total_timesteps | 293544   |\n","| train/             |          |\n","|    actor_loss      | 69.3     |\n","|    critic_loss     | 11.3     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 293424   |\n","---------------------------------\n","Eval num_timesteps=293592, episode_reward=-103.56 +/- 3.50\n","Episode length: 58.80 +/- 15.68\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 58.8     |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 293592   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 8.89     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 293472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3890     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2278     |\n","|    total_timesteps | 293808   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 7.48     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 293688   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3900     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2283     |\n","|    total_timesteps | 294696   |\n","| train/             |          |\n","|    actor_loss      | 67.9     |\n","|    critic_loss     | 11.1     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 294576   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3910     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2290     |\n","|    total_timesteps | 295728   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 5.64     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 295608   |\n","---------------------------------\n","Eval num_timesteps=295992, episode_reward=-107.13 +/- 0.10\n","Episode length: 48.80 +/- 3.92\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 48.8     |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 295992   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 4.86     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 295872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3920     |\n","|    fps             | 128      |\n","|    time_elapsed    | 2298     |\n","|    total_timesteps | 296448   |\n","| train/             |          |\n","|    actor_loss      | 67.9     |\n","|    critic_loss     | 8.3      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 296328   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3930     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2300     |\n","|    total_timesteps | 296856   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 8.09     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 296736   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3940     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2306     |\n","|    total_timesteps | 297744   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 8.73     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 297624   |\n","---------------------------------\n","Eval num_timesteps=298392, episode_reward=-100.70 +/- 1.64\n","Episode length: 77.00 +/- 9.80\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 77       |\n","|    mean_reward     | -101     |\n","| time/              |          |\n","|    total_timesteps | 298392   |\n","| train/             |          |\n","|    actor_loss      | 67.9     |\n","|    critic_loss     | 7.31     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 298272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3950     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2313     |\n","|    total_timesteps | 298512   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 6.15     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 298392   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3960     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2321     |\n","|    total_timesteps | 299496   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 9.78     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 299376   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3970     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2324     |\n","|    total_timesteps | 299952   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 7.43     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 299832   |\n","---------------------------------\n","Eval num_timesteps=300792, episode_reward=-104.95 +/- 1.82\n","Episode length: 64.80 +/- 16.17\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 64.8     |\n","|    mean_reward     | -105     |\n","| time/              |          |\n","|    total_timesteps | 300792   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 9.26     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 300672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3980     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2331     |\n","|    total_timesteps | 300984   |\n","| train/             |          |\n","|    actor_loss      | 68       |\n","|    critic_loss     | 8.38     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 300864   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3990     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2333     |\n","|    total_timesteps | 301248   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 9.87     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 301128   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4000     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2342     |\n","|    total_timesteps | 302256   |\n","| train/             |          |\n","|    actor_loss      | 68.9     |\n","|    critic_loss     | 8.73     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 302136   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4010     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2344     |\n","|    total_timesteps | 302640   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 11       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 302520   |\n","---------------------------------\n","Eval num_timesteps=303192, episode_reward=-100.50 +/- 3.34\n","Episode length: 81.20 +/- 3.92\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 81.2     |\n","|    mean_reward     | -100     |\n","| time/              |          |\n","|    total_timesteps | 303192   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 6.12     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 303072   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4020     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2350     |\n","|    total_timesteps | 303336   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 5.96     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 303216   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4030     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2359     |\n","|    total_timesteps | 304440   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 8.79     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 304320   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4040     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2366     |\n","|    total_timesteps | 305424   |\n","| train/             |          |\n","|    actor_loss      | 67.9     |\n","|    critic_loss     | 8.72     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 305304   |\n","---------------------------------\n","Eval num_timesteps=305592, episode_reward=-106.04 +/- 3.46\n","Episode length: 58.60 +/- 14.21\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 58.6     |\n","|    mean_reward     | -106     |\n","| time/              |          |\n","|    total_timesteps | 305592   |\n","| train/             |          |\n","|    actor_loss      | 69       |\n","|    critic_loss     | 7.3      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 305472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4050     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2369     |\n","|    total_timesteps | 305832   |\n","| train/             |          |\n","|    actor_loss      | 69       |\n","|    critic_loss     | 9.09     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 305712   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4060     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2375     |\n","|    total_timesteps | 306600   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 8.43     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 306480   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4070     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2378     |\n","|    total_timesteps | 306960   |\n","| train/             |          |\n","|    actor_loss      | 68       |\n","|    critic_loss     | 5.96     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 306840   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4080     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2383     |\n","|    total_timesteps | 307680   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 10.3     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 307560   |\n","---------------------------------\n","Eval num_timesteps=307992, episode_reward=-106.25 +/- 4.29\n","Episode length: 66.00 +/- 19.60\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 66       |\n","|    mean_reward     | -106     |\n","| time/              |          |\n","|    total_timesteps | 307992   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 5.83     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 307872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4090     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2387     |\n","|    total_timesteps | 308160   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 8.55     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 308040   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4100     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2389     |\n","|    total_timesteps | 308424   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 7.71     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 308304   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4110     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2396     |\n","|    total_timesteps | 309360   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 8.86     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 309240   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4120     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2404     |\n","|    total_timesteps | 310344   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 6.51     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 310224   |\n","---------------------------------\n","Eval num_timesteps=310392, episode_reward=-104.14 +/- 0.39\n","Episode length: 100.20 +/- 5.88\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 100      |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 310392   |\n","| train/             |          |\n","|    actor_loss      | 69       |\n","|    critic_loss     | 6.99     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 310272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4130     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2408     |\n","|    total_timesteps | 310824   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 6.87     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 310704   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4140     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2414     |\n","|    total_timesteps | 311808   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 9.47     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 311688   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4150     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2422     |\n","|    total_timesteps | 312624   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 9.56     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 312504   |\n","---------------------------------\n","Eval num_timesteps=312792, episode_reward=-110.68 +/- 3.88\n","Episode length: 43.80 +/- 0.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 43.8     |\n","|    mean_reward     | -111     |\n","| time/              |          |\n","|    total_timesteps | 312792   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 10.6     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 312672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4160     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2424     |\n","|    total_timesteps | 312816   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 9.4      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 312696   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4170     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2429     |\n","|    total_timesteps | 313776   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 8.76     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 313656   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4180     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2436     |\n","|    total_timesteps | 314760   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 8.34     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 314640   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4190     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2439     |\n","|    total_timesteps | 315048   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 9.69     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 314928   |\n","---------------------------------\n","Eval num_timesteps=315192, episode_reward=-104.68 +/- 1.73\n","Episode length: 67.40 +/- 19.11\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 67.4     |\n","|    mean_reward     | -105     |\n","| time/              |          |\n","|    total_timesteps | 315192   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 7.99     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 315072   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4200     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2444     |\n","|    total_timesteps | 315456   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 13.8     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 315336   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4210     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2450     |\n","|    total_timesteps | 316536   |\n","| train/             |          |\n","|    actor_loss      | 69       |\n","|    critic_loss     | 9.41     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 316416   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4220     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2453     |\n","|    total_timesteps | 316992   |\n","| train/             |          |\n","|    actor_loss      | 68.9     |\n","|    critic_loss     | 5.92     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 316872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4230     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2457     |\n","|    total_timesteps | 317472   |\n","| train/             |          |\n","|    actor_loss      | 69.1     |\n","|    critic_loss     | 8.39     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 317352   |\n","---------------------------------\n","Eval num_timesteps=317592, episode_reward=-106.62 +/- 0.42\n","Episode length: 61.80 +/- 20.58\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 61.8     |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 317592   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 10.3     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 317472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4240     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2468     |\n","|    total_timesteps | 318768   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 10.9     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 318648   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4250     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2473     |\n","|    total_timesteps | 319512   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 7.18     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 319392   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4260     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2476     |\n","|    total_timesteps | 319896   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 8.29     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 319776   |\n","---------------------------------\n","Eval num_timesteps=319992, episode_reward=-105.36 +/- 0.44\n","Episode length: 90.40 +/- 4.41\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 90.4     |\n","|    mean_reward     | -105     |\n","| time/              |          |\n","|    total_timesteps | 319992   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 9.12     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 319872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4270     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2487     |\n","|    total_timesteps | 321120   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 6.72     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 321000   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4280     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2491     |\n","|    total_timesteps | 321768   |\n","| train/             |          |\n","|    actor_loss      | 69       |\n","|    critic_loss     | 7.31     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 321648   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4290     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2494     |\n","|    total_timesteps | 322320   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 9.15     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 322200   |\n","---------------------------------\n","Eval num_timesteps=322392, episode_reward=-106.19 +/- 0.18\n","Episode length: 58.40 +/- 15.19\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 58.4     |\n","|    mean_reward     | -106     |\n","| time/              |          |\n","|    total_timesteps | 322392   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 6.3      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 322272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4300     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2499     |\n","|    total_timesteps | 322728   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 7.65     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 322608   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4310     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2508     |\n","|    total_timesteps | 323832   |\n","| train/             |          |\n","|    actor_loss      | 68.9     |\n","|    critic_loss     | 6.99     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 323712   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4320     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2513     |\n","|    total_timesteps | 324624   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 8.64     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 324504   |\n","---------------------------------\n","Eval num_timesteps=324792, episode_reward=-103.92 +/- 0.11\n","Episode length: 83.40 +/- 0.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 83.4     |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 324792   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 6.88     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 324672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4330     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2522     |\n","|    total_timesteps | 325680   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 8.74     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 325560   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4340     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2525     |\n","|    total_timesteps | 326016   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 8.54     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 325896   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4350     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2531     |\n","|    total_timesteps | 326976   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 5.27     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 326856   |\n","---------------------------------\n","Eval num_timesteps=327192, episode_reward=-104.13 +/- 2.20\n","Episode length: 87.00 +/- 14.70\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 87       |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 327192   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 6.5      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 327072   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4360     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2540     |\n","|    total_timesteps | 328152   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 6.44     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 328032   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4370     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2548     |\n","|    total_timesteps | 329112   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 7.1      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 328992   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4380     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2549     |\n","|    total_timesteps | 329208   |\n","| train/             |          |\n","|    actor_loss      | 67.9     |\n","|    critic_loss     | 8.74     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 329088   |\n","---------------------------------\n","Eval num_timesteps=329592, episode_reward=-101.24 +/- 1.39\n","Episode length: 93.60 +/- 1.96\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 93.6     |\n","|    mean_reward     | -101     |\n","| time/              |          |\n","|    total_timesteps | 329592   |\n","| train/             |          |\n","|    actor_loss      | 68       |\n","|    critic_loss     | 11       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 329472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4390     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2556     |\n","|    total_timesteps | 330168   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 10.3     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 330048   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4400     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2558     |\n","|    total_timesteps | 330480   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 8.08     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 330360   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4410     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2563     |\n","|    total_timesteps | 331104   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 6.29     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 330984   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4420     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2568     |\n","|    total_timesteps | 331752   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 10.3     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 331632   |\n","---------------------------------\n","Eval num_timesteps=331992, episode_reward=-106.50 +/- 0.56\n","Episode length: 99.20 +/- 6.37\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 99.2     |\n","|    mean_reward     | -106     |\n","| time/              |          |\n","|    total_timesteps | 331992   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 6.73     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 331872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4430     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2576     |\n","|    total_timesteps | 332808   |\n","| train/             |          |\n","|    actor_loss      | 67.9     |\n","|    critic_loss     | 5.74     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 332688   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4440     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2578     |\n","|    total_timesteps | 333144   |\n","| train/             |          |\n","|    actor_loss      | 68.9     |\n","|    critic_loss     | 7.1      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 333024   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4450     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2586     |\n","|    total_timesteps | 333960   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 7.88     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 333840   |\n","---------------------------------\n","Eval num_timesteps=334392, episode_reward=-105.28 +/- 1.94\n","Episode length: 62.00 +/- 14.70\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 62       |\n","|    mean_reward     | -105     |\n","| time/              |          |\n","|    total_timesteps | 334392   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 6.07     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 334272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4460     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2592     |\n","|    total_timesteps | 334752   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 8.4      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 334632   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4470     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2596     |\n","|    total_timesteps | 335568   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 7.39     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 335448   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4480     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2605     |\n","|    total_timesteps | 336552   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 6.98     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 336432   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4490     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2606     |\n","|    total_timesteps | 336696   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 10       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 336576   |\n","---------------------------------\n","Eval num_timesteps=336792, episode_reward=-106.42 +/- 0.44\n","Episode length: 94.00 +/- 4.90\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 94       |\n","|    mean_reward     | -106     |\n","| time/              |          |\n","|    total_timesteps | 336792   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 6.42     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 336672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4500     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2614     |\n","|    total_timesteps | 337752   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 6.24     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 337632   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4510     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2619     |\n","|    total_timesteps | 338496   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 6.54     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 338376   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4520     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2622     |\n","|    total_timesteps | 338880   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 8.13     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 338760   |\n","---------------------------------\n","Eval num_timesteps=339192, episode_reward=-107.57 +/- 0.74\n","Episode length: 48.20 +/- 1.47\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 48.2     |\n","|    mean_reward     | -108     |\n","| time/              |          |\n","|    total_timesteps | 339192   |\n","| train/             |          |\n","|    actor_loss      | 68.9     |\n","|    critic_loss     | 8.87     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 339072   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4530     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2631     |\n","|    total_timesteps | 339912   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 8.7      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 339792   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4540     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2632     |\n","|    total_timesteps | 340176   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 11       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 340056   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4550     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2638     |\n","|    total_timesteps | 341160   |\n","| train/             |          |\n","|    actor_loss      | 69.3     |\n","|    critic_loss     | 11.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 341040   |\n","---------------------------------\n","Eval num_timesteps=341592, episode_reward=-105.90 +/- 3.14\n","Episode length: 94.80 +/- 1.47\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 94.8     |\n","|    mean_reward     | -106     |\n","| time/              |          |\n","|    total_timesteps | 341592   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 10.3     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 341472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4560     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2649     |\n","|    total_timesteps | 342240   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 8.32     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 342120   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4570     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2653     |\n","|    total_timesteps | 342840   |\n","| train/             |          |\n","|    actor_loss      | 69       |\n","|    critic_loss     | 6.97     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 342720   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4580     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2657     |\n","|    total_timesteps | 343440   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 7.37     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 343320   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4590     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2660     |\n","|    total_timesteps | 343896   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 5.63     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 343776   |\n","---------------------------------\n","Eval num_timesteps=343992, episode_reward=-106.91 +/- 0.14\n","Episode length: 66.00 +/- 26.94\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 66       |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 343992   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 4.41     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 343872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4600     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2670     |\n","|    total_timesteps | 344952   |\n","| train/             |          |\n","|    actor_loss      | 69.2     |\n","|    critic_loss     | 8.47     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 344832   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4610     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2676     |\n","|    total_timesteps | 345792   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 8.18     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 345672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4620     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2678     |\n","|    total_timesteps | 346224   |\n","| train/             |          |\n","|    actor_loss      | 69.2     |\n","|    critic_loss     | 10.9     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 346104   |\n","---------------------------------\n","Eval num_timesteps=346392, episode_reward=-100.87 +/- 2.18\n","Episode length: 88.00 +/- 2.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 88       |\n","|    mean_reward     | -101     |\n","| time/              |          |\n","|    total_timesteps | 346392   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 9.15     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 346272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4630     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2688     |\n","|    total_timesteps | 347208   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 7.87     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 347088   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4640     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2690     |\n","|    total_timesteps | 347424   |\n","| train/             |          |\n","|    actor_loss      | 69.1     |\n","|    critic_loss     | 7.23     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 347304   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4650     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2697     |\n","|    total_timesteps | 348552   |\n","| train/             |          |\n","|    actor_loss      | 69       |\n","|    critic_loss     | 7.21     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 348432   |\n","---------------------------------\n","Eval num_timesteps=348792, episode_reward=-103.44 +/- 1.63\n","Episode length: 87.00 +/- 7.35\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 87       |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 348792   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 6.77     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 348672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4660     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2705     |\n","|    total_timesteps | 349488   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 6.44     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 349368   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4670     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2709     |\n","|    total_timesteps | 349920   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 10.1     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 349800   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4680     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2713     |\n","|    total_timesteps | 350616   |\n","| train/             |          |\n","|    actor_loss      | 69.1     |\n","|    critic_loss     | 8.37     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 350496   |\n","---------------------------------\n","Eval num_timesteps=351192, episode_reward=-106.80 +/- 5.00\n","Episode length: 93.40 +/- 6.86\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 93.4     |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 351192   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 5.47     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 351072   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4690     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2726     |\n","|    total_timesteps | 352296   |\n","| train/             |          |\n","|    actor_loss      | 69.3     |\n","|    critic_loss     | 7.21     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 352176   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4700     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2729     |\n","|    total_timesteps | 352632   |\n","| train/             |          |\n","|    actor_loss      | 69.2     |\n","|    critic_loss     | 8.29     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 352512   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4710     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2732     |\n","|    total_timesteps | 353088   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 7.24     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 352968   |\n","---------------------------------\n","Eval num_timesteps=353592, episode_reward=-103.34 +/- 2.96\n","Episode length: 71.40 +/- 16.66\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 71.4     |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 353592   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 9.09     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 353472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4720     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2738     |\n","|    total_timesteps | 353832   |\n","| train/             |          |\n","|    actor_loss      | 69.2     |\n","|    critic_loss     | 7.05     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 353712   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4730     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2743     |\n","|    total_timesteps | 354624   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 8.37     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 354504   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4740     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2751     |\n","|    total_timesteps | 355608   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 7.25     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 355488   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4750     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2753     |\n","|    total_timesteps | 355920   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 9.47     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 355800   |\n","---------------------------------\n","Eval num_timesteps=355992, episode_reward=-103.01 +/- 0.18\n","Episode length: 92.40 +/- 4.41\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 92.4     |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 355992   |\n","| train/             |          |\n","|    actor_loss      | 68.9     |\n","|    critic_loss     | 8.07     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 355872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4760     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2760     |\n","|    total_timesteps | 356856   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 12.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 356736   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4770     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2765     |\n","|    total_timesteps | 357504   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 8.95     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 357384   |\n","---------------------------------\n","Eval num_timesteps=358392, episode_reward=-105.15 +/- 1.68\n","Episode length: 86.20 +/- 10.78\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 86.2     |\n","|    mean_reward     | -105     |\n","| time/              |          |\n","|    total_timesteps | 358392   |\n","| train/             |          |\n","|    actor_loss      | 69.2     |\n","|    critic_loss     | 9.18     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 358272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4780     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2780     |\n","|    total_timesteps | 359496   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 7.67     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 359376   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4790     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2785     |\n","|    total_timesteps | 360048   |\n","| train/             |          |\n","|    actor_loss      | 69.2     |\n","|    critic_loss     | 5.96     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 359928   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4800     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2790     |\n","|    total_timesteps | 360552   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 6.97     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 360432   |\n","---------------------------------\n","Eval num_timesteps=360792, episode_reward=-105.16 +/- 1.03\n","Episode length: 87.60 +/- 1.96\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 87.6     |\n","|    mean_reward     | -105     |\n","| time/              |          |\n","|    total_timesteps | 360792   |\n","| train/             |          |\n","|    actor_loss      | 69       |\n","|    critic_loss     | 9.03     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 360672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4810     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2800     |\n","|    total_timesteps | 361920   |\n","| train/             |          |\n","|    actor_loss      | 68.9     |\n","|    critic_loss     | 11.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 361800   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4820     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2806     |\n","|    total_timesteps | 362832   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 6.82     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 362712   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4830     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2809     |\n","|    total_timesteps | 363168   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 8.02     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 363048   |\n","---------------------------------\n","Eval num_timesteps=363192, episode_reward=-106.82 +/- 1.87\n","Episode length: 63.60 +/- 21.56\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 63.6     |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 363192   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 8.95     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 363072   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4840     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2817     |\n","|    total_timesteps | 364080   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 8.21     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 363960   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4850     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2818     |\n","|    total_timesteps | 364272   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 11       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 364152   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4860     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2825     |\n","|    total_timesteps | 365304   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 8.9      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 365184   |\n","---------------------------------\n","Eval num_timesteps=365592, episode_reward=-105.11 +/- 4.78\n","Episode length: 62.80 +/- 16.17\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 62.8     |\n","|    mean_reward     | -105     |\n","| time/              |          |\n","|    total_timesteps | 365592   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 7.4      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 365472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4870     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2837     |\n","|    total_timesteps | 366648   |\n","| train/             |          |\n","|    actor_loss      | 68       |\n","|    critic_loss     | 9.29     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 366528   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4880     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2839     |\n","|    total_timesteps | 367080   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 7.87     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 366960   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4890     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2844     |\n","|    total_timesteps | 367824   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 9.3      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 367704   |\n","---------------------------------\n","Eval num_timesteps=367992, episode_reward=-98.41 +/- 0.25\n","Episode length: 76.80 +/- 0.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 76.8     |\n","|    mean_reward     | -98.4    |\n","| time/              |          |\n","|    total_timesteps | 367992   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 9.37     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 367872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4900     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2853     |\n","|    total_timesteps | 368616   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 9.69     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 368496   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4910     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2858     |\n","|    total_timesteps | 369456   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 10.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 369336   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4920     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2861     |\n","|    total_timesteps | 370008   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 9.63     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 369888   |\n","---------------------------------\n","Eval num_timesteps=370392, episode_reward=-106.75 +/- 3.05\n","Episode length: 87.80 +/- 3.92\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 87.8     |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 370392   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 13.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 370272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4930     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2873     |\n","|    total_timesteps | 371208   |\n","| train/             |          |\n","|    actor_loss      | 67.6     |\n","|    critic_loss     | 4.63     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 371088   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4940     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2878     |\n","|    total_timesteps | 372024   |\n","| train/             |          |\n","|    actor_loss      | 67.8     |\n","|    critic_loss     | 6.03     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 371904   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4950     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2881     |\n","|    total_timesteps | 372408   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 9.43     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 372288   |\n","---------------------------------\n","Eval num_timesteps=372792, episode_reward=-103.41 +/- 0.10\n","Episode length: 76.80 +/- 3.92\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 76.8     |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 372792   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 5.57     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 372672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4960     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2885     |\n","|    total_timesteps | 372960   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 8.58     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 372840   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4970     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2886     |\n","|    total_timesteps | 373128   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 6.09     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 373008   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4980     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2894     |\n","|    total_timesteps | 374016   |\n","| train/             |          |\n","|    actor_loss      | 67.9     |\n","|    critic_loss     | 8.92     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 373896   |\n","---------------------------------\n","Eval num_timesteps=375192, episode_reward=-99.97 +/- 2.84\n","Episode length: 85.40 +/- 10.29\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 85.4     |\n","|    mean_reward     | -100     |\n","| time/              |          |\n","|    total_timesteps | 375192   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 6.38     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 375072   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4990     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2906     |\n","|    total_timesteps | 375768   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 8.54     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 375648   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5000     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2915     |\n","|    total_timesteps | 376752   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 8.21     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 376632   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5010     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2917     |\n","|    total_timesteps | 377016   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 6.22     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 376896   |\n","---------------------------------\n","Eval num_timesteps=377592, episode_reward=-105.11 +/- 2.02\n","Episode length: 58.80 +/- 10.78\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 58.8     |\n","|    mean_reward     | -105     |\n","| time/              |          |\n","|    total_timesteps | 377592   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 7.17     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 377472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5020     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2923     |\n","|    total_timesteps | 377856   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 6.56     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 377736   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5030     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2925     |\n","|    total_timesteps | 378216   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 7.51     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 378096   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5040     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2931     |\n","|    total_timesteps | 378960   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 8.28     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 378840   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5050     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2938     |\n","|    total_timesteps | 379848   |\n","| train/             |          |\n","|    actor_loss      | 67.9     |\n","|    critic_loss     | 7.68     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 379728   |\n","---------------------------------\n","Eval num_timesteps=379992, episode_reward=-108.03 +/- 3.83\n","Episode length: 62.40 +/- 24.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 62.4     |\n","|    mean_reward     | -108     |\n","| time/              |          |\n","|    total_timesteps | 379992   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 9.93     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 379872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5060     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2943     |\n","|    total_timesteps | 380568   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 8.98     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 380448   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5070     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2946     |\n","|    total_timesteps | 380976   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 7.88     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 380856   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5080     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2954     |\n","|    total_timesteps | 381936   |\n","| train/             |          |\n","|    actor_loss      | 67.4     |\n","|    critic_loss     | 7.27     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 381816   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5090     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2955     |\n","|    total_timesteps | 382032   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 8.74     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 381912   |\n","---------------------------------\n","Eval num_timesteps=382392, episode_reward=-103.60 +/- 3.62\n","Episode length: 84.60 +/- 7.84\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 84.6     |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 382392   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 6.83     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 382272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5100     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2959     |\n","|    total_timesteps | 382416   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 6.06     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 382296   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5110     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2966     |\n","|    total_timesteps | 383616   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 6.54     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 383496   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5120     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2971     |\n","|    total_timesteps | 384240   |\n","| train/             |          |\n","|    actor_loss      | 67.9     |\n","|    critic_loss     | 11.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 384120   |\n","---------------------------------\n","Eval num_timesteps=384792, episode_reward=-108.06 +/- 1.80\n","Episode length: 47.80 +/- 0.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 47.8     |\n","|    mean_reward     | -108     |\n","| time/              |          |\n","|    total_timesteps | 384792   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 7.43     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 384672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5130     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2980     |\n","|    total_timesteps | 385344   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 5.57     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 385224   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5140     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2984     |\n","|    total_timesteps | 385944   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 7.62     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 385824   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5150     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2988     |\n","|    total_timesteps | 386616   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 7.4      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 386496   |\n","---------------------------------\n","Eval num_timesteps=387192, episode_reward=-104.01 +/- 0.49\n","Episode length: 83.80 +/- 10.78\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 83.8     |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 387192   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 5.84     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 387072   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5160     |\n","|    fps             | 129      |\n","|    time_elapsed    | 2999     |\n","|    total_timesteps | 387720   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 5.44     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 387600   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5170     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3005     |\n","|    total_timesteps | 388704   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 11.6     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 388584   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5180     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3009     |\n","|    total_timesteps | 389328   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 7.77     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 389208   |\n","---------------------------------\n","Eval num_timesteps=389592, episode_reward=-106.62 +/- 0.04\n","Episode length: 44.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 44       |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 389592   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 7.98     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 389472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5190     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3022     |\n","|    total_timesteps | 390840   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 8.57     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 390720   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5200     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3028     |\n","|    total_timesteps | 391632   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 6.46     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 391512   |\n","---------------------------------\n","Eval num_timesteps=391992, episode_reward=-109.20 +/- 1.90\n","Episode length: 45.40 +/- 1.96\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 45.4     |\n","|    mean_reward     | -109     |\n","| time/              |          |\n","|    total_timesteps | 391992   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 11.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 391872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5210     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3040     |\n","|    total_timesteps | 392712   |\n","| train/             |          |\n","|    actor_loss      | 69.3     |\n","|    critic_loss     | 8.41     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 392592   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5220     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3044     |\n","|    total_timesteps | 393048   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 5.38     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 392928   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5230     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3049     |\n","|    total_timesteps | 393888   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 10.3     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 393768   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5240     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3052     |\n","|    total_timesteps | 394344   |\n","| train/             |          |\n","|    actor_loss      | 68.9     |\n","|    critic_loss     | 8.97     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 394224   |\n","---------------------------------\n","Eval num_timesteps=394392, episode_reward=-103.97 +/- 0.61\n","Episode length: 77.80 +/- 3.43\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 77.8     |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 394392   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 8.83     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 394272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5250     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3056     |\n","|    total_timesteps | 394752   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 8.32     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 394632   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5260     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3065     |\n","|    total_timesteps | 395808   |\n","| train/             |          |\n","|    actor_loss      | 69.1     |\n","|    critic_loss     | 9.17     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 395688   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5270     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3069     |\n","|    total_timesteps | 396408   |\n","| train/             |          |\n","|    actor_loss      | 68       |\n","|    critic_loss     | 9.17     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 396288   |\n","---------------------------------\n","Eval num_timesteps=396792, episode_reward=-108.52 +/- 0.04\n","Episode length: 44.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 44       |\n","|    mean_reward     | -109     |\n","| time/              |          |\n","|    total_timesteps | 396792   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 8.44     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 396672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5280     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3078     |\n","|    total_timesteps | 397680   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 6.2      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 397560   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5290     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3082     |\n","|    total_timesteps | 398088   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 6.1      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 397968   |\n","---------------------------------\n","Eval num_timesteps=399192, episode_reward=-103.58 +/- 1.09\n","Episode length: 78.80 +/- 10.78\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 78.8     |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 399192   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 8.99     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 399072   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5300     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3098     |\n","|    total_timesteps | 400224   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 9.04     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 400104   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5310     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3105     |\n","|    total_timesteps | 400992   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 7.26     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 400872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5320     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3107     |\n","|    total_timesteps | 401328   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 7.69     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 401208   |\n","---------------------------------\n","Eval num_timesteps=401592, episode_reward=-104.73 +/- 3.06\n","Episode length: 66.20 +/- 18.13\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 66.2     |\n","|    mean_reward     | -105     |\n","| time/              |          |\n","|    total_timesteps | 401592   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 6.66     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 401472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5330     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3111     |\n","|    total_timesteps | 401784   |\n","| train/             |          |\n","|    actor_loss      | 69       |\n","|    critic_loss     | 8.78     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 401664   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5340     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3117     |\n","|    total_timesteps | 402696   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 8.06     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 402576   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5350     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3126     |\n","|    total_timesteps | 403728   |\n","| train/             |          |\n","|    actor_loss      | 67.7     |\n","|    critic_loss     | 6.48     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 403608   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5360     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3127     |\n","|    total_timesteps | 403872   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 10.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 403752   |\n","---------------------------------\n","Eval num_timesteps=403992, episode_reward=-103.48 +/- 0.91\n","Episode length: 86.60 +/- 10.29\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 86.6     |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 403992   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 5.99     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 403872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5370     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3133     |\n","|    total_timesteps | 404616   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 10.6     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 404496   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5380     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3142     |\n","|    total_timesteps | 405816   |\n","| train/             |          |\n","|    actor_loss      | 67.8     |\n","|    critic_loss     | 7.54     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 405696   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5390     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3144     |\n","|    total_timesteps | 406008   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 6.93     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 405888   |\n","---------------------------------\n","Eval num_timesteps=406392, episode_reward=-104.22 +/- 1.49\n","Episode length: 73.00 +/- 2.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 73       |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 406392   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 7.68     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 406272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5400     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3149     |\n","|    total_timesteps | 406512   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 6        |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 406392   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5410     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3155     |\n","|    total_timesteps | 407544   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 8.69     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 407424   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5420     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3161     |\n","|    total_timesteps | 408336   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 9.81     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 408216   |\n","---------------------------------\n","Eval num_timesteps=408792, episode_reward=-106.99 +/- 0.58\n","Episode length: 89.80 +/- 11.27\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 89.8     |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 408792   |\n","| train/             |          |\n","|    actor_loss      | 68.9     |\n","|    critic_loss     | 8.01     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 408672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5430     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3168     |\n","|    total_timesteps | 408888   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 9.28     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 408768   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5440     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3172     |\n","|    total_timesteps | 409560   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 7.29     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 409440   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5450     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3177     |\n","|    total_timesteps | 410400   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 13.7     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 410280   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5460     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3180     |\n","|    total_timesteps | 410712   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 8.2      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 410592   |\n","---------------------------------\n","Eval num_timesteps=411192, episode_reward=-103.40 +/- 4.04\n","Episode length: 58.40 +/- 15.19\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 58.4     |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 411192   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 9.33     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 411072   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5470     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3193     |\n","|    total_timesteps | 412320   |\n","| train/             |          |\n","|    actor_loss      | 69.1     |\n","|    critic_loss     | 6.26     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 412200   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5480     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3196     |\n","|    total_timesteps | 412728   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 8.26     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 412608   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5490     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3202     |\n","|    total_timesteps | 413472   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 6.47     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 413352   |\n","---------------------------------\n","Eval num_timesteps=413592, episode_reward=-107.20 +/- 2.12\n","Episode length: 63.80 +/- 20.58\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 63.8     |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 413592   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 9.97     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 413472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5500     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3211     |\n","|    total_timesteps | 414456   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 7.75     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 414336   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5510     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3214     |\n","|    total_timesteps | 414840   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 12       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 414720   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5520     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3219     |\n","|    total_timesteps | 415704   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 10       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 415584   |\n","---------------------------------\n","Eval num_timesteps=415992, episode_reward=-100.00 +/- 0.49\n","Episode length: 86.00 +/- 9.80\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 86       |\n","|    mean_reward     | -100     |\n","| time/              |          |\n","|    total_timesteps | 415992   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 6.75     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 415872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5530     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3230     |\n","|    total_timesteps | 416712   |\n","| train/             |          |\n","|    actor_loss      | 69.7     |\n","|    critic_loss     | 7.08     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 416592   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5540     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3232     |\n","|    total_timesteps | 417048   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 9.3      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 416928   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5550     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3239     |\n","|    total_timesteps | 418152   |\n","| train/             |          |\n","|    actor_loss      | 69.1     |\n","|    critic_loss     | 7.89     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 418032   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5560     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3240     |\n","|    total_timesteps | 418296   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 5.97     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 418176   |\n","---------------------------------\n","Eval num_timesteps=418392, episode_reward=-102.50 +/- 0.22\n","Episode length: 69.60 +/- 0.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 69.6     |\n","|    mean_reward     | -102     |\n","| time/              |          |\n","|    total_timesteps | 418392   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 9.83     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 418272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5570     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3248     |\n","|    total_timesteps | 419016   |\n","| train/             |          |\n","|    actor_loss      | 69       |\n","|    critic_loss     | 10.3     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 418896   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5580     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3252     |\n","|    total_timesteps | 419496   |\n","| train/             |          |\n","|    actor_loss      | 68.9     |\n","|    critic_loss     | 8.6      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 419376   |\n","---------------------------------\n","Eval num_timesteps=420792, episode_reward=-103.78 +/- 2.37\n","Episode length: 83.00 +/- 14.70\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 83       |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 420792   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 7.54     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 420672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5590     |\n","|    fps             | 129      |\n","|    time_elapsed    | 3262     |\n","|    total_timesteps | 420936   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 11.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 420816   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5600     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3270     |\n","|    total_timesteps | 421776   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 7.57     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 421656   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5610     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3274     |\n","|    total_timesteps | 422232   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 10.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 422112   |\n","---------------------------------\n","Eval num_timesteps=423192, episode_reward=-107.21 +/- 1.60\n","Episode length: 64.40 +/- 22.54\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 64.4     |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 423192   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 8.78     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 423072   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5620     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3286     |\n","|    total_timesteps | 423936   |\n","| train/             |          |\n","|    actor_loss      | 69       |\n","|    critic_loss     | 10.9     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 423816   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5630     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3294     |\n","|    total_timesteps | 424752   |\n","| train/             |          |\n","|    actor_loss      | 69.1     |\n","|    critic_loss     | 8.06     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 424632   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5640     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3296     |\n","|    total_timesteps | 425136   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 8.07     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 425016   |\n","---------------------------------\n","Eval num_timesteps=425592, episode_reward=-104.42 +/- 2.07\n","Episode length: 75.40 +/- 26.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 75.4     |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 425592   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 7.66     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 425472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5650     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3303     |\n","|    total_timesteps | 426120   |\n","| train/             |          |\n","|    actor_loss      | 69       |\n","|    critic_loss     | 8.92     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 426000   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5660     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3306     |\n","|    total_timesteps | 426456   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 9.12     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 426336   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5670     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3315     |\n","|    total_timesteps | 427488   |\n","| train/             |          |\n","|    actor_loss      | 68       |\n","|    critic_loss     | 12.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 427368   |\n","---------------------------------\n","Eval num_timesteps=427992, episode_reward=-106.42 +/- 2.45\n","Episode length: 59.60 +/- 19.11\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 59.6     |\n","|    mean_reward     | -106     |\n","| time/              |          |\n","|    total_timesteps | 427992   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 7.99     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 427872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5680     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3320     |\n","|    total_timesteps | 428160   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 9.58     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 428040   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5690     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3327     |\n","|    total_timesteps | 429072   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 6.36     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 428952   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5700     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3335     |\n","|    total_timesteps | 430008   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 7.92     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 429888   |\n","---------------------------------\n","Eval num_timesteps=430392, episode_reward=-103.91 +/- 2.40\n","Episode length: 76.00 +/- 7.35\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 76       |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 430392   |\n","| train/             |          |\n","|    actor_loss      | 68       |\n","|    critic_loss     | 10       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 430272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5710     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3338     |\n","|    total_timesteps | 430416   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 6.7      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 430296   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5720     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3340     |\n","|    total_timesteps | 430680   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 9.92     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 430560   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5730     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3345     |\n","|    total_timesteps | 431472   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 8.17     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 431352   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5740     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3353     |\n","|    total_timesteps | 432360   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 8.13     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 432240   |\n","---------------------------------\n","Eval num_timesteps=432792, episode_reward=-103.38 +/- 1.57\n","Episode length: 76.60 +/- 4.41\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 76.6     |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 432792   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 10.9     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 432672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5750     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3359     |\n","|    total_timesteps | 433008   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 8.22     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 432888   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5760     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3364     |\n","|    total_timesteps | 433824   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 10.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 433704   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5770     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3371     |\n","|    total_timesteps | 434736   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 7.14     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 434616   |\n","---------------------------------\n","Eval num_timesteps=435192, episode_reward=-102.50 +/- 1.95\n","Episode length: 95.40 +/- 7.84\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 95.4     |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 435192   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 7.26     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 435072   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5780     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3378     |\n","|    total_timesteps | 435312   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 11.3     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 435192   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5790     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3381     |\n","|    total_timesteps | 435840   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 3.44     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 435720   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5800     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3386     |\n","|    total_timesteps | 436608   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 7.69     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 436488   |\n","---------------------------------\n","Eval num_timesteps=437592, episode_reward=-104.82 +/- 5.07\n","Episode length: 57.80 +/- 13.23\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 57.8     |\n","|    mean_reward     | -105     |\n","| time/              |          |\n","|    total_timesteps | 437592   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 7.84     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 437472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5810     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3398     |\n","|    total_timesteps | 437904   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 8.71     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 437784   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5820     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3399     |\n","|    total_timesteps | 438120   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 8.45     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 438000   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5830     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3406     |\n","|    total_timesteps | 439200   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 9.45     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 439080   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5840     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3411     |\n","|    total_timesteps | 439776   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 7.48     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 439656   |\n","---------------------------------\n","Eval num_timesteps=439992, episode_reward=-103.59 +/- 1.94\n","Episode length: 94.00 +/- 4.90\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 94       |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 439992   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 10.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 439872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5850     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3423     |\n","|    total_timesteps | 441096   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 10.1     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 440976   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5860     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3426     |\n","|    total_timesteps | 441720   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 6.54     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 441600   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5870     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3430     |\n","|    total_timesteps | 442200   |\n","| train/             |          |\n","|    actor_loss      | 68.9     |\n","|    critic_loss     | 7.06     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 442080   |\n","---------------------------------\n","Eval num_timesteps=442392, episode_reward=-106.78 +/- 3.78\n","Episode length: 73.60 +/- 24.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 73.6     |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 442392   |\n","| train/             |          |\n","|    actor_loss      | 69       |\n","|    critic_loss     | 6.29     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 442272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5880     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3440     |\n","|    total_timesteps | 443160   |\n","| train/             |          |\n","|    actor_loss      | 69       |\n","|    critic_loss     | 9.99     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 443040   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5890     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3446     |\n","|    total_timesteps | 444072   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 10.9     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 443952   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5900     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3448     |\n","|    total_timesteps | 444408   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 9.45     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 444288   |\n","---------------------------------\n","Eval num_timesteps=444792, episode_reward=-108.10 +/- 2.52\n","Episode length: 72.60 +/- 24.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 72.6     |\n","|    mean_reward     | -108     |\n","| time/              |          |\n","|    total_timesteps | 444792   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 7.1      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 444672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5910     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3454     |\n","|    total_timesteps | 444912   |\n","| train/             |          |\n","|    actor_loss      | 67.8     |\n","|    critic_loss     | 6.24     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 444792   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5920     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3465     |\n","|    total_timesteps | 446376   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 7.76     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 446256   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5930     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3467     |\n","|    total_timesteps | 446760   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 12.1     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 446640   |\n","---------------------------------\n","Eval num_timesteps=447192, episode_reward=-107.95 +/- 0.61\n","Episode length: 44.40 +/- 0.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 44.4     |\n","|    mean_reward     | -108     |\n","| time/              |          |\n","|    total_timesteps | 447192   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 11.9     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 447072   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5940     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3478     |\n","|    total_timesteps | 447960   |\n","| train/             |          |\n","|    actor_loss      | 67.7     |\n","|    critic_loss     | 11.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 447840   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5950     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3482     |\n","|    total_timesteps | 448512   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 9.83     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 448392   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5960     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3486     |\n","|    total_timesteps | 449040   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 8.16     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 448920   |\n","---------------------------------\n","Eval num_timesteps=449592, episode_reward=-103.78 +/- 0.63\n","Episode length: 80.80 +/- 8.33\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 80.8     |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 449592   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 7.53     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 449472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5970     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3492     |\n","|    total_timesteps | 449880   |\n","| train/             |          |\n","|    actor_loss      | 68.9     |\n","|    critic_loss     | 12.6     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 449760   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5980     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3497     |\n","|    total_timesteps | 450360   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 7.18     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 450240   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5990     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3504     |\n","|    total_timesteps | 451248   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 7.51     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 451128   |\n","---------------------------------\n","Eval num_timesteps=451992, episode_reward=-103.28 +/- 0.32\n","Episode length: 73.00 +/- 4.90\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 73       |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 451992   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 10       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 451872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6000     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3509     |\n","|    total_timesteps | 452016   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 8.74     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 451896   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6010     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3517     |\n","|    total_timesteps | 453024   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 5.65     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 452904   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6020     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3523     |\n","|    total_timesteps | 453648   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 7.4      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 453528   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6030     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3525     |\n","|    total_timesteps | 454032   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 5.8      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 453912   |\n","---------------------------------\n","Eval num_timesteps=454392, episode_reward=-108.94 +/- 1.41\n","Episode length: 107.60 +/- 0.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 108      |\n","|    mean_reward     | -109     |\n","| time/              |          |\n","|    total_timesteps | 454392   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 9.18     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 454272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6040     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3533     |\n","|    total_timesteps | 455088   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 8.45     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 454968   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6050     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3536     |\n","|    total_timesteps | 455496   |\n","| train/             |          |\n","|    actor_loss      | 68       |\n","|    critic_loss     | 7.89     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 455376   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6060     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3543     |\n","|    total_timesteps | 456264   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 5.05     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 456144   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6070     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3545     |\n","|    total_timesteps | 456648   |\n","| train/             |          |\n","|    actor_loss      | 68       |\n","|    critic_loss     | 5.65     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 456528   |\n","---------------------------------\n","Eval num_timesteps=456792, episode_reward=-103.57 +/- 1.31\n","Episode length: 85.00 +/- 7.35\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 85       |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 456792   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 12.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 456672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6080     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3552     |\n","|    total_timesteps | 457584   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 8.24     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 457464   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6090     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3558     |\n","|    total_timesteps | 458280   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 8.26     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 458160   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6100     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3563     |\n","|    total_timesteps | 458880   |\n","| train/             |          |\n","|    actor_loss      | 68.9     |\n","|    critic_loss     | 7.27     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 458760   |\n","---------------------------------\n","Eval num_timesteps=459192, episode_reward=-106.66 +/- 2.00\n","Episode length: 65.60 +/- 24.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 65.6     |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 459192   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 8.5      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 459072   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6110     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3568     |\n","|    total_timesteps | 459384   |\n","| train/             |          |\n","|    actor_loss      | 67.8     |\n","|    critic_loss     | 7.16     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 459264   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6120     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3578     |\n","|    total_timesteps | 460848   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 8.67     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 460728   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6130     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3583     |\n","|    total_timesteps | 461400   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 7.11     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 461280   |\n","---------------------------------\n","Eval num_timesteps=461592, episode_reward=-106.10 +/- 1.28\n","Episode length: 57.00 +/- 17.15\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 57       |\n","|    mean_reward     | -106     |\n","| time/              |          |\n","|    total_timesteps | 461592   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 11.1     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 461472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6140     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3587     |\n","|    total_timesteps | 461904   |\n","| train/             |          |\n","|    actor_loss      | 68.9     |\n","|    critic_loss     | 6.22     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 461784   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6150     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3587     |\n","|    total_timesteps | 461952   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 8.35     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 461832   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6160     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3595     |\n","|    total_timesteps | 463128   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 10       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 463008   |\n","---------------------------------\n","Eval num_timesteps=463992, episode_reward=-107.42 +/- 1.23\n","Episode length: 63.00 +/- 19.60\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 63       |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 463992   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 10.8     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 463872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6170     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3605     |\n","|    total_timesteps | 464040   |\n","| train/             |          |\n","|    actor_loss      | 67.7     |\n","|    critic_loss     | 10.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 463920   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6180     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3611     |\n","|    total_timesteps | 465024   |\n","| train/             |          |\n","|    actor_loss      | 69       |\n","|    critic_loss     | 8.74     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 464904   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6190     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3612     |\n","|    total_timesteps | 465216   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 9.48     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 465096   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6200     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3618     |\n","|    total_timesteps | 466056   |\n","| train/             |          |\n","|    actor_loss      | 68       |\n","|    critic_loss     | 8.5      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 465936   |\n","---------------------------------\n","Eval num_timesteps=466392, episode_reward=-103.52 +/- 4.48\n","Episode length: 71.60 +/- 20.09\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 71.6     |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 466392   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 8.83     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 466272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6210     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3625     |\n","|    total_timesteps | 466608   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 7.43     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 466488   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6220     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3631     |\n","|    total_timesteps | 467544   |\n","| train/             |          |\n","|    actor_loss      | 68       |\n","|    critic_loss     | 5.54     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 467424   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6230     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3636     |\n","|    total_timesteps | 468408   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 8.97     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 468288   |\n","---------------------------------\n","Eval num_timesteps=468792, episode_reward=-107.12 +/- 1.26\n","Episode length: 70.60 +/- 20.09\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 70.6     |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 468792   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 6.5      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 468672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6240     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3647     |\n","|    total_timesteps | 469416   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 8.17     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 469296   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6250     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3649     |\n","|    total_timesteps | 469848   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 7.37     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 469728   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6260     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3657     |\n","|    total_timesteps | 470760   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 11.9     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 470640   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6270     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3659     |\n","|    total_timesteps | 470952   |\n","| train/             |          |\n","|    actor_loss      | 68       |\n","|    critic_loss     | 11.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 470832   |\n","---------------------------------\n","Eval num_timesteps=471192, episode_reward=-109.27 +/- 3.38\n","Episode length: 63.20 +/- 23.52\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 63.2     |\n","|    mean_reward     | -109     |\n","| time/              |          |\n","|    total_timesteps | 471192   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 6.98     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 471072   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6280     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3664     |\n","|    total_timesteps | 471264   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 7.61     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 471144   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6290     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3673     |\n","|    total_timesteps | 472320   |\n","| train/             |          |\n","|    actor_loss      | 67.8     |\n","|    critic_loss     | 7.28     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 472200   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6300     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3677     |\n","|    total_timesteps | 473064   |\n","| train/             |          |\n","|    actor_loss      | 69       |\n","|    critic_loss     | 10.7     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 472944   |\n","---------------------------------\n","Eval num_timesteps=473592, episode_reward=-103.55 +/- 0.43\n","Episode length: 79.20 +/- 8.82\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 79.2     |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 473592   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 6.38     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 473472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6310     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3683     |\n","|    total_timesteps | 473688   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 9.93     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 473568   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6320     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3692     |\n","|    total_timesteps | 474720   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 5.28     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 474600   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6330     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3696     |\n","|    total_timesteps | 475344   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 9.99     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 475224   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6340     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3699     |\n","|    total_timesteps | 475776   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 6.62     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 475656   |\n","---------------------------------\n","Eval num_timesteps=475992, episode_reward=-108.23 +/- 0.84\n","Episode length: 79.40 +/- 26.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 79.4     |\n","|    mean_reward     | -108     |\n","| time/              |          |\n","|    total_timesteps | 475992   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 10.5     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 475872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6350     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3710     |\n","|    total_timesteps | 477000   |\n","| train/             |          |\n","|    actor_loss      | 68       |\n","|    critic_loss     | 10.1     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 476880   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6360     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3714     |\n","|    total_timesteps | 477552   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 9.9      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 477432   |\n","---------------------------------\n","Eval num_timesteps=478392, episode_reward=-102.64 +/- 5.62\n","Episode length: 88.40 +/- 5.39\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 88.4     |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 478392   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 9.87     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 478272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6370     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3724     |\n","|    total_timesteps | 478992   |\n","| train/             |          |\n","|    actor_loss      | 68       |\n","|    critic_loss     | 9.17     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 478872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6380     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3728     |\n","|    total_timesteps | 479424   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 9.01     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 479304   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6390     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3734     |\n","|    total_timesteps | 480096   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 10.8     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 479976   |\n","---------------------------------\n","Eval num_timesteps=480792, episode_reward=-103.47 +/- 6.30\n","Episode length: 58.80 +/- 18.13\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 58.8     |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 480792   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 8.77     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 480672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6400     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3740     |\n","|    total_timesteps | 480984   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 9.01     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 480864   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6410     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3741     |\n","|    total_timesteps | 481152   |\n","| train/             |          |\n","|    actor_loss      | 68.8     |\n","|    critic_loss     | 8.94     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 481032   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6420     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3749     |\n","|    total_timesteps | 482064   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 7.51     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 481944   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6430     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3754     |\n","|    total_timesteps | 482640   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 10.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 482520   |\n","---------------------------------\n","Eval num_timesteps=483192, episode_reward=-109.23 +/- 1.41\n","Episode length: 44.40 +/- 1.96\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 44.4     |\n","|    mean_reward     | -109     |\n","| time/              |          |\n","|    total_timesteps | 483192   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 8.24     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 483072   |\n","---------------------------------\n","-------------------------------\n","| time/              |        |\n","|    episodes        | 6440   |\n","|    fps             | 128    |\n","|    time_elapsed    | 3757   |\n","|    total_timesteps | 483192 |\n","-------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6450     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3762     |\n","|    total_timesteps | 483912   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 6.09     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 483792   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6460     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3764     |\n","|    total_timesteps | 484248   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 8.38     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 484128   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6470     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3772     |\n","|    total_timesteps | 485088   |\n","| train/             |          |\n","|    actor_loss      | 67.8     |\n","|    critic_loss     | 10.8     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 484968   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6480     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3775     |\n","|    total_timesteps | 485424   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 7.79     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 485304   |\n","---------------------------------\n","Eval num_timesteps=485592, episode_reward=-101.36 +/- 0.58\n","Episode length: 90.80 +/- 0.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 90.8     |\n","|    mean_reward     | -101     |\n","| time/              |          |\n","|    total_timesteps | 485592   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 7.55     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 485472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6490     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3783     |\n","|    total_timesteps | 486600   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 12       |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 486480   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6500     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3790     |\n","|    total_timesteps | 487536   |\n","| train/             |          |\n","|    actor_loss      | 69       |\n","|    critic_loss     | 7.89     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 487416   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6510     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3793     |\n","|    total_timesteps | 487848   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 3.75     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 487728   |\n","---------------------------------\n","Eval num_timesteps=487992, episode_reward=-104.72 +/- 1.82\n","Episode length: 82.80 +/- 10.78\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 82.8     |\n","|    mean_reward     | -105     |\n","| time/              |          |\n","|    total_timesteps | 487992   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 9.23     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 487872   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6520     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3807     |\n","|    total_timesteps | 489744   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 8.06     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 489624   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6530     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3810     |\n","|    total_timesteps | 490032   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 9.76     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 489912   |\n","---------------------------------\n","Eval num_timesteps=490392, episode_reward=-103.35 +/- 2.90\n","Episode length: 56.40 +/- 5.39\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 56.4     |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 490392   |\n","| train/             |          |\n","|    actor_loss      | 67.7     |\n","|    critic_loss     | 10.7     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 490272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6540     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3816     |\n","|    total_timesteps | 490632   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 10.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 490512   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6550     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3818     |\n","|    total_timesteps | 491064   |\n","| train/             |          |\n","|    actor_loss      | 69.1     |\n","|    critic_loss     | 8.48     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 490944   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6560     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3825     |\n","|    total_timesteps | 492096   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 8.64     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 491976   |\n","---------------------------------\n","Eval num_timesteps=492792, episode_reward=-104.36 +/- 0.98\n","Episode length: 70.00 +/- 19.60\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 70       |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 492792   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 9.85     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 492672   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6570     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3833     |\n","|    total_timesteps | 492888   |\n","| train/             |          |\n","|    actor_loss      | 67.9     |\n","|    critic_loss     | 5.12     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 492768   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6580     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3839     |\n","|    total_timesteps | 493752   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 8.72     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 493632   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6590     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3846     |\n","|    total_timesteps | 494832   |\n","| train/             |          |\n","|    actor_loss      | 68.9     |\n","|    critic_loss     | 5.79     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 494712   |\n","---------------------------------\n","Eval num_timesteps=495192, episode_reward=-101.59 +/- 0.33\n","Episode length: 82.60 +/- 0.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 82.6     |\n","|    mean_reward     | -102     |\n","| time/              |          |\n","|    total_timesteps | 495192   |\n","| train/             |          |\n","|    actor_loss      | 68.4     |\n","|    critic_loss     | 7.02     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 495072   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6600     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3859     |\n","|    total_timesteps | 496272   |\n","| train/             |          |\n","|    actor_loss      | 69.2     |\n","|    critic_loss     | 9.73     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 496152   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6610     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3861     |\n","|    total_timesteps | 496584   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 11.4     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 496464   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6620     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3865     |\n","|    total_timesteps | 497280   |\n","| train/             |          |\n","|    actor_loss      | 68.1     |\n","|    critic_loss     | 6.97     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 497160   |\n","---------------------------------\n","Eval num_timesteps=497592, episode_reward=-103.89 +/- 0.74\n","Episode length: 92.60 +/- 4.41\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 92.6     |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 497592   |\n","| train/             |          |\n","|    actor_loss      | 68.5     |\n","|    critic_loss     | 8.19     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 497472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6630     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3877     |\n","|    total_timesteps | 498504   |\n","| train/             |          |\n","|    actor_loss      | 68.6     |\n","|    critic_loss     | 7.11     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 498384   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6640     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3883     |\n","|    total_timesteps | 499392   |\n","| train/             |          |\n","|    actor_loss      | 68.3     |\n","|    critic_loss     | 7.98     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 499272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6650     |\n","|    fps             | 128      |\n","|    time_elapsed    | 3886     |\n","|    total_timesteps | 499896   |\n","| train/             |          |\n","|    actor_loss      | 68.2     |\n","|    critic_loss     | 6.18     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 499776   |\n","---------------------------------\n","Eval num_timesteps=499992, episode_reward=-102.97 +/- 1.80\n","Episode length: 99.00 +/- 12.25\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 99       |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 499992   |\n","| train/             |          |\n","|    actor_loss      | 68.7     |\n","|    critic_loss     | 7.42     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 499872   |\n","---------------------------------\n"]}],"source":["from stable_baselines3.common.logger import configure\n","\n","tmp_path = \"multiwalker_ddpg_log_eval/\"\n","# set up logger\n","new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n","\n","model = DDPG(\"MlpPolicy\", env, verbose=3,train_freq=1)\n","\n","model.set_logger(new_logger)\n","model.learn(total_timesteps=500000, log_interval=10,callback=eval_callback)\n","model.save(\"multiwalker_ddpg\")"]},{"cell_type":"code","source":["############# PPO multiwalker"],"metadata":{"id":"I0Q7lFc3xn-c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3 import PPO\n","# from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n","from pettingzoo.sisl import multiwalker_v9\n","import supersuit as ss\n","from stable_baselines3.common.monitor import Monitor"],"metadata":{"id":"7QHhIWDlxxi4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env = multiwalker_v9.parallel_env(render_mode=\"rgb_array\")\n","\n","# #---------------------------------\n","# log_dir = \"./policy_log\"\n","# os.makedirs(log_dir, exist_ok=True)\n","# env = Monitor(env, log_dir)\n","# #---------------------------------\n","# env = ss.color_reduction_v0(env, mode='B')\n","# env = ss.black_death_v3(env)\n","# env = ss.resize_v1(env, x_size=84, y_size=84)\n","env = ss.frame_stack_v1(env, 3)\n","env = ss.pettingzoo_env_to_vec_env_v1(env)\n","env = ss.concat_vec_envs_v1(env, 8, num_cpus=4, base_class='stable_baselines3')"],"metadata":{"id":"mOueaPEHx6iN"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H0lKH0t2kIP9"},"outputs":[],"source":["from stable_baselines3.common.callbacks import EvalCallback\n","eval_callback = EvalCallback(env, best_model_save_path=\"./multiwalker_ppo_log_eval/\",\n","                             log_path=\"./multiwalker_ppo_log_eval/\", eval_freq=500,\n","                             deterministic=True, render=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F9i2kMahkL8M","outputId":"b77a36f6-8c8a-470e-8b42-075bbd3d2c89","executionInfo":{"status":"ok","timestamp":1697655893522,"user_tz":-120,"elapsed":5978625,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Logging to ./multiwalker_ppo_log_eval/\n","Using cuda device\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 2.43         |\n","| time/                   |              |\n","|    total_timesteps      | 2832000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020025198 |\n","|    clip_fraction        | 0.0801       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.2        |\n","|    explained_variance   | -0.0931      |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0882      |\n","|    n_updates            | 1150         |\n","|    policy_gradient_loss | -0.00413     |\n","|    std                  | 3.09         |\n","|    value_loss           | 0.586        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 231     |\n","|    time_elapsed    | 3458    |\n","|    total_timesteps | 2838528 |\n","--------------------------------\n","Eval num_timesteps=2844000, episode_reward=2.85 +/- 2.02\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 2.85         |\n","| time/                   |              |\n","|    total_timesteps      | 2844000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0024337461 |\n","|    clip_fraction        | 0.121        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.2        |\n","|    explained_variance   | 0.228        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.277        |\n","|    n_updates            | 1155         |\n","|    policy_gradient_loss | -0.00337     |\n","|    std                  | 3.11         |\n","|    value_loss           | 15.5         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 232     |\n","|    time_elapsed    | 3472    |\n","|    total_timesteps | 2850816 |\n","--------------------------------\n","Eval num_timesteps=2856000, episode_reward=2.42 +/- 0.46\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 2.42         |\n","| time/                   |              |\n","|    total_timesteps      | 2856000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021688398 |\n","|    clip_fraction        | 0.107        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.2        |\n","|    explained_variance   | -0.11        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0927      |\n","|    n_updates            | 1160         |\n","|    policy_gradient_loss | -0.00375     |\n","|    std                  | 3.11         |\n","|    value_loss           | 0.416        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 821     |\n","|    iterations      | 233     |\n","|    time_elapsed    | 3485    |\n","|    total_timesteps | 2863104 |\n","--------------------------------\n","Eval num_timesteps=2868000, episode_reward=1.94 +/- 0.30\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 1.94         |\n","| time/                   |              |\n","|    total_timesteps      | 2868000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0024111157 |\n","|    clip_fraction        | 0.115        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.2        |\n","|    explained_variance   | 0.569        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0233      |\n","|    n_updates            | 1165         |\n","|    policy_gradient_loss | -0.00267     |\n","|    std                  | 3.14         |\n","|    value_loss           | 16           |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 821     |\n","|    iterations      | 234     |\n","|    time_elapsed    | 3501    |\n","|    total_timesteps | 2875392 |\n","--------------------------------\n","Eval num_timesteps=2880000, episode_reward=3.51 +/- 1.37\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 3.51         |\n","| time/                   |              |\n","|    total_timesteps      | 2880000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0025300572 |\n","|    clip_fraction        | 0.122        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.2        |\n","|    explained_variance   | 0.637        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.739        |\n","|    n_updates            | 1170         |\n","|    policy_gradient_loss | -0.00345     |\n","|    std                  | 3.17         |\n","|    value_loss           | 14.2         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 821     |\n","|    iterations      | 235     |\n","|    time_elapsed    | 3516    |\n","|    total_timesteps | 2887680 |\n","--------------------------------\n","Eval num_timesteps=2892000, episode_reward=2.76 +/- 0.63\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 2.76         |\n","| time/                   |              |\n","|    total_timesteps      | 2892000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020061887 |\n","|    clip_fraction        | 0.11         |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.3        |\n","|    explained_variance   | 0.599        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.488        |\n","|    n_updates            | 1175         |\n","|    policy_gradient_loss | -0.00225     |\n","|    std                  | 3.19         |\n","|    value_loss           | 16.5         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 821     |\n","|    iterations      | 236     |\n","|    time_elapsed    | 3529    |\n","|    total_timesteps | 2899968 |\n","--------------------------------\n","Eval num_timesteps=2904000, episode_reward=4.67 +/- 0.37\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 4.67        |\n","| time/                   |             |\n","|    total_timesteps      | 2904000     |\n","| train/                  |             |\n","|    approx_kl            | 0.003398962 |\n","|    clip_fraction        | 0.14        |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10.3       |\n","|    explained_variance   | 0.589       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 0.668       |\n","|    n_updates            | 1180        |\n","|    policy_gradient_loss | -0.00296    |\n","|    std                  | 3.24        |\n","|    value_loss           | 18.1        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 821     |\n","|    iterations      | 237     |\n","|    time_elapsed    | 3545    |\n","|    total_timesteps | 2912256 |\n","--------------------------------\n","Eval num_timesteps=2916000, episode_reward=3.60 +/- 0.35\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 3.6          |\n","| time/                   |              |\n","|    total_timesteps      | 2916000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021709611 |\n","|    clip_fraction        | 0.0923       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.4        |\n","|    explained_variance   | -0.48        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0684      |\n","|    n_updates            | 1185         |\n","|    policy_gradient_loss | -0.00328     |\n","|    std                  | 3.26         |\n","|    value_loss           | 0.684        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 821     |\n","|    iterations      | 238     |\n","|    time_elapsed    | 3560    |\n","|    total_timesteps | 2924544 |\n","--------------------------------\n","Eval num_timesteps=2928000, episode_reward=4.96 +/- 0.15\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 4.96         |\n","| time/                   |              |\n","|    total_timesteps      | 2928000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0033927623 |\n","|    clip_fraction        | 0.142        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.4        |\n","|    explained_variance   | 0.666        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 1.21         |\n","|    n_updates            | 1190         |\n","|    policy_gradient_loss | -0.00302     |\n","|    std                  | 3.29         |\n","|    value_loss           | 16.6         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 239     |\n","|    time_elapsed    | 3572    |\n","|    total_timesteps | 2936832 |\n","--------------------------------\n","Eval num_timesteps=2940000, episode_reward=4.95 +/- 1.06\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 4.95         |\n","| time/                   |              |\n","|    total_timesteps      | 2940000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0031250613 |\n","|    clip_fraction        | 0.137        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.4        |\n","|    explained_variance   | 0.672        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.217        |\n","|    n_updates            | 1195         |\n","|    policy_gradient_loss | -0.00249     |\n","|    std                  | 3.32         |\n","|    value_loss           | 14.8         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 821     |\n","|    iterations      | 240     |\n","|    time_elapsed    | 3588    |\n","|    total_timesteps | 2949120 |\n","--------------------------------\n","Eval num_timesteps=2952000, episode_reward=9.39 +/- 7.56\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 9.39         |\n","| time/                   |              |\n","|    total_timesteps      | 2952000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022323418 |\n","|    clip_fraction        | 0.104        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.5        |\n","|    explained_variance   | -0.338       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0898      |\n","|    n_updates            | 1200         |\n","|    policy_gradient_loss | -0.00349     |\n","|    std                  | 3.33         |\n","|    value_loss           | 0.66         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 821     |\n","|    iterations      | 241     |\n","|    time_elapsed    | 3603    |\n","|    total_timesteps | 2961408 |\n","--------------------------------\n","Eval num_timesteps=2964000, episode_reward=8.25 +/- 1.92\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 8.25         |\n","| time/                   |              |\n","|    total_timesteps      | 2964000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0023945079 |\n","|    clip_fraction        | 0.1          |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.5        |\n","|    explained_variance   | -0.147       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0937      |\n","|    n_updates            | 1205         |\n","|    policy_gradient_loss | -0.00395     |\n","|    std                  | 3.35         |\n","|    value_loss           | 0.42         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 242     |\n","|    time_elapsed    | 3615    |\n","|    total_timesteps | 2973696 |\n","--------------------------------\n","Eval num_timesteps=2976000, episode_reward=10.44 +/- 1.88\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 10.4        |\n","| time/                   |             |\n","|    total_timesteps      | 2976000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002361142 |\n","|    clip_fraction        | 0.101       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10.5       |\n","|    explained_variance   | 0.1         |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0991     |\n","|    n_updates            | 1210        |\n","|    policy_gradient_loss | -0.00333    |\n","|    std                  | 3.37        |\n","|    value_loss           | 0.353       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 243     |\n","|    time_elapsed    | 3631    |\n","|    total_timesteps | 2985984 |\n","--------------------------------\n","Eval num_timesteps=2988000, episode_reward=20.02 +/- 1.02\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 20          |\n","| time/                   |             |\n","|    total_timesteps      | 2988000     |\n","| train/                  |             |\n","|    approx_kl            | 0.001953288 |\n","|    clip_fraction        | 0.0787      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10.5       |\n","|    explained_variance   | -0.0358     |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0942     |\n","|    n_updates            | 1215        |\n","|    policy_gradient_loss | -0.00368    |\n","|    std                  | 3.38        |\n","|    value_loss           | 0.42        |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 244     |\n","|    time_elapsed    | 3645    |\n","|    total_timesteps | 2998272 |\n","--------------------------------\n","Eval num_timesteps=3000000, episode_reward=20.85 +/- 0.00\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 20.9         |\n","| time/                   |              |\n","|    total_timesteps      | 3000000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018261079 |\n","|    clip_fraction        | 0.08         |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.5        |\n","|    explained_variance   | 0.117        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.099       |\n","|    n_updates            | 1220         |\n","|    policy_gradient_loss | -0.00364     |\n","|    std                  | 3.39         |\n","|    value_loss           | 0.34         |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 245     |\n","|    time_elapsed    | 3658    |\n","|    total_timesteps | 3010560 |\n","--------------------------------\n","Eval num_timesteps=3012000, episode_reward=27.20 +/- 2.71\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 27.2         |\n","| time/                   |              |\n","|    total_timesteps      | 3012000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020432826 |\n","|    clip_fraction        | 0.0875       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.5        |\n","|    explained_variance   | 0.157        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0963      |\n","|    n_updates            | 1225         |\n","|    policy_gradient_loss | -0.00324     |\n","|    std                  | 3.4          |\n","|    value_loss           | 0.363        |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 246     |\n","|    time_elapsed    | 3674    |\n","|    total_timesteps | 3022848 |\n","--------------------------------\n","Eval num_timesteps=3024000, episode_reward=24.76 +/- 6.96\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 24.8         |\n","| time/                   |              |\n","|    total_timesteps      | 3024000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0027862664 |\n","|    clip_fraction        | 0.149        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.6        |\n","|    explained_variance   | 0.0094       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 1.7          |\n","|    n_updates            | 1230         |\n","|    policy_gradient_loss | -0.00357     |\n","|    std                  | 3.45         |\n","|    value_loss           | 22.2         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 247     |\n","|    time_elapsed    | 3688    |\n","|    total_timesteps | 3035136 |\n","--------------------------------\n","Eval num_timesteps=3036000, episode_reward=18.71 +/- 0.53\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 18.7         |\n","| time/                   |              |\n","|    total_timesteps      | 3036000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0026443407 |\n","|    clip_fraction        | 0.11         |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.6        |\n","|    explained_variance   | -0.0107      |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0921      |\n","|    n_updates            | 1235         |\n","|    policy_gradient_loss | -0.00372     |\n","|    std                  | 3.46         |\n","|    value_loss           | 0.466        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 823     |\n","|    iterations      | 248     |\n","|    time_elapsed    | 3700    |\n","|    total_timesteps | 3047424 |\n","--------------------------------\n","Eval num_timesteps=3048000, episode_reward=8.00 +/- 2.69\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 8            |\n","| time/                   |              |\n","|    total_timesteps      | 3048000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021116824 |\n","|    clip_fraction        | 0.0989       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.6        |\n","|    explained_variance   | 0.261        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0927      |\n","|    n_updates            | 1240         |\n","|    policy_gradient_loss | -0.00404     |\n","|    std                  | 3.46         |\n","|    value_loss           | 0.434        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 823     |\n","|    iterations      | 249     |\n","|    time_elapsed    | 3716    |\n","|    total_timesteps | 3059712 |\n","--------------------------------\n","Eval num_timesteps=3060000, episode_reward=1.74 +/- 0.43\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 1.74         |\n","| time/                   |              |\n","|    total_timesteps      | 3060000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021471388 |\n","|    clip_fraction        | 0.0963       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.6        |\n","|    explained_variance   | 0.212        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0997      |\n","|    n_updates            | 1245         |\n","|    policy_gradient_loss | -0.00369     |\n","|    std                  | 3.48         |\n","|    value_loss           | 0.334        |\n","------------------------------------------\n","Eval num_timesteps=3072000, episode_reward=9.77 +/- 3.79\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 9.77     |\n","| time/              |          |\n","|    total_timesteps | 3072000  |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 250     |\n","|    time_elapsed    | 3736    |\n","|    total_timesteps | 3072000 |\n","--------------------------------\n","Eval num_timesteps=3084000, episode_reward=1.94 +/- 0.14\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 1.94         |\n","| time/                   |              |\n","|    total_timesteps      | 3084000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0025126329 |\n","|    clip_fraction        | 0.122        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.6        |\n","|    explained_variance   | 0.151        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0878      |\n","|    n_updates            | 1250         |\n","|    policy_gradient_loss | -0.00412     |\n","|    std                  | 3.49         |\n","|    value_loss           | 0.528        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 251     |\n","|    time_elapsed    | 3749    |\n","|    total_timesteps | 3084288 |\n","--------------------------------\n","Eval num_timesteps=3096000, episode_reward=12.06 +/- 8.66\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 12.1         |\n","| time/                   |              |\n","|    total_timesteps      | 3096000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020772782 |\n","|    clip_fraction        | 0.0937       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.6        |\n","|    explained_variance   | 0.256        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.1         |\n","|    n_updates            | 1255         |\n","|    policy_gradient_loss | -0.00394     |\n","|    std                  | 3.49         |\n","|    value_loss           | 0.34         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 252     |\n","|    time_elapsed    | 3763    |\n","|    total_timesteps | 3096576 |\n","--------------------------------\n","Eval num_timesteps=3108000, episode_reward=1.20 +/- 0.56\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 1.2          |\n","| time/                   |              |\n","|    total_timesteps      | 3108000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021151642 |\n","|    clip_fraction        | 0.0918       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.6        |\n","|    explained_variance   | 0.292        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0986      |\n","|    n_updates            | 1260         |\n","|    policy_gradient_loss | -0.00432     |\n","|    std                  | 3.5          |\n","|    value_loss           | 0.375        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 253     |\n","|    time_elapsed    | 3779    |\n","|    total_timesteps | 3108864 |\n","--------------------------------\n","Eval num_timesteps=3120000, episode_reward=3.21 +/- 0.08\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 3.21         |\n","| time/                   |              |\n","|    total_timesteps      | 3120000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0036643443 |\n","|    clip_fraction        | 0.179        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.7        |\n","|    explained_variance   | 0.558        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.561        |\n","|    n_updates            | 1265         |\n","|    policy_gradient_loss | -0.00355     |\n","|    std                  | 3.54         |\n","|    value_loss           | 16.6         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 823     |\n","|    iterations      | 254     |\n","|    time_elapsed    | 3791    |\n","|    total_timesteps | 3121152 |\n","--------------------------------\n","Eval num_timesteps=3132000, episode_reward=2.14 +/- 0.54\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 2.14         |\n","| time/                   |              |\n","|    total_timesteps      | 3132000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020608234 |\n","|    clip_fraction        | 0.0936       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.7        |\n","|    explained_variance   | 0.0947       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0919      |\n","|    n_updates            | 1270         |\n","|    policy_gradient_loss | -0.0035      |\n","|    std                  | 3.55         |\n","|    value_loss           | 0.484        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 823     |\n","|    iterations      | 255     |\n","|    time_elapsed    | 3805    |\n","|    total_timesteps | 3133440 |\n","--------------------------------\n","Eval num_timesteps=3144000, episode_reward=9.17 +/- 5.90\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 9.17         |\n","| time/                   |              |\n","|    total_timesteps      | 3144000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0032769442 |\n","|    clip_fraction        | 0.124        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.7        |\n","|    explained_variance   | 0.574        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.084        |\n","|    n_updates            | 1275         |\n","|    policy_gradient_loss | -0.00294     |\n","|    std                  | 3.58         |\n","|    value_loss           | 15.8         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 823     |\n","|    iterations      | 256     |\n","|    time_elapsed    | 3821    |\n","|    total_timesteps | 3145728 |\n","--------------------------------\n","Eval num_timesteps=3156000, episode_reward=5.22 +/- 2.14\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 5.22         |\n","| time/                   |              |\n","|    total_timesteps      | 3156000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021173647 |\n","|    clip_fraction        | 0.106        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.7        |\n","|    explained_variance   | 0.108        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.097       |\n","|    n_updates            | 1280         |\n","|    policy_gradient_loss | -0.0035      |\n","|    std                  | 3.59         |\n","|    value_loss           | 0.377        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 823     |\n","|    iterations      | 257     |\n","|    time_elapsed    | 3833    |\n","|    total_timesteps | 3158016 |\n","--------------------------------\n","Eval num_timesteps=3168000, episode_reward=13.14 +/- 8.15\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 13.1         |\n","| time/                   |              |\n","|    total_timesteps      | 3168000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021762617 |\n","|    clip_fraction        | 0.0918       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.8        |\n","|    explained_variance   | 0.793        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0956      |\n","|    n_updates            | 1285         |\n","|    policy_gradient_loss | -0.00392     |\n","|    std                  | 3.61         |\n","|    value_loss           | 0.496        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 823     |\n","|    iterations      | 258     |\n","|    time_elapsed    | 3848    |\n","|    total_timesteps | 3170304 |\n","--------------------------------\n","Eval num_timesteps=3180000, episode_reward=-40.05 +/- 40.05\n","Episode length: 433.40 +/- 54.38\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 433          |\n","|    mean_reward          | -40.1        |\n","| time/                   |              |\n","|    total_timesteps      | 3180000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0033037271 |\n","|    clip_fraction        | 0.145        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.8        |\n","|    explained_variance   | 0.612        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.0531       |\n","|    n_updates            | 1290         |\n","|    policy_gradient_loss | -0.00233     |\n","|    std                  | 3.65         |\n","|    value_loss           | 16.5         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 823     |\n","|    iterations      | 259     |\n","|    time_elapsed    | 3865    |\n","|    total_timesteps | 3182592 |\n","--------------------------------\n","Eval num_timesteps=3192000, episode_reward=11.15 +/- 10.71\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 11.2         |\n","| time/                   |              |\n","|    total_timesteps      | 3192000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0016562546 |\n","|    clip_fraction        | 0.0771       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.8        |\n","|    explained_variance   | 0.205        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.102       |\n","|    n_updates            | 1295         |\n","|    policy_gradient_loss | -0.00334     |\n","|    std                  | 3.66         |\n","|    value_loss           | 0.402        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 823     |\n","|    iterations      | 260     |\n","|    time_elapsed    | 3877    |\n","|    total_timesteps | 3194880 |\n","--------------------------------\n","Eval num_timesteps=3204000, episode_reward=3.36 +/- 1.94\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 3.36        |\n","| time/                   |             |\n","|    total_timesteps      | 3204000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002175981 |\n","|    clip_fraction        | 0.0898      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10.8       |\n","|    explained_variance   | 0.193       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.102      |\n","|    n_updates            | 1300        |\n","|    policy_gradient_loss | -0.00406    |\n","|    std                  | 3.68        |\n","|    value_loss           | 0.374       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 824     |\n","|    iterations      | 261     |\n","|    time_elapsed    | 3892    |\n","|    total_timesteps | 3207168 |\n","--------------------------------\n","Eval num_timesteps=3216000, episode_reward=18.90 +/- 7.71\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 18.9        |\n","| time/                   |             |\n","|    total_timesteps      | 3216000     |\n","| train/                  |             |\n","|    approx_kl            | 0.001764702 |\n","|    clip_fraction        | 0.0757      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10.9       |\n","|    explained_variance   | 0.207       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0938     |\n","|    n_updates            | 1305        |\n","|    policy_gradient_loss | -0.00335    |\n","|    std                  | 3.69        |\n","|    value_loss           | 0.385       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 823     |\n","|    iterations      | 262     |\n","|    time_elapsed    | 3908    |\n","|    total_timesteps | 3219456 |\n","--------------------------------\n","Eval num_timesteps=3228000, episode_reward=11.53 +/- 2.27\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 11.5        |\n","| time/                   |             |\n","|    total_timesteps      | 3228000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002179153 |\n","|    clip_fraction        | 0.0916      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10.9       |\n","|    explained_variance   | 0.279       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0987     |\n","|    n_updates            | 1310        |\n","|    policy_gradient_loss | -0.00349    |\n","|    std                  | 3.7         |\n","|    value_loss           | 0.354       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 824     |\n","|    iterations      | 263     |\n","|    time_elapsed    | 3920    |\n","|    total_timesteps | 3231744 |\n","--------------------------------\n","Eval num_timesteps=3240000, episode_reward=9.49 +/- 6.92\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 9.49         |\n","| time/                   |              |\n","|    total_timesteps      | 3240000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021130936 |\n","|    clip_fraction        | 0.0916       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.9        |\n","|    explained_variance   | 0.281        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.102       |\n","|    n_updates            | 1315         |\n","|    policy_gradient_loss | -0.00374     |\n","|    std                  | 3.71         |\n","|    value_loss           | 0.343        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 824     |\n","|    iterations      | 264     |\n","|    time_elapsed    | 3935    |\n","|    total_timesteps | 3244032 |\n","--------------------------------\n","Eval num_timesteps=3252000, episode_reward=11.89 +/- 8.42\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 11.9         |\n","| time/                   |              |\n","|    total_timesteps      | 3252000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0017735377 |\n","|    clip_fraction        | 0.077        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.9        |\n","|    explained_variance   | 0.273        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0995      |\n","|    n_updates            | 1320         |\n","|    policy_gradient_loss | -0.0035      |\n","|    std                  | 3.72         |\n","|    value_loss           | 0.327        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 824     |\n","|    iterations      | 265     |\n","|    time_elapsed    | 3950    |\n","|    total_timesteps | 3256320 |\n","--------------------------------\n","Eval num_timesteps=3264000, episode_reward=8.42 +/- 6.51\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 8.42         |\n","| time/                   |              |\n","|    total_timesteps      | 3264000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018466538 |\n","|    clip_fraction        | 0.0857       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -10.9        |\n","|    explained_variance   | 0.261        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.102       |\n","|    n_updates            | 1325         |\n","|    policy_gradient_loss | -0.00354     |\n","|    std                  | 3.74         |\n","|    value_loss           | 0.364        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 824     |\n","|    iterations      | 266     |\n","|    time_elapsed    | 3962    |\n","|    total_timesteps | 3268608 |\n","--------------------------------\n","Eval num_timesteps=3276000, episode_reward=10.81 +/- 10.65\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 10.8        |\n","| time/                   |             |\n","|    total_timesteps      | 3276000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002647177 |\n","|    clip_fraction        | 0.0923      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -10.9       |\n","|    explained_variance   | 0.201       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 0.0189      |\n","|    n_updates            | 1330        |\n","|    policy_gradient_loss | -0.00305    |\n","|    std                  | 3.8         |\n","|    value_loss           | 6.42        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 824     |\n","|    iterations      | 267     |\n","|    time_elapsed    | 3977    |\n","|    total_timesteps | 3280896 |\n","--------------------------------\n","Eval num_timesteps=3288000, episode_reward=13.35 +/- 7.00\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 13.3         |\n","| time/                   |              |\n","|    total_timesteps      | 3288000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0027278678 |\n","|    clip_fraction        | 0.135        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11          |\n","|    explained_variance   | 0.0795       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.101        |\n","|    n_updates            | 1335         |\n","|    policy_gradient_loss | -0.00335     |\n","|    std                  | 3.82         |\n","|    value_loss           | 21.6         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 824     |\n","|    iterations      | 268     |\n","|    time_elapsed    | 3992    |\n","|    total_timesteps | 3293184 |\n","--------------------------------\n","Eval num_timesteps=3300000, episode_reward=7.12 +/- 0.35\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 7.12        |\n","| time/                   |             |\n","|    total_timesteps      | 3300000     |\n","| train/                  |             |\n","|    approx_kl            | 0.001868988 |\n","|    clip_fraction        | 0.0837      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11         |\n","|    explained_variance   | -0.166      |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0851     |\n","|    n_updates            | 1340        |\n","|    policy_gradient_loss | -0.00399    |\n","|    std                  | 3.82        |\n","|    value_loss           | 1.06        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 825     |\n","|    iterations      | 269     |\n","|    time_elapsed    | 4004    |\n","|    total_timesteps | 3305472 |\n","--------------------------------\n","Eval num_timesteps=3312000, episode_reward=5.08 +/- 2.81\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 5.08         |\n","| time/                   |              |\n","|    total_timesteps      | 3312000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021728394 |\n","|    clip_fraction        | 0.0885       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11          |\n","|    explained_variance   | 0.132        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.103       |\n","|    n_updates            | 1345         |\n","|    policy_gradient_loss | -0.00391     |\n","|    std                  | 3.82         |\n","|    value_loss           | 0.323        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 825     |\n","|    iterations      | 270     |\n","|    time_elapsed    | 4019    |\n","|    total_timesteps | 3317760 |\n","--------------------------------\n","Eval num_timesteps=3324000, episode_reward=3.26 +/- 2.97\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 3.26        |\n","| time/                   |             |\n","|    total_timesteps      | 3324000     |\n","| train/                  |             |\n","|    approx_kl            | 0.001966012 |\n","|    clip_fraction        | 0.0868      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11         |\n","|    explained_variance   | 0.241       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.104      |\n","|    n_updates            | 1350        |\n","|    policy_gradient_loss | -0.0038     |\n","|    std                  | 3.83        |\n","|    value_loss           | 0.329       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 825     |\n","|    iterations      | 271     |\n","|    time_elapsed    | 4035    |\n","|    total_timesteps | 3330048 |\n","--------------------------------\n","Eval num_timesteps=3336000, episode_reward=15.10 +/- 4.32\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 15.1         |\n","| time/                   |              |\n","|    total_timesteps      | 3336000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0026176209 |\n","|    clip_fraction        | 0.118        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11          |\n","|    explained_variance   | 0.612        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 1.31         |\n","|    n_updates            | 1355         |\n","|    policy_gradient_loss | -0.00346     |\n","|    std                  | 3.85         |\n","|    value_loss           | 17           |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 825     |\n","|    iterations      | 272     |\n","|    time_elapsed    | 4046    |\n","|    total_timesteps | 3342336 |\n","--------------------------------\n","Eval num_timesteps=3348000, episode_reward=30.89 +/- 3.06\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 30.9        |\n","| time/                   |             |\n","|    total_timesteps      | 3348000     |\n","| train/                  |             |\n","|    approx_kl            | 0.001658517 |\n","|    clip_fraction        | 0.0563      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11         |\n","|    explained_variance   | 0.552       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 1.42        |\n","|    n_updates            | 1360        |\n","|    policy_gradient_loss | -0.00246    |\n","|    std                  | 3.88        |\n","|    value_loss           | 27.6        |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 825     |\n","|    iterations      | 273     |\n","|    time_elapsed    | 4062    |\n","|    total_timesteps | 3354624 |\n","--------------------------------\n","Eval num_timesteps=3360000, episode_reward=20.74 +/- 12.42\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 20.7         |\n","| time/                   |              |\n","|    total_timesteps      | 3360000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018226443 |\n","|    clip_fraction        | 0.0807       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.1        |\n","|    explained_variance   | -0.000518    |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.105       |\n","|    n_updates            | 1365         |\n","|    policy_gradient_loss | -0.00367     |\n","|    std                  | 3.89         |\n","|    value_loss           | 0.38         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 825     |\n","|    iterations      | 274     |\n","|    time_elapsed    | 4076    |\n","|    total_timesteps | 3366912 |\n","--------------------------------\n","Eval num_timesteps=3372000, episode_reward=-64.89 +/- 0.56\n","Episode length: 365.80 +/- 54.87\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 366         |\n","|    mean_reward          | -64.9       |\n","| time/                   |             |\n","|    total_timesteps      | 3372000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002198157 |\n","|    clip_fraction        | 0.0876      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11.1       |\n","|    explained_variance   | 0.104       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0971     |\n","|    n_updates            | 1370        |\n","|    policy_gradient_loss | -0.00395    |\n","|    std                  | 3.9         |\n","|    value_loss           | 0.433       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 826     |\n","|    iterations      | 275     |\n","|    time_elapsed    | 4087    |\n","|    total_timesteps | 3379200 |\n","--------------------------------\n","Eval num_timesteps=3384000, episode_reward=-6.27 +/- 42.90\n","Episode length: 476.80 +/- 28.41\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 477          |\n","|    mean_reward          | -6.27        |\n","| time/                   |              |\n","|    total_timesteps      | 3384000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020858014 |\n","|    clip_fraction        | 0.114        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.1        |\n","|    explained_variance   | 0.701        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.458        |\n","|    n_updates            | 1375         |\n","|    policy_gradient_loss | -0.00276     |\n","|    std                  | 3.92         |\n","|    value_loss           | 15.1         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 826     |\n","|    iterations      | 276     |\n","|    time_elapsed    | 4103    |\n","|    total_timesteps | 3391488 |\n","--------------------------------\n","Eval num_timesteps=3396000, episode_reward=-27.72 +/- 43.49\n","Episode length: 454.40 +/- 37.23\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 454          |\n","|    mean_reward          | -27.7        |\n","| time/                   |              |\n","|    total_timesteps      | 3396000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022217908 |\n","|    clip_fraction        | 0.0894       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.1        |\n","|    explained_variance   | -0.088       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0829      |\n","|    n_updates            | 1380         |\n","|    policy_gradient_loss | -0.00417     |\n","|    std                  | 3.96         |\n","|    value_loss           | 1.46         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 826     |\n","|    iterations      | 277     |\n","|    time_elapsed    | 4116    |\n","|    total_timesteps | 3403776 |\n","--------------------------------\n","Eval num_timesteps=3408000, episode_reward=31.75 +/- 3.55\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 31.7         |\n","| time/                   |              |\n","|    total_timesteps      | 3408000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021067883 |\n","|    clip_fraction        | 0.0964       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.2        |\n","|    explained_variance   | 0.083        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.1         |\n","|    n_updates            | 1385         |\n","|    policy_gradient_loss | -0.00385     |\n","|    std                  | 3.97         |\n","|    value_loss           | 0.389        |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 827     |\n","|    iterations      | 278     |\n","|    time_elapsed    | 4129    |\n","|    total_timesteps | 3416064 |\n","--------------------------------\n","Eval num_timesteps=3420000, episode_reward=-25.73 +/- 41.92\n","Episode length: 440.00 +/- 48.99\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 440          |\n","|    mean_reward          | -25.7        |\n","| time/                   |              |\n","|    total_timesteps      | 3420000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020851288 |\n","|    clip_fraction        | 0.0779       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.2        |\n","|    explained_variance   | 0.102        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.103       |\n","|    n_updates            | 1390         |\n","|    policy_gradient_loss | -0.00347     |\n","|    std                  | 3.98         |\n","|    value_loss           | 0.359        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 827     |\n","|    iterations      | 279     |\n","|    time_elapsed    | 4145    |\n","|    total_timesteps | 3428352 |\n","--------------------------------\n","Eval num_timesteps=3432000, episode_reward=30.95 +/- 2.35\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 31          |\n","| time/                   |             |\n","|    total_timesteps      | 3432000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002619261 |\n","|    clip_fraction        | 0.118       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11.2       |\n","|    explained_variance   | 0.626       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 0.207       |\n","|    n_updates            | 1395        |\n","|    policy_gradient_loss | -0.00325    |\n","|    std                  | 4.04        |\n","|    value_loss           | 13.9        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 827     |\n","|    iterations      | 280     |\n","|    time_elapsed    | 4157    |\n","|    total_timesteps | 3440640 |\n","--------------------------------\n","Eval num_timesteps=3444000, episode_reward=-66.59 +/- 3.04\n","Episode length: 354.00 +/- 17.15\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 354         |\n","|    mean_reward          | -66.6       |\n","| time/                   |             |\n","|    total_timesteps      | 3444000     |\n","| train/                  |             |\n","|    approx_kl            | 0.001827931 |\n","|    clip_fraction        | 0.0953      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11.2       |\n","|    explained_variance   | -0.00959    |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0896     |\n","|    n_updates            | 1400        |\n","|    policy_gradient_loss | -0.00403    |\n","|    std                  | 4.05        |\n","|    value_loss           | 0.982       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 828     |\n","|    iterations      | 281     |\n","|    time_elapsed    | 4169    |\n","|    total_timesteps | 3452928 |\n","--------------------------------\n","Eval num_timesteps=3456000, episode_reward=10.66 +/- 2.27\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 10.7         |\n","| time/                   |              |\n","|    total_timesteps      | 3456000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0037792437 |\n","|    clip_fraction        | 0.148        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.3        |\n","|    explained_variance   | 0.0319       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 1.87         |\n","|    n_updates            | 1405         |\n","|    policy_gradient_loss | -0.00359     |\n","|    std                  | 4.09         |\n","|    value_loss           | 57.1         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 828     |\n","|    iterations      | 282     |\n","|    time_elapsed    | 4184    |\n","|    total_timesteps | 3465216 |\n","--------------------------------\n","Eval num_timesteps=3468000, episode_reward=19.47 +/- 0.62\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 19.5         |\n","| time/                   |              |\n","|    total_timesteps      | 3468000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018003505 |\n","|    clip_fraction        | 0.0853       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.3        |\n","|    explained_variance   | -0.00334     |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.102       |\n","|    n_updates            | 1410         |\n","|    policy_gradient_loss | -0.00454     |\n","|    std                  | 4.09         |\n","|    value_loss           | 0.414        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 828     |\n","|    iterations      | 283     |\n","|    time_elapsed    | 4196    |\n","|    total_timesteps | 3477504 |\n","--------------------------------\n","Eval num_timesteps=3480000, episode_reward=14.38 +/- 6.22\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 14.4         |\n","| time/                   |              |\n","|    total_timesteps      | 3480000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0024242934 |\n","|    clip_fraction        | 0.127        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.3        |\n","|    explained_variance   | 0.591        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.559        |\n","|    n_updates            | 1415         |\n","|    policy_gradient_loss | -0.00294     |\n","|    std                  | 4.13         |\n","|    value_loss           | 15.8         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 828     |\n","|    iterations      | 284     |\n","|    time_elapsed    | 4211    |\n","|    total_timesteps | 3489792 |\n","--------------------------------\n","Eval num_timesteps=3492000, episode_reward=-30.04 +/- 42.75\n","Episode length: 423.80 +/- 62.22\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 424          |\n","|    mean_reward          | -30          |\n","| time/                   |              |\n","|    total_timesteps      | 3492000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018813378 |\n","|    clip_fraction        | 0.0793       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.3        |\n","|    explained_variance   | -0.628       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.104       |\n","|    n_updates            | 1420         |\n","|    policy_gradient_loss | -0.00391     |\n","|    std                  | 4.14         |\n","|    value_loss           | 0.482        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 828     |\n","|    iterations      | 285     |\n","|    time_elapsed    | 4227    |\n","|    total_timesteps | 3502080 |\n","--------------------------------\n","Eval num_timesteps=3504000, episode_reward=33.91 +/- 0.85\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 33.9         |\n","| time/                   |              |\n","|    total_timesteps      | 3504000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021614765 |\n","|    clip_fraction        | 0.096        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.3        |\n","|    explained_variance   | -0.0271      |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.111       |\n","|    n_updates            | 1425         |\n","|    policy_gradient_loss | -0.00435     |\n","|    std                  | 4.15         |\n","|    value_loss           | 0.349        |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 829     |\n","|    iterations      | 286     |\n","|    time_elapsed    | 4239    |\n","|    total_timesteps | 3514368 |\n","--------------------------------\n","Eval num_timesteps=3516000, episode_reward=27.82 +/- 4.06\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 27.8         |\n","| time/                   |              |\n","|    total_timesteps      | 3516000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0026449924 |\n","|    clip_fraction        | 0.114        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.4        |\n","|    explained_variance   | 0.618        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.0586       |\n","|    n_updates            | 1430         |\n","|    policy_gradient_loss | -0.00302     |\n","|    std                  | 4.2          |\n","|    value_loss           | 26.9         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 828     |\n","|    iterations      | 287     |\n","|    time_elapsed    | 4254    |\n","|    total_timesteps | 3526656 |\n","--------------------------------\n","Eval num_timesteps=3528000, episode_reward=-65.15 +/- 0.94\n","Episode length: 410.00 +/- 12.25\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 410          |\n","|    mean_reward          | -65.1        |\n","| time/                   |              |\n","|    total_timesteps      | 3528000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019838528 |\n","|    clip_fraction        | 0.104        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.4        |\n","|    explained_variance   | -0.864       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0911      |\n","|    n_updates            | 1435         |\n","|    policy_gradient_loss | -0.00367     |\n","|    std                  | 4.2          |\n","|    value_loss           | 0.955        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 829     |\n","|    iterations      | 288     |\n","|    time_elapsed    | 4268    |\n","|    total_timesteps | 3538944 |\n","--------------------------------\n","Eval num_timesteps=3540000, episode_reward=22.49 +/- 4.23\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 22.5         |\n","| time/                   |              |\n","|    total_timesteps      | 3540000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019697635 |\n","|    clip_fraction        | 0.0939       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.4        |\n","|    explained_variance   | 0.179        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.105       |\n","|    n_updates            | 1440         |\n","|    policy_gradient_loss | -0.00375     |\n","|    std                  | 4.22         |\n","|    value_loss           | 0.537        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 828     |\n","|    iterations      | 289     |\n","|    time_elapsed    | 4285    |\n","|    total_timesteps | 3551232 |\n","--------------------------------\n","Eval num_timesteps=3552000, episode_reward=28.05 +/- 3.69\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 28           |\n","| time/                   |              |\n","|    total_timesteps      | 3552000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019627726 |\n","|    clip_fraction        | 0.0915       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.4        |\n","|    explained_variance   | 0.241        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.107       |\n","|    n_updates            | 1445         |\n","|    policy_gradient_loss | -0.00422     |\n","|    std                  | 4.22         |\n","|    value_loss           | 0.241        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 828     |\n","|    iterations      | 290     |\n","|    time_elapsed    | 4300    |\n","|    total_timesteps | 3563520 |\n","--------------------------------\n","Eval num_timesteps=3564000, episode_reward=-27.22 +/- 40.80\n","Episode length: 483.80 +/- 13.23\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 484         |\n","|    mean_reward          | -27.2       |\n","| time/                   |             |\n","|    total_timesteps      | 3564000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002022389 |\n","|    clip_fraction        | 0.0995      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11.4       |\n","|    explained_variance   | 0.184       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.11       |\n","|    n_updates            | 1450        |\n","|    policy_gradient_loss | -0.00385    |\n","|    std                  | 4.24        |\n","|    value_loss           | 0.379       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 828     |\n","|    iterations      | 291     |\n","|    time_elapsed    | 4316    |\n","|    total_timesteps | 3575808 |\n","--------------------------------\n","Eval num_timesteps=3576000, episode_reward=-9.50 +/- 43.90\n","Episode length: 484.40 +/- 19.11\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 484          |\n","|    mean_reward          | -9.5         |\n","| time/                   |              |\n","|    total_timesteps      | 3576000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022678021 |\n","|    clip_fraction        | 0.11         |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.4        |\n","|    explained_variance   | 0.105        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.104       |\n","|    n_updates            | 1455         |\n","|    policy_gradient_loss | -0.0046      |\n","|    std                  | 4.25         |\n","|    value_loss           | 0.431        |\n","------------------------------------------\n","Eval num_timesteps=3588000, episode_reward=-31.99 +/- 37.48\n","Episode length: 490.40 +/- 7.84\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 490      |\n","|    mean_reward     | -32      |\n","| time/              |          |\n","|    total_timesteps | 3588000  |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 827     |\n","|    iterations      | 292     |\n","|    time_elapsed    | 4336    |\n","|    total_timesteps | 3588096 |\n","--------------------------------\n","Eval num_timesteps=3600000, episode_reward=11.62 +/- 7.82\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 11.6         |\n","| time/                   |              |\n","|    total_timesteps      | 3600000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020077007 |\n","|    clip_fraction        | 0.0937       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.4        |\n","|    explained_variance   | 0.223        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.106       |\n","|    n_updates            | 1460         |\n","|    policy_gradient_loss | -0.00291     |\n","|    std                  | 4.25         |\n","|    value_loss           | 0.412        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 828     |\n","|    iterations      | 293     |\n","|    time_elapsed    | 4348    |\n","|    total_timesteps | 3600384 |\n","--------------------------------\n","Eval num_timesteps=3612000, episode_reward=17.13 +/- 8.00\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 17.1         |\n","| time/                   |              |\n","|    total_timesteps      | 3612000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019589504 |\n","|    clip_fraction        | 0.0876       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.4        |\n","|    explained_variance   | 0.378        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.105       |\n","|    n_updates            | 1465         |\n","|    policy_gradient_loss | -0.00387     |\n","|    std                  | 4.27         |\n","|    value_loss           | 0.344        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 827     |\n","|    iterations      | 294     |\n","|    time_elapsed    | 4363    |\n","|    total_timesteps | 3612672 |\n","--------------------------------\n","Eval num_timesteps=3624000, episode_reward=-26.77 +/- 48.18\n","Episode length: 496.40 +/- 2.94\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 496          |\n","|    mean_reward          | -26.8        |\n","| time/                   |              |\n","|    total_timesteps      | 3624000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020318888 |\n","|    clip_fraction        | 0.0899       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.5        |\n","|    explained_variance   | 0.426        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.107       |\n","|    n_updates            | 1470         |\n","|    policy_gradient_loss | -0.00341     |\n","|    std                  | 4.3          |\n","|    value_loss           | 0.337        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 827     |\n","|    iterations      | 295     |\n","|    time_elapsed    | 4378    |\n","|    total_timesteps | 3624960 |\n","--------------------------------\n","Eval num_timesteps=3636000, episode_reward=28.39 +/- 4.61\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 28.4         |\n","| time/                   |              |\n","|    total_timesteps      | 3636000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018034742 |\n","|    clip_fraction        | 0.0807       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.5        |\n","|    explained_variance   | 0.423        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.105       |\n","|    n_updates            | 1475         |\n","|    policy_gradient_loss | -0.00381     |\n","|    std                  | 4.32         |\n","|    value_loss           | 0.35         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 828     |\n","|    iterations      | 296     |\n","|    time_elapsed    | 4390    |\n","|    total_timesteps | 3637248 |\n","--------------------------------\n","Eval num_timesteps=3648000, episode_reward=25.55 +/- 4.44\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 25.6         |\n","| time/                   |              |\n","|    total_timesteps      | 3648000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020596944 |\n","|    clip_fraction        | 0.0945       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.5        |\n","|    explained_variance   | 0.406        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.107       |\n","|    n_updates            | 1480         |\n","|    policy_gradient_loss | -0.00376     |\n","|    std                  | 4.32         |\n","|    value_loss           | 0.369        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 828     |\n","|    iterations      | 297     |\n","|    time_elapsed    | 4407    |\n","|    total_timesteps | 3649536 |\n","--------------------------------\n","Eval num_timesteps=3660000, episode_reward=-62.26 +/- 0.20\n","Episode length: 463.20 +/- 15.68\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 463          |\n","|    mean_reward          | -62.3        |\n","| time/                   |              |\n","|    total_timesteps      | 3660000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0016431366 |\n","|    clip_fraction        | 0.0733       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.5        |\n","|    explained_variance   | 0.576        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.11        |\n","|    n_updates            | 1485         |\n","|    policy_gradient_loss | -0.00383     |\n","|    std                  | 4.32         |\n","|    value_loss           | 0.286        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 827     |\n","|    iterations      | 298     |\n","|    time_elapsed    | 4422    |\n","|    total_timesteps | 3661824 |\n","--------------------------------\n","Eval num_timesteps=3672000, episode_reward=3.13 +/- 49.01\n","Episode length: 481.60 +/- 22.54\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 482          |\n","|    mean_reward          | 3.13         |\n","| time/                   |              |\n","|    total_timesteps      | 3672000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020014623 |\n","|    clip_fraction        | 0.0905       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.5        |\n","|    explained_variance   | 0.613        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.101       |\n","|    n_updates            | 1490         |\n","|    policy_gradient_loss | -0.00373     |\n","|    std                  | 4.33         |\n","|    value_loss           | 0.341        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 828     |\n","|    iterations      | 299     |\n","|    time_elapsed    | 4434    |\n","|    total_timesteps | 3674112 |\n","--------------------------------\n","Eval num_timesteps=3684000, episode_reward=-12.71 +/- 39.95\n","Episode length: 461.60 +/- 47.03\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 462         |\n","|    mean_reward          | -12.7       |\n","| time/                   |             |\n","|    total_timesteps      | 3684000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002585315 |\n","|    clip_fraction        | 0.111       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11.5       |\n","|    explained_variance   | 0.00933     |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 0.374       |\n","|    n_updates            | 1495        |\n","|    policy_gradient_loss | -0.00316    |\n","|    std                  | 4.39        |\n","|    value_loss           | 17.5        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 828     |\n","|    iterations      | 300     |\n","|    time_elapsed    | 4449    |\n","|    total_timesteps | 3686400 |\n","--------------------------------\n","Eval num_timesteps=3696000, episode_reward=-61.85 +/- 3.30\n","Episode length: 385.00 +/- 24.49\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 385          |\n","|    mean_reward          | -61.9        |\n","| time/                   |              |\n","|    total_timesteps      | 3696000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0017903118 |\n","|    clip_fraction        | 0.0897       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.6        |\n","|    explained_variance   | 0.00849      |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.101       |\n","|    n_updates            | 1500         |\n","|    policy_gradient_loss | -0.00429     |\n","|    std                  | 4.39         |\n","|    value_loss           | 1.03         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 828     |\n","|    iterations      | 301     |\n","|    time_elapsed    | 4462    |\n","|    total_timesteps | 3698688 |\n","--------------------------------\n","Eval num_timesteps=3708000, episode_reward=-61.32 +/- 0.76\n","Episode length: 410.40 +/- 0.49\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 410          |\n","|    mean_reward          | -61.3        |\n","| time/                   |              |\n","|    total_timesteps      | 3708000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0026573793 |\n","|    clip_fraction        | 0.117        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.6        |\n","|    explained_variance   | 0.3          |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 1.01         |\n","|    n_updates            | 1505         |\n","|    policy_gradient_loss | -0.00331     |\n","|    std                  | 4.43         |\n","|    value_loss           | 26           |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 829     |\n","|    iterations      | 302     |\n","|    time_elapsed    | 4474    |\n","|    total_timesteps | 3710976 |\n","--------------------------------\n","Eval num_timesteps=3720000, episode_reward=-26.36 +/- 45.07\n","Episode length: 423.20 +/- 62.71\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 423         |\n","|    mean_reward          | -26.4       |\n","| time/                   |             |\n","|    total_timesteps      | 3720000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002330699 |\n","|    clip_fraction        | 0.0985      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11.6       |\n","|    explained_variance   | 0.509       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 0.0851      |\n","|    n_updates            | 1510        |\n","|    policy_gradient_loss | -0.00331    |\n","|    std                  | 4.46        |\n","|    value_loss           | 13.5        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 829     |\n","|    iterations      | 303     |\n","|    time_elapsed    | 4489    |\n","|    total_timesteps | 3723264 |\n","--------------------------------\n","Eval num_timesteps=3732000, episode_reward=-21.76 +/- 45.47\n","Episode length: 424.40 +/- 61.73\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 424          |\n","|    mean_reward          | -21.8        |\n","| time/                   |              |\n","|    total_timesteps      | 3732000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0023147103 |\n","|    clip_fraction        | 0.0897       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.6        |\n","|    explained_variance   | -1.64        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0975      |\n","|    n_updates            | 1515         |\n","|    policy_gradient_loss | -0.00338     |\n","|    std                  | 4.47         |\n","|    value_loss           | 0.791        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 829     |\n","|    iterations      | 304     |\n","|    time_elapsed    | 4501    |\n","|    total_timesteps | 3735552 |\n","--------------------------------\n","Eval num_timesteps=3744000, episode_reward=-56.57 +/- 0.83\n","Episode length: 457.60 +/- 2.94\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 458         |\n","|    mean_reward          | -56.6       |\n","| time/                   |             |\n","|    total_timesteps      | 3744000     |\n","| train/                  |             |\n","|    approx_kl            | 0.001963874 |\n","|    clip_fraction        | 0.0966      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11.6       |\n","|    explained_variance   | -0.0389     |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.107      |\n","|    n_updates            | 1520        |\n","|    policy_gradient_loss | -0.00399    |\n","|    std                  | 4.47        |\n","|    value_loss           | 0.5         |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 830     |\n","|    iterations      | 305     |\n","|    time_elapsed    | 4515    |\n","|    total_timesteps | 3747840 |\n","--------------------------------\n","Eval num_timesteps=3756000, episode_reward=-23.16 +/- 45.42\n","Episode length: 444.20 +/- 45.56\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 444         |\n","|    mean_reward          | -23.2       |\n","| time/                   |             |\n","|    total_timesteps      | 3756000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002783579 |\n","|    clip_fraction        | 0.129       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11.7       |\n","|    explained_variance   | 0.415       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 0.926       |\n","|    n_updates            | 1525        |\n","|    policy_gradient_loss | -0.00292    |\n","|    std                  | 4.53        |\n","|    value_loss           | 21.9        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 829     |\n","|    iterations      | 306     |\n","|    time_elapsed    | 4530    |\n","|    total_timesteps | 3760128 |\n","--------------------------------\n","Eval num_timesteps=3768000, episode_reward=-58.09 +/- 1.88\n","Episode length: 419.40 +/- 33.80\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 419         |\n","|    mean_reward          | -58.1       |\n","| time/                   |             |\n","|    total_timesteps      | 3768000     |\n","| train/                  |             |\n","|    approx_kl            | 0.001963451 |\n","|    clip_fraction        | 0.0873      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11.7       |\n","|    explained_variance   | 0.689       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.104      |\n","|    n_updates            | 1530        |\n","|    policy_gradient_loss | -0.00444    |\n","|    std                  | 4.56        |\n","|    value_loss           | 0.49        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 830     |\n","|    iterations      | 307     |\n","|    time_elapsed    | 4541    |\n","|    total_timesteps | 3772416 |\n","--------------------------------\n","Eval num_timesteps=3780000, episode_reward=30.39 +/- 2.58\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 30.4         |\n","| time/                   |              |\n","|    total_timesteps      | 3780000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0028671157 |\n","|    clip_fraction        | 0.154        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.7        |\n","|    explained_variance   | 0.47         |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 5.26         |\n","|    n_updates            | 1535         |\n","|    policy_gradient_loss | -0.00426     |\n","|    std                  | 4.6          |\n","|    value_loss           | 53.1         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 830     |\n","|    iterations      | 308     |\n","|    time_elapsed    | 4558    |\n","|    total_timesteps | 3784704 |\n","--------------------------------\n","Eval num_timesteps=3792000, episode_reward=-3.79 +/- 43.16\n","Episode length: 488.40 +/- 14.21\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 488          |\n","|    mean_reward          | -3.79        |\n","| time/                   |              |\n","|    total_timesteps      | 3792000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018603848 |\n","|    clip_fraction        | 0.0744       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.8        |\n","|    explained_variance   | 0.0708       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.11        |\n","|    n_updates            | 1540         |\n","|    policy_gradient_loss | -0.00426     |\n","|    std                  | 4.62         |\n","|    value_loss           | 0.495        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 829     |\n","|    iterations      | 309     |\n","|    time_elapsed    | 4574    |\n","|    total_timesteps | 3796992 |\n","--------------------------------\n","Eval num_timesteps=3804000, episode_reward=-8.17 +/- 43.94\n","Episode length: 466.00 +/- 41.64\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 466         |\n","|    mean_reward          | -8.17       |\n","| time/                   |             |\n","|    total_timesteps      | 3804000     |\n","| train/                  |             |\n","|    approx_kl            | 0.001751545 |\n","|    clip_fraction        | 0.0778      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11.8       |\n","|    explained_variance   | 0.239       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.104      |\n","|    n_updates            | 1545        |\n","|    policy_gradient_loss | -0.0038     |\n","|    std                  | 4.64        |\n","|    value_loss           | 0.417       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 830     |\n","|    iterations      | 310     |\n","|    time_elapsed    | 4587    |\n","|    total_timesteps | 3809280 |\n","--------------------------------\n","Eval num_timesteps=3816000, episode_reward=-60.52 +/- 0.25\n","Episode length: 382.00 +/- 2.45\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 382         |\n","|    mean_reward          | -60.5       |\n","| time/                   |             |\n","|    total_timesteps      | 3816000     |\n","| train/                  |             |\n","|    approx_kl            | 0.001934691 |\n","|    clip_fraction        | 0.0825      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11.8       |\n","|    explained_variance   | 0.35        |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.111      |\n","|    n_updates            | 1550        |\n","|    policy_gradient_loss | -0.0037     |\n","|    std                  | 4.63        |\n","|    value_loss           | 0.315       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 830     |\n","|    iterations      | 311     |\n","|    time_elapsed    | 4600    |\n","|    total_timesteps | 3821568 |\n","--------------------------------\n","Eval num_timesteps=3828000, episode_reward=-23.90 +/- 46.10\n","Episode length: 463.40 +/- 29.88\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 463          |\n","|    mean_reward          | -23.9        |\n","| time/                   |              |\n","|    total_timesteps      | 3828000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0027988676 |\n","|    clip_fraction        | 0.119        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.8        |\n","|    explained_variance   | 0.486        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.535        |\n","|    n_updates            | 1555         |\n","|    policy_gradient_loss | -0.00209     |\n","|    std                  | 4.69         |\n","|    value_loss           | 11.1         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 830     |\n","|    iterations      | 312     |\n","|    time_elapsed    | 4616    |\n","|    total_timesteps | 3833856 |\n","--------------------------------\n","Eval num_timesteps=3840000, episode_reward=28.67 +/- 2.48\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 28.7        |\n","| time/                   |             |\n","|    total_timesteps      | 3840000     |\n","| train/                  |             |\n","|    approx_kl            | 0.001762336 |\n","|    clip_fraction        | 0.0794      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11.8       |\n","|    explained_variance   | -0.0531     |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.104      |\n","|    n_updates            | 1560        |\n","|    policy_gradient_loss | -0.00409    |\n","|    std                  | 4.68        |\n","|    value_loss           | 0.397       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 831     |\n","|    iterations      | 313     |\n","|    time_elapsed    | 4627    |\n","|    total_timesteps | 3846144 |\n","--------------------------------\n","Eval num_timesteps=3852000, episode_reward=-5.58 +/- 43.33\n","Episode length: 462.40 +/- 46.05\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 462          |\n","|    mean_reward          | -5.58        |\n","| time/                   |              |\n","|    total_timesteps      | 3852000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022105142 |\n","|    clip_fraction        | 0.129        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.8        |\n","|    explained_variance   | 0.559        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 1.5          |\n","|    n_updates            | 1565         |\n","|    policy_gradient_loss | -0.00376     |\n","|    std                  | 4.71         |\n","|    value_loss           | 17.4         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 830     |\n","|    iterations      | 314     |\n","|    time_elapsed    | 4643    |\n","|    total_timesteps | 3858432 |\n","--------------------------------\n","Eval num_timesteps=3864000, episode_reward=-55.66 +/- 0.63\n","Episode length: 441.40 +/- 7.84\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 441         |\n","|    mean_reward          | -55.7       |\n","| time/                   |             |\n","|    total_timesteps      | 3864000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002389395 |\n","|    clip_fraction        | 0.088       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -11.9       |\n","|    explained_variance   | 0.282       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.107      |\n","|    n_updates            | 1570        |\n","|    policy_gradient_loss | -0.0038     |\n","|    std                  | 4.73        |\n","|    value_loss           | 0.379       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 831     |\n","|    iterations      | 315     |\n","|    time_elapsed    | 4657    |\n","|    total_timesteps | 3870720 |\n","--------------------------------\n","Eval num_timesteps=3876000, episode_reward=-57.35 +/- 1.47\n","Episode length: 434.00 +/- 9.80\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 434          |\n","|    mean_reward          | -57.3        |\n","| time/                   |              |\n","|    total_timesteps      | 3876000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0034110062 |\n","|    clip_fraction        | 0.137        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.9        |\n","|    explained_variance   | 0.523        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.328        |\n","|    n_updates            | 1575         |\n","|    policy_gradient_loss | -0.00293     |\n","|    std                  | 4.79         |\n","|    value_loss           | 16.4         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 831     |\n","|    iterations      | 316     |\n","|    time_elapsed    | 4668    |\n","|    total_timesteps | 3883008 |\n","--------------------------------\n","Eval num_timesteps=3888000, episode_reward=-57.30 +/- 4.50\n","Episode length: 445.60 +/- 51.93\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 446          |\n","|    mean_reward          | -57.3        |\n","| time/                   |              |\n","|    total_timesteps      | 3888000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0036596179 |\n","|    clip_fraction        | 0.15         |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -11.9        |\n","|    explained_variance   | 0.212        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.415        |\n","|    n_updates            | 1580         |\n","|    policy_gradient_loss | -0.00315     |\n","|    std                  | 4.83         |\n","|    value_loss           | 21           |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 831     |\n","|    iterations      | 317     |\n","|    time_elapsed    | 4684    |\n","|    total_timesteps | 3895296 |\n","--------------------------------\n","Eval num_timesteps=3900000, episode_reward=-14.84 +/- 49.79\n","Episode length: 458.00 +/- 34.29\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 458         |\n","|    mean_reward          | -14.8       |\n","| time/                   |             |\n","|    total_timesteps      | 3900000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002132269 |\n","|    clip_fraction        | 0.0906      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12         |\n","|    explained_variance   | 0.312       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.112      |\n","|    n_updates            | 1585        |\n","|    policy_gradient_loss | -0.00413    |\n","|    std                  | 4.86        |\n","|    value_loss           | 0.483       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 831     |\n","|    iterations      | 318     |\n","|    time_elapsed    | 4697    |\n","|    total_timesteps | 3907584 |\n","--------------------------------\n","Eval num_timesteps=3912000, episode_reward=-57.14 +/- 1.55\n","Episode length: 449.40 +/- 5.39\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 449        |\n","|    mean_reward          | -57.1      |\n","| time/                   |            |\n","|    total_timesteps      | 3912000    |\n","| train/                  |            |\n","|    approx_kl            | 0.00268968 |\n","|    clip_fraction        | 0.101      |\n","|    clip_range           | 0.1        |\n","|    entropy_loss         | -12        |\n","|    explained_variance   | 0.312      |\n","|    learning_rate        | 0.000622   |\n","|    loss                 | 1.36       |\n","|    n_updates            | 1590       |\n","|    policy_gradient_loss | -0.00444   |\n","|    std                  | 4.93       |\n","|    value_loss           | 42.7       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 832     |\n","|    iterations      | 319     |\n","|    time_elapsed    | 4710    |\n","|    total_timesteps | 3919872 |\n","--------------------------------\n","Eval num_timesteps=3924000, episode_reward=-55.78 +/- 0.42\n","Episode length: 432.20 +/- 30.37\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 432         |\n","|    mean_reward          | -55.8       |\n","| time/                   |             |\n","|    total_timesteps      | 3924000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002377905 |\n","|    clip_fraction        | 0.111       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12         |\n","|    explained_variance   | -0.474      |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0987     |\n","|    n_updates            | 1595        |\n","|    policy_gradient_loss | -0.00437    |\n","|    std                  | 4.96        |\n","|    value_loss           | 0.702       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 831     |\n","|    iterations      | 320     |\n","|    time_elapsed    | 4726    |\n","|    total_timesteps | 3932160 |\n","--------------------------------\n","Eval num_timesteps=3936000, episode_reward=37.18 +/- 1.12\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 37.2         |\n","| time/                   |              |\n","|    total_timesteps      | 3936000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019209386 |\n","|    clip_fraction        | 0.0823       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.1        |\n","|    explained_variance   | 0.121        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.11        |\n","|    n_updates            | 1600         |\n","|    policy_gradient_loss | -0.00412     |\n","|    std                  | 4.98         |\n","|    value_loss           | 0.469        |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 832     |\n","|    iterations      | 321     |\n","|    time_elapsed    | 4739    |\n","|    total_timesteps | 3944448 |\n","--------------------------------\n","Eval num_timesteps=3948000, episode_reward=-53.85 +/- 1.84\n","Episode length: 430.60 +/- 5.39\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 431         |\n","|    mean_reward          | -53.9       |\n","| time/                   |             |\n","|    total_timesteps      | 3948000     |\n","| train/                  |             |\n","|    approx_kl            | 0.001934009 |\n","|    clip_fraction        | 0.0949      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12.1       |\n","|    explained_variance   | -0.218      |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.0929     |\n","|    n_updates            | 1605        |\n","|    policy_gradient_loss | -0.00386    |\n","|    std                  | 4.99        |\n","|    value_loss           | 1.51        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 832     |\n","|    iterations      | 322     |\n","|    time_elapsed    | 4752    |\n","|    total_timesteps | 3956736 |\n","--------------------------------\n","Eval num_timesteps=3960000, episode_reward=-57.76 +/- 0.75\n","Episode length: 395.60 +/- 10.29\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 396         |\n","|    mean_reward          | -57.8       |\n","| time/                   |             |\n","|    total_timesteps      | 3960000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002740999 |\n","|    clip_fraction        | 0.118       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12.1       |\n","|    explained_variance   | 0.542       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 0.75        |\n","|    n_updates            | 1610        |\n","|    policy_gradient_loss | -0.00269    |\n","|    std                  | 5.05        |\n","|    value_loss           | 16.3        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 832     |\n","|    iterations      | 323     |\n","|    time_elapsed    | 4767    |\n","|    total_timesteps | 3969024 |\n","--------------------------------\n","Eval num_timesteps=3972000, episode_reward=-57.87 +/- 0.02\n","Episode length: 441.80 +/- 57.81\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 442          |\n","|    mean_reward          | -57.9        |\n","| time/                   |              |\n","|    total_timesteps      | 3972000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0031424146 |\n","|    clip_fraction        | 0.103        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.1        |\n","|    explained_variance   | 0.567        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.818        |\n","|    n_updates            | 1615         |\n","|    policy_gradient_loss | -0.00339     |\n","|    std                  | 5.1          |\n","|    value_loss           | 24.4         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 833     |\n","|    iterations      | 324     |\n","|    time_elapsed    | 4778    |\n","|    total_timesteps | 3981312 |\n","--------------------------------\n","Eval num_timesteps=3984000, episode_reward=-21.29 +/- 45.81\n","Episode length: 416.00 +/- 68.59\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 416          |\n","|    mean_reward          | -21.3        |\n","| time/                   |              |\n","|    total_timesteps      | 3984000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0028050132 |\n","|    clip_fraction        | 0.117        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.2        |\n","|    explained_variance   | -0.123       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.469        |\n","|    n_updates            | 1620         |\n","|    policy_gradient_loss | -0.00337     |\n","|    std                  | 5.15         |\n","|    value_loss           | 20.8         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 832     |\n","|    iterations      | 325     |\n","|    time_elapsed    | 4794    |\n","|    total_timesteps | 3993600 |\n","--------------------------------\n","Eval num_timesteps=3996000, episode_reward=-60.12 +/- 1.11\n","Episode length: 392.80 +/- 15.68\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 393          |\n","|    mean_reward          | -60.1        |\n","| time/                   |              |\n","|    total_timesteps      | 3996000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020927587 |\n","|    clip_fraction        | 0.0884       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.2        |\n","|    explained_variance   | -0.459       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0988      |\n","|    n_updates            | 1625         |\n","|    policy_gradient_loss | -0.00341     |\n","|    std                  | 5.15         |\n","|    value_loss           | 0.828        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 833     |\n","|    iterations      | 326     |\n","|    time_elapsed    | 4808    |\n","|    total_timesteps | 4005888 |\n","--------------------------------\n","Eval num_timesteps=4008000, episode_reward=-52.83 +/- 0.70\n","Episode length: 397.20 +/- 11.27\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 397          |\n","|    mean_reward          | -52.8        |\n","| time/                   |              |\n","|    total_timesteps      | 4008000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0028573612 |\n","|    clip_fraction        | 0.11         |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.2        |\n","|    explained_variance   | 0.305        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.502        |\n","|    n_updates            | 1630         |\n","|    policy_gradient_loss | -0.00267     |\n","|    std                  | 5.19         |\n","|    value_loss           | 23.5         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 833     |\n","|    iterations      | 327     |\n","|    time_elapsed    | 4820    |\n","|    total_timesteps | 4018176 |\n","--------------------------------\n","Eval num_timesteps=4020000, episode_reward=-59.84 +/- 1.23\n","Episode length: 434.00 +/- 51.44\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 434          |\n","|    mean_reward          | -59.8        |\n","| time/                   |              |\n","|    total_timesteps      | 4020000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0028427567 |\n","|    clip_fraction        | 0.135        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.3        |\n","|    explained_variance   | 0.645        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.983        |\n","|    n_updates            | 1635         |\n","|    policy_gradient_loss | -0.00446     |\n","|    std                  | 5.25         |\n","|    value_loss           | 44.4         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 833     |\n","|    iterations      | 328     |\n","|    time_elapsed    | 4836    |\n","|    total_timesteps | 4030464 |\n","--------------------------------\n","Eval num_timesteps=4032000, episode_reward=1.03 +/- 49.53\n","Episode length: 474.40 +/- 31.35\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 474          |\n","|    mean_reward          | 1.03         |\n","| time/                   |              |\n","|    total_timesteps      | 4032000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020914823 |\n","|    clip_fraction        | 0.0896       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.3        |\n","|    explained_variance   | -0.772       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0993      |\n","|    n_updates            | 1640         |\n","|    policy_gradient_loss | -0.0043      |\n","|    std                  | 5.25         |\n","|    value_loss           | 1.66         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 833     |\n","|    iterations      | 329     |\n","|    time_elapsed    | 4850    |\n","|    total_timesteps | 4042752 |\n","--------------------------------\n","Eval num_timesteps=4044000, episode_reward=-57.72 +/- 0.18\n","Episode length: 463.80 +/- 30.86\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 464         |\n","|    mean_reward          | -57.7       |\n","| time/                   |             |\n","|    total_timesteps      | 4044000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002114992 |\n","|    clip_fraction        | 0.0755      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12.3       |\n","|    explained_variance   | 0.429       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.11       |\n","|    n_updates            | 1645        |\n","|    policy_gradient_loss | -0.00467    |\n","|    std                  | 5.25        |\n","|    value_loss           | 0.367       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 833     |\n","|    iterations      | 330     |\n","|    time_elapsed    | 4863    |\n","|    total_timesteps | 4055040 |\n","--------------------------------\n","Eval num_timesteps=4056000, episode_reward=3.30 +/- 51.62\n","Episode length: 453.60 +/- 56.83\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 454          |\n","|    mean_reward          | 3.3          |\n","| time/                   |              |\n","|    total_timesteps      | 4056000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020647608 |\n","|    clip_fraction        | 0.0983       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.3        |\n","|    explained_variance   | 0.414        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.084       |\n","|    n_updates            | 1650         |\n","|    policy_gradient_loss | -0.00492     |\n","|    std                  | 5.28         |\n","|    value_loss           | 0.738        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 833     |\n","|    iterations      | 331     |\n","|    time_elapsed    | 4879    |\n","|    total_timesteps | 4067328 |\n","--------------------------------\n","Eval num_timesteps=4068000, episode_reward=-55.22 +/- 2.99\n","Episode length: 447.20 +/- 35.76\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 447          |\n","|    mean_reward          | -55.2        |\n","| time/                   |              |\n","|    total_timesteps      | 4068000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0029678158 |\n","|    clip_fraction        | 0.106        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.3        |\n","|    explained_variance   | 0.285        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 1.03         |\n","|    n_updates            | 1655         |\n","|    policy_gradient_loss | -0.00407     |\n","|    std                  | 5.34         |\n","|    value_loss           | 24.1         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 833     |\n","|    iterations      | 332     |\n","|    time_elapsed    | 4893    |\n","|    total_timesteps | 4079616 |\n","--------------------------------\n","Eval num_timesteps=4080000, episode_reward=-12.84 +/- 50.96\n","Episode length: 480.80 +/- 15.68\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 481          |\n","|    mean_reward          | -12.8        |\n","| time/                   |              |\n","|    total_timesteps      | 4080000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0024836473 |\n","|    clip_fraction        | 0.0884       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.3        |\n","|    explained_variance   | 0.407        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.108        |\n","|    n_updates            | 1660         |\n","|    policy_gradient_loss | -0.00371     |\n","|    std                  | 5.37         |\n","|    value_loss           | 10.9         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 833     |\n","|    iterations      | 333     |\n","|    time_elapsed    | 4907    |\n","|    total_timesteps | 4091904 |\n","--------------------------------\n","Eval num_timesteps=4092000, episode_reward=-54.45 +/- 0.04\n","Episode length: 453.60 +/- 28.90\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 454          |\n","|    mean_reward          | -54.5        |\n","| time/                   |              |\n","|    total_timesteps      | 4092000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021286176 |\n","|    clip_fraction        | 0.0875       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.4        |\n","|    explained_variance   | 0.178        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0767      |\n","|    n_updates            | 1665         |\n","|    policy_gradient_loss | -0.00366     |\n","|    std                  | 5.37         |\n","|    value_loss           | 1.63         |\n","------------------------------------------\n","Eval num_timesteps=4104000, episode_reward=0.39 +/- 40.42\n","Episode length: 488.00 +/- 14.70\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 488      |\n","|    mean_reward     | 0.391    |\n","| time/              |          |\n","|    total_timesteps | 4104000  |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 832     |\n","|    iterations      | 334     |\n","|    time_elapsed    | 4927    |\n","|    total_timesteps | 4104192 |\n","--------------------------------\n","Eval num_timesteps=4116000, episode_reward=-0.04 +/- 50.06\n","Episode length: 444.80 +/- 67.61\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 445          |\n","|    mean_reward          | -0.0356      |\n","| time/                   |              |\n","|    total_timesteps      | 4116000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0017448198 |\n","|    clip_fraction        | 0.0607       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.4        |\n","|    explained_variance   | -0.464       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.101       |\n","|    n_updates            | 1670         |\n","|    policy_gradient_loss | -0.00345     |\n","|    std                  | 5.38         |\n","|    value_loss           | 0.856        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 832     |\n","|    iterations      | 335     |\n","|    time_elapsed    | 4943    |\n","|    total_timesteps | 4116480 |\n","--------------------------------\n","Eval num_timesteps=4128000, episode_reward=36.74 +/- 1.12\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 36.7         |\n","| time/                   |              |\n","|    total_timesteps      | 4128000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022124483 |\n","|    clip_fraction        | 0.0938       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.4        |\n","|    explained_variance   | -0.0282      |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.109       |\n","|    n_updates            | 1675         |\n","|    policy_gradient_loss | -0.00387     |\n","|    std                  | 5.41         |\n","|    value_loss           | 0.635        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 832     |\n","|    iterations      | 336     |\n","|    time_elapsed    | 4956    |\n","|    total_timesteps | 4128768 |\n","--------------------------------\n","Eval num_timesteps=4140000, episode_reward=-54.70 +/- 1.84\n","Episode length: 451.00 +/- 7.35\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 451          |\n","|    mean_reward          | -54.7        |\n","| time/                   |              |\n","|    total_timesteps      | 4140000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021388296 |\n","|    clip_fraction        | 0.0988       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.4        |\n","|    explained_variance   | 0.318        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.105       |\n","|    n_updates            | 1680         |\n","|    policy_gradient_loss | -0.00428     |\n","|    std                  | 5.42         |\n","|    value_loss           | 0.501        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 833     |\n","|    iterations      | 337     |\n","|    time_elapsed    | 4970    |\n","|    total_timesteps | 4141056 |\n","--------------------------------\n","Eval num_timesteps=4152000, episode_reward=3.02 +/- 50.80\n","Episode length: 469.60 +/- 37.23\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 470          |\n","|    mean_reward          | 3.02         |\n","| time/                   |              |\n","|    total_timesteps      | 4152000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0024326975 |\n","|    clip_fraction        | 0.0994       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.4        |\n","|    explained_variance   | 0.392        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.128        |\n","|    n_updates            | 1685         |\n","|    policy_gradient_loss | -0.0035      |\n","|    std                  | 5.49         |\n","|    value_loss           | 16.4         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 833     |\n","|    iterations      | 338     |\n","|    time_elapsed    | 4985    |\n","|    total_timesteps | 4153344 |\n","--------------------------------\n","Eval num_timesteps=4164000, episode_reward=34.77 +/- 1.22\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 34.8         |\n","| time/                   |              |\n","|    total_timesteps      | 4164000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0017969852 |\n","|    clip_fraction        | 0.0738       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.5        |\n","|    explained_variance   | 0.806        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.105       |\n","|    n_updates            | 1690         |\n","|    policy_gradient_loss | -0.00403     |\n","|    std                  | 5.52         |\n","|    value_loss           | 0.841        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 833     |\n","|    iterations      | 339     |\n","|    time_elapsed    | 4995    |\n","|    total_timesteps | 4165632 |\n","--------------------------------\n","Eval num_timesteps=4176000, episode_reward=-58.49 +/- 0.22\n","Episode length: 397.60 +/- 11.76\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 398          |\n","|    mean_reward          | -58.5        |\n","| time/                   |              |\n","|    total_timesteps      | 4176000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020759732 |\n","|    clip_fraction        | 0.113        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.5        |\n","|    explained_variance   | 0.326        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.109       |\n","|    n_updates            | 1695         |\n","|    policy_gradient_loss | -0.00294     |\n","|    std                  | 5.54         |\n","|    value_loss           | 6.44         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 833     |\n","|    iterations      | 340     |\n","|    time_elapsed    | 5010    |\n","|    total_timesteps | 4177920 |\n","--------------------------------\n","Eval num_timesteps=4188000, episode_reward=-14.52 +/- 44.12\n","Episode length: 475.40 +/- 20.09\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 475         |\n","|    mean_reward          | -14.5       |\n","| time/                   |             |\n","|    total_timesteps      | 4188000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002915025 |\n","|    clip_fraction        | 0.115       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12.5       |\n","|    explained_variance   | 0.689       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 0.511       |\n","|    n_updates            | 1700        |\n","|    policy_gradient_loss | -0.00322    |\n","|    std                  | 5.59        |\n","|    value_loss           | 23.4        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 834     |\n","|    iterations      | 341     |\n","|    time_elapsed    | 5024    |\n","|    total_timesteps | 4190208 |\n","--------------------------------\n","Eval num_timesteps=4200000, episode_reward=-18.65 +/- 47.88\n","Episode length: 417.80 +/- 67.12\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 418          |\n","|    mean_reward          | -18.7        |\n","| time/                   |              |\n","|    total_timesteps      | 4200000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018673538 |\n","|    clip_fraction        | 0.0982       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.5        |\n","|    explained_variance   | -3.48        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.112       |\n","|    n_updates            | 1705         |\n","|    policy_gradient_loss | -0.0022      |\n","|    std                  | 5.62         |\n","|    value_loss           | 0.597        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 834     |\n","|    iterations      | 342     |\n","|    time_elapsed    | 5036    |\n","|    total_timesteps | 4202496 |\n","--------------------------------\n","Eval num_timesteps=4212000, episode_reward=-52.69 +/- 2.18\n","Episode length: 452.00 +/- 29.39\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 452          |\n","|    mean_reward          | -52.7        |\n","| time/                   |              |\n","|    total_timesteps      | 4212000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0026233175 |\n","|    clip_fraction        | 0.115        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.6        |\n","|    explained_variance   | 0.556        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.292        |\n","|    n_updates            | 1710         |\n","|    policy_gradient_loss | -0.00342     |\n","|    std                  | 5.65         |\n","|    value_loss           | 13.5         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 834     |\n","|    iterations      | 343     |\n","|    time_elapsed    | 5051    |\n","|    total_timesteps | 4214784 |\n","--------------------------------\n","Eval num_timesteps=4224000, episode_reward=-59.86 +/- 2.51\n","Episode length: 392.00 +/- 44.09\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 392          |\n","|    mean_reward          | -59.9        |\n","| time/                   |              |\n","|    total_timesteps      | 4224000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0028066002 |\n","|    clip_fraction        | 0.0995       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.6        |\n","|    explained_variance   | 0.479        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.255        |\n","|    n_updates            | 1715         |\n","|    policy_gradient_loss | -0.00313     |\n","|    std                  | 5.69         |\n","|    value_loss           | 21.1         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 834     |\n","|    iterations      | 344     |\n","|    time_elapsed    | 5063    |\n","|    total_timesteps | 4227072 |\n","--------------------------------\n","Eval num_timesteps=4236000, episode_reward=-54.50 +/- 2.57\n","Episode length: 414.80 +/- 20.58\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 415          |\n","|    mean_reward          | -54.5        |\n","| time/                   |              |\n","|    total_timesteps      | 4236000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022003793 |\n","|    clip_fraction        | 0.0838       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.6        |\n","|    explained_variance   | 0.464        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.704        |\n","|    n_updates            | 1720         |\n","|    policy_gradient_loss | -0.00343     |\n","|    std                  | 5.72         |\n","|    value_loss           | 39.7         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 835     |\n","|    iterations      | 345     |\n","|    time_elapsed    | 5077    |\n","|    total_timesteps | 4239360 |\n","--------------------------------\n","Eval num_timesteps=4248000, episode_reward=-54.07 +/- 1.70\n","Episode length: 405.60 +/- 29.88\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 406         |\n","|    mean_reward          | -54.1       |\n","| time/                   |             |\n","|    total_timesteps      | 4248000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002195066 |\n","|    clip_fraction        | 0.0798      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12.6       |\n","|    explained_variance   | 0.589       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 1.39        |\n","|    n_updates            | 1725        |\n","|    policy_gradient_loss | -0.00286    |\n","|    std                  | 5.76        |\n","|    value_loss           | 42.9        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 835     |\n","|    iterations      | 346     |\n","|    time_elapsed    | 5091    |\n","|    total_timesteps | 4251648 |\n","--------------------------------\n","Eval num_timesteps=4260000, episode_reward=4.10 +/- 50.14\n","Episode length: 458.40 +/- 50.95\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 458          |\n","|    mean_reward          | 4.1          |\n","| time/                   |              |\n","|    total_timesteps      | 4260000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0026939884 |\n","|    clip_fraction        | 0.106        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.7        |\n","|    explained_variance   | 0.856        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.498        |\n","|    n_updates            | 1730         |\n","|    policy_gradient_loss | -0.00347     |\n","|    std                  | 5.8          |\n","|    value_loss           | 12.7         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 835     |\n","|    iterations      | 347     |\n","|    time_elapsed    | 5102    |\n","|    total_timesteps | 4263936 |\n","--------------------------------\n","Eval num_timesteps=4272000, episode_reward=-53.06 +/- 3.34\n","Episode length: 398.00 +/- 31.84\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 398          |\n","|    mean_reward          | -53.1        |\n","| time/                   |              |\n","|    total_timesteps      | 4272000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021714307 |\n","|    clip_fraction        | 0.0867       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.7        |\n","|    explained_variance   | 0.016        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.119       |\n","|    n_updates            | 1735         |\n","|    policy_gradient_loss | -0.00465     |\n","|    std                  | 5.79         |\n","|    value_loss           | 0.322        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 835     |\n","|    iterations      | 348     |\n","|    time_elapsed    | 5118    |\n","|    total_timesteps | 4276224 |\n","--------------------------------\n","Eval num_timesteps=4284000, episode_reward=44.50 +/- 3.45\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 44.5         |\n","| time/                   |              |\n","|    total_timesteps      | 4284000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0029042661 |\n","|    clip_fraction        | 0.154        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.7        |\n","|    explained_variance   | 0.525        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.29         |\n","|    n_updates            | 1740         |\n","|    policy_gradient_loss | -0.00364     |\n","|    std                  | 5.85         |\n","|    value_loss           | 14.6         |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 835     |\n","|    iterations      | 349     |\n","|    time_elapsed    | 5135    |\n","|    total_timesteps | 4288512 |\n","--------------------------------\n","Eval num_timesteps=4296000, episode_reward=-53.50 +/- 0.56\n","Episode length: 434.20 +/- 38.21\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 434         |\n","|    mean_reward          | -53.5       |\n","| time/                   |             |\n","|    total_timesteps      | 4296000     |\n","| train/                  |             |\n","|    approx_kl            | 0.001960899 |\n","|    clip_fraction        | 0.0815      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12.7       |\n","|    explained_variance   | -0.0834     |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.119      |\n","|    n_updates            | 1745        |\n","|    policy_gradient_loss | -0.00436    |\n","|    std                  | 5.85        |\n","|    value_loss           | 0.374       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 835     |\n","|    iterations      | 350     |\n","|    time_elapsed    | 5148    |\n","|    total_timesteps | 4300800 |\n","--------------------------------\n","Eval num_timesteps=4308000, episode_reward=-51.47 +/- 0.09\n","Episode length: 488.60 +/- 4.41\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 489         |\n","|    mean_reward          | -51.5       |\n","| time/                   |             |\n","|    total_timesteps      | 4308000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002000313 |\n","|    clip_fraction        | 0.0833      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12.7       |\n","|    explained_variance   | 0.116       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.118      |\n","|    n_updates            | 1750        |\n","|    policy_gradient_loss | -0.00439    |\n","|    std                  | 5.87        |\n","|    value_loss           | 0.386       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 835     |\n","|    iterations      | 351     |\n","|    time_elapsed    | 5164    |\n","|    total_timesteps | 4313088 |\n","--------------------------------\n","Eval num_timesteps=4320000, episode_reward=47.27 +/- 1.89\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 47.3        |\n","| time/                   |             |\n","|    total_timesteps      | 4320000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002351756 |\n","|    clip_fraction        | 0.097       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12.7       |\n","|    explained_variance   | -2.07       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.102      |\n","|    n_updates            | 1755        |\n","|    policy_gradient_loss | -0.00377    |\n","|    std                  | 5.88        |\n","|    value_loss           | 1.07        |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 834     |\n","|    iterations      | 352     |\n","|    time_elapsed    | 5181    |\n","|    total_timesteps | 4325376 |\n","--------------------------------\n","Eval num_timesteps=4332000, episode_reward=-53.03 +/- 0.62\n","Episode length: 439.60 +/- 38.70\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 440         |\n","|    mean_reward          | -53         |\n","| time/                   |             |\n","|    total_timesteps      | 4332000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002068092 |\n","|    clip_fraction        | 0.0844      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12.7       |\n","|    explained_variance   | 0.184       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.122      |\n","|    n_updates            | 1760        |\n","|    policy_gradient_loss | -0.00466    |\n","|    std                  | 5.91        |\n","|    value_loss           | 0.311       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 834     |\n","|    iterations      | 353     |\n","|    time_elapsed    | 5196    |\n","|    total_timesteps | 4337664 |\n","--------------------------------\n","Eval num_timesteps=4344000, episode_reward=-55.18 +/- 2.65\n","Episode length: 422.60 +/- 20.09\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 423         |\n","|    mean_reward          | -55.2       |\n","| time/                   |             |\n","|    total_timesteps      | 4344000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002282982 |\n","|    clip_fraction        | 0.0956      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12.8       |\n","|    explained_variance   | 0.175       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 0.224       |\n","|    n_updates            | 1765        |\n","|    policy_gradient_loss | -0.00314    |\n","|    std                  | 5.99        |\n","|    value_loss           | 16.8        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 834     |\n","|    iterations      | 354     |\n","|    time_elapsed    | 5209    |\n","|    total_timesteps | 4349952 |\n","--------------------------------\n","Eval num_timesteps=4356000, episode_reward=31.57 +/- 7.79\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 31.6         |\n","| time/                   |              |\n","|    total_timesteps      | 4356000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020843013 |\n","|    clip_fraction        | 0.0756       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.8        |\n","|    explained_variance   | 0.615        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.707        |\n","|    n_updates            | 1770         |\n","|    policy_gradient_loss | -0.00385     |\n","|    std                  | 6.05         |\n","|    value_loss           | 39.5         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 834     |\n","|    iterations      | 355     |\n","|    time_elapsed    | 5227    |\n","|    total_timesteps | 4362240 |\n","--------------------------------\n","Eval num_timesteps=4368000, episode_reward=-16.88 +/- 41.78\n","Episode length: 471.80 +/- 23.03\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 472          |\n","|    mean_reward          | -16.9        |\n","| time/                   |              |\n","|    total_timesteps      | 4368000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0024527677 |\n","|    clip_fraction        | 0.102        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.8        |\n","|    explained_variance   | 0.641        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.365        |\n","|    n_updates            | 1775         |\n","|    policy_gradient_loss | -0.00351     |\n","|    std                  | 6.08         |\n","|    value_loss           | 18.3         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 834     |\n","|    iterations      | 356     |\n","|    time_elapsed    | 5243    |\n","|    total_timesteps | 4374528 |\n","--------------------------------\n","Eval num_timesteps=4380000, episode_reward=43.16 +/- 4.65\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 43.2        |\n","| time/                   |             |\n","|    total_timesteps      | 4380000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002381942 |\n","|    clip_fraction        | 0.105       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -12.9       |\n","|    explained_variance   | 0.583       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 0.404       |\n","|    n_updates            | 1780        |\n","|    policy_gradient_loss | -0.00299    |\n","|    std                  | 6.15        |\n","|    value_loss           | 11.2        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 834     |\n","|    iterations      | 357     |\n","|    time_elapsed    | 5257    |\n","|    total_timesteps | 4386816 |\n","--------------------------------\n","Eval num_timesteps=4392000, episode_reward=-17.15 +/- 44.48\n","Episode length: 481.40 +/- 15.19\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 481          |\n","|    mean_reward          | -17.1        |\n","| time/                   |              |\n","|    total_timesteps      | 4392000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0023926261 |\n","|    clip_fraction        | 0.0998       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.9        |\n","|    explained_variance   | -0.714       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0786      |\n","|    n_updates            | 1785         |\n","|    policy_gradient_loss | -0.00419     |\n","|    std                  | 6.18         |\n","|    value_loss           | 1.49         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 834     |\n","|    iterations      | 358     |\n","|    time_elapsed    | 5272    |\n","|    total_timesteps | 4399104 |\n","--------------------------------\n","Eval num_timesteps=4404000, episode_reward=41.82 +/- 2.60\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 41.8         |\n","| time/                   |              |\n","|    total_timesteps      | 4404000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020221279 |\n","|    clip_fraction        | 0.0875       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.9        |\n","|    explained_variance   | -0.32        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.114       |\n","|    n_updates            | 1790         |\n","|    policy_gradient_loss | -0.00478     |\n","|    std                  | 6.18         |\n","|    value_loss           | 0.557        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 833     |\n","|    iterations      | 359     |\n","|    time_elapsed    | 5289    |\n","|    total_timesteps | 4411392 |\n","--------------------------------\n","Eval num_timesteps=4416000, episode_reward=-52.92 +/- 2.18\n","Episode length: 458.20 +/- 42.62\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 458          |\n","|    mean_reward          | -52.9        |\n","| time/                   |              |\n","|    total_timesteps      | 4416000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0028073436 |\n","|    clip_fraction        | 0.151        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -12.9        |\n","|    explained_variance   | 0.446        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 2.18         |\n","|    n_updates            | 1795         |\n","|    policy_gradient_loss | -0.00512     |\n","|    std                  | 6.21         |\n","|    value_loss           | 33.3         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 833     |\n","|    iterations      | 360     |\n","|    time_elapsed    | 5304    |\n","|    total_timesteps | 4423680 |\n","--------------------------------\n","Eval num_timesteps=4428000, episode_reward=-23.65 +/- 44.89\n","Episode length: 473.60 +/- 21.56\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 474          |\n","|    mean_reward          | -23.7        |\n","| time/                   |              |\n","|    total_timesteps      | 4428000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0024443392 |\n","|    clip_fraction        | 0.0884       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13          |\n","|    explained_variance   | 0.619        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.0338       |\n","|    n_updates            | 1800         |\n","|    policy_gradient_loss | -0.00358     |\n","|    std                  | 6.27         |\n","|    value_loss           | 14.4         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 834     |\n","|    iterations      | 361     |\n","|    time_elapsed    | 5318    |\n","|    total_timesteps | 4435968 |\n","--------------------------------\n","Eval num_timesteps=4440000, episode_reward=5.16 +/- 51.46\n","Episode length: 467.20 +/- 40.17\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 467          |\n","|    mean_reward          | 5.16         |\n","| time/                   |              |\n","|    total_timesteps      | 4440000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020607144 |\n","|    clip_fraction        | 0.103        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13          |\n","|    explained_variance   | -0.365       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.122       |\n","|    n_updates            | 1805         |\n","|    policy_gradient_loss | -0.00409     |\n","|    std                  | 6.3          |\n","|    value_loss           | 0.438        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 833     |\n","|    iterations      | 362     |\n","|    time_elapsed    | 5335    |\n","|    total_timesteps | 4448256 |\n","--------------------------------\n","Eval num_timesteps=4452000, episode_reward=46.17 +/- 3.08\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 46.2         |\n","| time/                   |              |\n","|    total_timesteps      | 4452000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022288775 |\n","|    clip_fraction        | 0.0828       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13          |\n","|    explained_variance   | -0.0505      |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.118       |\n","|    n_updates            | 1810         |\n","|    policy_gradient_loss | -0.00459     |\n","|    std                  | 6.27         |\n","|    value_loss           | 0.878        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 833     |\n","|    iterations      | 363     |\n","|    time_elapsed    | 5352    |\n","|    total_timesteps | 4460544 |\n","--------------------------------\n","Eval num_timesteps=4464000, episode_reward=34.77 +/- 2.67\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 34.8         |\n","| time/                   |              |\n","|    total_timesteps      | 4464000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020816317 |\n","|    clip_fraction        | 0.083        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13          |\n","|    explained_variance   | 0.289        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.124       |\n","|    n_updates            | 1815         |\n","|    policy_gradient_loss | -0.00483     |\n","|    std                  | 6.28         |\n","|    value_loss           | 0.444        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 833     |\n","|    iterations      | 364     |\n","|    time_elapsed    | 5364    |\n","|    total_timesteps | 4472832 |\n","--------------------------------\n","Eval num_timesteps=4476000, episode_reward=4.67 +/- 47.59\n","Episode length: 495.60 +/- 5.39\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 496          |\n","|    mean_reward          | 4.67         |\n","| time/                   |              |\n","|    total_timesteps      | 4476000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021510476 |\n","|    clip_fraction        | 0.0846       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13          |\n","|    explained_variance   | 0.492        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0871      |\n","|    n_updates            | 1820         |\n","|    policy_gradient_loss | -0.0032      |\n","|    std                  | 6.3          |\n","|    value_loss           | 1.83         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 833     |\n","|    iterations      | 365     |\n","|    time_elapsed    | 5379    |\n","|    total_timesteps | 4485120 |\n","--------------------------------\n","Eval num_timesteps=4488000, episode_reward=-12.21 +/- 46.95\n","Episode length: 474.20 +/- 21.07\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 474          |\n","|    mean_reward          | -12.2        |\n","| time/                   |              |\n","|    total_timesteps      | 4488000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020755816 |\n","|    clip_fraction        | 0.0999       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13          |\n","|    explained_variance   | 0.222        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.12        |\n","|    n_updates            | 1825         |\n","|    policy_gradient_loss | -0.0043      |\n","|    std                  | 6.3          |\n","|    value_loss           | 0.464        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 833     |\n","|    iterations      | 366     |\n","|    time_elapsed    | 5395    |\n","|    total_timesteps | 4497408 |\n","--------------------------------\n","Eval num_timesteps=4500000, episode_reward=46.79 +/- 1.61\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 46.8         |\n","| time/                   |              |\n","|    total_timesteps      | 4500000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020717161 |\n","|    clip_fraction        | 0.0856       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13          |\n","|    explained_variance   | 0.236        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.124       |\n","|    n_updates            | 1830         |\n","|    policy_gradient_loss | -0.0038      |\n","|    std                  | 6.31         |\n","|    value_loss           | 0.426        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 833     |\n","|    iterations      | 367     |\n","|    time_elapsed    | 5407    |\n","|    total_timesteps | 4509696 |\n","--------------------------------\n","Eval num_timesteps=4512000, episode_reward=6.31 +/- 53.49\n","Episode length: 478.80 +/- 25.96\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 479          |\n","|    mean_reward          | 6.31         |\n","| time/                   |              |\n","|    total_timesteps      | 4512000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019383699 |\n","|    clip_fraction        | 0.0839       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13          |\n","|    explained_variance   | 0.18         |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.116       |\n","|    n_updates            | 1835         |\n","|    policy_gradient_loss | -0.00356     |\n","|    std                  | 6.37         |\n","|    value_loss           | 0.51         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 833     |\n","|    iterations      | 368     |\n","|    time_elapsed    | 5422    |\n","|    total_timesteps | 4521984 |\n","--------------------------------\n","Eval num_timesteps=4524000, episode_reward=-18.90 +/- 52.94\n","Episode length: 464.60 +/- 28.90\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 465          |\n","|    mean_reward          | -18.9        |\n","| time/                   |              |\n","|    total_timesteps      | 4524000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020388952 |\n","|    clip_fraction        | 0.0816       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13          |\n","|    explained_variance   | 0.317        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.126       |\n","|    n_updates            | 1840         |\n","|    policy_gradient_loss | -0.00382     |\n","|    std                  | 6.39         |\n","|    value_loss           | 0.4          |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 833     |\n","|    iterations      | 369     |\n","|    time_elapsed    | 5437    |\n","|    total_timesteps | 4534272 |\n","--------------------------------\n","Eval num_timesteps=4536000, episode_reward=-55.91 +/- 1.42\n","Episode length: 389.80 +/- 18.62\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 390          |\n","|    mean_reward          | -55.9        |\n","| time/                   |              |\n","|    total_timesteps      | 4536000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019197227 |\n","|    clip_fraction        | 0.0706       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.1        |\n","|    explained_variance   | 0.364        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.119       |\n","|    n_updates            | 1845         |\n","|    policy_gradient_loss | -0.00354     |\n","|    std                  | 6.41         |\n","|    value_loss           | 0.441        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 834     |\n","|    iterations      | 370     |\n","|    time_elapsed    | 5447    |\n","|    total_timesteps | 4546560 |\n","--------------------------------\n","Eval num_timesteps=4548000, episode_reward=36.72 +/- 1.46\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 36.7         |\n","| time/                   |              |\n","|    total_timesteps      | 4548000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018580118 |\n","|    clip_fraction        | 0.0879       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.1        |\n","|    explained_variance   | 0.637        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.642        |\n","|    n_updates            | 1850         |\n","|    policy_gradient_loss | -0.00272     |\n","|    std                  | 6.46         |\n","|    value_loss           | 24.9         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 834     |\n","|    iterations      | 371     |\n","|    time_elapsed    | 5463    |\n","|    total_timesteps | 4558848 |\n","--------------------------------\n","Eval num_timesteps=4560000, episode_reward=9.31 +/- 53.52\n","Episode length: 456.80 +/- 52.91\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 457         |\n","|    mean_reward          | 9.31        |\n","| time/                   |             |\n","|    total_timesteps      | 4560000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002112137 |\n","|    clip_fraction        | 0.0903      |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -13.1       |\n","|    explained_variance   | 0.543       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | -0.1        |\n","|    n_updates            | 1855        |\n","|    policy_gradient_loss | -0.00454    |\n","|    std                  | 6.5         |\n","|    value_loss           | 1.16        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 834     |\n","|    iterations      | 372     |\n","|    time_elapsed    | 5478    |\n","|    total_timesteps | 4571136 |\n","--------------------------------\n","Eval num_timesteps=4572000, episode_reward=-49.98 +/- 0.38\n","Episode length: 432.60 +/- 1.96\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 433          |\n","|    mean_reward          | -50          |\n","| time/                   |              |\n","|    total_timesteps      | 4572000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021681117 |\n","|    clip_fraction        | 0.109        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.1        |\n","|    explained_variance   | 0.152        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0882      |\n","|    n_updates            | 1860         |\n","|    policy_gradient_loss | -0.00482     |\n","|    std                  | 6.51         |\n","|    value_loss           | 1.45         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 834     |\n","|    iterations      | 373     |\n","|    time_elapsed    | 5489    |\n","|    total_timesteps | 4583424 |\n","--------------------------------\n","Eval num_timesteps=4584000, episode_reward=46.79 +/- 2.08\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 46.8         |\n","| time/                   |              |\n","|    total_timesteps      | 4584000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0025284563 |\n","|    clip_fraction        | 0.116        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.1        |\n","|    explained_variance   | 0.629        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.31         |\n","|    n_updates            | 1865         |\n","|    policy_gradient_loss | -0.00353     |\n","|    std                  | 6.56         |\n","|    value_loss           | 14.4         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 834     |\n","|    iterations      | 374     |\n","|    time_elapsed    | 5505    |\n","|    total_timesteps | 4595712 |\n","--------------------------------\n","Eval num_timesteps=4596000, episode_reward=4.72 +/- 38.98\n","Episode length: 496.40 +/- 4.41\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 496          |\n","|    mean_reward          | 4.72         |\n","| time/                   |              |\n","|    total_timesteps      | 4596000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019658117 |\n","|    clip_fraction        | 0.0815       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.2        |\n","|    explained_variance   | 0.238        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.46         |\n","|    n_updates            | 1870         |\n","|    policy_gradient_loss | -0.00325     |\n","|    std                  | 6.61         |\n","|    value_loss           | 14.8         |\n","------------------------------------------\n","Eval num_timesteps=4608000, episode_reward=-53.97 +/- 0.73\n","Episode length: 415.80 +/- 10.78\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 416      |\n","|    mean_reward     | -54      |\n","| time/              |          |\n","|    total_timesteps | 4608000  |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 833     |\n","|    iterations      | 375     |\n","|    time_elapsed    | 5525    |\n","|    total_timesteps | 4608000 |\n","--------------------------------\n","Eval num_timesteps=4620000, episode_reward=8.04 +/- 51.24\n","Episode length: 466.00 +/- 41.64\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 466         |\n","|    mean_reward          | 8.04        |\n","| time/                   |             |\n","|    total_timesteps      | 4620000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002396534 |\n","|    clip_fraction        | 0.102       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -13.2       |\n","|    explained_variance   | 0.545       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 1.49        |\n","|    n_updates            | 1875        |\n","|    policy_gradient_loss | -0.00361    |\n","|    std                  | 6.66        |\n","|    value_loss           | 18.1        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 834     |\n","|    iterations      | 376     |\n","|    time_elapsed    | 5538    |\n","|    total_timesteps | 4620288 |\n","--------------------------------\n","Eval num_timesteps=4632000, episode_reward=8.57 +/- 56.31\n","Episode length: 458.40 +/- 50.95\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 458          |\n","|    mean_reward          | 8.57         |\n","| time/                   |              |\n","|    total_timesteps      | 4632000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0023036806 |\n","|    clip_fraction        | 0.0973       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.2        |\n","|    explained_variance   | 0.521        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.0856       |\n","|    n_updates            | 1880         |\n","|    policy_gradient_loss | -0.00482     |\n","|    std                  | 6.67         |\n","|    value_loss           | 3.21         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 834     |\n","|    iterations      | 377     |\n","|    time_elapsed    | 5552    |\n","|    total_timesteps | 4632576 |\n","--------------------------------\n","Eval num_timesteps=4644000, episode_reward=-57.36 +/- 2.92\n","Episode length: 397.20 +/- 3.43\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 397          |\n","|    mean_reward          | -57.4        |\n","| time/                   |              |\n","|    total_timesteps      | 4644000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0020661438 |\n","|    clip_fraction        | 0.0684       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.2        |\n","|    explained_variance   | 0.671        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.115        |\n","|    n_updates            | 1885         |\n","|    policy_gradient_loss | -0.00411     |\n","|    std                  | 6.74         |\n","|    value_loss           | 14.9         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 834     |\n","|    iterations      | 378     |\n","|    time_elapsed    | 5567    |\n","|    total_timesteps | 4644864 |\n","--------------------------------\n","Eval num_timesteps=4656000, episode_reward=-21.69 +/- 49.78\n","Episode length: 411.20 +/- 72.50\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 411          |\n","|    mean_reward          | -21.7        |\n","| time/                   |              |\n","|    total_timesteps      | 4656000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0023594047 |\n","|    clip_fraction        | 0.118        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.3        |\n","|    explained_variance   | 0.818        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.191        |\n","|    n_updates            | 1890         |\n","|    policy_gradient_loss | -0.00313     |\n","|    std                  | 6.76         |\n","|    value_loss           | 7.63         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 834     |\n","|    iterations      | 379     |\n","|    time_elapsed    | 5579    |\n","|    total_timesteps | 4657152 |\n","--------------------------------\n","Eval num_timesteps=4668000, episode_reward=50.85 +/- 1.76\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 50.8         |\n","| time/                   |              |\n","|    total_timesteps      | 4668000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0026712886 |\n","|    clip_fraction        | 0.0966       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.3        |\n","|    explained_variance   | 0.678        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.181        |\n","|    n_updates            | 1895         |\n","|    policy_gradient_loss | -0.00308     |\n","|    std                  | 6.81         |\n","|    value_loss           | 12.8         |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 834     |\n","|    iterations      | 380     |\n","|    time_elapsed    | 5594    |\n","|    total_timesteps | 4669440 |\n","--------------------------------\n","Eval num_timesteps=4680000, episode_reward=45.46 +/- 1.08\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 45.5       |\n","| time/                   |            |\n","|    total_timesteps      | 4680000    |\n","| train/                  |            |\n","|    approx_kl            | 0.00205316 |\n","|    clip_fraction        | 0.0794     |\n","|    clip_range           | 0.1        |\n","|    entropy_loss         | -13.3      |\n","|    explained_variance   | 0.196      |\n","|    learning_rate        | 0.000622   |\n","|    loss                 | -0.118     |\n","|    n_updates            | 1900       |\n","|    policy_gradient_loss | -0.00456   |\n","|    std                  | 6.82       |\n","|    value_loss           | 0.582      |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 834     |\n","|    iterations      | 381     |\n","|    time_elapsed    | 5609    |\n","|    total_timesteps | 4681728 |\n","--------------------------------\n","Eval num_timesteps=4692000, episode_reward=-15.29 +/- 46.72\n","Episode length: 461.60 +/- 31.35\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 462          |\n","|    mean_reward          | -15.3        |\n","| time/                   |              |\n","|    total_timesteps      | 4692000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0016425736 |\n","|    clip_fraction        | 0.0618       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.3        |\n","|    explained_variance   | 0.393        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.117       |\n","|    n_updates            | 1905         |\n","|    policy_gradient_loss | -0.00418     |\n","|    std                  | 6.82         |\n","|    value_loss           | 0.54         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 834     |\n","|    iterations      | 382     |\n","|    time_elapsed    | 5621    |\n","|    total_timesteps | 4694016 |\n","--------------------------------\n","Eval num_timesteps=4704000, episode_reward=3.34 +/- 50.55\n","Episode length: 454.80 +/- 55.36\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 455          |\n","|    mean_reward          | 3.34         |\n","| time/                   |              |\n","|    total_timesteps      | 4704000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019117739 |\n","|    clip_fraction        | 0.0884       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.3        |\n","|    explained_variance   | 0.207        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.123       |\n","|    n_updates            | 1910         |\n","|    policy_gradient_loss | -0.00444     |\n","|    std                  | 6.85         |\n","|    value_loss           | 0.448        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 834     |\n","|    iterations      | 383     |\n","|    time_elapsed    | 5636    |\n","|    total_timesteps | 4706304 |\n","--------------------------------\n","Eval num_timesteps=4716000, episode_reward=-55.72 +/- 0.12\n","Episode length: 401.60 +/- 31.35\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 402          |\n","|    mean_reward          | -55.7        |\n","| time/                   |              |\n","|    total_timesteps      | 4716000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0030994301 |\n","|    clip_fraction        | 0.139        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.4        |\n","|    explained_variance   | 0.000207     |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.351        |\n","|    n_updates            | 1915         |\n","|    policy_gradient_loss | -0.00295     |\n","|    std                  | 6.93         |\n","|    value_loss           | 20           |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 834     |\n","|    iterations      | 384     |\n","|    time_elapsed    | 5651    |\n","|    total_timesteps | 4718592 |\n","--------------------------------\n","Eval num_timesteps=4728000, episode_reward=4.10 +/- 45.77\n","Episode length: 466.40 +/- 41.15\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 466         |\n","|    mean_reward          | 4.1         |\n","| time/                   |             |\n","|    total_timesteps      | 4728000     |\n","| train/                  |             |\n","|    approx_kl            | 0.002998409 |\n","|    clip_fraction        | 0.113       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -13.4       |\n","|    explained_variance   | 0.691       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 0.395       |\n","|    n_updates            | 1920        |\n","|    policy_gradient_loss | -0.00386    |\n","|    std                  | 7           |\n","|    value_loss           | 15.2        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 835     |\n","|    iterations      | 385     |\n","|    time_elapsed    | 5662    |\n","|    total_timesteps | 4730880 |\n","--------------------------------\n","Eval num_timesteps=4740000, episode_reward=-56.09 +/- 2.89\n","Episode length: 427.40 +/- 68.10\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 427          |\n","|    mean_reward          | -56.1        |\n","| time/                   |              |\n","|    total_timesteps      | 4740000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0029194306 |\n","|    clip_fraction        | 0.109        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.4        |\n","|    explained_variance   | 0.558        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.105       |\n","|    n_updates            | 1925         |\n","|    policy_gradient_loss | -0.00292     |\n","|    std                  | 7.09         |\n","|    value_loss           | 16.8         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 835     |\n","|    iterations      | 386     |\n","|    time_elapsed    | 5678    |\n","|    total_timesteps | 4743168 |\n","--------------------------------\n","Eval num_timesteps=4752000, episode_reward=-17.98 +/- 43.91\n","Episode length: 464.60 +/- 28.90\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 465          |\n","|    mean_reward          | -18          |\n","| time/                   |              |\n","|    total_timesteps      | 4752000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0026983593 |\n","|    clip_fraction        | 0.102        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.5        |\n","|    explained_variance   | 0.439        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 1.15         |\n","|    n_updates            | 1930         |\n","|    policy_gradient_loss | -0.00363     |\n","|    std                  | 7.14         |\n","|    value_loss           | 33.3         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 835     |\n","|    iterations      | 387     |\n","|    time_elapsed    | 5692    |\n","|    total_timesteps | 4755456 |\n","--------------------------------\n","Eval num_timesteps=4764000, episode_reward=-54.10 +/- 2.11\n","Episode length: 422.80 +/- 25.47\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 423          |\n","|    mean_reward          | -54.1        |\n","| time/                   |              |\n","|    total_timesteps      | 4764000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0016986117 |\n","|    clip_fraction        | 0.0755       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.5        |\n","|    explained_variance   | -0.236       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.127       |\n","|    n_updates            | 1935         |\n","|    policy_gradient_loss | -0.00476     |\n","|    std                  | 7.16         |\n","|    value_loss           | 0.623        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 835     |\n","|    iterations      | 388     |\n","|    time_elapsed    | 5704    |\n","|    total_timesteps | 4767744 |\n","--------------------------------\n","Eval num_timesteps=4776000, episode_reward=48.23 +/- 1.19\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 48.2         |\n","| time/                   |              |\n","|    total_timesteps      | 4776000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022682387 |\n","|    clip_fraction        | 0.0884       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.5        |\n","|    explained_variance   | 0.532        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.503        |\n","|    n_updates            | 1940         |\n","|    policy_gradient_loss | -0.00335     |\n","|    std                  | 7.22         |\n","|    value_loss           | 25.1         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 835     |\n","|    iterations      | 389     |\n","|    time_elapsed    | 5719    |\n","|    total_timesteps | 4780032 |\n","--------------------------------\n","Eval num_timesteps=4788000, episode_reward=-12.12 +/- 45.03\n","Episode length: 497.60 +/- 1.96\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 498          |\n","|    mean_reward          | -12.1        |\n","| time/                   |              |\n","|    total_timesteps      | 4788000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021536518 |\n","|    clip_fraction        | 0.103        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.6        |\n","|    explained_variance   | 0.607        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.804        |\n","|    n_updates            | 1945         |\n","|    policy_gradient_loss | -0.00374     |\n","|    std                  | 7.27         |\n","|    value_loss           | 17.6         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 835     |\n","|    iterations      | 390     |\n","|    time_elapsed    | 5733    |\n","|    total_timesteps | 4792320 |\n","--------------------------------\n","Eval num_timesteps=4800000, episode_reward=45.06 +/- 3.71\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 45.1         |\n","| time/                   |              |\n","|    total_timesteps      | 4800000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0019117921 |\n","|    clip_fraction        | 0.0776       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.6        |\n","|    explained_variance   | -0.225       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.127       |\n","|    n_updates            | 1950         |\n","|    policy_gradient_loss | -0.00432     |\n","|    std                  | 7.26         |\n","|    value_loss           | 0.443        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 836     |\n","|    iterations      | 391     |\n","|    time_elapsed    | 5746    |\n","|    total_timesteps | 4804608 |\n","--------------------------------\n","Eval num_timesteps=4812000, episode_reward=-16.41 +/- 40.49\n","Episode length: 488.00 +/- 9.80\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 488          |\n","|    mean_reward          | -16.4        |\n","| time/                   |              |\n","|    total_timesteps      | 4812000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0028472517 |\n","|    clip_fraction        | 0.117        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.6        |\n","|    explained_variance   | 0.662        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.39         |\n","|    n_updates            | 1955         |\n","|    policy_gradient_loss | -0.00333     |\n","|    std                  | 7.33         |\n","|    value_loss           | 17.7         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 835     |\n","|    iterations      | 392     |\n","|    time_elapsed    | 5763    |\n","|    total_timesteps | 4816896 |\n","--------------------------------\n","Eval num_timesteps=4824000, episode_reward=-50.91 +/- 0.22\n","Episode length: 475.60 +/- 15.19\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 476          |\n","|    mean_reward          | -50.9        |\n","| time/                   |              |\n","|    total_timesteps      | 4824000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022352093 |\n","|    clip_fraction        | 0.0925       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.6        |\n","|    explained_variance   | -0.878       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.122       |\n","|    n_updates            | 1960         |\n","|    policy_gradient_loss | -0.00494     |\n","|    std                  | 7.36         |\n","|    value_loss           | 0.517        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 836     |\n","|    iterations      | 393     |\n","|    time_elapsed    | 5776    |\n","|    total_timesteps | 4829184 |\n","--------------------------------\n","Eval num_timesteps=4836000, episode_reward=39.52 +/- 9.25\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 39.5         |\n","| time/                   |              |\n","|    total_timesteps      | 4836000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021637122 |\n","|    clip_fraction        | 0.0968       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.6        |\n","|    explained_variance   | 0.621        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0515      |\n","|    n_updates            | 1965         |\n","|    policy_gradient_loss | -0.00364     |\n","|    std                  | 7.45         |\n","|    value_loss           | 10.6         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 836     |\n","|    iterations      | 394     |\n","|    time_elapsed    | 5790    |\n","|    total_timesteps | 4841472 |\n","--------------------------------\n","Eval num_timesteps=4848000, episode_reward=-56.58 +/- 2.94\n","Episode length: 370.20 +/- 21.07\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 370          |\n","|    mean_reward          | -56.6        |\n","| time/                   |              |\n","|    total_timesteps      | 4848000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022143442 |\n","|    clip_fraction        | 0.0731       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.7        |\n","|    explained_variance   | 0.0199       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.128       |\n","|    n_updates            | 1970         |\n","|    policy_gradient_loss | -0.00468     |\n","|    std                  | 7.45         |\n","|    value_loss           | 0.372        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 836     |\n","|    iterations      | 395     |\n","|    time_elapsed    | 5805    |\n","|    total_timesteps | 4853760 |\n","--------------------------------\n","Eval num_timesteps=4860000, episode_reward=-52.66 +/- 6.95\n","Episode length: 434.40 +/- 65.65\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 434          |\n","|    mean_reward          | -52.7        |\n","| time/                   |              |\n","|    total_timesteps      | 4860000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022593793 |\n","|    clip_fraction        | 0.0746       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.7        |\n","|    explained_variance   | 0.452        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 1.9          |\n","|    n_updates            | 1975         |\n","|    policy_gradient_loss | -0.00237     |\n","|    std                  | 7.49         |\n","|    value_loss           | 34.3         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 836     |\n","|    iterations      | 396     |\n","|    time_elapsed    | 5816    |\n","|    total_timesteps | 4866048 |\n","--------------------------------\n","Eval num_timesteps=4872000, episode_reward=-48.07 +/- 0.55\n","Episode length: 482.80 +/- 18.62\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 483          |\n","|    mean_reward          | -48.1        |\n","| time/                   |              |\n","|    total_timesteps      | 4872000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0024208378 |\n","|    clip_fraction        | 0.11         |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.7        |\n","|    explained_variance   | -0.256       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.12        |\n","|    n_updates            | 1980         |\n","|    policy_gradient_loss | -0.00377     |\n","|    std                  | 7.51         |\n","|    value_loss           | 0.636        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 836     |\n","|    iterations      | 397     |\n","|    time_elapsed    | 5831    |\n","|    total_timesteps | 4878336 |\n","--------------------------------\n","Eval num_timesteps=4884000, episode_reward=-53.46 +/- 0.45\n","Episode length: 408.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 408          |\n","|    mean_reward          | -53.5        |\n","| time/                   |              |\n","|    total_timesteps      | 4884000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0027189162 |\n","|    clip_fraction        | 0.121        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.7        |\n","|    explained_variance   | 0.0728       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.295        |\n","|    n_updates            | 1985         |\n","|    policy_gradient_loss | -0.00322     |\n","|    std                  | 7.61         |\n","|    value_loss           | 22           |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 836     |\n","|    iterations      | 398     |\n","|    time_elapsed    | 5846    |\n","|    total_timesteps | 4890624 |\n","--------------------------------\n","Eval num_timesteps=4896000, episode_reward=-15.87 +/- 42.91\n","Episode length: 473.60 +/- 21.56\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 474          |\n","|    mean_reward          | -15.9        |\n","| time/                   |              |\n","|    total_timesteps      | 4896000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0026890717 |\n","|    clip_fraction        | 0.107        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.8        |\n","|    explained_variance   | 0.612        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.434        |\n","|    n_updates            | 1990         |\n","|    policy_gradient_loss | -0.00358     |\n","|    std                  | 7.68         |\n","|    value_loss           | 26.2         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 837     |\n","|    iterations      | 399     |\n","|    time_elapsed    | 5857    |\n","|    total_timesteps | 4902912 |\n","--------------------------------\n","Eval num_timesteps=4908000, episode_reward=-17.42 +/- 42.25\n","Episode length: 465.80 +/- 27.92\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 466         |\n","|    mean_reward          | -17.4       |\n","| time/                   |             |\n","|    total_timesteps      | 4908000     |\n","| train/                  |             |\n","|    approx_kl            | 0.003008825 |\n","|    clip_fraction        | 0.112       |\n","|    clip_range           | 0.1         |\n","|    entropy_loss         | -13.8       |\n","|    explained_variance   | 0.785       |\n","|    learning_rate        | 0.000622    |\n","|    loss                 | 0.171       |\n","|    n_updates            | 1995        |\n","|    policy_gradient_loss | -0.00287    |\n","|    std                  | 7.78        |\n","|    value_loss           | 8.1         |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 836     |\n","|    iterations      | 400     |\n","|    time_elapsed    | 5873    |\n","|    total_timesteps | 4915200 |\n","--------------------------------\n","Eval num_timesteps=4920000, episode_reward=-55.65 +/- 4.13\n","Episode length: 404.00 +/- 31.84\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 404          |\n","|    mean_reward          | -55.6        |\n","| time/                   |              |\n","|    total_timesteps      | 4920000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021481605 |\n","|    clip_fraction        | 0.09         |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.9        |\n","|    explained_variance   | 0.559        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.0331      |\n","|    n_updates            | 2000         |\n","|    policy_gradient_loss | -0.00288     |\n","|    std                  | 7.84         |\n","|    value_loss           | 17           |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 837     |\n","|    iterations      | 401     |\n","|    time_elapsed    | 5886    |\n","|    total_timesteps | 4927488 |\n","--------------------------------\n","Eval num_timesteps=4932000, episode_reward=46.09 +/- 1.75\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 46.1         |\n","| time/                   |              |\n","|    total_timesteps      | 4932000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0023405333 |\n","|    clip_fraction        | 0.0894       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.9        |\n","|    explained_variance   | 0.824        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.644        |\n","|    n_updates            | 2005         |\n","|    policy_gradient_loss | -0.00331     |\n","|    std                  | 7.92         |\n","|    value_loss           | 26.3         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 837     |\n","|    iterations      | 402     |\n","|    time_elapsed    | 5899    |\n","|    total_timesteps | 4939776 |\n","--------------------------------\n","Eval num_timesteps=4944000, episode_reward=10.03 +/- 46.59\n","Episode length: 486.40 +/- 16.66\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 486          |\n","|    mean_reward          | 10           |\n","| time/                   |              |\n","|    total_timesteps      | 4944000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0022886982 |\n","|    clip_fraction        | 0.0896       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -13.9        |\n","|    explained_variance   | 0.592        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.937        |\n","|    n_updates            | 2010         |\n","|    policy_gradient_loss | -0.00396     |\n","|    std                  | 8            |\n","|    value_loss           | 31.3         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 837     |\n","|    iterations      | 403     |\n","|    time_elapsed    | 5915    |\n","|    total_timesteps | 4952064 |\n","--------------------------------\n","Eval num_timesteps=4956000, episode_reward=44.82 +/- 5.30\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 44.8         |\n","| time/                   |              |\n","|    total_timesteps      | 4956000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0021024772 |\n","|    clip_fraction        | 0.0875       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -14          |\n","|    explained_variance   | -0.0167      |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.105       |\n","|    n_updates            | 2015         |\n","|    policy_gradient_loss | -0.00449     |\n","|    std                  | 8            |\n","|    value_loss           | 1.21         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 837     |\n","|    iterations      | 404     |\n","|    time_elapsed    | 5928    |\n","|    total_timesteps | 4964352 |\n","--------------------------------\n","Eval num_timesteps=4968000, episode_reward=-13.74 +/- 43.17\n","Episode length: 489.80 +/- 8.33\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 490          |\n","|    mean_reward          | -13.7        |\n","| time/                   |              |\n","|    total_timesteps      | 4968000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0023619605 |\n","|    clip_fraction        | 0.103        |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -14          |\n","|    explained_variance   | 0.0838       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.518        |\n","|    n_updates            | 2020         |\n","|    policy_gradient_loss | -0.0033      |\n","|    std                  | 8.09         |\n","|    value_loss           | 19           |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 837     |\n","|    iterations      | 405     |\n","|    time_elapsed    | 5941    |\n","|    total_timesteps | 4976640 |\n","--------------------------------\n","Eval num_timesteps=4980000, episode_reward=34.27 +/- 2.65\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 34.3         |\n","| time/                   |              |\n","|    total_timesteps      | 4980000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0018865977 |\n","|    clip_fraction        | 0.0773       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -14          |\n","|    explained_variance   | -0.203       |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | -0.132       |\n","|    n_updates            | 2025         |\n","|    policy_gradient_loss | -0.0048      |\n","|    std                  | 8.1          |\n","|    value_loss           | 0.465        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 837     |\n","|    iterations      | 406     |\n","|    time_elapsed    | 5956    |\n","|    total_timesteps | 4988928 |\n","--------------------------------\n","Eval num_timesteps=4992000, episode_reward=-52.63 +/- 0.42\n","Episode length: 429.80 +/- 13.23\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 430          |\n","|    mean_reward          | -52.6        |\n","| time/                   |              |\n","|    total_timesteps      | 4992000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0023233423 |\n","|    clip_fraction        | 0.0881       |\n","|    clip_range           | 0.1          |\n","|    entropy_loss         | -14          |\n","|    explained_variance   | 0.515        |\n","|    learning_rate        | 0.000622     |\n","|    loss                 | 0.0334       |\n","|    n_updates            | 2030         |\n","|    policy_gradient_loss | -0.00383     |\n","|    std                  | 8.18         |\n","|    value_loss           | 9.25         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 837     |\n","|    iterations      | 407     |\n","|    time_elapsed    | 5968    |\n","|    total_timesteps | 5001216 |\n","--------------------------------\n"]}],"source":["from stable_baselines3.common.logger import configure\n","\n","tmp_path = \"./multiwalker_ppo_log_eval/\"\n","# set up logger\n","new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n","\n","model = PPO(\"MlpPolicy\", env, verbose=3, gamma=0.99, n_steps=512, ent_coef=0.01, learning_rate=0.00062211, vf_coef=0.042202, max_grad_norm=0.9, gae_lambda=0.95, n_epochs=5, clip_range=0.1, batch_size=512)\n","# model.set_logger()\n","model.set_logger(new_logger)\n","model.learn(total_timesteps=5000000,callback=eval_callback)\n","model.save(\"multiwalker_ppo\")"]},{"cell_type":"code","source":["from stable_baselines3 import PPO\n","# from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n","from pettingzoo.sisl import multiwalker_v9\n","import supersuit as ss\n","from stable_baselines3.common.monitor import Monitor"],"metadata":{"id":"MNRZ7Uu67dfg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env = multiwalker_v9.parallel_env(render_mode=\"rgb_array\")\n","\n","# #---------------------------------\n","# log_dir = \"./policy_log\"\n","# os.makedirs(log_dir, exist_ok=True)\n","# env = Monitor(env, log_dir)\n","# #---------------------------------\n","# env = ss.color_reduction_v0(env, mode='B')\n","# env = ss.black_death_v3(env)\n","# env = ss.resize_v1(env, x_size=84, y_size=84)\n","env = ss.frame_stack_v1(env, 3)\n","env = ss.pettingzoo_env_to_vec_env_v1(env)\n","env = ss.concat_vec_envs_v1(env, 8, num_cpus=4, base_class='stable_baselines3')"],"metadata":{"id":"SAh1CXvi7gIG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3.common.callbacks import EvalCallback\n","eval_callback = EvalCallback(env, best_model_save_path=\"./multiwalker_ppo_2_log_eval/\",\n","                             log_path=\"./multiwalker_ppo_2_log_eval/\", eval_freq=500,\n","                             deterministic=True, render=False)"],"metadata":{"id":"U9sXQH8q7iOG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3.common.logger import configure\n","\n","tmp_path = \"./multiwalker_ppo_2_log_eval/\"\n","# set up logger\n","new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n","\n","model = PPO(\"MlpPolicy\", env, verbose=3, n_steps=512,batch_size=512)\n","# model.set_logger()\n","model.set_logger(new_logger)\n","model.learn(total_timesteps=5000000,callback=eval_callback)\n","model.save(\"multiwalker_ppo_2\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I9knvCAg7iSY","executionInfo":{"status":"ok","timestamp":1698312987181,"user_tz":-120,"elapsed":6112840,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"21143525-94ad-400c-b54a-80c52bf8fe9d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Logging to ./multiwalker_ppo_2_log_eval/\n","Using cuda device\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n","|    mean_ep_length       | 391         |\n","|    mean_reward          | -15.6       |\n","| time/                   |             |\n","|    total_timesteps      | 2832000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011695099 |\n","|    clip_fraction        | 0.137       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.85       |\n","|    explained_variance   | 0.353       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 11          |\n","|    n_updates            | 2300        |\n","|    policy_gradient_loss | -0.00768    |\n","|    std                  | 0.498       |\n","|    value_loss           | 17.7        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 827     |\n","|    iterations      | 231     |\n","|    time_elapsed    | 3429    |\n","|    total_timesteps | 2838528 |\n","--------------------------------\n","Eval num_timesteps=2844000, episode_reward=40.40 +/- 1.20\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 40.4         |\n","| time/                   |              |\n","|    total_timesteps      | 2844000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0069289133 |\n","|    clip_fraction        | 0.0818       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.85        |\n","|    explained_variance   | 0.35         |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 3.6          |\n","|    n_updates            | 2310         |\n","|    policy_gradient_loss | -0.00574     |\n","|    std                  | 0.497        |\n","|    value_loss           | 16.5         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 827     |\n","|    iterations      | 232     |\n","|    time_elapsed    | 3445    |\n","|    total_timesteps | 2850816 |\n","--------------------------------\n","Eval num_timesteps=2856000, episode_reward=37.33 +/- 0.61\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 37.3        |\n","| time/                   |             |\n","|    total_timesteps      | 2856000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011757755 |\n","|    clip_fraction        | 0.133       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.85       |\n","|    explained_variance   | -0.012      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.96        |\n","|    n_updates            | 2320        |\n","|    policy_gradient_loss | -0.00857    |\n","|    std                  | 0.498       |\n","|    value_loss           | 22.4        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 827     |\n","|    iterations      | 233     |\n","|    time_elapsed    | 3461    |\n","|    total_timesteps | 2863104 |\n","--------------------------------\n","Eval num_timesteps=2868000, episode_reward=46.10 +/- 1.56\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 46.1        |\n","| time/                   |             |\n","|    total_timesteps      | 2868000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008752464 |\n","|    clip_fraction        | 0.0919      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.85       |\n","|    explained_variance   | 0.119       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 16.4        |\n","|    n_updates            | 2330        |\n","|    policy_gradient_loss | -0.00638    |\n","|    std                  | 0.498       |\n","|    value_loss           | 36.5        |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 825     |\n","|    iterations      | 234     |\n","|    time_elapsed    | 3482    |\n","|    total_timesteps | 2875392 |\n","--------------------------------\n","Eval num_timesteps=2880000, episode_reward=39.41 +/- 1.83\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 39.4        |\n","| time/                   |             |\n","|    total_timesteps      | 2880000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009464233 |\n","|    clip_fraction        | 0.108       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.85       |\n","|    explained_variance   | 0.165       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 15.5        |\n","|    n_updates            | 2340        |\n","|    policy_gradient_loss | -0.00672    |\n","|    std                  | 0.498       |\n","|    value_loss           | 40.1        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 826     |\n","|    iterations      | 235     |\n","|    time_elapsed    | 3495    |\n","|    total_timesteps | 2887680 |\n","--------------------------------\n","Eval num_timesteps=2892000, episode_reward=46.61 +/- 1.78\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 46.6        |\n","| time/                   |             |\n","|    total_timesteps      | 2892000     |\n","| train/                  |             |\n","|    approx_kl            | 0.017270414 |\n","|    clip_fraction        | 0.189       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.82       |\n","|    explained_variance   | -0.437      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.482       |\n","|    n_updates            | 2350        |\n","|    policy_gradient_loss | -0.00887    |\n","|    std                  | 0.492       |\n","|    value_loss           | 1.37        |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 825     |\n","|    iterations      | 236     |\n","|    time_elapsed    | 3511    |\n","|    total_timesteps | 2899968 |\n","--------------------------------\n","Eval num_timesteps=2904000, episode_reward=38.73 +/- 1.40\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 38.7       |\n","| time/                   |            |\n","|    total_timesteps      | 2904000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01564429 |\n","|    clip_fraction        | 0.181      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.78      |\n","|    explained_variance   | 0.336      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.351      |\n","|    n_updates            | 2360       |\n","|    policy_gradient_loss | -0.00981   |\n","|    std                  | 0.487      |\n","|    value_loss           | 0.782      |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 825     |\n","|    iterations      | 237     |\n","|    time_elapsed    | 3527    |\n","|    total_timesteps | 2912256 |\n","--------------------------------\n","Eval num_timesteps=2916000, episode_reward=34.28 +/- 3.65\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 34.3        |\n","| time/                   |             |\n","|    total_timesteps      | 2916000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013091922 |\n","|    clip_fraction        | 0.169       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.76       |\n","|    explained_variance   | 0.418       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.305       |\n","|    n_updates            | 2370        |\n","|    policy_gradient_loss | -0.00932    |\n","|    std                  | 0.485       |\n","|    value_loss           | 0.549       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 825     |\n","|    iterations      | 238     |\n","|    time_elapsed    | 3542    |\n","|    total_timesteps | 2924544 |\n","--------------------------------\n","Eval num_timesteps=2928000, episode_reward=47.08 +/- 4.19\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 47.1        |\n","| time/                   |             |\n","|    total_timesteps      | 2928000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011707485 |\n","|    clip_fraction        | 0.157       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.75       |\n","|    explained_variance   | 0.464       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.135       |\n","|    n_updates            | 2380        |\n","|    policy_gradient_loss | -0.0101     |\n","|    std                  | 0.485       |\n","|    value_loss           | 0.485       |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 825     |\n","|    iterations      | 239     |\n","|    time_elapsed    | 3555    |\n","|    total_timesteps | 2936832 |\n","--------------------------------\n","Eval num_timesteps=2940000, episode_reward=39.29 +/- 6.79\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 39.3        |\n","| time/                   |             |\n","|    total_timesteps      | 2940000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014036514 |\n","|    clip_fraction        | 0.15        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.74       |\n","|    explained_variance   | 0.351       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.212       |\n","|    n_updates            | 2390        |\n","|    policy_gradient_loss | -0.0113     |\n","|    std                  | 0.482       |\n","|    value_loss           | 0.508       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 825     |\n","|    iterations      | 240     |\n","|    time_elapsed    | 3572    |\n","|    total_timesteps | 2949120 |\n","--------------------------------\n","Eval num_timesteps=2952000, episode_reward=-30.26 +/- 59.03\n","Episode length: 416.60 +/- 68.10\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 417         |\n","|    mean_reward          | -30.3       |\n","| time/                   |             |\n","|    total_timesteps      | 2952000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013646564 |\n","|    clip_fraction        | 0.154       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.72       |\n","|    explained_variance   | 0.293       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.249       |\n","|    n_updates            | 2400        |\n","|    policy_gradient_loss | -0.00963    |\n","|    std                  | 0.481       |\n","|    value_loss           | 0.596       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 825     |\n","|    iterations      | 241     |\n","|    time_elapsed    | 3589    |\n","|    total_timesteps | 2961408 |\n","--------------------------------\n","Eval num_timesteps=2964000, episode_reward=40.48 +/- 2.31\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 40.5        |\n","| time/                   |             |\n","|    total_timesteps      | 2964000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013307518 |\n","|    clip_fraction        | 0.15        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.7        |\n","|    explained_variance   | 0.309       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.271       |\n","|    n_updates            | 2410        |\n","|    policy_gradient_loss | -0.00955    |\n","|    std                  | 0.479       |\n","|    value_loss           | 0.599       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 825     |\n","|    iterations      | 242     |\n","|    time_elapsed    | 3603    |\n","|    total_timesteps | 2973696 |\n","--------------------------------\n","Eval num_timesteps=2976000, episode_reward=-40.23 +/- 65.58\n","Episode length: 279.80 +/- 179.79\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 280         |\n","|    mean_reward          | -40.2       |\n","| time/                   |             |\n","|    total_timesteps      | 2976000     |\n","| train/                  |             |\n","|    approx_kl            | 0.007964163 |\n","|    clip_fraction        | 0.0968      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.7        |\n","|    explained_variance   | 0.0435      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 10.7        |\n","|    n_updates            | 2420        |\n","|    policy_gradient_loss | -0.00569    |\n","|    std                  | 0.479       |\n","|    value_loss           | 20.1        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 825     |\n","|    iterations      | 243     |\n","|    time_elapsed    | 3617    |\n","|    total_timesteps | 2985984 |\n","--------------------------------\n","Eval num_timesteps=2988000, episode_reward=40.53 +/- 4.20\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 40.5         |\n","| time/                   |              |\n","|    total_timesteps      | 2988000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0061666965 |\n","|    clip_fraction        | 0.0576       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.7         |\n","|    explained_variance   | 0.0602       |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 26.3         |\n","|    n_updates            | 2430         |\n","|    policy_gradient_loss | -0.00532     |\n","|    std                  | 0.479        |\n","|    value_loss           | 43           |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 824     |\n","|    iterations      | 244     |\n","|    time_elapsed    | 3634    |\n","|    total_timesteps | 2998272 |\n","--------------------------------\n","Eval num_timesteps=3000000, episode_reward=-23.01 +/- 51.87\n","Episode length: 428.60 +/- 58.30\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 429          |\n","|    mean_reward          | -23          |\n","| time/                   |              |\n","|    total_timesteps      | 3000000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0061532953 |\n","|    clip_fraction        | 0.0462       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.7         |\n","|    explained_variance   | 0.227        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 4.13         |\n","|    n_updates            | 2440         |\n","|    policy_gradient_loss | -0.00488     |\n","|    std                  | 0.479        |\n","|    value_loss           | 18           |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 824     |\n","|    iterations      | 245     |\n","|    time_elapsed    | 3650    |\n","|    total_timesteps | 3010560 |\n","--------------------------------\n","Eval num_timesteps=3012000, episode_reward=40.03 +/- 3.70\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 40         |\n","| time/                   |            |\n","|    total_timesteps      | 3012000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01068977 |\n","|    clip_fraction        | 0.0875     |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.7       |\n","|    explained_variance   | 0.337      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 3.68       |\n","|    n_updates            | 2450       |\n","|    policy_gradient_loss | -0.00602   |\n","|    std                  | 0.479      |\n","|    value_loss           | 10.3       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 825     |\n","|    iterations      | 246     |\n","|    time_elapsed    | 3663    |\n","|    total_timesteps | 3022848 |\n","--------------------------------\n","Eval num_timesteps=3024000, episode_reward=37.68 +/- 0.19\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 37.7        |\n","| time/                   |             |\n","|    total_timesteps      | 3024000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009006363 |\n","|    clip_fraction        | 0.0884      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.7        |\n","|    explained_variance   | 0.702       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.47        |\n","|    n_updates            | 2460        |\n","|    policy_gradient_loss | -0.00802    |\n","|    std                  | 0.479       |\n","|    value_loss           | 6.08        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 824     |\n","|    iterations      | 247     |\n","|    time_elapsed    | 3679    |\n","|    total_timesteps | 3035136 |\n","--------------------------------\n","Eval num_timesteps=3036000, episode_reward=42.46 +/- 1.32\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 42.5        |\n","| time/                   |             |\n","|    total_timesteps      | 3036000     |\n","| train/                  |             |\n","|    approx_kl            | 0.006760698 |\n","|    clip_fraction        | 0.0557      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.69       |\n","|    explained_variance   | 0.498       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 25.4        |\n","|    n_updates            | 2470        |\n","|    policy_gradient_loss | -0.006      |\n","|    std                  | 0.478       |\n","|    value_loss           | 35.6        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 824     |\n","|    iterations      | 248     |\n","|    time_elapsed    | 3695    |\n","|    total_timesteps | 3047424 |\n","--------------------------------\n","Eval num_timesteps=3048000, episode_reward=-45.03 +/- 59.95\n","Episode length: 284.60 +/- 175.87\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 285         |\n","|    mean_reward          | -45         |\n","| time/                   |             |\n","|    total_timesteps      | 3048000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014114096 |\n","|    clip_fraction        | 0.143       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.66       |\n","|    explained_variance   | 0.16        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.868       |\n","|    n_updates            | 2480        |\n","|    policy_gradient_loss | -0.00824    |\n","|    std                  | 0.471       |\n","|    value_loss           | 2.11        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 824     |\n","|    iterations      | 249     |\n","|    time_elapsed    | 3709    |\n","|    total_timesteps | 3059712 |\n","--------------------------------\n","Eval num_timesteps=3060000, episode_reward=31.94 +/- 0.23\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 31.9        |\n","| time/                   |             |\n","|    total_timesteps      | 3060000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010794774 |\n","|    clip_fraction        | 0.126       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.63       |\n","|    explained_variance   | 0.106       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.836       |\n","|    n_updates            | 2490        |\n","|    policy_gradient_loss | -0.00638    |\n","|    std                  | 0.472       |\n","|    value_loss           | 18.2        |\n","-----------------------------------------\n","Eval num_timesteps=3072000, episode_reward=41.32 +/- 0.20\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 41.3     |\n","| time/              |          |\n","|    total_timesteps | 3072000  |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 823     |\n","|    iterations      | 250     |\n","|    time_elapsed    | 3731    |\n","|    total_timesteps | 3072000 |\n","--------------------------------\n","Eval num_timesteps=3084000, episode_reward=35.33 +/- 4.29\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 35.3        |\n","| time/                   |             |\n","|    total_timesteps      | 3084000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011269969 |\n","|    clip_fraction        | 0.111       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.63       |\n","|    explained_variance   | 0.673       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 5.41        |\n","|    n_updates            | 2500        |\n","|    policy_gradient_loss | -0.00645    |\n","|    std                  | 0.471       |\n","|    value_loss           | 9.52        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 823     |\n","|    iterations      | 251     |\n","|    time_elapsed    | 3744    |\n","|    total_timesteps | 3084288 |\n","--------------------------------\n","Eval num_timesteps=3096000, episode_reward=47.48 +/- 7.92\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 47.5         |\n","| time/                   |              |\n","|    total_timesteps      | 3096000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0063386285 |\n","|    clip_fraction        | 0.0768       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.63        |\n","|    explained_variance   | 0.619        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 8.16         |\n","|    n_updates            | 2510         |\n","|    policy_gradient_loss | -0.00667     |\n","|    std                  | 0.471        |\n","|    value_loss           | 25.4         |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 252     |\n","|    time_elapsed    | 3764    |\n","|    total_timesteps | 3096576 |\n","--------------------------------\n","Eval num_timesteps=3108000, episode_reward=40.22 +/- 6.93\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 40.2        |\n","| time/                   |             |\n","|    total_timesteps      | 3108000     |\n","| train/                  |             |\n","|    approx_kl            | 0.007140199 |\n","|    clip_fraction        | 0.0799      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.64       |\n","|    explained_variance   | 0.141       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 9.93        |\n","|    n_updates            | 2520        |\n","|    policy_gradient_loss | -0.00648    |\n","|    std                  | 0.472       |\n","|    value_loss           | 21.4        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 253     |\n","|    time_elapsed    | 3780    |\n","|    total_timesteps | 3108864 |\n","--------------------------------\n","Eval num_timesteps=3120000, episode_reward=39.64 +/- 1.27\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 39.6        |\n","| time/                   |             |\n","|    total_timesteps      | 3120000     |\n","| train/                  |             |\n","|    approx_kl            | 0.021183044 |\n","|    clip_fraction        | 0.175       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.63       |\n","|    explained_variance   | -0.323      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.678       |\n","|    n_updates            | 2530        |\n","|    policy_gradient_loss | -0.00967    |\n","|    std                  | 0.469       |\n","|    value_loss           | 1.62        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 254     |\n","|    time_elapsed    | 3792    |\n","|    total_timesteps | 3121152 |\n","--------------------------------\n","Eval num_timesteps=3132000, episode_reward=31.38 +/- 11.50\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 31.4        |\n","| time/                   |             |\n","|    total_timesteps      | 3132000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012115944 |\n","|    clip_fraction        | 0.178       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.62       |\n","|    explained_variance   | 0.187       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.292       |\n","|    n_updates            | 2540        |\n","|    policy_gradient_loss | -0.00914    |\n","|    std                  | 0.467       |\n","|    value_loss           | 0.659       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 255     |\n","|    time_elapsed    | 3808    |\n","|    total_timesteps | 3133440 |\n","--------------------------------\n","Eval num_timesteps=3144000, episode_reward=23.96 +/- 6.82\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 24          |\n","| time/                   |             |\n","|    total_timesteps      | 3144000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015906082 |\n","|    clip_fraction        | 0.177       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.58       |\n","|    explained_variance   | 0.333       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.251       |\n","|    n_updates            | 2550        |\n","|    policy_gradient_loss | -0.0101     |\n","|    std                  | 0.463       |\n","|    value_loss           | 0.535       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 256     |\n","|    time_elapsed    | 3824    |\n","|    total_timesteps | 3145728 |\n","--------------------------------\n","Eval num_timesteps=3156000, episode_reward=42.06 +/- 2.06\n","Episode length: 500.00 +/- 0.00\n","---------------------------------------\n","| eval/                   |           |\n","|    mean_ep_length       | 500       |\n","|    mean_reward          | 42.1      |\n","| time/                   |           |\n","|    total_timesteps      | 3156000   |\n","| train/                  |           |\n","|    approx_kl            | 0.0151014 |\n","|    clip_fraction        | 0.15      |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -2.56     |\n","|    explained_variance   | 0.527     |\n","|    learning_rate        | 0.0003    |\n","|    loss                 | 0.322     |\n","|    n_updates            | 2560      |\n","|    policy_gradient_loss | -0.00798  |\n","|    std                  | 0.461     |\n","|    value_loss           | 0.81      |\n","---------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 257     |\n","|    time_elapsed    | 3838    |\n","|    total_timesteps | 3158016 |\n","--------------------------------\n","Eval num_timesteps=3168000, episode_reward=45.82 +/- 3.06\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 45.8         |\n","| time/                   |              |\n","|    total_timesteps      | 3168000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0130749345 |\n","|    clip_fraction        | 0.155        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.55        |\n","|    explained_variance   | 0.725        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.222        |\n","|    n_updates            | 2570         |\n","|    policy_gradient_loss | -0.0102      |\n","|    std                  | 0.46         |\n","|    value_loss           | 0.521        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 258     |\n","|    time_elapsed    | 3852    |\n","|    total_timesteps | 3170304 |\n","--------------------------------\n","Eval num_timesteps=3180000, episode_reward=32.96 +/- 4.41\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 33         |\n","| time/                   |            |\n","|    total_timesteps      | 3180000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01295558 |\n","|    clip_fraction        | 0.145      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.53      |\n","|    explained_variance   | 0.425      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.362      |\n","|    n_updates            | 2580       |\n","|    policy_gradient_loss | -0.0108    |\n","|    std                  | 0.457      |\n","|    value_loss           | 0.638      |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 259     |\n","|    time_elapsed    | 3868    |\n","|    total_timesteps | 3182592 |\n","--------------------------------\n","Eval num_timesteps=3192000, episode_reward=46.16 +/- 9.09\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 46.2        |\n","| time/                   |             |\n","|    total_timesteps      | 3192000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011348774 |\n","|    clip_fraction        | 0.143       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.52       |\n","|    explained_variance   | 0.659       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.288       |\n","|    n_updates            | 2590        |\n","|    policy_gradient_loss | -0.00962    |\n","|    std                  | 0.458       |\n","|    value_loss           | 0.568       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 260     |\n","|    time_elapsed    | 3884    |\n","|    total_timesteps | 3194880 |\n","--------------------------------\n","Eval num_timesteps=3204000, episode_reward=46.57 +/- 1.20\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 46.6        |\n","| time/                   |             |\n","|    total_timesteps      | 3204000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012888228 |\n","|    clip_fraction        | 0.15        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.5        |\n","|    explained_variance   | 0.807       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.211       |\n","|    n_updates            | 2600        |\n","|    policy_gradient_loss | -0.0112     |\n","|    std                  | 0.453       |\n","|    value_loss           | 0.563       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 823     |\n","|    iterations      | 261     |\n","|    time_elapsed    | 3896    |\n","|    total_timesteps | 3207168 |\n","--------------------------------\n","Eval num_timesteps=3216000, episode_reward=39.59 +/- 5.71\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 39.6        |\n","| time/                   |             |\n","|    total_timesteps      | 3216000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012731894 |\n","|    clip_fraction        | 0.143       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.48       |\n","|    explained_variance   | 0.469       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.256       |\n","|    n_updates            | 2610        |\n","|    policy_gradient_loss | -0.0102     |\n","|    std                  | 0.452       |\n","|    value_loss           | 0.679       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 262     |\n","|    time_elapsed    | 3912    |\n","|    total_timesteps | 3219456 |\n","--------------------------------\n","Eval num_timesteps=3228000, episode_reward=43.60 +/- 0.84\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 43.6        |\n","| time/                   |             |\n","|    total_timesteps      | 3228000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011555451 |\n","|    clip_fraction        | 0.135       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.48       |\n","|    explained_variance   | 0.654       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.326       |\n","|    n_updates            | 2620        |\n","|    policy_gradient_loss | -0.0111     |\n","|    std                  | 0.453       |\n","|    value_loss           | 0.626       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 263     |\n","|    time_elapsed    | 3928    |\n","|    total_timesteps | 3231744 |\n","--------------------------------\n","Eval num_timesteps=3240000, episode_reward=2.56 +/- 51.70\n","Episode length: 480.40 +/- 24.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 480        |\n","|    mean_reward          | 2.56       |\n","| time/                   |            |\n","|    total_timesteps      | 3240000    |\n","| train/                  |            |\n","|    approx_kl            | 0.00762035 |\n","|    clip_fraction        | 0.0875     |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.48      |\n","|    explained_variance   | 0.0607     |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 9.7        |\n","|    n_updates            | 2630       |\n","|    policy_gradient_loss | -0.00611   |\n","|    std                  | 0.453      |\n","|    value_loss           | 17.1       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 264     |\n","|    time_elapsed    | 3942    |\n","|    total_timesteps | 3244032 |\n","--------------------------------\n","Eval num_timesteps=3252000, episode_reward=38.38 +/- 5.63\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 38.4       |\n","| time/                   |            |\n","|    total_timesteps      | 3252000    |\n","| train/                  |            |\n","|    approx_kl            | 0.00764281 |\n","|    clip_fraction        | 0.0838     |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.48      |\n","|    explained_variance   | 0.101      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 10.1       |\n","|    n_updates            | 2640       |\n","|    policy_gradient_loss | -0.00661   |\n","|    std                  | 0.453      |\n","|    value_loss           | 20.4       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 823     |\n","|    iterations      | 265     |\n","|    time_elapsed    | 3956    |\n","|    total_timesteps | 3256320 |\n","--------------------------------\n","Eval num_timesteps=3264000, episode_reward=-4.62 +/- 66.89\n","Episode length: 412.80 +/- 106.80\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 413         |\n","|    mean_reward          | -4.62       |\n","| time/                   |             |\n","|    total_timesteps      | 3264000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010939433 |\n","|    clip_fraction        | 0.135       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.48       |\n","|    explained_variance   | 0.495       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.305       |\n","|    n_updates            | 2650        |\n","|    policy_gradient_loss | -0.00909    |\n","|    std                  | 0.453       |\n","|    value_loss           | 0.798       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 266     |\n","|    time_elapsed    | 3972    |\n","|    total_timesteps | 3268608 |\n","--------------------------------\n","Eval num_timesteps=3276000, episode_reward=-4.09 +/- 54.56\n","Episode length: 474.80 +/- 30.86\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 475         |\n","|    mean_reward          | -4.09       |\n","| time/                   |             |\n","|    total_timesteps      | 3276000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012856486 |\n","|    clip_fraction        | 0.134       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.47       |\n","|    explained_variance   | 0.705       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.266       |\n","|    n_updates            | 2660        |\n","|    policy_gradient_loss | -0.0103     |\n","|    std                  | 0.451       |\n","|    value_loss           | 0.599       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 267     |\n","|    time_elapsed    | 3988    |\n","|    total_timesteps | 3280896 |\n","--------------------------------\n","Eval num_timesteps=3288000, episode_reward=32.14 +/- 12.28\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 32.1       |\n","| time/                   |            |\n","|    total_timesteps      | 3288000    |\n","| train/                  |            |\n","|    approx_kl            | 0.00889372 |\n","|    clip_fraction        | 0.126      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.46      |\n","|    explained_variance   | 0.266      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 1.35       |\n","|    n_updates            | 2670       |\n","|    policy_gradient_loss | -0.00722   |\n","|    std                  | 0.451      |\n","|    value_loss           | 17         |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 823     |\n","|    iterations      | 268     |\n","|    time_elapsed    | 4000    |\n","|    total_timesteps | 3293184 |\n","--------------------------------\n","Eval num_timesteps=3300000, episode_reward=-22.03 +/- 59.80\n","Episode length: 425.00 +/- 61.24\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 425         |\n","|    mean_reward          | -22         |\n","| time/                   |             |\n","|    total_timesteps      | 3300000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014206846 |\n","|    clip_fraction        | 0.141       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.45       |\n","|    explained_variance   | 0.541       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.532       |\n","|    n_updates            | 2680        |\n","|    policy_gradient_loss | -0.0114     |\n","|    std                  | 0.449       |\n","|    value_loss           | 1.18        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 823     |\n","|    iterations      | 269     |\n","|    time_elapsed    | 4016    |\n","|    total_timesteps | 3305472 |\n","--------------------------------\n","Eval num_timesteps=3312000, episode_reward=35.04 +/- 4.06\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 35           |\n","| time/                   |              |\n","|    total_timesteps      | 3312000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0064391084 |\n","|    clip_fraction        | 0.0753       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.44        |\n","|    explained_variance   | 0.24         |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 24.6         |\n","|    n_updates            | 2690         |\n","|    policy_gradient_loss | -0.00361     |\n","|    std                  | 0.449        |\n","|    value_loss           | 43.8         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 270     |\n","|    time_elapsed    | 4032    |\n","|    total_timesteps | 3317760 |\n","--------------------------------\n","Eval num_timesteps=3324000, episode_reward=-33.05 +/- 58.06\n","Episode length: 368.00 +/- 107.78\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 368         |\n","|    mean_reward          | -33         |\n","| time/                   |             |\n","|    total_timesteps      | 3324000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008769573 |\n","|    clip_fraction        | 0.0907      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.44       |\n","|    explained_variance   | 0.288       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.856       |\n","|    n_updates            | 2700        |\n","|    policy_gradient_loss | -0.00879    |\n","|    std                  | 0.448       |\n","|    value_loss           | 3.36        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 821     |\n","|    iterations      | 271     |\n","|    time_elapsed    | 4052    |\n","|    total_timesteps | 3330048 |\n","--------------------------------\n","Eval num_timesteps=3336000, episode_reward=-2.13 +/- 55.02\n","Episode length: 463.20 +/- 45.07\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 463         |\n","|    mean_reward          | -2.13       |\n","| time/                   |             |\n","|    total_timesteps      | 3336000     |\n","| train/                  |             |\n","|    approx_kl            | 0.007567575 |\n","|    clip_fraction        | 0.0708      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.44       |\n","|    explained_variance   | 0.697       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 9.01        |\n","|    n_updates            | 2710        |\n","|    policy_gradient_loss | -0.00745    |\n","|    std                  | 0.449       |\n","|    value_loss           | 34.2        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 272     |\n","|    time_elapsed    | 4065    |\n","|    total_timesteps | 3342336 |\n","--------------------------------\n","Eval num_timesteps=3348000, episode_reward=1.72 +/- 47.71\n","Episode length: 464.80 +/- 43.11\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 465         |\n","|    mean_reward          | 1.72        |\n","| time/                   |             |\n","|    total_timesteps      | 3348000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014136761 |\n","|    clip_fraction        | 0.153       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.42       |\n","|    explained_variance   | 0.502       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.413       |\n","|    n_updates            | 2720        |\n","|    policy_gradient_loss | -0.0109     |\n","|    std                  | 0.445       |\n","|    value_loss           | 1.13        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 273     |\n","|    time_elapsed    | 4080    |\n","|    total_timesteps | 3354624 |\n","--------------------------------\n","Eval num_timesteps=3360000, episode_reward=-4.02 +/- 53.63\n","Episode length: 454.40 +/- 55.85\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 454         |\n","|    mean_reward          | -4.02       |\n","| time/                   |             |\n","|    total_timesteps      | 3360000     |\n","| train/                  |             |\n","|    approx_kl            | 0.007281974 |\n","|    clip_fraction        | 0.0983      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.41       |\n","|    explained_variance   | 0.489       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 16.6        |\n","|    n_updates            | 2730        |\n","|    policy_gradient_loss | -0.00513    |\n","|    std                  | 0.445       |\n","|    value_loss           | 26.5        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 821     |\n","|    iterations      | 274     |\n","|    time_elapsed    | 4096    |\n","|    total_timesteps | 3366912 |\n","--------------------------------\n","Eval num_timesteps=3372000, episode_reward=46.25 +/- 4.61\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 46.2        |\n","| time/                   |             |\n","|    total_timesteps      | 3372000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014000383 |\n","|    clip_fraction        | 0.131       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.41       |\n","|    explained_variance   | 0.502       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 13.8        |\n","|    n_updates            | 2740        |\n","|    policy_gradient_loss | -0.00733    |\n","|    std                  | 0.446       |\n","|    value_loss           | 17.7        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 275     |\n","|    time_elapsed    | 4110    |\n","|    total_timesteps | 3379200 |\n","--------------------------------\n","Eval num_timesteps=3384000, episode_reward=40.58 +/- 1.23\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 40.6        |\n","| time/                   |             |\n","|    total_timesteps      | 3384000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008123434 |\n","|    clip_fraction        | 0.0857      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.43       |\n","|    explained_variance   | 0.424       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 21          |\n","|    n_updates            | 2750        |\n","|    policy_gradient_loss | -0.00665    |\n","|    std                  | 0.447       |\n","|    value_loss           | 49.2        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 276     |\n","|    time_elapsed    | 4124    |\n","|    total_timesteps | 3391488 |\n","--------------------------------\n","Eval num_timesteps=3396000, episode_reward=-0.58 +/- 44.87\n","Episode length: 492.40 +/- 9.31\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 492        |\n","|    mean_reward          | -0.576     |\n","| time/                   |            |\n","|    total_timesteps      | 3396000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01560839 |\n","|    clip_fraction        | 0.176      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.42      |\n","|    explained_variance   | 0.15       |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.408      |\n","|    n_updates            | 2760       |\n","|    policy_gradient_loss | -0.00956   |\n","|    std                  | 0.446      |\n","|    value_loss           | 0.995      |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 277     |\n","|    time_elapsed    | 4140    |\n","|    total_timesteps | 3403776 |\n","--------------------------------\n","Eval num_timesteps=3408000, episode_reward=40.29 +/- 3.33\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 40.3        |\n","| time/                   |             |\n","|    total_timesteps      | 3408000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009831417 |\n","|    clip_fraction        | 0.114       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.41       |\n","|    explained_variance   | 0.821       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.857       |\n","|    n_updates            | 2770        |\n","|    policy_gradient_loss | -0.00832    |\n","|    std                  | 0.445       |\n","|    value_loss           | 4.08        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 821     |\n","|    iterations      | 278     |\n","|    time_elapsed    | 4156    |\n","|    total_timesteps | 3416064 |\n","--------------------------------\n","Eval num_timesteps=3420000, episode_reward=1.00 +/- 60.16\n","Episode length: 435.20 +/- 79.36\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 435         |\n","|    mean_reward          | 1           |\n","| time/                   |             |\n","|    total_timesteps      | 3420000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008103033 |\n","|    clip_fraction        | 0.111       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.41       |\n","|    explained_variance   | 0.0918      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 20.7        |\n","|    n_updates            | 2780        |\n","|    policy_gradient_loss | -0.00754    |\n","|    std                  | 0.445       |\n","|    value_loss           | 36.4        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 279     |\n","|    time_elapsed    | 4168    |\n","|    total_timesteps | 3428352 |\n","--------------------------------\n","Eval num_timesteps=3432000, episode_reward=-17.73 +/- 54.69\n","Episode length: 439.60 +/- 73.97\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 440         |\n","|    mean_reward          | -17.7       |\n","| time/                   |             |\n","|    total_timesteps      | 3432000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010658748 |\n","|    clip_fraction        | 0.099       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.41       |\n","|    explained_variance   | 0.366       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.56        |\n","|    n_updates            | 2790        |\n","|    policy_gradient_loss | -0.00752    |\n","|    std                  | 0.445       |\n","|    value_loss           | 17.1        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 280     |\n","|    time_elapsed    | 4184    |\n","|    total_timesteps | 3440640 |\n","--------------------------------\n","Eval num_timesteps=3444000, episode_reward=-80.38 +/- 11.26\n","Episode length: 298.80 +/- 98.96\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 299         |\n","|    mean_reward          | -80.4       |\n","| time/                   |             |\n","|    total_timesteps      | 3444000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010397397 |\n","|    clip_fraction        | 0.123       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.41       |\n","|    explained_variance   | 0.556       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 10.4        |\n","|    n_updates            | 2800        |\n","|    policy_gradient_loss | -0.00723    |\n","|    std                  | 0.446       |\n","|    value_loss           | 13.3        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 281     |\n","|    time_elapsed    | 4199    |\n","|    total_timesteps | 3452928 |\n","--------------------------------\n","Eval num_timesteps=3456000, episode_reward=-4.90 +/- 63.37\n","Episode length: 411.20 +/- 108.76\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 411         |\n","|    mean_reward          | -4.9        |\n","| time/                   |             |\n","|    total_timesteps      | 3456000     |\n","| train/                  |             |\n","|    approx_kl            | 0.007938984 |\n","|    clip_fraction        | 0.077       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.41       |\n","|    explained_variance   | 0.651       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 10.7        |\n","|    n_updates            | 2810        |\n","|    policy_gradient_loss | -0.00708    |\n","|    std                  | 0.446       |\n","|    value_loss           | 34.2        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 282     |\n","|    time_elapsed    | 4212    |\n","|    total_timesteps | 3465216 |\n","--------------------------------\n","Eval num_timesteps=3468000, episode_reward=-28.12 +/- 57.17\n","Episode length: 405.80 +/- 76.91\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 406         |\n","|    mean_reward          | -28.1       |\n","| time/                   |             |\n","|    total_timesteps      | 3468000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009469247 |\n","|    clip_fraction        | 0.125       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.41       |\n","|    explained_variance   | 0.525       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.18        |\n","|    n_updates            | 2820        |\n","|    policy_gradient_loss | -0.00787    |\n","|    std                  | 0.443       |\n","|    value_loss           | 9.68        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 283     |\n","|    time_elapsed    | 4227    |\n","|    total_timesteps | 3477504 |\n","--------------------------------\n","Eval num_timesteps=3480000, episode_reward=2.24 +/- 56.95\n","Episode length: 460.40 +/- 48.50\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 460        |\n","|    mean_reward          | 2.24       |\n","| time/                   |            |\n","|    total_timesteps      | 3480000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01273775 |\n","|    clip_fraction        | 0.124      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.39      |\n","|    explained_variance   | 0.287      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 1.85       |\n","|    n_updates            | 2830       |\n","|    policy_gradient_loss | -0.00792   |\n","|    std                  | 0.443      |\n","|    value_loss           | 20.9       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 284     |\n","|    time_elapsed    | 4243    |\n","|    total_timesteps | 3489792 |\n","--------------------------------\n","Eval num_timesteps=3492000, episode_reward=-64.62 +/- 1.31\n","Episode length: 461.80 +/- 6.37\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 462        |\n","|    mean_reward          | -64.6      |\n","| time/                   |            |\n","|    total_timesteps      | 3492000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01170246 |\n","|    clip_fraction        | 0.132      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.39      |\n","|    explained_variance   | 0.509      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 5.61       |\n","|    n_updates            | 2840       |\n","|    policy_gradient_loss | -0.00821   |\n","|    std                  | 0.444      |\n","|    value_loss           | 23.9       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 285     |\n","|    time_elapsed    | 4257    |\n","|    total_timesteps | 3502080 |\n","--------------------------------\n","Eval num_timesteps=3504000, episode_reward=52.29 +/- 2.99\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 52.3        |\n","| time/                   |             |\n","|    total_timesteps      | 3504000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012601834 |\n","|    clip_fraction        | 0.175       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.38       |\n","|    explained_variance   | 0.667       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.43        |\n","|    n_updates            | 2850        |\n","|    policy_gradient_loss | -0.00552    |\n","|    std                  | 0.44        |\n","|    value_loss           | 16.8        |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 286     |\n","|    time_elapsed    | 4270    |\n","|    total_timesteps | 3514368 |\n","--------------------------------\n","Eval num_timesteps=3516000, episode_reward=-6.30 +/- 67.93\n","Episode length: 362.00 +/- 169.01\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 362         |\n","|    mean_reward          | -6.3        |\n","| time/                   |             |\n","|    total_timesteps      | 3516000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015514568 |\n","|    clip_fraction        | 0.173       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.36       |\n","|    explained_variance   | 0.916       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.909       |\n","|    n_updates            | 2860        |\n","|    policy_gradient_loss | -0.00944    |\n","|    std                  | 0.44        |\n","|    value_loss           | 4.45        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 287     |\n","|    time_elapsed    | 4286    |\n","|    total_timesteps | 3526656 |\n","--------------------------------\n","Eval num_timesteps=3528000, episode_reward=42.40 +/- 0.09\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 42.4        |\n","| time/                   |             |\n","|    total_timesteps      | 3528000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009617395 |\n","|    clip_fraction        | 0.119       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.36       |\n","|    explained_variance   | 0.702       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 13          |\n","|    n_updates            | 2870        |\n","|    policy_gradient_loss | -0.00826    |\n","|    std                  | 0.44        |\n","|    value_loss           | 32          |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 288     |\n","|    time_elapsed    | 4302    |\n","|    total_timesteps | 3538944 |\n","--------------------------------\n","Eval num_timesteps=3540000, episode_reward=45.96 +/- 5.41\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 46          |\n","| time/                   |             |\n","|    total_timesteps      | 3540000     |\n","| train/                  |             |\n","|    approx_kl            | 0.018297551 |\n","|    clip_fraction        | 0.165       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.34       |\n","|    explained_variance   | 0.34        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.03        |\n","|    n_updates            | 2880        |\n","|    policy_gradient_loss | -0.0068     |\n","|    std                  | 0.436       |\n","|    value_loss           | 4.51        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 289     |\n","|    time_elapsed    | 4315    |\n","|    total_timesteps | 3551232 |\n","--------------------------------\n","Eval num_timesteps=3552000, episode_reward=-32.56 +/- 68.35\n","Episode length: 324.20 +/- 143.54\n","---------------------------------------\n","| eval/                   |           |\n","|    mean_ep_length       | 324       |\n","|    mean_reward          | -32.6     |\n","| time/                   |           |\n","|    total_timesteps      | 3552000   |\n","| train/                  |           |\n","|    approx_kl            | 0.0096509 |\n","|    clip_fraction        | 0.132     |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -2.32     |\n","|    explained_variance   | 0.184     |\n","|    learning_rate        | 0.0003    |\n","|    loss                 | 25.9      |\n","|    n_updates            | 2890      |\n","|    policy_gradient_loss | -0.00837  |\n","|    std                  | 0.436     |\n","|    value_loss           | 66.4      |\n","---------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 822     |\n","|    iterations      | 290     |\n","|    time_elapsed    | 4334    |\n","|    total_timesteps | 3563520 |\n","--------------------------------\n","Eval num_timesteps=3564000, episode_reward=48.60 +/- 3.20\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 48.6        |\n","| time/                   |             |\n","|    total_timesteps      | 3564000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015847787 |\n","|    clip_fraction        | 0.182       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.33       |\n","|    explained_variance   | 0.593       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 6.84        |\n","|    n_updates            | 2900        |\n","|    policy_gradient_loss | -0.00653    |\n","|    std                  | 0.435       |\n","|    value_loss           | 6.53        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 821     |\n","|    iterations      | 291     |\n","|    time_elapsed    | 4350    |\n","|    total_timesteps | 3575808 |\n","--------------------------------\n","Eval num_timesteps=3576000, episode_reward=-16.70 +/- 56.22\n","Episode length: 479.60 +/- 16.66\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 480          |\n","|    mean_reward          | -16.7        |\n","| time/                   |              |\n","|    total_timesteps      | 3576000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0078903185 |\n","|    clip_fraction        | 0.0971       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.32        |\n","|    explained_variance   | 0.511        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 15.7         |\n","|    n_updates            | 2910         |\n","|    policy_gradient_loss | -0.0086      |\n","|    std                  | 0.435        |\n","|    value_loss           | 48.1         |\n","------------------------------------------\n","Eval num_timesteps=3588000, episode_reward=44.76 +/- 1.50\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 44.8     |\n","| time/              |          |\n","|    total_timesteps | 3588000  |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 292     |\n","|    time_elapsed    | 4371    |\n","|    total_timesteps | 3588096 |\n","--------------------------------\n","Eval num_timesteps=3600000, episode_reward=51.37 +/- 1.33\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 51.4       |\n","| time/                   |            |\n","|    total_timesteps      | 3600000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01600224 |\n","|    clip_fraction        | 0.174      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.32      |\n","|    explained_variance   | -1.67      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.928      |\n","|    n_updates            | 2920       |\n","|    policy_gradient_loss | -0.00702   |\n","|    std                  | 0.435      |\n","|    value_loss           | 5.16       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 821     |\n","|    iterations      | 293     |\n","|    time_elapsed    | 4385    |\n","|    total_timesteps | 3600384 |\n","--------------------------------\n","Eval num_timesteps=3612000, episode_reward=-26.39 +/- 59.63\n","Episode length: 419.00 +/- 66.14\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 419         |\n","|    mean_reward          | -26.4       |\n","| time/                   |             |\n","|    total_timesteps      | 3612000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014560859 |\n","|    clip_fraction        | 0.176       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.32       |\n","|    explained_variance   | 0.688       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.966       |\n","|    n_updates            | 2930        |\n","|    policy_gradient_loss | -0.00784    |\n","|    std                  | 0.436       |\n","|    value_loss           | 4.34        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 821     |\n","|    iterations      | 294     |\n","|    time_elapsed    | 4399    |\n","|    total_timesteps | 3612672 |\n","--------------------------------\n","Eval num_timesteps=3624000, episode_reward=45.04 +/- 7.90\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 45          |\n","| time/                   |             |\n","|    total_timesteps      | 3624000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011819668 |\n","|    clip_fraction        | 0.15        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.32       |\n","|    explained_variance   | 0.888       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.76        |\n","|    n_updates            | 2940        |\n","|    policy_gradient_loss | -0.00896    |\n","|    std                  | 0.436       |\n","|    value_loss           | 6.8         |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 295     |\n","|    time_elapsed    | 4415    |\n","|    total_timesteps | 3624960 |\n","--------------------------------\n","Eval num_timesteps=3636000, episode_reward=50.13 +/- 7.39\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 50.1        |\n","| time/                   |             |\n","|    total_timesteps      | 3636000     |\n","| train/                  |             |\n","|    approx_kl            | 0.021508813 |\n","|    clip_fraction        | 0.249       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.28       |\n","|    explained_variance   | 0.486       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.603       |\n","|    n_updates            | 2950        |\n","|    policy_gradient_loss | -0.00874    |\n","|    std                  | 0.428       |\n","|    value_loss           | 1.4         |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 296     |\n","|    time_elapsed    | 4430    |\n","|    total_timesteps | 3637248 |\n","--------------------------------\n","Eval num_timesteps=3648000, episode_reward=52.09 +/- 2.93\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 52.1        |\n","| time/                   |             |\n","|    total_timesteps      | 3648000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014551851 |\n","|    clip_fraction        | 0.197       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.23       |\n","|    explained_variance   | 0.576       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.11        |\n","|    n_updates            | 2960        |\n","|    policy_gradient_loss | -0.00985    |\n","|    std                  | 0.423       |\n","|    value_loss           | 2.28        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 821     |\n","|    iterations      | 297     |\n","|    time_elapsed    | 4442    |\n","|    total_timesteps | 3649536 |\n","--------------------------------\n","Eval num_timesteps=3660000, episode_reward=50.39 +/- 1.80\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 50.4        |\n","| time/                   |             |\n","|    total_timesteps      | 3660000     |\n","| train/                  |             |\n","|    approx_kl            | 0.007954455 |\n","|    clip_fraction        | 0.113       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.2        |\n","|    explained_variance   | 0.319       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.69        |\n","|    n_updates            | 2970        |\n","|    policy_gradient_loss | -0.00815    |\n","|    std                  | 0.423       |\n","|    value_loss           | 26.2        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 821     |\n","|    iterations      | 298     |\n","|    time_elapsed    | 4458    |\n","|    total_timesteps | 3661824 |\n","--------------------------------\n","Eval num_timesteps=3672000, episode_reward=51.74 +/- 4.01\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 51.7        |\n","| time/                   |             |\n","|    total_timesteps      | 3672000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009387534 |\n","|    clip_fraction        | 0.113       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.2        |\n","|    explained_variance   | 0.478       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 10.3        |\n","|    n_updates            | 2980        |\n","|    policy_gradient_loss | -0.00791    |\n","|    std                  | 0.423       |\n","|    value_loss           | 17          |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 299     |\n","|    time_elapsed    | 4475    |\n","|    total_timesteps | 3674112 |\n","--------------------------------\n","Eval num_timesteps=3684000, episode_reward=45.58 +/- 3.61\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 45.6       |\n","| time/                   |            |\n","|    total_timesteps      | 3684000    |\n","| train/                  |            |\n","|    approx_kl            | 0.00946338 |\n","|    clip_fraction        | 0.11       |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.21      |\n","|    explained_variance   | 0.508      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 7.38       |\n","|    n_updates            | 2990       |\n","|    policy_gradient_loss | -0.00902   |\n","|    std                  | 0.424      |\n","|    value_loss           | 26.3       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 821     |\n","|    iterations      | 300     |\n","|    time_elapsed    | 4488    |\n","|    total_timesteps | 3686400 |\n","--------------------------------\n","Eval num_timesteps=3696000, episode_reward=9.76 +/- 52.44\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 9.76        |\n","| time/                   |             |\n","|    total_timesteps      | 3696000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008754029 |\n","|    clip_fraction        | 0.0958      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.22       |\n","|    explained_variance   | 0.575       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.76        |\n","|    n_updates            | 3000        |\n","|    policy_gradient_loss | -0.00737    |\n","|    std                  | 0.424       |\n","|    value_loss           | 8.62        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 821     |\n","|    iterations      | 301     |\n","|    time_elapsed    | 4502    |\n","|    total_timesteps | 3698688 |\n","--------------------------------\n","Eval num_timesteps=3708000, episode_reward=51.34 +/- 4.00\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 51.3        |\n","| time/                   |             |\n","|    total_timesteps      | 3708000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009725977 |\n","|    clip_fraction        | 0.103       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.22       |\n","|    explained_variance   | 0.409       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 22.2        |\n","|    n_updates            | 3010        |\n","|    policy_gradient_loss | -0.00803    |\n","|    std                  | 0.424       |\n","|    value_loss           | 33.4        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 821     |\n","|    iterations      | 302     |\n","|    time_elapsed    | 4518    |\n","|    total_timesteps | 3710976 |\n","--------------------------------\n","Eval num_timesteps=3720000, episode_reward=41.95 +/- 10.47\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 41.9        |\n","| time/                   |             |\n","|    total_timesteps      | 3720000     |\n","| train/                  |             |\n","|    approx_kl            | 0.019904783 |\n","|    clip_fraction        | 0.234       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.21       |\n","|    explained_variance   | 0.0191      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.582       |\n","|    n_updates            | 3020        |\n","|    policy_gradient_loss | -0.00772    |\n","|    std                  | 0.423       |\n","|    value_loss           | 1.66        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 821     |\n","|    iterations      | 303     |\n","|    time_elapsed    | 4533    |\n","|    total_timesteps | 3723264 |\n","--------------------------------\n","Eval num_timesteps=3732000, episode_reward=51.66 +/- 0.90\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 51.7         |\n","| time/                   |              |\n","|    total_timesteps      | 3732000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0099335965 |\n","|    clip_fraction        | 0.14         |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.2         |\n","|    explained_variance   | 0.676        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 1.59         |\n","|    n_updates            | 3030         |\n","|    policy_gradient_loss | -0.00696     |\n","|    std                  | 0.423        |\n","|    value_loss           | 8.38         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 821     |\n","|    iterations      | 304     |\n","|    time_elapsed    | 4546    |\n","|    total_timesteps | 3735552 |\n","--------------------------------\n","Eval num_timesteps=3744000, episode_reward=50.80 +/- 7.41\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 50.8        |\n","| time/                   |             |\n","|    total_timesteps      | 3744000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015387517 |\n","|    clip_fraction        | 0.174       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.2        |\n","|    explained_variance   | 0.0985      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 9           |\n","|    n_updates            | 3040        |\n","|    policy_gradient_loss | -0.00705    |\n","|    std                  | 0.422       |\n","|    value_loss           | 22.7        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 821     |\n","|    iterations      | 305     |\n","|    time_elapsed    | 4562    |\n","|    total_timesteps | 3747840 |\n","--------------------------------\n","Eval num_timesteps=3756000, episode_reward=51.10 +/- 0.06\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 51.1        |\n","| time/                   |             |\n","|    total_timesteps      | 3756000     |\n","| train/                  |             |\n","|    approx_kl            | 0.017965162 |\n","|    clip_fraction        | 0.216       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.19       |\n","|    explained_variance   | 0.357       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.494       |\n","|    n_updates            | 3050        |\n","|    policy_gradient_loss | -0.0127     |\n","|    std                  | 0.42        |\n","|    value_loss           | 1.04        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 821     |\n","|    iterations      | 306     |\n","|    time_elapsed    | 4578    |\n","|    total_timesteps | 3760128 |\n","--------------------------------\n","Eval num_timesteps=3768000, episode_reward=54.72 +/- 1.34\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 54.7       |\n","| time/                   |            |\n","|    total_timesteps      | 3768000    |\n","| train/                  |            |\n","|    approx_kl            | 0.00839672 |\n","|    clip_fraction        | 0.117      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.17      |\n","|    explained_variance   | 0.79       |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 1.46       |\n","|    n_updates            | 3060       |\n","|    policy_gradient_loss | -0.00659   |\n","|    std                  | 0.42       |\n","|    value_loss           | 5.56       |\n","----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 821     |\n","|    iterations      | 307     |\n","|    time_elapsed    | 4591    |\n","|    total_timesteps | 3772416 |\n","--------------------------------\n","Eval num_timesteps=3780000, episode_reward=52.78 +/- 5.21\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 52.8        |\n","| time/                   |             |\n","|    total_timesteps      | 3780000     |\n","| train/                  |             |\n","|    approx_kl            | 0.017612213 |\n","|    clip_fraction        | 0.193       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.16       |\n","|    explained_variance   | 0.295       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.424       |\n","|    n_updates            | 3070        |\n","|    policy_gradient_loss | -0.0106     |\n","|    std                  | 0.416       |\n","|    value_loss           | 1.55        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 308     |\n","|    time_elapsed    | 4610    |\n","|    total_timesteps | 3784704 |\n","--------------------------------\n","Eval num_timesteps=3792000, episode_reward=56.93 +/- 4.78\n","Episode length: 500.00 +/- 0.00\n","---------------------------------------\n","| eval/                   |           |\n","|    mean_ep_length       | 500       |\n","|    mean_reward          | 56.9      |\n","| time/                   |           |\n","|    total_timesteps      | 3792000   |\n","| train/                  |           |\n","|    approx_kl            | 0.0176828 |\n","|    clip_fraction        | 0.212     |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -2.13     |\n","|    explained_variance   | 0.414     |\n","|    learning_rate        | 0.0003    |\n","|    loss                 | 0.304     |\n","|    n_updates            | 3080      |\n","|    policy_gradient_loss | -0.0114   |\n","|    std                  | 0.414     |\n","|    value_loss           | 0.923     |\n","---------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 309     |\n","|    time_elapsed    | 4626    |\n","|    total_timesteps | 3796992 |\n","--------------------------------\n","Eval num_timesteps=3804000, episode_reward=50.49 +/- 1.16\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 50.5        |\n","| time/                   |             |\n","|    total_timesteps      | 3804000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012557899 |\n","|    clip_fraction        | 0.166       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.13       |\n","|    explained_variance   | 0.474       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.347       |\n","|    n_updates            | 3090        |\n","|    policy_gradient_loss | -0.0106     |\n","|    std                  | 0.415       |\n","|    value_loss           | 0.795       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 310     |\n","|    time_elapsed    | 4643    |\n","|    total_timesteps | 3809280 |\n","--------------------------------\n","Eval num_timesteps=3816000, episode_reward=48.11 +/- 0.84\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 48.1        |\n","| time/                   |             |\n","|    total_timesteps      | 3816000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015054099 |\n","|    clip_fraction        | 0.16        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.14       |\n","|    explained_variance   | 0.339       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.456       |\n","|    n_updates            | 3100        |\n","|    policy_gradient_loss | -0.00997    |\n","|    std                  | 0.416       |\n","|    value_loss           | 1.16        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 311     |\n","|    time_elapsed    | 4656    |\n","|    total_timesteps | 3821568 |\n","--------------------------------\n","Eval num_timesteps=3828000, episode_reward=50.57 +/- 2.14\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 50.6         |\n","| time/                   |              |\n","|    total_timesteps      | 3828000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0077393427 |\n","|    clip_fraction        | 0.0964       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.14        |\n","|    explained_variance   | 0.485        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 33.6         |\n","|    n_updates            | 3110         |\n","|    policy_gradient_loss | -0.00511     |\n","|    std                  | 0.417        |\n","|    value_loss           | 25.4         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 312     |\n","|    time_elapsed    | 4670    |\n","|    total_timesteps | 3833856 |\n","--------------------------------\n","Eval num_timesteps=3840000, episode_reward=58.50 +/- 2.20\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 58.5        |\n","| time/                   |             |\n","|    total_timesteps      | 3840000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012710206 |\n","|    clip_fraction        | 0.155       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.13       |\n","|    explained_variance   | 0.58        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.423       |\n","|    n_updates            | 3120        |\n","|    policy_gradient_loss | -0.0111     |\n","|    std                  | 0.414       |\n","|    value_loss           | 1.02        |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 313     |\n","|    time_elapsed    | 4687    |\n","|    total_timesteps | 3846144 |\n","--------------------------------\n","Eval num_timesteps=3852000, episode_reward=48.23 +/- 6.45\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 48.2        |\n","| time/                   |             |\n","|    total_timesteps      | 3852000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013834921 |\n","|    clip_fraction        | 0.155       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.11       |\n","|    explained_variance   | 0.506       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.545       |\n","|    n_updates            | 3130        |\n","|    policy_gradient_loss | -0.0115     |\n","|    std                  | 0.413       |\n","|    value_loss           | 1.01        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 314     |\n","|    time_elapsed    | 4702    |\n","|    total_timesteps | 3858432 |\n","--------------------------------\n","Eval num_timesteps=3864000, episode_reward=51.93 +/- 3.58\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 51.9        |\n","| time/                   |             |\n","|    total_timesteps      | 3864000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012458066 |\n","|    clip_fraction        | 0.164       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.09       |\n","|    explained_variance   | 0.665       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.403       |\n","|    n_updates            | 3140        |\n","|    policy_gradient_loss | -0.0103     |\n","|    std                  | 0.411       |\n","|    value_loss           | 0.849       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 315     |\n","|    time_elapsed    | 4715    |\n","|    total_timesteps | 3870720 |\n","--------------------------------\n","Eval num_timesteps=3876000, episode_reward=12.82 +/- 57.76\n","Episode length: 480.40 +/- 24.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 480         |\n","|    mean_reward          | 12.8        |\n","| time/                   |             |\n","|    total_timesteps      | 3876000     |\n","| train/                  |             |\n","|    approx_kl            | 0.016079403 |\n","|    clip_fraction        | 0.178       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.09       |\n","|    explained_variance   | 0.592       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.552       |\n","|    n_updates            | 3150        |\n","|    policy_gradient_loss | -0.0124     |\n","|    std                  | 0.412       |\n","|    value_loss           | 0.9         |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 316     |\n","|    time_elapsed    | 4730    |\n","|    total_timesteps | 3883008 |\n","--------------------------------\n","Eval num_timesteps=3888000, episode_reward=52.10 +/- 2.59\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 52.1        |\n","| time/                   |             |\n","|    total_timesteps      | 3888000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013468608 |\n","|    clip_fraction        | 0.154       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.08       |\n","|    explained_variance   | 0.413       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.414       |\n","|    n_updates            | 3160        |\n","|    policy_gradient_loss | -0.0104     |\n","|    std                  | 0.409       |\n","|    value_loss           | 1.09        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 317     |\n","|    time_elapsed    | 4746    |\n","|    total_timesteps | 3895296 |\n","--------------------------------\n","Eval num_timesteps=3900000, episode_reward=48.35 +/- 3.50\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 48.4        |\n","| time/                   |             |\n","|    total_timesteps      | 3900000     |\n","| train/                  |             |\n","|    approx_kl            | 0.007082104 |\n","|    clip_fraction        | 0.103       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.07       |\n","|    explained_variance   | 0.0724      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 10.2        |\n","|    n_updates            | 3170        |\n","|    policy_gradient_loss | -0.00668    |\n","|    std                  | 0.409       |\n","|    value_loss           | 40.3        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 318     |\n","|    time_elapsed    | 4760    |\n","|    total_timesteps | 3907584 |\n","--------------------------------\n","Eval num_timesteps=3912000, episode_reward=-11.93 +/- 45.93\n","Episode length: 465.80 +/- 27.92\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 466         |\n","|    mean_reward          | -11.9       |\n","| time/                   |             |\n","|    total_timesteps      | 3912000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008147147 |\n","|    clip_fraction        | 0.0763      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.07       |\n","|    explained_variance   | 0.0589      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 14.9        |\n","|    n_updates            | 3180        |\n","|    policy_gradient_loss | -0.00716    |\n","|    std                  | 0.408       |\n","|    value_loss           | 26.4        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 821     |\n","|    iterations      | 319     |\n","|    time_elapsed    | 4774    |\n","|    total_timesteps | 3919872 |\n","--------------------------------\n","Eval num_timesteps=3924000, episode_reward=49.82 +/- 2.02\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 49.8        |\n","| time/                   |             |\n","|    total_timesteps      | 3924000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012913848 |\n","|    clip_fraction        | 0.136       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.05       |\n","|    explained_variance   | 0.519       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.661       |\n","|    n_updates            | 3190        |\n","|    policy_gradient_loss | -0.0111     |\n","|    std                  | 0.406       |\n","|    value_loss           | 1.8         |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 320     |\n","|    time_elapsed    | 4790    |\n","|    total_timesteps | 3932160 |\n","--------------------------------\n","Eval num_timesteps=3936000, episode_reward=46.96 +/- 0.87\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 47          |\n","| time/                   |             |\n","|    total_timesteps      | 3936000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008788572 |\n","|    clip_fraction        | 0.0971      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.04       |\n","|    explained_variance   | 0.724       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.21        |\n","|    n_updates            | 3200        |\n","|    policy_gradient_loss | -0.00607    |\n","|    std                  | 0.407       |\n","|    value_loss           | 19.6        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 321     |\n","|    time_elapsed    | 4805    |\n","|    total_timesteps | 3944448 |\n","--------------------------------\n","Eval num_timesteps=3948000, episode_reward=50.00 +/- 7.84\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 50          |\n","| time/                   |             |\n","|    total_timesteps      | 3948000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015871456 |\n","|    clip_fraction        | 0.194       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.03       |\n","|    explained_variance   | 0.405       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.377       |\n","|    n_updates            | 3210        |\n","|    policy_gradient_loss | -0.0119     |\n","|    std                  | 0.404       |\n","|    value_loss           | 1.02        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 821     |\n","|    iterations      | 322     |\n","|    time_elapsed    | 4818    |\n","|    total_timesteps | 3956736 |\n","--------------------------------\n","Eval num_timesteps=3960000, episode_reward=48.92 +/- 0.38\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 48.9        |\n","| time/                   |             |\n","|    total_timesteps      | 3960000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013573575 |\n","|    clip_fraction        | 0.169       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.99       |\n","|    explained_variance   | 0.622       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.379       |\n","|    n_updates            | 3220        |\n","|    policy_gradient_loss | -0.0112     |\n","|    std                  | 0.4         |\n","|    value_loss           | 0.885       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 821     |\n","|    iterations      | 323     |\n","|    time_elapsed    | 4833    |\n","|    total_timesteps | 3969024 |\n","--------------------------------\n","Eval num_timesteps=3972000, episode_reward=54.76 +/- 2.17\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 54.8        |\n","| time/                   |             |\n","|    total_timesteps      | 3972000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014205066 |\n","|    clip_fraction        | 0.172       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.97       |\n","|    explained_variance   | 0.727       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.416       |\n","|    n_updates            | 3230        |\n","|    policy_gradient_loss | -0.0117     |\n","|    std                  | 0.398       |\n","|    value_loss           | 0.818       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 324     |\n","|    time_elapsed    | 4849    |\n","|    total_timesteps | 3981312 |\n","--------------------------------\n","Eval num_timesteps=3984000, episode_reward=51.45 +/- 7.26\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 51.5        |\n","| time/                   |             |\n","|    total_timesteps      | 3984000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012355927 |\n","|    clip_fraction        | 0.161       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.94       |\n","|    explained_variance   | 0.641       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.386       |\n","|    n_updates            | 3240        |\n","|    policy_gradient_loss | -0.0107     |\n","|    std                  | 0.395       |\n","|    value_loss           | 0.754       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 821     |\n","|    iterations      | 325     |\n","|    time_elapsed    | 4863    |\n","|    total_timesteps | 3993600 |\n","--------------------------------\n","Eval num_timesteps=3996000, episode_reward=53.56 +/- 4.53\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 53.6         |\n","| time/                   |              |\n","|    total_timesteps      | 3996000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0072406344 |\n","|    clip_fraction        | 0.0856       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -1.92        |\n","|    explained_variance   | 0.418        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 7.45         |\n","|    n_updates            | 3250         |\n","|    policy_gradient_loss | -0.00489     |\n","|    std                  | 0.394        |\n","|    value_loss           | 14.5         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 821     |\n","|    iterations      | 326     |\n","|    time_elapsed    | 4877    |\n","|    total_timesteps | 4005888 |\n","--------------------------------\n","Eval num_timesteps=4008000, episode_reward=49.74 +/- 3.56\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 49.7        |\n","| time/                   |             |\n","|    total_timesteps      | 4008000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013573895 |\n","|    clip_fraction        | 0.114       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.91       |\n","|    explained_variance   | 0.465       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.786       |\n","|    n_updates            | 3260        |\n","|    policy_gradient_loss | -0.00837    |\n","|    std                  | 0.393       |\n","|    value_loss           | 2.15        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 327     |\n","|    time_elapsed    | 4896    |\n","|    total_timesteps | 4018176 |\n","--------------------------------\n","Eval num_timesteps=4020000, episode_reward=50.39 +/- 4.13\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 50.4        |\n","| time/                   |             |\n","|    total_timesteps      | 4020000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011032837 |\n","|    clip_fraction        | 0.122       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.91       |\n","|    explained_variance   | 0.112       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 10.1        |\n","|    n_updates            | 3270        |\n","|    policy_gradient_loss | -0.00612    |\n","|    std                  | 0.394       |\n","|    value_loss           | 44.9        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 328     |\n","|    time_elapsed    | 4913    |\n","|    total_timesteps | 4030464 |\n","--------------------------------\n","Eval num_timesteps=4032000, episode_reward=51.73 +/- 0.91\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 51.7        |\n","| time/                   |             |\n","|    total_timesteps      | 4032000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014601138 |\n","|    clip_fraction        | 0.158       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.91       |\n","|    explained_variance   | 0.659       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.585       |\n","|    n_updates            | 3280        |\n","|    policy_gradient_loss | -0.0113     |\n","|    std                  | 0.393       |\n","|    value_loss           | 0.946       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 329     |\n","|    time_elapsed    | 4925    |\n","|    total_timesteps | 4042752 |\n","--------------------------------\n","Eval num_timesteps=4044000, episode_reward=39.25 +/- 1.73\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 39.3        |\n","| time/                   |             |\n","|    total_timesteps      | 4044000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013573129 |\n","|    clip_fraction        | 0.191       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.91       |\n","|    explained_variance   | 0.596       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.421       |\n","|    n_updates            | 3290        |\n","|    policy_gradient_loss | -0.0111     |\n","|    std                  | 0.394       |\n","|    value_loss           | 0.807       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 330     |\n","|    time_elapsed    | 4940    |\n","|    total_timesteps | 4055040 |\n","--------------------------------\n","Eval num_timesteps=4056000, episode_reward=49.33 +/- 3.22\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 49.3       |\n","| time/                   |            |\n","|    total_timesteps      | 4056000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01611422 |\n","|    clip_fraction        | 0.173      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.89      |\n","|    explained_variance   | 0.528      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.39       |\n","|    n_updates            | 3300       |\n","|    policy_gradient_loss | -0.0114    |\n","|    std                  | 0.39       |\n","|    value_loss           | 0.95       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 331     |\n","|    time_elapsed    | 4957    |\n","|    total_timesteps | 4067328 |\n","--------------------------------\n","Eval num_timesteps=4068000, episode_reward=55.50 +/- 1.51\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 55.5        |\n","| time/                   |             |\n","|    total_timesteps      | 4068000     |\n","| train/                  |             |\n","|    approx_kl            | 0.016507423 |\n","|    clip_fraction        | 0.17        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.87       |\n","|    explained_variance   | 0.626       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.51        |\n","|    n_updates            | 3310        |\n","|    policy_gradient_loss | -0.00718    |\n","|    std                  | 0.388       |\n","|    value_loss           | 3.71        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 332     |\n","|    time_elapsed    | 4970    |\n","|    total_timesteps | 4079616 |\n","--------------------------------\n","Eval num_timesteps=4080000, episode_reward=52.33 +/- 1.08\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 52.3        |\n","| time/                   |             |\n","|    total_timesteps      | 4080000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013699259 |\n","|    clip_fraction        | 0.165       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.85       |\n","|    explained_variance   | 0.607       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.273       |\n","|    n_updates            | 3320        |\n","|    policy_gradient_loss | -0.0113     |\n","|    std                  | 0.387       |\n","|    value_loss           | 0.729       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 333     |\n","|    time_elapsed    | 4984    |\n","|    total_timesteps | 4091904 |\n","--------------------------------\n","Eval num_timesteps=4092000, episode_reward=52.68 +/- 2.19\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 52.7         |\n","| time/                   |              |\n","|    total_timesteps      | 4092000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0147340195 |\n","|    clip_fraction        | 0.179        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -1.85        |\n","|    explained_variance   | 0.569        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.273        |\n","|    n_updates            | 3330         |\n","|    policy_gradient_loss | -0.0106      |\n","|    std                  | 0.387        |\n","|    value_loss           | 0.725        |\n","------------------------------------------\n","Eval num_timesteps=4104000, episode_reward=45.83 +/- 0.96\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 45.8     |\n","| time/              |          |\n","|    total_timesteps | 4104000  |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 334     |\n","|    time_elapsed    | 5005    |\n","|    total_timesteps | 4104192 |\n","--------------------------------\n","Eval num_timesteps=4116000, episode_reward=50.61 +/- 10.82\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 50.6        |\n","| time/                   |             |\n","|    total_timesteps      | 4116000     |\n","| train/                  |             |\n","|    approx_kl            | 0.016601438 |\n","|    clip_fraction        | 0.179       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.84       |\n","|    explained_variance   | 0.649       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.486       |\n","|    n_updates            | 3340        |\n","|    policy_gradient_loss | -0.0126     |\n","|    std                  | 0.386       |\n","|    value_loss           | 1.19        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 335     |\n","|    time_elapsed    | 5022    |\n","|    total_timesteps | 4116480 |\n","--------------------------------\n","Eval num_timesteps=4128000, episode_reward=45.83 +/- 2.52\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 45.8         |\n","| time/                   |              |\n","|    total_timesteps      | 4128000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0124769835 |\n","|    clip_fraction        | 0.156        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -1.84        |\n","|    explained_variance   | 0.556        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.293        |\n","|    n_updates            | 3350         |\n","|    policy_gradient_loss | -0.0111      |\n","|    std                  | 0.386        |\n","|    value_loss           | 0.841        |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 336     |\n","|    time_elapsed    | 5037    |\n","|    total_timesteps | 4128768 |\n","--------------------------------\n","Eval num_timesteps=4140000, episode_reward=45.52 +/- 7.70\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 45.5        |\n","| time/                   |             |\n","|    total_timesteps      | 4140000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012712609 |\n","|    clip_fraction        | 0.157       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.82       |\n","|    explained_variance   | 0.566       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.481       |\n","|    n_updates            | 3360        |\n","|    policy_gradient_loss | -0.0119     |\n","|    std                  | 0.383       |\n","|    value_loss           | 0.951       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 337     |\n","|    time_elapsed    | 5050    |\n","|    total_timesteps | 4141056 |\n","--------------------------------\n","Eval num_timesteps=4152000, episode_reward=52.12 +/- 3.20\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 52.1        |\n","| time/                   |             |\n","|    total_timesteps      | 4152000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014774046 |\n","|    clip_fraction        | 0.159       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.82       |\n","|    explained_variance   | 0.568       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.484       |\n","|    n_updates            | 3370        |\n","|    policy_gradient_loss | -0.0109     |\n","|    std                  | 0.384       |\n","|    value_loss           | 0.921       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 338     |\n","|    time_elapsed    | 5065    |\n","|    total_timesteps | 4153344 |\n","--------------------------------\n","Eval num_timesteps=4164000, episode_reward=32.35 +/- 6.00\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 32.4        |\n","| time/                   |             |\n","|    total_timesteps      | 4164000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011849843 |\n","|    clip_fraction        | 0.15        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.83       |\n","|    explained_variance   | 0.679       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.307       |\n","|    n_updates            | 3380        |\n","|    policy_gradient_loss | -0.0102     |\n","|    std                  | 0.385       |\n","|    value_loss           | 0.788       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 339     |\n","|    time_elapsed    | 5081    |\n","|    total_timesteps | 4165632 |\n","--------------------------------\n","Eval num_timesteps=4176000, episode_reward=-18.02 +/- 42.36\n","Episode length: 483.20 +/- 13.72\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 483         |\n","|    mean_reward          | -18         |\n","| time/                   |             |\n","|    total_timesteps      | 4176000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011671851 |\n","|    clip_fraction        | 0.138       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.83       |\n","|    explained_variance   | 0.689       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.311       |\n","|    n_updates            | 3390        |\n","|    policy_gradient_loss | -0.00944    |\n","|    std                  | 0.385       |\n","|    value_loss           | 0.802       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 340     |\n","|    time_elapsed    | 5095    |\n","|    total_timesteps | 4177920 |\n","--------------------------------\n","Eval num_timesteps=4188000, episode_reward=58.21 +/- 3.07\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 58.2       |\n","| time/                   |            |\n","|    total_timesteps      | 4188000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01355942 |\n","|    clip_fraction        | 0.161      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.83      |\n","|    explained_variance   | 0.463      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.353      |\n","|    n_updates            | 3400       |\n","|    policy_gradient_loss | -0.0114    |\n","|    std                  | 0.385      |\n","|    value_loss           | 1.08       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 341     |\n","|    time_elapsed    | 5109    |\n","|    total_timesteps | 4190208 |\n","--------------------------------\n","Eval num_timesteps=4200000, episode_reward=51.37 +/- 0.88\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 51.4        |\n","| time/                   |             |\n","|    total_timesteps      | 4200000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012307237 |\n","|    clip_fraction        | 0.146       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.83       |\n","|    explained_variance   | 0.554       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.544       |\n","|    n_updates            | 3410        |\n","|    policy_gradient_loss | -0.0134     |\n","|    std                  | 0.386       |\n","|    value_loss           | 0.944       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 342     |\n","|    time_elapsed    | 5125    |\n","|    total_timesteps | 4202496 |\n","--------------------------------\n","Eval num_timesteps=4212000, episode_reward=54.74 +/- 1.88\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 54.7       |\n","| time/                   |            |\n","|    total_timesteps      | 4212000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01694811 |\n","|    clip_fraction        | 0.15       |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.84      |\n","|    explained_variance   | 0.685      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.355      |\n","|    n_updates            | 3420       |\n","|    policy_gradient_loss | -0.00928   |\n","|    std                  | 0.386      |\n","|    value_loss           | 0.76       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 343     |\n","|    time_elapsed    | 5141    |\n","|    total_timesteps | 4214784 |\n","--------------------------------\n","Eval num_timesteps=4224000, episode_reward=53.05 +/- 1.06\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 53.1        |\n","| time/                   |             |\n","|    total_timesteps      | 4224000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013329023 |\n","|    clip_fraction        | 0.147       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.85       |\n","|    explained_variance   | 0.586       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.421       |\n","|    n_updates            | 3430        |\n","|    policy_gradient_loss | -0.0123     |\n","|    std                  | 0.386       |\n","|    value_loss           | 0.916       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 344     |\n","|    time_elapsed    | 5153    |\n","|    total_timesteps | 4227072 |\n","--------------------------------\n","Eval num_timesteps=4236000, episode_reward=59.58 +/- 2.99\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 59.6        |\n","| time/                   |             |\n","|    total_timesteps      | 4236000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008431583 |\n","|    clip_fraction        | 0.123       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.85       |\n","|    explained_variance   | 0.261       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 7.61        |\n","|    n_updates            | 3440        |\n","|    policy_gradient_loss | -0.00733    |\n","|    std                  | 0.387       |\n","|    value_loss           | 16.4        |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 345     |\n","|    time_elapsed    | 5168    |\n","|    total_timesteps | 4239360 |\n","--------------------------------\n","Eval num_timesteps=4248000, episode_reward=57.36 +/- 0.48\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 57.4        |\n","| time/                   |             |\n","|    total_timesteps      | 4248000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008313214 |\n","|    clip_fraction        | 0.1         |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.85       |\n","|    explained_variance   | 0.464       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.963       |\n","|    n_updates            | 3450        |\n","|    policy_gradient_loss | -0.00856    |\n","|    std                  | 0.387       |\n","|    value_loss           | 1.9         |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 346     |\n","|    time_elapsed    | 5186    |\n","|    total_timesteps | 4251648 |\n","--------------------------------\n","Eval num_timesteps=4260000, episode_reward=57.57 +/- 0.89\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 57.6        |\n","| time/                   |             |\n","|    total_timesteps      | 4260000     |\n","| train/                  |             |\n","|    approx_kl            | 0.016122736 |\n","|    clip_fraction        | 0.17        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.83       |\n","|    explained_variance   | 0.737       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.392       |\n","|    n_updates            | 3460        |\n","|    policy_gradient_loss | -0.0109     |\n","|    std                  | 0.383       |\n","|    value_loss           | 1.12        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 347     |\n","|    time_elapsed    | 5199    |\n","|    total_timesteps | 4263936 |\n","--------------------------------\n","Eval num_timesteps=4272000, episode_reward=50.36 +/- 2.89\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 50.4        |\n","| time/                   |             |\n","|    total_timesteps      | 4272000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013322979 |\n","|    clip_fraction        | 0.164       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.82       |\n","|    explained_variance   | 0.698       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.387       |\n","|    n_updates            | 3470        |\n","|    policy_gradient_loss | -0.0105     |\n","|    std                  | 0.383       |\n","|    value_loss           | 0.824       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 348     |\n","|    time_elapsed    | 5215    |\n","|    total_timesteps | 4276224 |\n","--------------------------------\n","Eval num_timesteps=4284000, episode_reward=56.93 +/- 2.04\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 56.9        |\n","| time/                   |             |\n","|    total_timesteps      | 4284000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008050343 |\n","|    clip_fraction        | 0.0877      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.82       |\n","|    explained_variance   | 0.407       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.25        |\n","|    n_updates            | 3480        |\n","|    policy_gradient_loss | -0.00697    |\n","|    std                  | 0.384       |\n","|    value_loss           | 20.2        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 349     |\n","|    time_elapsed    | 5231    |\n","|    total_timesteps | 4288512 |\n","--------------------------------\n","Eval num_timesteps=4296000, episode_reward=48.57 +/- 2.30\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 48.6        |\n","| time/                   |             |\n","|    total_timesteps      | 4296000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015340631 |\n","|    clip_fraction        | 0.166       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.78       |\n","|    explained_variance   | 0.197       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.542       |\n","|    n_updates            | 3490        |\n","|    policy_gradient_loss | -0.00982    |\n","|    std                  | 0.377       |\n","|    value_loss           | 1.28        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 350     |\n","|    time_elapsed    | 5243    |\n","|    total_timesteps | 4300800 |\n","--------------------------------\n","Eval num_timesteps=4308000, episode_reward=57.95 +/- 0.37\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 58         |\n","| time/                   |            |\n","|    total_timesteps      | 4308000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01385953 |\n","|    clip_fraction        | 0.17       |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.75      |\n","|    explained_variance   | 0.335      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.364      |\n","|    n_updates            | 3500       |\n","|    policy_gradient_loss | -0.00955   |\n","|    std                  | 0.376      |\n","|    value_loss           | 1.08       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 351     |\n","|    time_elapsed    | 5259    |\n","|    total_timesteps | 4313088 |\n","--------------------------------\n","Eval num_timesteps=4320000, episode_reward=60.28 +/- 1.19\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 60.3        |\n","| time/                   |             |\n","|    total_timesteps      | 4320000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014826062 |\n","|    clip_fraction        | 0.181       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.73       |\n","|    explained_variance   | 0.549       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.586       |\n","|    n_updates            | 3510        |\n","|    policy_gradient_loss | -0.0112     |\n","|    std                  | 0.376       |\n","|    value_loss           | 0.828       |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 352     |\n","|    time_elapsed    | 5275    |\n","|    total_timesteps | 4325376 |\n","--------------------------------\n","Eval num_timesteps=4332000, episode_reward=52.85 +/- 1.59\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 52.9        |\n","| time/                   |             |\n","|    total_timesteps      | 4332000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012691152 |\n","|    clip_fraction        | 0.147       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.73       |\n","|    explained_variance   | 0.836       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.267       |\n","|    n_updates            | 3520        |\n","|    policy_gradient_loss | -0.00915    |\n","|    std                  | 0.375       |\n","|    value_loss           | 0.663       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 353     |\n","|    time_elapsed    | 5288    |\n","|    total_timesteps | 4337664 |\n","--------------------------------\n","Eval num_timesteps=4344000, episode_reward=55.80 +/- 5.15\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 55.8        |\n","| time/                   |             |\n","|    total_timesteps      | 4344000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013880205 |\n","|    clip_fraction        | 0.157       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.72       |\n","|    explained_variance   | 0.871       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.3         |\n","|    n_updates            | 3530        |\n","|    policy_gradient_loss | -0.0103     |\n","|    std                  | 0.373       |\n","|    value_loss           | 0.588       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 354     |\n","|    time_elapsed    | 5302    |\n","|    total_timesteps | 4349952 |\n","--------------------------------\n","Eval num_timesteps=4356000, episode_reward=47.28 +/- 4.73\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 47.3        |\n","| time/                   |             |\n","|    total_timesteps      | 4356000     |\n","| train/                  |             |\n","|    approx_kl            | 0.012498089 |\n","|    clip_fraction        | 0.16        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.69       |\n","|    explained_variance   | 0.832       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.222       |\n","|    n_updates            | 3540        |\n","|    policy_gradient_loss | -0.00924    |\n","|    std                  | 0.371       |\n","|    value_loss           | 0.67        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 355     |\n","|    time_elapsed    | 5318    |\n","|    total_timesteps | 4362240 |\n","--------------------------------\n","Eval num_timesteps=4368000, episode_reward=54.34 +/- 7.31\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 54.3        |\n","| time/                   |             |\n","|    total_timesteps      | 4368000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014070594 |\n","|    clip_fraction        | 0.175       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.67       |\n","|    explained_variance   | 0.817       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.42        |\n","|    n_updates            | 3550        |\n","|    policy_gradient_loss | -0.0135     |\n","|    std                  | 0.369       |\n","|    value_loss           | 0.73        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 356     |\n","|    time_elapsed    | 5333    |\n","|    total_timesteps | 4374528 |\n","--------------------------------\n","Eval num_timesteps=4380000, episode_reward=45.77 +/- 2.28\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 45.8        |\n","| time/                   |             |\n","|    total_timesteps      | 4380000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008683637 |\n","|    clip_fraction        | 0.107       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.66       |\n","|    explained_variance   | 0.263       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.01        |\n","|    n_updates            | 3560        |\n","|    policy_gradient_loss | -0.00614    |\n","|    std                  | 0.37        |\n","|    value_loss           | 14.7        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 357     |\n","|    time_elapsed    | 5346    |\n","|    total_timesteps | 4386816 |\n","--------------------------------\n","Eval num_timesteps=4392000, episode_reward=50.30 +/- 5.21\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 50.3        |\n","| time/                   |             |\n","|    total_timesteps      | 4392000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010242361 |\n","|    clip_fraction        | 0.0928      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.67       |\n","|    explained_variance   | 0.563       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.83        |\n","|    n_updates            | 3570        |\n","|    policy_gradient_loss | -0.00704    |\n","|    std                  | 0.37        |\n","|    value_loss           | 8.51        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 358     |\n","|    time_elapsed    | 5362    |\n","|    total_timesteps | 4399104 |\n","--------------------------------\n","Eval num_timesteps=4404000, episode_reward=52.80 +/- 7.31\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 52.8        |\n","| time/                   |             |\n","|    total_timesteps      | 4404000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014553168 |\n","|    clip_fraction        | 0.157       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.66       |\n","|    explained_variance   | 0.383       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.435       |\n","|    n_updates            | 3580        |\n","|    policy_gradient_loss | -0.00974    |\n","|    std                  | 0.368       |\n","|    value_loss           | 1.16        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 359     |\n","|    time_elapsed    | 5378    |\n","|    total_timesteps | 4411392 |\n","--------------------------------\n","Eval num_timesteps=4416000, episode_reward=57.57 +/- 0.97\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 57.6        |\n","| time/                   |             |\n","|    total_timesteps      | 4416000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014539176 |\n","|    clip_fraction        | 0.17        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.64       |\n","|    explained_variance   | 0.748       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.39        |\n","|    n_updates            | 3590        |\n","|    policy_gradient_loss | -0.0119     |\n","|    std                  | 0.366       |\n","|    value_loss           | 0.914       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 360     |\n","|    time_elapsed    | 5391    |\n","|    total_timesteps | 4423680 |\n","--------------------------------\n","Eval num_timesteps=4428000, episode_reward=6.02 +/- 54.63\n","Episode length: 474.80 +/- 30.86\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 475         |\n","|    mean_reward          | 6.02        |\n","| time/                   |             |\n","|    total_timesteps      | 4428000     |\n","| train/                  |             |\n","|    approx_kl            | 0.018407369 |\n","|    clip_fraction        | 0.192       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.62       |\n","|    explained_variance   | 0.674       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.403       |\n","|    n_updates            | 3600        |\n","|    policy_gradient_loss | -0.0109     |\n","|    std                  | 0.365       |\n","|    value_loss           | 0.82        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 361     |\n","|    time_elapsed    | 5405    |\n","|    total_timesteps | 4435968 |\n","--------------------------------\n","Eval num_timesteps=4440000, episode_reward=60.28 +/- 1.10\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 60.3        |\n","| time/                   |             |\n","|    total_timesteps      | 4440000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015049246 |\n","|    clip_fraction        | 0.184       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.6        |\n","|    explained_variance   | 0.757       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.37        |\n","|    n_updates            | 3610        |\n","|    policy_gradient_loss | -0.0113     |\n","|    std                  | 0.362       |\n","|    value_loss           | 0.886       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 362     |\n","|    time_elapsed    | 5422    |\n","|    total_timesteps | 4448256 |\n","--------------------------------\n","Eval num_timesteps=4452000, episode_reward=58.78 +/- 1.96\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 58.8       |\n","| time/                   |            |\n","|    total_timesteps      | 4452000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01425876 |\n","|    clip_fraction        | 0.177      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.58      |\n","|    explained_variance   | 0.716      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.415      |\n","|    n_updates            | 3620       |\n","|    policy_gradient_loss | -0.0122    |\n","|    std                  | 0.361      |\n","|    value_loss           | 0.835      |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 363     |\n","|    time_elapsed    | 5437    |\n","|    total_timesteps | 4460544 |\n","--------------------------------\n","Eval num_timesteps=4464000, episode_reward=65.53 +/- 2.50\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 65.5         |\n","| time/                   |              |\n","|    total_timesteps      | 4464000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0098229945 |\n","|    clip_fraction        | 0.117        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -1.57        |\n","|    explained_variance   | 0.234        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 3.32         |\n","|    n_updates            | 3630         |\n","|    policy_gradient_loss | -0.00756     |\n","|    std                  | 0.362        |\n","|    value_loss           | 12.1         |\n","------------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 364     |\n","|    time_elapsed    | 5449    |\n","|    total_timesteps | 4472832 |\n","--------------------------------\n","Eval num_timesteps=4476000, episode_reward=63.60 +/- 0.87\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 63.6        |\n","| time/                   |             |\n","|    total_timesteps      | 4476000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014412765 |\n","|    clip_fraction        | 0.168       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.57       |\n","|    explained_variance   | 0.589       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.447       |\n","|    n_updates            | 3640        |\n","|    policy_gradient_loss | -0.0123     |\n","|    std                  | 0.36        |\n","|    value_loss           | 0.916       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 365     |\n","|    time_elapsed    | 5470    |\n","|    total_timesteps | 4485120 |\n","--------------------------------\n","Eval num_timesteps=4488000, episode_reward=61.28 +/- 4.15\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 61.3        |\n","| time/                   |             |\n","|    total_timesteps      | 4488000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009042395 |\n","|    clip_fraction        | 0.102       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.56       |\n","|    explained_variance   | 0.487       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.9         |\n","|    n_updates            | 3650        |\n","|    policy_gradient_loss | -0.00492    |\n","|    std                  | 0.36        |\n","|    value_loss           | 8.85        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 366     |\n","|    time_elapsed    | 5486    |\n","|    total_timesteps | 4497408 |\n","--------------------------------\n","Eval num_timesteps=4500000, episode_reward=-26.32 +/- 54.04\n","Episode length: 404.00 +/- 78.38\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 404         |\n","|    mean_reward          | -26.3       |\n","| time/                   |             |\n","|    total_timesteps      | 4500000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013819062 |\n","|    clip_fraction        | 0.151       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.55       |\n","|    explained_variance   | 0.476       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.586       |\n","|    n_updates            | 3660        |\n","|    policy_gradient_loss | -0.00995    |\n","|    std                  | 0.358       |\n","|    value_loss           | 1.47        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 367     |\n","|    time_elapsed    | 5500    |\n","|    total_timesteps | 4509696 |\n","--------------------------------\n","Eval num_timesteps=4512000, episode_reward=56.29 +/- 4.36\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 56.3        |\n","| time/                   |             |\n","|    total_timesteps      | 4512000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014338653 |\n","|    clip_fraction        | 0.175       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.53       |\n","|    explained_variance   | 0.755       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.441       |\n","|    n_updates            | 3670        |\n","|    policy_gradient_loss | -0.0102     |\n","|    std                  | 0.356       |\n","|    value_loss           | 0.974       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 368     |\n","|    time_elapsed    | 5513    |\n","|    total_timesteps | 4521984 |\n","--------------------------------\n","Eval num_timesteps=4524000, episode_reward=54.88 +/- 4.46\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 54.9        |\n","| time/                   |             |\n","|    total_timesteps      | 4524000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013389173 |\n","|    clip_fraction        | 0.182       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.5        |\n","|    explained_variance   | 0.714       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.363       |\n","|    n_updates            | 3680        |\n","|    policy_gradient_loss | -0.00962    |\n","|    std                  | 0.355       |\n","|    value_loss           | 0.728       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 369     |\n","|    time_elapsed    | 5530    |\n","|    total_timesteps | 4534272 |\n","--------------------------------\n","Eval num_timesteps=4536000, episode_reward=51.90 +/- 5.09\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 51.9        |\n","| time/                   |             |\n","|    total_timesteps      | 4536000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010256687 |\n","|    clip_fraction        | 0.138       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.5        |\n","|    explained_variance   | 0.2         |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.44        |\n","|    n_updates            | 3690        |\n","|    policy_gradient_loss | -0.00686    |\n","|    std                  | 0.355       |\n","|    value_loss           | 18.4        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 370     |\n","|    time_elapsed    | 5545    |\n","|    total_timesteps | 4546560 |\n","--------------------------------\n","Eval num_timesteps=4548000, episode_reward=61.39 +/- 5.40\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 61.4        |\n","| time/                   |             |\n","|    total_timesteps      | 4548000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015024595 |\n","|    clip_fraction        | 0.166       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.5        |\n","|    explained_variance   | 0.712       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.382       |\n","|    n_updates            | 3700        |\n","|    policy_gradient_loss | -0.0134     |\n","|    std                  | 0.354       |\n","|    value_loss           | 1.02        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 371     |\n","|    time_elapsed    | 5557    |\n","|    total_timesteps | 4558848 |\n","--------------------------------\n","Eval num_timesteps=4560000, episode_reward=49.74 +/- 8.22\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 49.7        |\n","| time/                   |             |\n","|    total_timesteps      | 4560000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015251656 |\n","|    clip_fraction        | 0.182       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.5        |\n","|    explained_variance   | 0.728       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.314       |\n","|    n_updates            | 3710        |\n","|    policy_gradient_loss | -0.0109     |\n","|    std                  | 0.355       |\n","|    value_loss           | 0.826       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 372     |\n","|    time_elapsed    | 5573    |\n","|    total_timesteps | 4571136 |\n","--------------------------------\n","Eval num_timesteps=4572000, episode_reward=14.79 +/- 62.37\n","Episode length: 450.00 +/- 61.24\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 450         |\n","|    mean_reward          | 14.8        |\n","| time/                   |             |\n","|    total_timesteps      | 4572000     |\n","| train/                  |             |\n","|    approx_kl            | 0.006932448 |\n","|    clip_fraction        | 0.0835      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.51       |\n","|    explained_variance   | 0.186       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 19.1        |\n","|    n_updates            | 3720        |\n","|    policy_gradient_loss | -0.00685    |\n","|    std                  | 0.356       |\n","|    value_loss           | 19          |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 373     |\n","|    time_elapsed    | 5589    |\n","|    total_timesteps | 4583424 |\n","--------------------------------\n","Eval num_timesteps=4584000, episode_reward=61.29 +/- 2.07\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 61.3        |\n","| time/                   |             |\n","|    total_timesteps      | 4584000     |\n","| train/                  |             |\n","|    approx_kl            | 0.008968384 |\n","|    clip_fraction        | 0.0829      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.51       |\n","|    explained_variance   | 0.21        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 5.33        |\n","|    n_updates            | 3730        |\n","|    policy_gradient_loss | -0.00716    |\n","|    std                  | 0.356       |\n","|    value_loss           | 18.8        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 374     |\n","|    time_elapsed    | 5602    |\n","|    total_timesteps | 4595712 |\n","--------------------------------\n","Eval num_timesteps=4596000, episode_reward=54.02 +/- 4.67\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 54          |\n","| time/                   |             |\n","|    total_timesteps      | 4596000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014502254 |\n","|    clip_fraction        | 0.163       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.51       |\n","|    explained_variance   | 0.459       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.592       |\n","|    n_updates            | 3740        |\n","|    policy_gradient_loss | -0.00936    |\n","|    std                  | 0.355       |\n","|    value_loss           | 1.55        |\n","-----------------------------------------\n","Eval num_timesteps=4608000, episode_reward=57.87 +/- 0.51\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 57.9     |\n","| time/              |          |\n","|    total_timesteps | 4608000  |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 375     |\n","|    time_elapsed    | 5623    |\n","|    total_timesteps | 4608000 |\n","--------------------------------\n","Eval num_timesteps=4620000, episode_reward=53.86 +/- 1.30\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 53.9        |\n","| time/                   |             |\n","|    total_timesteps      | 4620000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009758524 |\n","|    clip_fraction        | 0.117       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.51       |\n","|    explained_variance   | 0.453       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 5.32        |\n","|    n_updates            | 3750        |\n","|    policy_gradient_loss | -0.00821    |\n","|    std                  | 0.356       |\n","|    value_loss           | 12.2        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 376     |\n","|    time_elapsed    | 5637    |\n","|    total_timesteps | 4620288 |\n","--------------------------------\n","Eval num_timesteps=4632000, episode_reward=55.31 +/- 2.93\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 55.3        |\n","| time/                   |             |\n","|    total_timesteps      | 4632000     |\n","| train/                  |             |\n","|    approx_kl            | 0.016714985 |\n","|    clip_fraction        | 0.214       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.5        |\n","|    explained_variance   | 0.499       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.408       |\n","|    n_updates            | 3760        |\n","|    policy_gradient_loss | -0.0118     |\n","|    std                  | 0.354       |\n","|    value_loss           | 1.02        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 377     |\n","|    time_elapsed    | 5654    |\n","|    total_timesteps | 4632576 |\n","--------------------------------\n","Eval num_timesteps=4644000, episode_reward=57.25 +/- 3.58\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 57.3       |\n","| time/                   |            |\n","|    total_timesteps      | 4644000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01576449 |\n","|    clip_fraction        | 0.186      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.49      |\n","|    explained_variance   | 0.775      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.443      |\n","|    n_updates            | 3770       |\n","|    policy_gradient_loss | -0.00937   |\n","|    std                  | 0.353      |\n","|    value_loss           | 1.17       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 378     |\n","|    time_elapsed    | 5668    |\n","|    total_timesteps | 4644864 |\n","--------------------------------\n","Eval num_timesteps=4656000, episode_reward=54.02 +/- 1.36\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 54          |\n","| time/                   |             |\n","|    total_timesteps      | 4656000     |\n","| train/                  |             |\n","|    approx_kl            | 0.016194073 |\n","|    clip_fraction        | 0.195       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.46       |\n","|    explained_variance   | 0.727       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.73        |\n","|    n_updates            | 3780        |\n","|    policy_gradient_loss | -0.0116     |\n","|    std                  | 0.35        |\n","|    value_loss           | 1.01        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 379     |\n","|    time_elapsed    | 5681    |\n","|    total_timesteps | 4657152 |\n","--------------------------------\n","Eval num_timesteps=4668000, episode_reward=60.05 +/- 2.84\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 60.1        |\n","| time/                   |             |\n","|    total_timesteps      | 4668000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015060373 |\n","|    clip_fraction        | 0.164       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.44       |\n","|    explained_variance   | 0.879       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.228       |\n","|    n_updates            | 3790        |\n","|    policy_gradient_loss | -0.0116     |\n","|    std                  | 0.348       |\n","|    value_loss           | 0.548       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 380     |\n","|    time_elapsed    | 5697    |\n","|    total_timesteps | 4669440 |\n","--------------------------------\n","Eval num_timesteps=4680000, episode_reward=54.77 +/- 4.07\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 54.8         |\n","| time/                   |              |\n","|    total_timesteps      | 4680000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0077765286 |\n","|    clip_fraction        | 0.102        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -1.43        |\n","|    explained_variance   | 0.196        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 3.49         |\n","|    n_updates            | 3800         |\n","|    policy_gradient_loss | -0.00588     |\n","|    std                  | 0.349        |\n","|    value_loss           | 19.1         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 381     |\n","|    time_elapsed    | 5713    |\n","|    total_timesteps | 4681728 |\n","--------------------------------\n","Eval num_timesteps=4692000, episode_reward=59.24 +/- 2.03\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 59.2         |\n","| time/                   |              |\n","|    total_timesteps      | 4692000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0069894423 |\n","|    clip_fraction        | 0.067        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -1.43        |\n","|    explained_variance   | 0.522        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 13           |\n","|    n_updates            | 3810         |\n","|    policy_gradient_loss | -0.00639     |\n","|    std                  | 0.349        |\n","|    value_loss           | 23.3         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 382     |\n","|    time_elapsed    | 5726    |\n","|    total_timesteps | 4694016 |\n","--------------------------------\n","Eval num_timesteps=4704000, episode_reward=55.85 +/- 2.55\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 55.9        |\n","| time/                   |             |\n","|    total_timesteps      | 4704000     |\n","| train/                  |             |\n","|    approx_kl            | 0.017604562 |\n","|    clip_fraction        | 0.169       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.43       |\n","|    explained_variance   | 0.83        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.562       |\n","|    n_updates            | 3820        |\n","|    policy_gradient_loss | -0.0102     |\n","|    std                  | 0.348       |\n","|    value_loss           | 1.16        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 383     |\n","|    time_elapsed    | 5740    |\n","|    total_timesteps | 4706304 |\n","--------------------------------\n","Eval num_timesteps=4716000, episode_reward=56.61 +/- 2.51\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 56.6        |\n","| time/                   |             |\n","|    total_timesteps      | 4716000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010420595 |\n","|    clip_fraction        | 0.126       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.42       |\n","|    explained_variance   | 0.21        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 11          |\n","|    n_updates            | 3830        |\n","|    policy_gradient_loss | -0.00617    |\n","|    std                  | 0.348       |\n","|    value_loss           | 17.9        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 384     |\n","|    time_elapsed    | 5759    |\n","|    total_timesteps | 4718592 |\n","--------------------------------\n","Eval num_timesteps=4728000, episode_reward=53.94 +/- 0.74\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 53.9        |\n","| time/                   |             |\n","|    total_timesteps      | 4728000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014778733 |\n","|    clip_fraction        | 0.164       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.41       |\n","|    explained_variance   | 0.609       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.561       |\n","|    n_updates            | 3840        |\n","|    policy_gradient_loss | -0.00992    |\n","|    std                  | 0.345       |\n","|    value_loss           | 1.94        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 385     |\n","|    time_elapsed    | 5772    |\n","|    total_timesteps | 4730880 |\n","--------------------------------\n","Eval num_timesteps=4740000, episode_reward=58.12 +/- 0.73\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 58.1        |\n","| time/                   |             |\n","|    total_timesteps      | 4740000     |\n","| train/                  |             |\n","|    approx_kl            | 0.018928047 |\n","|    clip_fraction        | 0.236       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.4        |\n","|    explained_variance   | 0.47        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.54        |\n","|    n_updates            | 3850        |\n","|    policy_gradient_loss | -0.0124     |\n","|    std                  | 0.346       |\n","|    value_loss           | 1.27        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 386     |\n","|    time_elapsed    | 5787    |\n","|    total_timesteps | 4743168 |\n","--------------------------------\n","Eval num_timesteps=4752000, episode_reward=65.67 +/- 1.87\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 65.7        |\n","| time/                   |             |\n","|    total_timesteps      | 4752000     |\n","| train/                  |             |\n","|    approx_kl            | 0.018225454 |\n","|    clip_fraction        | 0.215       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.39       |\n","|    explained_variance   | 0.729       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.476       |\n","|    n_updates            | 3860        |\n","|    policy_gradient_loss | -0.0104     |\n","|    std                  | 0.344       |\n","|    value_loss           | 1.02        |\n","-----------------------------------------\n","New best mean reward!\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 387     |\n","|    time_elapsed    | 5804    |\n","|    total_timesteps | 4755456 |\n","--------------------------------\n","Eval num_timesteps=4764000, episode_reward=61.11 +/- 0.88\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 61.1        |\n","| time/                   |             |\n","|    total_timesteps      | 4764000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015001338 |\n","|    clip_fraction        | 0.197       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.36       |\n","|    explained_variance   | 0.87        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.362       |\n","|    n_updates            | 3870        |\n","|    policy_gradient_loss | -0.0103     |\n","|    std                  | 0.341       |\n","|    value_loss           | 0.638       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 388     |\n","|    time_elapsed    | 5816    |\n","|    total_timesteps | 4767744 |\n","--------------------------------\n","Eval num_timesteps=4776000, episode_reward=62.00 +/- 1.44\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 62          |\n","| time/                   |             |\n","|    total_timesteps      | 4776000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014630429 |\n","|    clip_fraction        | 0.176       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.34       |\n","|    explained_variance   | 0.836       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.314       |\n","|    n_updates            | 3880        |\n","|    policy_gradient_loss | -0.0119     |\n","|    std                  | 0.34        |\n","|    value_loss           | 0.764       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 389     |\n","|    time_elapsed    | 5831    |\n","|    total_timesteps | 4780032 |\n","--------------------------------\n","Eval num_timesteps=4788000, episode_reward=57.38 +/- 5.85\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 57.4        |\n","| time/                   |             |\n","|    total_timesteps      | 4788000     |\n","| train/                  |             |\n","|    approx_kl            | 0.015543729 |\n","|    clip_fraction        | 0.19        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.31       |\n","|    explained_variance   | 0.796       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.278       |\n","|    n_updates            | 3890        |\n","|    policy_gradient_loss | -0.0115     |\n","|    std                  | 0.337       |\n","|    value_loss           | 0.885       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 390     |\n","|    time_elapsed    | 5847    |\n","|    total_timesteps | 4792320 |\n","--------------------------------\n","Eval num_timesteps=4800000, episode_reward=65.56 +/- 0.58\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 65.6        |\n","| time/                   |             |\n","|    total_timesteps      | 4800000     |\n","| train/                  |             |\n","|    approx_kl            | 0.016171403 |\n","|    clip_fraction        | 0.191       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.3        |\n","|    explained_variance   | 0.757       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.37        |\n","|    n_updates            | 3900        |\n","|    policy_gradient_loss | -0.0115     |\n","|    std                  | 0.337       |\n","|    value_loss           | 0.777       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 391     |\n","|    time_elapsed    | 5861    |\n","|    total_timesteps | 4804608 |\n","--------------------------------\n","Eval num_timesteps=4812000, episode_reward=59.49 +/- 1.45\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 59.5        |\n","| time/                   |             |\n","|    total_timesteps      | 4812000     |\n","| train/                  |             |\n","|    approx_kl            | 0.014832933 |\n","|    clip_fraction        | 0.163       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.3        |\n","|    explained_variance   | 0.762       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.443       |\n","|    n_updates            | 3910        |\n","|    policy_gradient_loss | -0.0119     |\n","|    std                  | 0.337       |\n","|    value_loss           | 1.05        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 392     |\n","|    time_elapsed    | 5874    |\n","|    total_timesteps | 4816896 |\n","--------------------------------\n","Eval num_timesteps=4824000, episode_reward=64.07 +/- 5.61\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 64.1        |\n","| time/                   |             |\n","|    total_timesteps      | 4824000     |\n","| train/                  |             |\n","|    approx_kl            | 0.016851448 |\n","|    clip_fraction        | 0.19        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.29       |\n","|    explained_variance   | 0.792       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.366       |\n","|    n_updates            | 3920        |\n","|    policy_gradient_loss | -0.0113     |\n","|    std                  | 0.336       |\n","|    value_loss           | 0.798       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 393     |\n","|    time_elapsed    | 5890    |\n","|    total_timesteps | 4829184 |\n","--------------------------------\n","Eval num_timesteps=4836000, episode_reward=63.64 +/- 3.83\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 63.6        |\n","| time/                   |             |\n","|    total_timesteps      | 4836000     |\n","| train/                  |             |\n","|    approx_kl            | 0.009745176 |\n","|    clip_fraction        | 0.141       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.28       |\n","|    explained_variance   | 0.325       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.32        |\n","|    n_updates            | 3930        |\n","|    policy_gradient_loss | -0.00709    |\n","|    std                  | 0.335       |\n","|    value_loss           | 15.5        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 394     |\n","|    time_elapsed    | 5906    |\n","|    total_timesteps | 4841472 |\n","--------------------------------\n","Eval num_timesteps=4848000, episode_reward=-2.14 +/- 85.79\n","Episode length: 328.80 +/- 209.68\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 329          |\n","|    mean_reward          | -2.14        |\n","| time/                   |              |\n","|    total_timesteps      | 4848000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0063940845 |\n","|    clip_fraction        | 0.0705       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -1.28        |\n","|    explained_variance   | 0.314        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 10.1         |\n","|    n_updates            | 3940         |\n","|    policy_gradient_loss | -0.00783     |\n","|    std                  | 0.335        |\n","|    value_loss           | 32.8         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 395     |\n","|    time_elapsed    | 5918    |\n","|    total_timesteps | 4853760 |\n","--------------------------------\n","Eval num_timesteps=4860000, episode_reward=61.92 +/- 6.07\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 61.9         |\n","| time/                   |              |\n","|    total_timesteps      | 4860000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0062018204 |\n","|    clip_fraction        | 0.056        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -1.28        |\n","|    explained_variance   | 0.377        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 8.26         |\n","|    n_updates            | 3950         |\n","|    policy_gradient_loss | -0.0072      |\n","|    std                  | 0.335        |\n","|    value_loss           | 30.5         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 396     |\n","|    time_elapsed    | 5933    |\n","|    total_timesteps | 4866048 |\n","--------------------------------\n","Eval num_timesteps=4872000, episode_reward=64.36 +/- 1.68\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 64.4        |\n","| time/                   |             |\n","|    total_timesteps      | 4872000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011109516 |\n","|    clip_fraction        | 0.108       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.27       |\n","|    explained_variance   | 0.615       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.54        |\n","|    n_updates            | 3960        |\n","|    policy_gradient_loss | -0.00974    |\n","|    std                  | 0.332       |\n","|    value_loss           | 5.77        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 397     |\n","|    time_elapsed    | 5949    |\n","|    total_timesteps | 4878336 |\n","--------------------------------\n","Eval num_timesteps=4884000, episode_reward=63.08 +/- 3.98\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 63.1        |\n","| time/                   |             |\n","|    total_timesteps      | 4884000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010269827 |\n","|    clip_fraction        | 0.116       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.25       |\n","|    explained_variance   | 0.683       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 6.69        |\n","|    n_updates            | 3970        |\n","|    policy_gradient_loss | -0.00851    |\n","|    std                  | 0.334       |\n","|    value_loss           | 9.84        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 398     |\n","|    time_elapsed    | 5963    |\n","|    total_timesteps | 4890624 |\n","--------------------------------\n","Eval num_timesteps=4896000, episode_reward=20.62 +/- 57.44\n","Episode length: 484.00 +/- 19.60\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 484         |\n","|    mean_reward          | 20.6        |\n","| time/                   |             |\n","|    total_timesteps      | 4896000     |\n","| train/                  |             |\n","|    approx_kl            | 0.019906202 |\n","|    clip_fraction        | 0.186       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.23       |\n","|    explained_variance   | 0.688       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.637       |\n","|    n_updates            | 3980        |\n","|    policy_gradient_loss | -0.00901    |\n","|    std                  | 0.329       |\n","|    value_loss           | 1.36        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 399     |\n","|    time_elapsed    | 5977    |\n","|    total_timesteps | 4902912 |\n","--------------------------------\n","Eval num_timesteps=4908000, episode_reward=61.61 +/- 0.80\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 61.6        |\n","| time/                   |             |\n","|    total_timesteps      | 4908000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010661746 |\n","|    clip_fraction        | 0.141       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.2        |\n","|    explained_variance   | 0.365       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 6.43        |\n","|    n_updates            | 3990        |\n","|    policy_gradient_loss | -0.0069     |\n","|    std                  | 0.329       |\n","|    value_loss           | 10.5        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 400     |\n","|    time_elapsed    | 5993    |\n","|    total_timesteps | 4915200 |\n","--------------------------------\n","Eval num_timesteps=4920000, episode_reward=61.96 +/- 0.22\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 62          |\n","| time/                   |             |\n","|    total_timesteps      | 4920000     |\n","| train/                  |             |\n","|    approx_kl            | 0.021062905 |\n","|    clip_fraction        | 0.23        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.2        |\n","|    explained_variance   | 0.612       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.786       |\n","|    n_updates            | 4000        |\n","|    policy_gradient_loss | -0.0105     |\n","|    std                  | 0.328       |\n","|    value_loss           | 1.43        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 401     |\n","|    time_elapsed    | 6008    |\n","|    total_timesteps | 4927488 |\n","--------------------------------\n","Eval num_timesteps=4932000, episode_reward=64.64 +/- 2.53\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 64.6        |\n","| time/                   |             |\n","|    total_timesteps      | 4932000     |\n","| train/                  |             |\n","|    approx_kl            | 0.011688075 |\n","|    clip_fraction        | 0.167       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.2        |\n","|    explained_variance   | 0.777       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.652       |\n","|    n_updates            | 4010        |\n","|    policy_gradient_loss | -0.00822    |\n","|    std                  | 0.328       |\n","|    value_loss           | 4.92        |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 820     |\n","|    iterations      | 402     |\n","|    time_elapsed    | 6020    |\n","|    total_timesteps | 4939776 |\n","--------------------------------\n","Eval num_timesteps=4944000, episode_reward=53.68 +/- 3.17\n","Episode length: 500.00 +/- 0.00\n","----------------------------------------\n","| eval/                   |            |\n","|    mean_ep_length       | 500        |\n","|    mean_reward          | 53.7       |\n","| time/                   |            |\n","|    total_timesteps      | 4944000    |\n","| train/                  |            |\n","|    approx_kl            | 0.01789172 |\n","|    clip_fraction        | 0.235      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.2       |\n","|    explained_variance   | 0.593      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.459      |\n","|    n_updates            | 4020       |\n","|    policy_gradient_loss | -0.0118    |\n","|    std                  | 0.328      |\n","|    value_loss           | 1.05       |\n","----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 403     |\n","|    time_elapsed    | 6040    |\n","|    total_timesteps | 4952064 |\n","--------------------------------\n","Eval num_timesteps=4956000, episode_reward=60.48 +/- 1.97\n","Episode length: 500.00 +/- 0.00\n","------------------------------------------\n","| eval/                   |              |\n","|    mean_ep_length       | 500          |\n","|    mean_reward          | 60.5         |\n","| time/                   |              |\n","|    total_timesteps      | 4956000      |\n","| train/                  |              |\n","|    approx_kl            | 0.0102860145 |\n","|    clip_fraction        | 0.131        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -1.2         |\n","|    explained_variance   | 0.506        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 9.28         |\n","|    n_updates            | 4030         |\n","|    policy_gradient_loss | -0.0104      |\n","|    std                  | 0.329        |\n","|    value_loss           | 14.9         |\n","------------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 404     |\n","|    time_elapsed    | 6057    |\n","|    total_timesteps | 4964352 |\n","--------------------------------\n","Eval num_timesteps=4968000, episode_reward=6.69 +/- 60.47\n","Episode length: 449.60 +/- 61.73\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 450         |\n","|    mean_reward          | 6.69        |\n","| time/                   |             |\n","|    total_timesteps      | 4968000     |\n","| train/                  |             |\n","|    approx_kl            | 0.010574353 |\n","|    clip_fraction        | 0.11        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.21       |\n","|    explained_variance   | 0.22        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 6.2         |\n","|    n_updates            | 4040        |\n","|    policy_gradient_loss | -0.00751    |\n","|    std                  | 0.329       |\n","|    value_loss           | 23          |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 405     |\n","|    time_elapsed    | 6071    |\n","|    total_timesteps | 4976640 |\n","--------------------------------\n","Eval num_timesteps=4980000, episode_reward=63.84 +/- 6.25\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 63.8        |\n","| time/                   |             |\n","|    total_timesteps      | 4980000     |\n","| train/                  |             |\n","|    approx_kl            | 0.013653538 |\n","|    clip_fraction        | 0.129       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.2        |\n","|    explained_variance   | 0.619       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.22        |\n","|    n_updates            | 4050        |\n","|    policy_gradient_loss | -0.00918    |\n","|    std                  | 0.327       |\n","|    value_loss           | 3.2         |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 406     |\n","|    time_elapsed    | 6084    |\n","|    total_timesteps | 4988928 |\n","--------------------------------\n","Eval num_timesteps=4992000, episode_reward=58.81 +/- 1.89\n","Episode length: 500.00 +/- 0.00\n","-----------------------------------------\n","| eval/                   |             |\n","|    mean_ep_length       | 500         |\n","|    mean_reward          | 58.8        |\n","| time/                   |             |\n","|    total_timesteps      | 4992000     |\n","| train/                  |             |\n","|    approx_kl            | 0.017715884 |\n","|    clip_fraction        | 0.207       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.2        |\n","|    explained_variance   | 0.817       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.278       |\n","|    n_updates            | 4060        |\n","|    policy_gradient_loss | -0.0108     |\n","|    std                  | 0.329       |\n","|    value_loss           | 0.956       |\n","-----------------------------------------\n","--------------------------------\n","| time/              |         |\n","|    fps             | 819     |\n","|    iterations      | 407     |\n","|    time_elapsed    | 6100    |\n","|    total_timesteps | 5001216 |\n","--------------------------------\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyMQhMX1Se9YKrDtZysCwoWe"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}