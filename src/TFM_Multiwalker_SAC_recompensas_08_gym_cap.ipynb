{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"d8JAmEUyj9De"},"outputs":[],"source":["# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n","mount='/content/gdrive'\n","drive_root = mount + \"/My Drive/TFM\"\n","\n","try:\n","  from google.colab import drive\n","  IN_COLAB=True\n","except:\n","  IN_COLAB=False"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12787,"status":"ok","timestamp":1698933872357,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-60},"id":"DrEo9QnxkAne","outputId":"1a49e2d4-b132-4317-897e-5cac36625359"},"outputs":[{"output_type":"stream","name":"stdout","text":["We're running Colab\n","Colab: mounting Google drive on  /content/gdrive\n","Mounted at /content/gdrive\n","\n","Colab: making sure  /content/gdrive/My Drive/TFM  exists.\n","\n","Colab: Changing directory to  /content/gdrive/My Drive/TFM\n","/content/gdrive/My Drive/TFM\n","Archivos en el directorio: \n","['Entrenamientos antiguos sin logs', '.ipynb_checkpoints', 'Entrenamientos_log_no_eval', 'PPO_policies', 'DQN_new_pettingzoo_gym_cap.ipynb', 'multi_car_racing', 'policy_log_eval', 'DQN_policies', 'results_rllib', 'MCR_TFM.ipynb', 'multiwalker_ddpg_log_eval', 'multiwalker_sac_log_eval', 'multiwalker_ddpg.zip', 'multiwalker_ppo_log_eval', 'multiwalker_ppo.zip', 'multiwalker_td3_log_eval', 'multiwalker_sac2_log_eval', 'multiwalker_td3_2_log_eval', 'multiwalker_sac3_log_eval', 'multiwalker_sac3.zip', 'multiwalker_ppo_2_log_eval', 'multiwalker_ddpg2_log_eval', 'multiwalker_ppo_2.zip', 'TFM_Multiwalker_DDPG_PPO_gym_cap.ipynb', 'multiwalker_td3_3_log_eval', 'TFM_Multiwalker_TD3_gym_cap.ipynb', 'TFM_Multiwalker_SAC_gym_cap.ipynb', 'TFM_PPO_KAZ_gym_cap.ipynb', 'TFM_PPO_new_KAZ_gym_cap.ipynb', 'TFM_Multiwalker_DDPG_recompensas_gym_cap.ipynb', 'multiwalker_ddpg2_5_log_eval', 'multiwalker_ppo_rew_08_log_eval', 'multiwalker_ppo_rew_08.zip', 'multiwalker_ppo_08_2_log_eval', 'multiwalker_ddpg2_6_log_eval', 'multiwalker_ppo_08_2.zip', 'multiwalker_sac_08_log_eval', 'TFM_Multiwalker_DDPG_gym_cap.ipynb', '=2.13', 'multiwalker_sac2_08_log_eval', 'multiwalker_ppo_rew_04_log_eval', 'multiwalker_ppo_rew_04.zip', 'multiwalker_ppo_04_2_log_eval', 'multiwalker_ppo_04_2.zip', 'TFM_Multiwalker_PPO_recompensas_gym_cap.ipynb', 'TFM_Multiwalker_SAC_recompensas_gym_cap.ipynb']\n"]}],"source":["# Switch to the directory on the Google Drive that you want to use\n","import os\n","if IN_COLAB:\n","  print(\"We're running Colab\")\n","\n","  if IN_COLAB:\n","    # Mount the Google Drive at mount\n","    print(\"Colab: mounting Google drive on \", mount)\n","\n","    drive.mount(mount)\n","\n","    # Create drive_root if it doesn't exist\n","    create_drive_root = True\n","    if create_drive_root:\n","      print(\"\\nColab: making sure \", drive_root, \" exists.\")\n","      os.makedirs(drive_root, exist_ok=True)\n","\n","    # Change to the directory\n","    print(\"\\nColab: Changing directory to \", drive_root)\n","    %cd $drive_root\n","# Verify we're in the correct working directory\n","%pwd\n","print(\"Archivos en el directorio: \")\n","print(os.listdir())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xAZDg478kEbs","outputId":"a806aeae-4d57-495a-d5c4-258046281615","executionInfo":{"status":"ok","timestamp":1698934096897,"user_tz":-60,"elapsed":224543,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gym==0.17.3\n","  Downloading gym-0.17.3.tar.gz (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym==0.17.3) (1.11.3)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.17.3) (1.23.5)\n","Collecting pyglet<=1.5.0,>=1.4.0 (from gym==0.17.3)\n","  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cloudpickle<1.7.0,>=1.2.0 (from gym==0.17.3)\n","  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3) (0.18.3)\n","Building wheels for collected packages: gym\n","  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym: filename=gym-0.17.3-py3-none-any.whl size=1654617 sha256=62a26c7ae447869f86831e779943f9dc05a1a90897c6eff30f502855df412484\n","  Stored in directory: /root/.cache/pip/wheels/af/4b/74/fcfc8238472c34d7f96508a63c962ff3ac9485a9a4137afd4e\n","Successfully built gym\n","Installing collected packages: pyglet, cloudpickle, gym\n","  Attempting uninstall: cloudpickle\n","    Found existing installation: cloudpickle 2.2.1\n","    Uninstalling cloudpickle-2.2.1:\n","      Successfully uninstalled cloudpickle-2.2.1\n","  Attempting uninstall: gym\n","    Found existing installation: gym 0.25.2\n","    Uninstalling gym-0.25.2:\n","      Successfully uninstalled gym-0.25.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","bigframes 0.10.0 requires cloudpickle>=2.0.0, but you have cloudpickle 1.6.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed cloudpickle-1.6.0 gym-0.17.3 pyglet-1.5.0\n","Collecting git+https://github.com/Kojoley/atari-py.git\n","  Cloning https://github.com/Kojoley/atari-py.git to /tmp/pip-req-build-dqr6ntj8\n","  Running command git clone --filter=blob:none --quiet https://github.com/Kojoley/atari-py.git /tmp/pip-req-build-dqr6ntj8\n","  Resolved https://github.com/Kojoley/atari-py.git to commit 86a1e05c0a95e9e6233c3a413521fdb34ca8a089\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from atari-py==1.2.2) (1.23.5)\n","Building wheels for collected packages: atari-py\n","  Building wheel for atari-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for atari-py: filename=atari_py-1.2.2-cp310-cp310-linux_x86_64.whl size=4738550 sha256=ae1a3c26bbe7490f99492ebe90ca5f04e08feb337cfb64a1e18392fa92092705\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-2h1ho1l_/wheels/6c/9b/f1/8a80a63a233d6f4eb2e2ee8c7357f4875f21c3ffc7f2613f9f\n","Successfully built atari-py\n","Installing collected packages: atari-py\n","Successfully installed atari-py-1.2.2\n","Collecting keras-rl2==1.0.5\n","  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from keras-rl2==1.0.5) (2.14.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (23.5.26)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (16.0.6)\n","Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.2.0)\n","Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.23.5)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (23.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (4.5.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.34.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.59.0)\n","Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.14.1)\n","Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.14.0)\n","Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.14.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2==1.0.5) (0.41.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.2.2)\n","Installing collected packages: keras-rl2\n","Successfully installed keras-rl2-1.0.5\n","Collecting tensorflow==2.8\n","  Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl (497.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.6.3)\n","Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (23.5.26)\n","Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.9.0)\n","Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.8)\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (16.0.6)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.23.5)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.3.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (4.5.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.14.1)\n","Collecting tensorboard<2.9,>=2.8 (from tensorflow==2.8)\n","  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tf-estimator-nightly==2.8.0.dev2021122109 (from tensorflow==2.8)\n","  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting keras<2.9,>=2.8.0rc0 (from tensorflow==2.8)\n","  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.34.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.59.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.8) (0.41.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.17.3)\n","Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.31.0)\n","Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n","  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n","  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.2.2)\n","Installing collected packages: tf-estimator-nightly, tensorboard-plugin-wit, keras, tensorboard-data-server, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.14.0\n","    Uninstalling keras-2.14.0:\n","      Successfully uninstalled keras-2.14.0\n","  Attempting uninstall: tensorboard-data-server\n","    Found existing installation: tensorboard-data-server 0.7.2\n","    Uninstalling tensorboard-data-server-0.7.2:\n","      Successfully uninstalled tensorboard-data-server-0.7.2\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 1.0.0\n","    Uninstalling google-auth-oauthlib-1.0.0:\n","      Successfully uninstalled google-auth-oauthlib-1.0.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.14.1\n","    Uninstalling tensorboard-2.14.1:\n","      Successfully uninstalled tensorboard-2.14.1\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.14.0\n","    Uninstalling tensorflow-2.14.0:\n","      Successfully uninstalled tensorflow-2.14.0\n","Successfully installed google-auth-oauthlib-0.4.6 keras-2.8.0 keras-preprocessing-1.1.2 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tf-estimator-nightly-2.8.0.dev2021122109\n","Collecting gym-cap\n","  Downloading gym_cap-0.1.4.12-py3-none-any.whl (32 kB)\n","Requirement already satisfied: gym>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from gym-cap) (0.17.3)\n","Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from gym-cap) (1.23.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym>=0.16.0->gym-cap) (1.11.3)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.16.0->gym-cap) (1.5.0)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.16.0->gym-cap) (1.6.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.16.0->gym-cap) (0.18.3)\n","Installing collected packages: gym-cap\n","Successfully installed gym-cap-0.1.4.12\n","Collecting pettingzoo[butterfly]\n","  Downloading pettingzoo-1.24.1-py3-none-any.whl (840 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.8/840.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[butterfly]) (1.23.5)\n","Collecting gymnasium>=0.28.0 (from pettingzoo[butterfly])\n","  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pygame==2.3.0 (from pettingzoo[butterfly])\n","  Downloading pygame-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pymunk==6.2.0 (from pettingzoo[butterfly])\n","  Downloading pymunk-6.2.0.zip (7.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m103.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: cffi>1.14.0 in /usr/local/lib/python3.10/dist-packages (from pymunk==6.2.0->pettingzoo[butterfly]) (1.16.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[butterfly]) (1.6.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[butterfly]) (4.5.0)\n","Collecting farama-notifications>=0.0.1 (from gymnasium>=0.28.0->pettingzoo[butterfly])\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>1.14.0->pymunk==6.2.0->pettingzoo[butterfly]) (2.21)\n","Building wheels for collected packages: pymunk\n","  Building wheel for pymunk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pymunk: filename=pymunk-6.2.0-cp310-cp310-linux_x86_64.whl size=801630 sha256=7a20593983984694cbe594c5d0f9e7ad057a9b6a47b4f2795581174bb2090c4b\n","  Stored in directory: /root/.cache/pip/wheels/2e/81/a2/f941a9ff417bb4020c37ae218fb7117d12d3fc019ea493d66f\n","Successfully built pymunk\n","Installing collected packages: farama-notifications, pygame, gymnasium, pymunk, pettingzoo\n","  Attempting uninstall: pygame\n","    Found existing installation: pygame 2.5.2\n","    Uninstalling pygame-2.5.2:\n","      Successfully uninstalled pygame-2.5.2\n","Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1 pettingzoo-1.24.1 pygame-2.3.0 pymunk-6.2.0\n"]}],"source":["if IN_COLAB:\n","  %pip install gym==0.17.3\n","  %pip install git+https://github.com/Kojoley/atari-py.git\n","  %pip install keras-rl2==1.0.5\n","  %pip install tensorflow==2.8\n","  %pip install gym-cap\n","  %pip install pettingzoo[butterfly]\n","else:\n","  %pip install gym==0.17.3\n","  %pip install git+https://github.com/Kojoley/atari-py.git\n","  %pip install pyglet==1.5.0\n","  %pip install h5py==3.1.0\n","  %pip install Pillow==9.5.0\n","  %pip install keras-rl2==1.0.5\n","  %pip install Keras==2.2.4\n","  %pip install tensorflow==2.5.3\n","  %pip install torch==2.0.1\n","  %pip install agents==1.4.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G8cw-IX3laE9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698934096899,"user_tz":-60,"elapsed":36,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"5f9f0d42-ea8a-480e-8ac0-5325ea24555d"},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'multi_car_racing' already exists and is not an empty directory.\n"]}],"source":["!git clone https://github.com/igilitschenski/multi_car_racing.git\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IqbMo3gK7vBG"},"outputs":[],"source":["!cd multi_car_racing\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jekec6f98b3A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698934106444,"user_tz":-60,"elapsed":9254,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"41f24642-2176-4aed-b93a-5cb387cac571"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting swig\n","  Downloading swig-4.1.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: swig\n","Successfully installed swig-4.1.1\n"]}],"source":["!pip install swig"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KKxRPBFx85k6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698934172247,"user_tz":-60,"elapsed":65811,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"e1d15aea-cebd-4a9f-e139-a4159ecc3893"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting box2d\n","  Downloading Box2D-2.3.2.tar.gz (427 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.9/427.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: box2d\n","  Building wheel for box2d (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d: filename=Box2D-2.3.2-cp310-cp310-linux_x86_64.whl size=2391351 sha256=4d3f2adc593f6f49a4fd06f8740d4153dea5965e324792c0f1f94ddcb8806e13\n","  Stored in directory: /root/.cache/pip/wheels/eb/cb/be/e663f3ce9aba6580611c0febaf7cd3cf7603f87047de2a52f9\n","Successfully built box2d\n","Installing collected packages: box2d\n","Successfully installed box2d-2.3.2\n"]}],"source":["!pip install box2d"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ijp5V0i09MRF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698934236253,"user_tz":-60,"elapsed":64054,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"73db3815-e196-4358-b608-2b682adce2ab"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting box2d-py\n","  Downloading box2d-py-2.3.8.tar.gz (374 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/374.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/374.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m368.6/374.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.5/374.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: box2d-py\n","  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-py: filename=box2d_py-2.3.8-cp310-cp310-linux_x86_64.whl size=2373077 sha256=b92a4dc4ee0ba5e524fa4f4984d628b9bae87f6d120da763fc1fc2b1f80ce3a2\n","  Stored in directory: /root/.cache/pip/wheels/47/01/d2/6a780da77ccb98b1d2facdd520a8d10838a03b590f6f8d50c0\n","Successfully built box2d-py\n","Installing collected packages: box2d-py\n","Successfully installed box2d-py-2.3.8\n"]}],"source":["!pip install box2d-py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BwjugqI99g0I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698934269293,"user_tz":-60,"elapsed":33090,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"5b6d9fa8-165e-4110-ce4b-43e0157e0866"},"outputs":[{"output_type":"stream","name":"stdout","text":["Obtaining file:///content/gdrive/MyDrive/TFM/multi_car_racing\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: box2d-py~=2.3.5 in /usr/local/lib/python3.10/dist-packages (from gym-multi-car-racing==0.0.1) (2.3.8)\n","Collecting shapely~=1.7.0 (from gym-multi-car-racing==0.0.1)\n","  Downloading Shapely-1.7.1.tar.gz (383 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.2/383.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from gym-multi-car-racing==0.0.1) (1.23.5)\n","Requirement already satisfied: gym~=0.17.2 in /usr/local/lib/python3.10/dist-packages (from gym-multi-car-racing==0.0.1) (0.17.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym~=0.17.2->gym-multi-car-racing==0.0.1) (1.11.3)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from gym~=0.17.2->gym-multi-car-racing==0.0.1) (1.5.0)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym~=0.17.2->gym-multi-car-racing==0.0.1) (1.6.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym~=0.17.2->gym-multi-car-racing==0.0.1) (0.18.3)\n","Building wheels for collected packages: shapely\n","  Building wheel for shapely (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for shapely: filename=Shapely-1.7.1-cp310-cp310-linux_x86_64.whl size=997164 sha256=5332023125c0738f89f4e9952267e49065ea78803277c925d30960eb57f58c88\n","  Stored in directory: /root/.cache/pip/wheels/2e/fa/97/c85f587c35afcaf4a81c481741d36592518d1e50445572f0d4\n","Successfully built shapely\n","Installing collected packages: shapely, gym-multi-car-racing\n","  Attempting uninstall: shapely\n","    Found existing installation: shapely 2.0.2\n","    Uninstalling shapely-2.0.2:\n","      Successfully uninstalled shapely-2.0.2\n","  Running setup.py develop for gym-multi-car-racing\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires fastapi, which is not installed.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed gym-multi-car-racing-0.0.1 shapely-1.7.1\n"]}],"source":["!pip install -e multi_car_racing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IrAvXzCW-Z3e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698934278292,"user_tz":-60,"elapsed":9046,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"b267959e-c459-499f-9a63-2fcf2b5bf371"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  freeglut3 libegl-dev libgl-dev libgl1-mesa-dev libgles-dev libgles1\n","  libglu1-mesa libglu1-mesa-dev libglvnd-core-dev libglvnd-dev libglx-dev\n","  libice-dev libopengl-dev libsm-dev libxt-dev\n","Suggested packages:\n","  libice-doc libsm-doc libxt-doc\n","The following NEW packages will be installed:\n","  freeglut3 freeglut3-dev libegl-dev libgl-dev libgl1-mesa-dev libgles-dev\n","  libgles1 libglu1-mesa libglu1-mesa-dev libglvnd-core-dev libglvnd-dev\n","  libglx-dev libice-dev libopengl-dev libsm-dev libxt-dev\n","0 upgraded, 16 newly installed, 0 to remove and 19 not upgraded.\n","Need to get 1,261 kB of archives.\n","After this operation, 6,753 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3 amd64 2.8.1-6 [74.0 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglx-dev amd64 1.4.0-1 [14.1 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgl-dev amd64 1.4.0-1 [101 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-core-dev amd64 1.4.0-1 [12.7 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libegl-dev amd64 1.4.0-1 [18.0 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles1 amd64 1.4.0-1 [11.5 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles-dev amd64 1.4.0-1 [49.4 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libopengl-dev amd64 1.4.0-1 [3,400 B]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-dev amd64 1.4.0-1 [3,162 B]\n","Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgl1-mesa-dev amd64 23.0.4-0ubuntu1~22.04.1 [6,510 B]\n","Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n","Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa-dev amd64 9.0.2-1 [231 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 libice-dev amd64 2:1.0.10-1build2 [51.4 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsm-dev amd64 2:1.2.3-1build2 [18.1 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3-dev amd64 2.8.1-6 [126 kB]\n","Fetched 1,261 kB in 0s (3,547 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 16.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package freeglut3:amd64.\n","(Reading database ... 120874 files and directories currently installed.)\n","Preparing to unpack .../00-freeglut3_2.8.1-6_amd64.deb ...\n","Unpacking freeglut3:amd64 (2.8.1-6) ...\n","Selecting previously unselected package libglx-dev:amd64.\n","Preparing to unpack .../01-libglx-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglx-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgl-dev:amd64.\n","Preparing to unpack .../02-libgl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libgl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libglvnd-core-dev:amd64.\n","Preparing to unpack .../03-libglvnd-core-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglvnd-core-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libegl-dev:amd64.\n","Preparing to unpack .../04-libegl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libegl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgles1:amd64.\n","Preparing to unpack .../05-libgles1_1.4.0-1_amd64.deb ...\n","Unpacking libgles1:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgles-dev:amd64.\n","Preparing to unpack .../06-libgles-dev_1.4.0-1_amd64.deb ...\n","Unpacking libgles-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libopengl-dev:amd64.\n","Preparing to unpack .../07-libopengl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libopengl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libglvnd-dev:amd64.\n","Preparing to unpack .../08-libglvnd-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglvnd-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgl1-mesa-dev:amd64.\n","Preparing to unpack .../09-libgl1-mesa-dev_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n","Unpacking libgl1-mesa-dev:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Selecting previously unselected package libglu1-mesa:amd64.\n","Preparing to unpack .../10-libglu1-mesa_9.0.2-1_amd64.deb ...\n","Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n","Selecting previously unselected package libglu1-mesa-dev:amd64.\n","Preparing to unpack .../11-libglu1-mesa-dev_9.0.2-1_amd64.deb ...\n","Unpacking libglu1-mesa-dev:amd64 (9.0.2-1) ...\n","Selecting previously unselected package libice-dev:amd64.\n","Preparing to unpack .../12-libice-dev_2%3a1.0.10-1build2_amd64.deb ...\n","Unpacking libice-dev:amd64 (2:1.0.10-1build2) ...\n","Selecting previously unselected package libsm-dev:amd64.\n","Preparing to unpack .../13-libsm-dev_2%3a1.2.3-1build2_amd64.deb ...\n","Unpacking libsm-dev:amd64 (2:1.2.3-1build2) ...\n","Selecting previously unselected package libxt-dev:amd64.\n","Preparing to unpack .../14-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n","Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n","Selecting previously unselected package freeglut3-dev:amd64.\n","Preparing to unpack .../15-freeglut3-dev_2.8.1-6_amd64.deb ...\n","Unpacking freeglut3-dev:amd64 (2.8.1-6) ...\n","Setting up freeglut3:amd64 (2.8.1-6) ...\n","Setting up libglvnd-core-dev:amd64 (1.4.0-1) ...\n","Setting up libice-dev:amd64 (2:1.0.10-1build2) ...\n","Setting up libsm-dev:amd64 (2:1.2.3-1build2) ...\n","Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n","Setting up libgles1:amd64 (1.4.0-1) ...\n","Setting up libglx-dev:amd64 (1.4.0-1) ...\n","Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n","Setting up libopengl-dev:amd64 (1.4.0-1) ...\n","Setting up libgl-dev:amd64 (1.4.0-1) ...\n","Setting up libegl-dev:amd64 (1.4.0-1) ...\n","Setting up libglu1-mesa-dev:amd64 (9.0.2-1) ...\n","Setting up libgles-dev:amd64 (1.4.0-1) ...\n","Setting up libglvnd-dev:amd64 (1.4.0-1) ...\n","Setting up libgl1-mesa-dev:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Setting up freeglut3-dev:amd64 (2.8.1-6) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n"]}],"source":["!sudo apt-get install freeglut3-dev"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cgGdQ6n9EERW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698934303784,"user_tz":-60,"elapsed":25500,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"13a1ceaa-e52e-4d19-f5a9-d67541328c23"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n","  xserver-common\n","The following NEW packages will be installed:\n","  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n","  xserver-common xvfb\n","0 upgraded, 9 newly installed, 0 to remove and 19 not upgraded.\n","Need to get 7,814 kB of archives.\n","After this operation, 11.9 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.2 [28.1 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.2 [864 kB]\n","Fetched 7,814 kB in 1s (15.2 MB/s)\n","Selecting previously unselected package libfontenc1:amd64.\n","(Reading database ... 121332 files and directories currently installed.)\n","Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n","Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n","Selecting previously unselected package libxfont2:amd64.\n","Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n","Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n","Selecting previously unselected package libxkbfile1:amd64.\n","Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n","Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n","Selecting previously unselected package x11-xkb-utils.\n","Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n","Unpacking x11-xkb-utils (7.7+5build4) ...\n","Selecting previously unselected package xfonts-encodings.\n","Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n","Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n","Selecting previously unselected package xfonts-utils.\n","Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n","Unpacking xfonts-utils (1:7.7+6build2) ...\n","Selecting previously unselected package xfonts-base.\n","Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n","Unpacking xfonts-base (1:1.0.5) ...\n","Selecting previously unselected package xserver-common.\n","Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.2_all.deb ...\n","Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n","Selecting previously unselected package xvfb.\n","Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.2_amd64.deb ...\n","Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.2) ...\n","Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n","Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n","Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n","Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n","Setting up x11-xkb-utils (7.7+5build4) ...\n","Setting up xfonts-utils (1:7.7+6build2) ...\n","Setting up xfonts-base (1:1.0.5) ...\n","Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n","Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.2) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","Collecting pyvirtualdisplay\n","  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n","Installing collected packages: pyvirtualdisplay\n","Successfully installed pyvirtualdisplay-3.0\n","Collecting piglet\n","  Downloading piglet-1.0.0-py2.py3-none-any.whl (2.2 kB)\n","Collecting piglet-templates (from piglet)\n","  Downloading piglet_templates-1.3.0-py3-none-any.whl (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.5/67.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from piglet-templates->piglet) (3.1.1)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from piglet-templates->piglet) (23.1.0)\n","Requirement already satisfied: astunparse in /usr/local/lib/python3.10/dist-packages (from piglet-templates->piglet) (1.6.3)\n","Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from piglet-templates->piglet) (2.1.3)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse->piglet-templates->piglet) (0.41.2)\n","Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from astunparse->piglet-templates->piglet) (1.16.0)\n","Installing collected packages: piglet-templates, piglet\n","Successfully installed piglet-1.0.0 piglet-templates-1.3.0\n"]}],"source":["!apt install xvfb -y\n","!pip install pyvirtualdisplay\n","!pip install piglet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5OaWkBSmhm6R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698934323663,"user_tz":-60,"elapsed":19889,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"04df45a9-424c-4d0a-924e-b78cee5820a5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting supersuit\n","  Downloading SuperSuit-3.9.0-py3-none-any.whl (49 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/49.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from supersuit) (1.23.5)\n","Requirement already satisfied: gymnasium>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from supersuit) (0.29.1)\n","Collecting tinyscaler>=1.2.6 (from supersuit)\n","  Downloading tinyscaler-1.2.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (517 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.1/517.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (1.6.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (0.0.4)\n","Installing collected packages: tinyscaler, supersuit\n","Successfully installed supersuit-3.9.0 tinyscaler-1.2.7\n","Collecting stable_baselines3\n","  Downloading stable_baselines3-2.1.0-py3-none-any.whl (178 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (0.29.1)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.23.5)\n","Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.1.0+cu118)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.6.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.5.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (3.7.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (0.0.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.12.4)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2.1.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.1.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (4.43.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (23.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable_baselines3) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable_baselines3) (1.3.0)\n","Installing collected packages: stable_baselines3\n","Successfully installed stable_baselines3-2.1.0\n"]}],"source":["!pip install supersuit\n","!pip install stable_baselines3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"thmOvcHdjKHw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698934329537,"user_tz":-60,"elapsed":5880,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"2bfeac6d-3f59-4698-aaf9-bce6f8fd80dc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting shimmy>=0.2.1\n","  Downloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from shimmy>=0.2.1) (1.23.5)\n","Requirement already satisfied: gymnasium>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from shimmy>=0.2.1) (0.29.1)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy>=0.2.1) (1.6.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy>=0.2.1) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy>=0.2.1) (0.0.4)\n","Installing collected packages: shimmy\n","Successfully installed shimmy-1.3.0\n"]}],"source":["!pip install 'shimmy>=0.2.1'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k0iVvep_spQz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698934357473,"user_tz":-60,"elapsed":27944,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"02e9442c-2bfa-406a-874b-bd905006e290"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tf-agents-nightly\n","  Downloading tf_agents_nightly-0.19.0.dev20231010-py3-none-any.whl (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.4.0)\n","Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.6.0)\n","Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (0.5.0)\n","Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (0.17.3)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.23.5)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (9.4.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.16.0)\n","Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (3.20.3)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.14.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (4.5.0)\n","Collecting pygame==2.1.3 (from tf-agents-nightly)\n","  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tfp-nightly (from tf-agents-nightly)\n","  Downloading tfp_nightly-0.23.0.dev20231102-py2.py3-none-any.whl (6.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents-nightly) (1.11.3)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents-nightly) (1.5.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tfp-nightly->tf-agents-nightly) (4.4.2)\n","Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tfp-nightly->tf-agents-nightly) (0.5.4)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tfp-nightly->tf-agents-nightly) (0.1.8)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<=0.23.0,>=0.17.0->tf-agents-nightly) (0.18.3)\n","Installing collected packages: tfp-nightly, pygame, tf-agents-nightly\n","  Attempting uninstall: pygame\n","    Found existing installation: pygame 2.3.0\n","    Uninstalling pygame-2.3.0:\n","      Successfully uninstalled pygame-2.3.0\n","Successfully installed pygame-2.1.3 tf-agents-nightly-0.19.0.dev20231010 tfp-nightly-0.23.0.dev20231102\n"]}],"source":["!pip install tf-agents-nightly"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UlXxViz9tdvH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698934378092,"user_tz":-60,"elapsed":20630,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"125f7913-a4c2-4746-d56f-0e8eb5c34d46"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: tensorflow 2.8.0\n","Uninstalling tensorflow-2.8.0:\n","  Would remove:\n","    /usr/local/bin/estimator_ckpt_converter\n","    /usr/local/bin/import_pb_to_tensorboard\n","    /usr/local/bin/saved_model_cli\n","    /usr/local/bin/tensorboard\n","    /usr/local/bin/tf_upgrade_v2\n","    /usr/local/bin/tflite_convert\n","    /usr/local/bin/toco\n","    /usr/local/bin/toco_from_protos\n","    /usr/local/lib/python3.10/dist-packages/tensorflow-2.8.0.dist-info/*\n","    /usr/local/lib/python3.10/dist-packages/tensorflow/*\n","Proceed (Y/n)? Y\n","  Successfully uninstalled tensorflow-2.8.0\n"]}],"source":["!pip uninstall tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dsWlVQ6MtKLj"},"outputs":[],"source":["!pip install tensorflow>=2.13"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wE5AiVtFtZDc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698934430614,"user_tz":-60,"elapsed":7022,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"ddbf4191-8aad-4ded-cc4d-cead6ae19a00"},"outputs":[{"output_type":"stream","name":"stdout","text":["Name: tensorflow\n","Version: 2.14.0\n","Summary: TensorFlow is an open source machine learning framework for everyone.\n","Home-page: https://www.tensorflow.org/\n","Author: Google Inc.\n","Author-email: packages@tensorflow.org\n","License: Apache 2.0\n","Location: /usr/local/lib/python3.10/dist-packages\n","Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, libclang, ml-dtypes, numpy, opt-einsum, packaging, protobuf, setuptools, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n","Required-by: dopamine-rl, keras-rl2\n"]}],"source":["!pip show tensorflow"]},{"cell_type":"code","source":["!pip install pettingzoo[sisl]"],"metadata":{"id":"PZa1qybXZKSX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698934497516,"user_tz":-60,"elapsed":66904,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"2c3d54d9-945a-49e7-f8c9-f0f5a58ece9d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pettingzoo[sisl] in /usr/local/lib/python3.10/dist-packages (1.24.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[sisl]) (1.23.5)\n","Requirement already satisfied: gymnasium>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[sisl]) (0.29.1)\n","Collecting pygame==2.3.0 (from pettingzoo[sisl])\n","  Using cached pygame-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\n","Requirement already satisfied: pymunk==6.2.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[sisl]) (6.2.0)\n","Collecting box2d-py==2.3.5 (from pettingzoo[sisl])\n","  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[sisl]) (1.11.3)\n","Requirement already satisfied: cffi>1.14.0 in /usr/local/lib/python3.10/dist-packages (from pymunk==6.2.0->pettingzoo[sisl]) (1.16.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[sisl]) (1.6.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[sisl]) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[sisl]) (0.0.4)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>1.14.0->pymunk==6.2.0->pettingzoo[sisl]) (2.21)\n","Building wheels for collected packages: box2d-py\n","  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2373077 sha256=a21465e38b5d1dd1032d715ddbbee0da298afea60f1e0241a1a403c42126231f\n","  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n","Successfully built box2d-py\n","Installing collected packages: box2d-py, pygame\n","  Attempting uninstall: box2d-py\n","    Found existing installation: box2d-py 2.3.8\n","    Uninstalling box2d-py-2.3.8:\n","      Successfully uninstalled box2d-py-2.3.8\n","  Attempting uninstall: pygame\n","    Found existing installation: pygame 2.1.3\n","    Uninstalling pygame-2.1.3:\n","      Successfully uninstalled pygame-2.1.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tf-agents-nightly 0.19.0.dev20231010 requires pygame==2.1.3, but you have pygame 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed box2d-py-2.3.5 pygame-2.3.0\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"NFzawQ1QZFwj"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JpA_YhKzCeC2"},"outputs":[],"source":["############################# Código para entrenar Multiwalker #############################"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tnz3fJDA33w8"},"outputs":[],"source":["# from stable_baselines3.dqn import MlpPolicy,CnnPolicy\n","from stable_baselines3 import SAC\n","from pettingzoo.sisl import multiwalker_v9\n","import supersuit as ss\n","from stable_baselines3.common.monitor import Monitor"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":218,"status":"ok","timestamp":1697644598142,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"6-kdhb3CI5VC","outputId":"4db87993-1f4b-464a-f0f0-20ed31aa0c34"},"outputs":[{"output_type":"stream","name":"stdout","text":["drive  gdrive  sample_data\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":395,"status":"ok","timestamp":1697644599460,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"04CbnRTvI9L2","outputId":"347477dc-2a63-4315-a08f-182fdf724421"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/TFM\n"]}],"source":["cd /content/drive/MyDrive/TFM"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":305,"status":"ok","timestamp":1697644601324,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"bUEH8254I_kb","outputId":"406e2b97-e693-46da-9e49-9d56c991f4e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["'=2.13'\t\t\t\t     multiwalker_sac_log_eval\n"," DQN_new_pettingzoo_gym_cap.ipynb    policy_log_eval\n"," DQN_policies\t\t\t     PPO_policies\n","'Entrenamientos antiguos sin logs'   results_rllib\n"," Entrenamientos_log_no_eval\t     TFM_Multiwalker_DDPG_gym_cap.ipynb\n"," MCR_TFM.ipynb\t\t\t     TFM_Multiwalker_SAC_gym_cap.ipynb\n"," multi_car_racing\t\t     TFM_PPO_new_pettingzoo_gym_cap.ipynb\n"," multiwalker_ddpg_log_eval\t     TFM_PPO_pettingzoo_gym_cap.ipynb\n"," multiwalker_ddpg.zip\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2HubK-2G3_vH"},"outputs":[],"source":["env = multiwalker_v9.parallel_env(render_mode=\"rgb_array\",shared_reward=0.8)\n","\n","# #---------------------------------\n","# log_dir = \"./policy_log\"\n","# os.makedirs(log_dir, exist_ok=True)\n","# env = Monitor(env, log_dir)\n","# #---------------------------------\n","# env = ss.color_reduction_v0(env, mode='B')\n","# env = ss.black_death_v3(env)\n","# env = ss.resize_v1(env, x_size=84, y_size=84)\n","env = ss.frame_stack_v1(env, 3)\n","env = ss.pettingzoo_env_to_vec_env_v1(env)\n","env = ss.concat_vec_envs_v1(env, 8, num_cpus=4, base_class='stable_baselines3')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"foI4bTFGbQo1"},"outputs":[],"source":["from stable_baselines3.common.callbacks import EvalCallback\n","eval_callback = EvalCallback(env, best_model_save_path=\"./multiwalker_sac_08_log_eval/\",\n","                             log_path=\"./multiwalker_sac_08_log_eval/\", eval_freq=100,\n","                             deterministic=False, render=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ib9VPIVgec47","outputId":"758d055e-4f9b-4bb8-af45-59eb767598e7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Logging to multiwalker_sac_08_log_eval/\n","Using cuda device\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 10       |\n","|    fps             | 268      |\n","|    time_elapsed    | 5        |\n","|    total_timesteps | 1488     |\n","| train/             |          |\n","|    actor_loss      | -5.31    |\n","|    critic_loss     | 0.196    |\n","|    ent_coef        | 0.983    |\n","|    ent_coef_loss   | -0.113   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 57       |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 20       |\n","|    fps             | 307      |\n","|    time_elapsed    | 6        |\n","|    total_timesteps | 1896     |\n","| train/             |          |\n","|    actor_loss      | -4.74    |\n","|    critic_loss     | 214      |\n","|    ent_coef        | 0.978    |\n","|    ent_coef_loss   | -0.148   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 74       |\n","---------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n","Eval num_timesteps=3352800, episode_reward=2.98 +/- 0.02\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.98     |\n","| time/              |          |\n","|    total_timesteps | 3352800  |\n","| train/             |          |\n","|    actor_loss      | -0.747   |\n","|    critic_loss     | 0.0246   |\n","|    ent_coef        | 0.00135  |\n","|    ent_coef_loss   | -2.19    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 139695   |\n","---------------------------------\n","Eval num_timesteps=3355200, episode_reward=3.79 +/- 1.90\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.79     |\n","| time/              |          |\n","|    total_timesteps | 3355200  |\n","| train/             |          |\n","|    actor_loss      | -0.782   |\n","|    critic_loss     | 0.0169   |\n","|    ent_coef        | 0.00132  |\n","|    ent_coef_loss   | 0.364    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 139795   |\n","---------------------------------\n","Eval num_timesteps=3357600, episode_reward=1.37 +/- 1.99\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.37     |\n","| time/              |          |\n","|    total_timesteps | 3357600  |\n","| train/             |          |\n","|    actor_loss      | -0.77    |\n","|    critic_loss     | 0.0362   |\n","|    ent_coef        | 0.00132  |\n","|    ent_coef_loss   | 0.149    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 139895   |\n","---------------------------------\n","Eval num_timesteps=3360000, episode_reward=1.52 +/- 0.95\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.52     |\n","| time/              |          |\n","|    total_timesteps | 3360000  |\n","| train/             |          |\n","|    actor_loss      | -0.749   |\n","|    critic_loss     | 0.061    |\n","|    ent_coef        | 0.00129  |\n","|    ent_coef_loss   | -5.33    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 139995   |\n","---------------------------------\n","Eval num_timesteps=3362400, episode_reward=-0.26 +/- 3.27\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.264   |\n","| time/              |          |\n","|    total_timesteps | 3362400  |\n","| train/             |          |\n","|    actor_loss      | -0.795   |\n","|    critic_loss     | 0.0248   |\n","|    ent_coef        | 0.00125  |\n","|    ent_coef_loss   | -4.45    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140095   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3190     |\n","|    fps             | 299      |\n","|    time_elapsed    | 11241    |\n","|    total_timesteps | 3364728  |\n","| train/             |          |\n","|    actor_loss      | -0.609   |\n","|    critic_loss     | 0.26     |\n","|    ent_coef        | 0.00123  |\n","|    ent_coef_loss   | 1.79     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140192   |\n","---------------------------------\n","Eval num_timesteps=3364800, episode_reward=2.80 +/- 0.46\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.8      |\n","| time/              |          |\n","|    total_timesteps | 3364800  |\n","| train/             |          |\n","|    actor_loss      | -0.526   |\n","|    critic_loss     | 0.0675   |\n","|    ent_coef        | 0.00123  |\n","|    ent_coef_loss   | 1.67     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140195   |\n","---------------------------------\n","Eval num_timesteps=3367200, episode_reward=2.66 +/- 1.60\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.66     |\n","| time/              |          |\n","|    total_timesteps | 3367200  |\n","| train/             |          |\n","|    actor_loss      | -0.807   |\n","|    critic_loss     | 0.0173   |\n","|    ent_coef        | 0.00123  |\n","|    ent_coef_loss   | 0.386    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140295   |\n","---------------------------------\n","Eval num_timesteps=3369600, episode_reward=2.54 +/- 0.11\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.54     |\n","| time/              |          |\n","|    total_timesteps | 3369600  |\n","| train/             |          |\n","|    actor_loss      | -0.664   |\n","|    critic_loss     | 0.0445   |\n","|    ent_coef        | 0.0012   |\n","|    ent_coef_loss   | -2.57    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140395   |\n","---------------------------------\n","Eval num_timesteps=3372000, episode_reward=1.82 +/- 1.24\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.82     |\n","| time/              |          |\n","|    total_timesteps | 3372000  |\n","| train/             |          |\n","|    actor_loss      | -0.793   |\n","|    critic_loss     | 0.0256   |\n","|    ent_coef        | 0.00126  |\n","|    ent_coef_loss   | 3.4      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140495   |\n","---------------------------------\n","Eval num_timesteps=3374400, episode_reward=0.98 +/- 0.41\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.985    |\n","| time/              |          |\n","|    total_timesteps | 3374400  |\n","| train/             |          |\n","|    actor_loss      | -0.608   |\n","|    critic_loss     | 0.0455   |\n","|    ent_coef        | 0.00124  |\n","|    ent_coef_loss   | -8.35    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140595   |\n","---------------------------------\n","Eval num_timesteps=3376800, episode_reward=2.98 +/- 0.46\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.98     |\n","| time/              |          |\n","|    total_timesteps | 3376800  |\n","| train/             |          |\n","|    actor_loss      | -0.838   |\n","|    critic_loss     | 0.0178   |\n","|    ent_coef        | 0.00119  |\n","|    ent_coef_loss   | -6.46    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140695   |\n","---------------------------------\n","Eval num_timesteps=3379200, episode_reward=1.10 +/- 0.25\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.1      |\n","| time/              |          |\n","|    total_timesteps | 3379200  |\n","| train/             |          |\n","|    actor_loss      | -0.74    |\n","|    critic_loss     | 0.0219   |\n","|    ent_coef        | 0.00116  |\n","|    ent_coef_loss   | -4.51    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140795   |\n","---------------------------------\n","Eval num_timesteps=3381600, episode_reward=4.83 +/- 0.69\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.83     |\n","| time/              |          |\n","|    total_timesteps | 3381600  |\n","| train/             |          |\n","|    actor_loss      | -0.599   |\n","|    critic_loss     | 0.0401   |\n","|    ent_coef        | 0.00114  |\n","|    ent_coef_loss   | 0.446    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140895   |\n","---------------------------------\n","Eval num_timesteps=3384000, episode_reward=2.21 +/- 0.57\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.21     |\n","| time/              |          |\n","|    total_timesteps | 3384000  |\n","| train/             |          |\n","|    actor_loss      | -0.649   |\n","|    critic_loss     | 0.0315   |\n","|    ent_coef        | 0.00114  |\n","|    ent_coef_loss   | 0.596    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 140995   |\n","---------------------------------\n","Eval num_timesteps=3386400, episode_reward=2.41 +/- 0.93\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.41     |\n","| time/              |          |\n","|    total_timesteps | 3386400  |\n","| train/             |          |\n","|    actor_loss      | -0.846   |\n","|    critic_loss     | 0.0222   |\n","|    ent_coef        | 0.00114  |\n","|    ent_coef_loss   | 15       |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141095   |\n","---------------------------------\n","Eval num_timesteps=3388800, episode_reward=3.35 +/- 0.02\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.35     |\n","| time/              |          |\n","|    total_timesteps | 3388800  |\n","| train/             |          |\n","|    actor_loss      | -0.651   |\n","|    critic_loss     | 0.044    |\n","|    ent_coef        | 0.00116  |\n","|    ent_coef_loss   | -2.38    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141195   |\n","---------------------------------\n","Eval num_timesteps=3391200, episode_reward=2.40 +/- 0.54\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.4      |\n","| time/              |          |\n","|    total_timesteps | 3391200  |\n","| train/             |          |\n","|    actor_loss      | -0.785   |\n","|    critic_loss     | 0.03     |\n","|    ent_coef        | 0.00117  |\n","|    ent_coef_loss   | -0.06    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141295   |\n","---------------------------------\n","Eval num_timesteps=3393600, episode_reward=3.98 +/- 1.93\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.98     |\n","| time/              |          |\n","|    total_timesteps | 3393600  |\n","| train/             |          |\n","|    actor_loss      | -0.51    |\n","|    critic_loss     | 0.0882   |\n","|    ent_coef        | 0.00117  |\n","|    ent_coef_loss   | -0.914   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141395   |\n","---------------------------------\n","Eval num_timesteps=3396000, episode_reward=-55.29 +/- 43.29\n","Episode length: 323.60 +/- 144.03\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 324      |\n","|    mean_reward     | -55.3    |\n","| time/              |          |\n","|    total_timesteps | 3396000  |\n","| train/             |          |\n","|    actor_loss      | -0.967   |\n","|    critic_loss     | 0.0336   |\n","|    ent_coef        | 0.00116  |\n","|    ent_coef_loss   | 6.63     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141495   |\n","---------------------------------\n","Eval num_timesteps=3398400, episode_reward=1.03 +/- 1.51\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.03     |\n","| time/              |          |\n","|    total_timesteps | 3398400  |\n","| train/             |          |\n","|    actor_loss      | -0.822   |\n","|    critic_loss     | 0.0259   |\n","|    ent_coef        | 0.00117  |\n","|    ent_coef_loss   | -4.42    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141595   |\n","---------------------------------\n","Eval num_timesteps=3400800, episode_reward=3.62 +/- 0.01\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.62     |\n","| time/              |          |\n","|    total_timesteps | 3400800  |\n","| train/             |          |\n","|    actor_loss      | -0.703   |\n","|    critic_loss     | 0.0343   |\n","|    ent_coef        | 0.00118  |\n","|    ent_coef_loss   | -2.68    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141695   |\n","---------------------------------\n","Eval num_timesteps=3403200, episode_reward=4.66 +/- 0.76\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.66     |\n","| time/              |          |\n","|    total_timesteps | 3403200  |\n","| train/             |          |\n","|    actor_loss      | -0.844   |\n","|    critic_loss     | 0.0115   |\n","|    ent_coef        | 0.00115  |\n","|    ent_coef_loss   | -3.21    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141795   |\n","---------------------------------\n","Eval num_timesteps=3405600, episode_reward=3.75 +/- 0.39\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.75     |\n","| time/              |          |\n","|    total_timesteps | 3405600  |\n","| train/             |          |\n","|    actor_loss      | -0.835   |\n","|    critic_loss     | 0.0356   |\n","|    ent_coef        | 0.00118  |\n","|    ent_coef_loss   | -2.06    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141895   |\n","---------------------------------\n","Eval num_timesteps=3408000, episode_reward=2.15 +/- 2.45\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.15     |\n","| time/              |          |\n","|    total_timesteps | 3408000  |\n","| train/             |          |\n","|    actor_loss      | -0.85    |\n","|    critic_loss     | 0.0172   |\n","|    ent_coef        | 0.00115  |\n","|    ent_coef_loss   | 0.439    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 141995   |\n","---------------------------------\n","Eval num_timesteps=3410400, episode_reward=4.53 +/- 1.24\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.53     |\n","| time/              |          |\n","|    total_timesteps | 3410400  |\n","| train/             |          |\n","|    actor_loss      | -0.357   |\n","|    critic_loss     | 0.307    |\n","|    ent_coef        | 0.00116  |\n","|    ent_coef_loss   | 5.88     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 142095   |\n","---------------------------------\n","Eval num_timesteps=3412800, episode_reward=2.05 +/- 0.84\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.05     |\n","| time/              |          |\n","|    total_timesteps | 3412800  |\n","| train/             |          |\n","|    actor_loss      | -0.785   |\n","|    critic_loss     | 0.0259   |\n","|    ent_coef        | 0.00119  |\n","|    ent_coef_loss   | 9.79     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 142195   |\n","---------------------------------\n","Eval num_timesteps=3415200, episode_reward=1.79 +/- 0.63\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.79     |\n","| time/              |          |\n","|    total_timesteps | 3415200  |\n","| train/             |          |\n","|    actor_loss      | -0.752   |\n","|    critic_loss     | 0.108    |\n","|    ent_coef        | 0.00123  |\n","|    ent_coef_loss   | -4.04    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 142295   |\n","---------------------------------\n","Eval num_timesteps=3417600, episode_reward=1.95 +/- 0.42\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.95     |\n","| time/              |          |\n","|    total_timesteps | 3417600  |\n","| train/             |          |\n","|    actor_loss      | -0.842   |\n","|    critic_loss     | 0.0311   |\n","|    ent_coef        | 0.00125  |\n","|    ent_coef_loss   | 6.92     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 142395   |\n","---------------------------------\n","Eval num_timesteps=3420000, episode_reward=3.46 +/- 1.16\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.46     |\n","| time/              |          |\n","|    total_timesteps | 3420000  |\n","| train/             |          |\n","|    actor_loss      | -0.842   |\n","|    critic_loss     | 0.0308   |\n","|    ent_coef        | 0.00127  |\n","|    ent_coef_loss   | 0.0802   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 142495   |\n","---------------------------------\n","Eval num_timesteps=3422400, episode_reward=2.00 +/- 0.43\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2        |\n","| time/              |          |\n","|    total_timesteps | 3422400  |\n","| train/             |          |\n","|    actor_loss      | -0.793   |\n","|    critic_loss     | 0.0334   |\n","|    ent_coef        | 0.00124  |\n","|    ent_coef_loss   | -2.99    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 142595   |\n","---------------------------------\n","Eval num_timesteps=3424800, episode_reward=3.70 +/- 2.24\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.7      |\n","| time/              |          |\n","|    total_timesteps | 3424800  |\n","| train/             |          |\n","|    actor_loss      | -0.747   |\n","|    critic_loss     | 0.0219   |\n","|    ent_coef        | 0.00124  |\n","|    ent_coef_loss   | -1.19    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 142695   |\n","---------------------------------\n","Eval num_timesteps=3427200, episode_reward=1.36 +/- 1.29\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.36     |\n","| time/              |          |\n","|    total_timesteps | 3427200  |\n","| train/             |          |\n","|    actor_loss      | -0.804   |\n","|    critic_loss     | 0.17     |\n","|    ent_coef        | 0.00124  |\n","|    ent_coef_loss   | 1.37     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 142795   |\n","---------------------------------\n","Eval num_timesteps=3429600, episode_reward=3.92 +/- 0.74\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.92     |\n","| time/              |          |\n","|    total_timesteps | 3429600  |\n","| train/             |          |\n","|    actor_loss      | -0.539   |\n","|    critic_loss     | 0.379    |\n","|    ent_coef        | 0.00125  |\n","|    ent_coef_loss   | -1.71    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 142895   |\n","---------------------------------\n","Eval num_timesteps=3432000, episode_reward=3.06 +/- 0.55\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.06     |\n","| time/              |          |\n","|    total_timesteps | 3432000  |\n","| train/             |          |\n","|    actor_loss      | -0.213   |\n","|    critic_loss     | 0.196    |\n","|    ent_coef        | 0.00125  |\n","|    ent_coef_loss   | -0.865   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 142995   |\n","---------------------------------\n","Eval num_timesteps=3434400, episode_reward=3.49 +/- 0.39\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.49     |\n","| time/              |          |\n","|    total_timesteps | 3434400  |\n","| train/             |          |\n","|    actor_loss      | -0.774   |\n","|    critic_loss     | 0.265    |\n","|    ent_coef        | 0.00129  |\n","|    ent_coef_loss   | 3.09     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 143095   |\n","---------------------------------\n","Eval num_timesteps=3436800, episode_reward=-2.43 +/- 0.25\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -2.43    |\n","| time/              |          |\n","|    total_timesteps | 3436800  |\n","| train/             |          |\n","|    actor_loss      | -0.989   |\n","|    critic_loss     | 0.0231   |\n","|    ent_coef        | 0.00131  |\n","|    ent_coef_loss   | 0.422    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 143195   |\n","---------------------------------\n","Eval num_timesteps=3439200, episode_reward=2.55 +/- 0.84\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.55     |\n","| time/              |          |\n","|    total_timesteps | 3439200  |\n","| train/             |          |\n","|    actor_loss      | -0.795   |\n","|    critic_loss     | 0.0283   |\n","|    ent_coef        | 0.00133  |\n","|    ent_coef_loss   | -1.57    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 143295   |\n","---------------------------------\n","Eval num_timesteps=3441600, episode_reward=3.91 +/- 0.16\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.91     |\n","| time/              |          |\n","|    total_timesteps | 3441600  |\n","| train/             |          |\n","|    actor_loss      | -0.712   |\n","|    critic_loss     | 29.7     |\n","|    ent_coef        | 0.00131  |\n","|    ent_coef_loss   | -2.06    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 143395   |\n","---------------------------------\n","Eval num_timesteps=3444000, episode_reward=-0.13 +/- 0.74\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.125   |\n","| time/              |          |\n","|    total_timesteps | 3444000  |\n","| train/             |          |\n","|    actor_loss      | -0.708   |\n","|    critic_loss     | 0.0287   |\n","|    ent_coef        | 0.00129  |\n","|    ent_coef_loss   | -1.25    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 143495   |\n","---------------------------------\n","Eval num_timesteps=3446400, episode_reward=0.85 +/- 2.38\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.846    |\n","| time/              |          |\n","|    total_timesteps | 3446400  |\n","| train/             |          |\n","|    actor_loss      | -0.787   |\n","|    critic_loss     | 0.0302   |\n","|    ent_coef        | 0.00129  |\n","|    ent_coef_loss   | -1.31    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 143595   |\n","---------------------------------\n","Eval num_timesteps=3448800, episode_reward=1.36 +/- 0.28\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.36     |\n","| time/              |          |\n","|    total_timesteps | 3448800  |\n","| train/             |          |\n","|    actor_loss      | -0.666   |\n","|    critic_loss     | 0.0316   |\n","|    ent_coef        | 0.00127  |\n","|    ent_coef_loss   | 2.65     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 143695   |\n","---------------------------------\n","Eval num_timesteps=3451200, episode_reward=-0.24 +/- 1.75\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.239   |\n","| time/              |          |\n","|    total_timesteps | 3451200  |\n","| train/             |          |\n","|    actor_loss      | -0.818   |\n","|    critic_loss     | 0.0412   |\n","|    ent_coef        | 0.00131  |\n","|    ent_coef_loss   | 13       |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 143795   |\n","---------------------------------\n","Eval num_timesteps=3453600, episode_reward=0.76 +/- 0.83\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.764    |\n","| time/              |          |\n","|    total_timesteps | 3453600  |\n","| train/             |          |\n","|    actor_loss      | -0.812   |\n","|    critic_loss     | 0.0319   |\n","|    ent_coef        | 0.00135  |\n","|    ent_coef_loss   | 9.72     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 143895   |\n","---------------------------------\n","Eval num_timesteps=3456000, episode_reward=1.39 +/- 0.34\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.39     |\n","| time/              |          |\n","|    total_timesteps | 3456000  |\n","| train/             |          |\n","|    actor_loss      | -0.833   |\n","|    critic_loss     | 0.0247   |\n","|    ent_coef        | 0.00133  |\n","|    ent_coef_loss   | -6.2     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 143995   |\n","---------------------------------\n","Eval num_timesteps=3458400, episode_reward=0.32 +/- 0.45\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.32     |\n","| time/              |          |\n","|    total_timesteps | 3458400  |\n","| train/             |          |\n","|    actor_loss      | -0.888   |\n","|    critic_loss     | 0.0525   |\n","|    ent_coef        | 0.00131  |\n","|    ent_coef_loss   | -0.3     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 144095   |\n","---------------------------------\n","Eval num_timesteps=3460800, episode_reward=0.47 +/- 1.35\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.472    |\n","| time/              |          |\n","|    total_timesteps | 3460800  |\n","| train/             |          |\n","|    actor_loss      | -0.859   |\n","|    critic_loss     | 0.123    |\n","|    ent_coef        | 0.00133  |\n","|    ent_coef_loss   | 0.168    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 144195   |\n","---------------------------------\n","Eval num_timesteps=3463200, episode_reward=3.96 +/- 0.81\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.96     |\n","| time/              |          |\n","|    total_timesteps | 3463200  |\n","| train/             |          |\n","|    actor_loss      | -0.826   |\n","|    critic_loss     | 0.0207   |\n","|    ent_coef        | 0.00133  |\n","|    ent_coef_loss   | -3.2     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 144295   |\n","---------------------------------\n","Eval num_timesteps=3465600, episode_reward=3.34 +/- 0.17\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.34     |\n","| time/              |          |\n","|    total_timesteps | 3465600  |\n","| train/             |          |\n","|    actor_loss      | -0.884   |\n","|    critic_loss     | 0.033    |\n","|    ent_coef        | 0.0013   |\n","|    ent_coef_loss   | -3.2     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 144395   |\n","---------------------------------\n","Eval num_timesteps=3468000, episode_reward=-0.60 +/- 2.70\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.6     |\n","| time/              |          |\n","|    total_timesteps | 3468000  |\n","| train/             |          |\n","|    actor_loss      | -0.789   |\n","|    critic_loss     | 0.0273   |\n","|    ent_coef        | 0.00129  |\n","|    ent_coef_loss   | 5.65     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 144495   |\n","---------------------------------\n","Eval num_timesteps=3470400, episode_reward=3.08 +/- 0.42\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.08     |\n","| time/              |          |\n","|    total_timesteps | 3470400  |\n","| train/             |          |\n","|    actor_loss      | -0.865   |\n","|    critic_loss     | 0.0401   |\n","|    ent_coef        | 0.00128  |\n","|    ent_coef_loss   | -0.262   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 144595   |\n","---------------------------------\n","Eval num_timesteps=3472800, episode_reward=3.24 +/- 0.29\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.24     |\n","| time/              |          |\n","|    total_timesteps | 3472800  |\n","| train/             |          |\n","|    actor_loss      | -0.771   |\n","|    critic_loss     | 0.0148   |\n","|    ent_coef        | 0.00128  |\n","|    ent_coef_loss   | -0.816   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 144695   |\n","---------------------------------\n","Eval num_timesteps=3475200, episode_reward=4.21 +/- 1.30\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.21     |\n","| time/              |          |\n","|    total_timesteps | 3475200  |\n","| train/             |          |\n","|    actor_loss      | -0.836   |\n","|    critic_loss     | 0.0245   |\n","|    ent_coef        | 0.00128  |\n","|    ent_coef_loss   | 4.53     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 144795   |\n","---------------------------------\n","Eval num_timesteps=3477600, episode_reward=4.28 +/- 1.06\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.28     |\n","| time/              |          |\n","|    total_timesteps | 3477600  |\n","| train/             |          |\n","|    actor_loss      | -0.846   |\n","|    critic_loss     | 0.0219   |\n","|    ent_coef        | 0.00132  |\n","|    ent_coef_loss   | 2.21     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 144895   |\n","---------------------------------\n","Eval num_timesteps=3480000, episode_reward=3.32 +/- 0.48\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.32     |\n","| time/              |          |\n","|    total_timesteps | 3480000  |\n","| train/             |          |\n","|    actor_loss      | -0.762   |\n","|    critic_loss     | 0.0174   |\n","|    ent_coef        | 0.00135  |\n","|    ent_coef_loss   | 2.5      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 144995   |\n","---------------------------------\n","Eval num_timesteps=3482400, episode_reward=2.95 +/- 0.21\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.95     |\n","| time/              |          |\n","|    total_timesteps | 3482400  |\n","| train/             |          |\n","|    actor_loss      | -0.885   |\n","|    critic_loss     | 0.0212   |\n","|    ent_coef        | 0.00137  |\n","|    ent_coef_loss   | 5.64     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 145095   |\n","---------------------------------\n","Eval num_timesteps=3484800, episode_reward=3.37 +/- 0.81\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.37     |\n","| time/              |          |\n","|    total_timesteps | 3484800  |\n","| train/             |          |\n","|    actor_loss      | -0.881   |\n","|    critic_loss     | 0.0476   |\n","|    ent_coef        | 0.00141  |\n","|    ent_coef_loss   | -3.62    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 145195   |\n","---------------------------------\n","Eval num_timesteps=3487200, episode_reward=2.33 +/- 0.06\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.33     |\n","| time/              |          |\n","|    total_timesteps | 3487200  |\n","| train/             |          |\n","|    actor_loss      | -0.792   |\n","|    critic_loss     | 0.0379   |\n","|    ent_coef        | 0.0014   |\n","|    ent_coef_loss   | -6.29    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 145295   |\n","---------------------------------\n","Eval num_timesteps=3489600, episode_reward=4.13 +/- 0.89\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.13     |\n","| time/              |          |\n","|    total_timesteps | 3489600  |\n","| train/             |          |\n","|    actor_loss      | -0.808   |\n","|    critic_loss     | 0.0192   |\n","|    ent_coef        | 0.0014   |\n","|    ent_coef_loss   | -0.575   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 145395   |\n","---------------------------------\n","Eval num_timesteps=3492000, episode_reward=0.24 +/- 1.52\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.242    |\n","| time/              |          |\n","|    total_timesteps | 3492000  |\n","| train/             |          |\n","|    actor_loss      | -0.829   |\n","|    critic_loss     | 0.0298   |\n","|    ent_coef        | 0.00138  |\n","|    ent_coef_loss   | -5.81    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 145495   |\n","---------------------------------\n","Eval num_timesteps=3494400, episode_reward=-52.89 +/- 46.04\n","Episode length: 258.80 +/- 196.94\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 259      |\n","|    mean_reward     | -52.9    |\n","| time/              |          |\n","|    total_timesteps | 3494400  |\n","| train/             |          |\n","|    actor_loss      | -0.938   |\n","|    critic_loss     | 0.0467   |\n","|    ent_coef        | 0.00135  |\n","|    ent_coef_loss   | 15.2     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 145595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3200     |\n","|    fps             | 298      |\n","|    time_elapsed    | 11706    |\n","|    total_timesteps | 3496536  |\n","| train/             |          |\n","|    actor_loss      | -0.773   |\n","|    critic_loss     | 0.0635   |\n","|    ent_coef        | 0.0014   |\n","|    ent_coef_loss   | -8.25    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 145684   |\n","---------------------------------\n","Eval num_timesteps=3496800, episode_reward=1.46 +/- 1.83\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.46     |\n","| time/              |          |\n","|    total_timesteps | 3496800  |\n","| train/             |          |\n","|    actor_loss      | -0.859   |\n","|    critic_loss     | 0.019    |\n","|    ent_coef        | 0.00139  |\n","|    ent_coef_loss   | -5.84    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 145695   |\n","---------------------------------\n","Eval num_timesteps=3499200, episode_reward=-1.85 +/- 3.95\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.85    |\n","| time/              |          |\n","|    total_timesteps | 3499200  |\n","| train/             |          |\n","|    actor_loss      | -0.712   |\n","|    critic_loss     | 0.0356   |\n","|    ent_coef        | 0.00136  |\n","|    ent_coef_loss   | 4.27     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 145795   |\n","---------------------------------\n","Eval num_timesteps=3501600, episode_reward=2.02 +/- 0.28\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.02     |\n","| time/              |          |\n","|    total_timesteps | 3501600  |\n","| train/             |          |\n","|    actor_loss      | -0.832   |\n","|    critic_loss     | 0.0251   |\n","|    ent_coef        | 0.0014   |\n","|    ent_coef_loss   | -0.341   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 145895   |\n","---------------------------------\n","Eval num_timesteps=3504000, episode_reward=5.61 +/- 5.73\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.61     |\n","| time/              |          |\n","|    total_timesteps | 3504000  |\n","| train/             |          |\n","|    actor_loss      | -0.859   |\n","|    critic_loss     | 0.0362   |\n","|    ent_coef        | 0.00143  |\n","|    ent_coef_loss   | -3.82    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 145995   |\n","---------------------------------\n","Eval num_timesteps=3506400, episode_reward=-0.94 +/- 2.13\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.942   |\n","| time/              |          |\n","|    total_timesteps | 3506400  |\n","| train/             |          |\n","|    actor_loss      | -0.756   |\n","|    critic_loss     | 0.0437   |\n","|    ent_coef        | 0.0014   |\n","|    ent_coef_loss   | 6.84     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 146095   |\n","---------------------------------\n","Eval num_timesteps=3508800, episode_reward=3.12 +/- 1.09\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.12     |\n","| time/              |          |\n","|    total_timesteps | 3508800  |\n","| train/             |          |\n","|    actor_loss      | -0.869   |\n","|    critic_loss     | 0.0165   |\n","|    ent_coef        | 0.00141  |\n","|    ent_coef_loss   | 0.0235   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 146195   |\n","---------------------------------\n","Eval num_timesteps=3511200, episode_reward=5.96 +/- 5.53\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.96     |\n","| time/              |          |\n","|    total_timesteps | 3511200  |\n","| train/             |          |\n","|    actor_loss      | -0.624   |\n","|    critic_loss     | 0.0657   |\n","|    ent_coef        | 0.00138  |\n","|    ent_coef_loss   | -1.63    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 146295   |\n","---------------------------------\n","Eval num_timesteps=3513600, episode_reward=-31.41 +/- 41.58\n","Episode length: 447.60 +/- 64.18\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 448      |\n","|    mean_reward     | -31.4    |\n","| time/              |          |\n","|    total_timesteps | 3513600  |\n","| train/             |          |\n","|    actor_loss      | -0.841   |\n","|    critic_loss     | 0.0658   |\n","|    ent_coef        | 0.00134  |\n","|    ent_coef_loss   | -6.54    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 146395   |\n","---------------------------------\n","Eval num_timesteps=3516000, episode_reward=2.02 +/- 0.13\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.02     |\n","| time/              |          |\n","|    total_timesteps | 3516000  |\n","| train/             |          |\n","|    actor_loss      | -0.719   |\n","|    critic_loss     | 0.0979   |\n","|    ent_coef        | 0.00132  |\n","|    ent_coef_loss   | -1.78    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 146495   |\n","---------------------------------\n","Eval num_timesteps=3518400, episode_reward=3.41 +/- 0.37\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.41     |\n","| time/              |          |\n","|    total_timesteps | 3518400  |\n","| train/             |          |\n","|    actor_loss      | -0.825   |\n","|    critic_loss     | 0.0671   |\n","|    ent_coef        | 0.00132  |\n","|    ent_coef_loss   | -3.05    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 146595   |\n","---------------------------------\n","Eval num_timesteps=3520800, episode_reward=2.21 +/- 0.19\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.21     |\n","| time/              |          |\n","|    total_timesteps | 3520800  |\n","| train/             |          |\n","|    actor_loss      | -0.666   |\n","|    critic_loss     | 0.0385   |\n","|    ent_coef        | 0.00134  |\n","|    ent_coef_loss   | -3.44    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 146695   |\n","---------------------------------\n","Eval num_timesteps=3523200, episode_reward=1.53 +/- 1.93\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.53     |\n","| time/              |          |\n","|    total_timesteps | 3523200  |\n","| train/             |          |\n","|    actor_loss      | -0.545   |\n","|    critic_loss     | 0.224    |\n","|    ent_coef        | 0.00134  |\n","|    ent_coef_loss   | -1.93    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 146795   |\n","---------------------------------\n","Eval num_timesteps=3525600, episode_reward=0.95 +/- 0.57\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.948    |\n","| time/              |          |\n","|    total_timesteps | 3525600  |\n","| train/             |          |\n","|    actor_loss      | -0.775   |\n","|    critic_loss     | 0.0204   |\n","|    ent_coef        | 0.00134  |\n","|    ent_coef_loss   | 9.7      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 146895   |\n","---------------------------------\n","Eval num_timesteps=3528000, episode_reward=4.41 +/- 0.94\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.41     |\n","| time/              |          |\n","|    total_timesteps | 3528000  |\n","| train/             |          |\n","|    actor_loss      | -0.827   |\n","|    critic_loss     | 0.0206   |\n","|    ent_coef        | 0.00139  |\n","|    ent_coef_loss   | 7.74     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 146995   |\n","---------------------------------\n","Eval num_timesteps=3530400, episode_reward=0.52 +/- 0.05\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.523    |\n","| time/              |          |\n","|    total_timesteps | 3530400  |\n","| train/             |          |\n","|    actor_loss      | -0.883   |\n","|    critic_loss     | 0.0282   |\n","|    ent_coef        | 0.00143  |\n","|    ent_coef_loss   | 0.422    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 147095   |\n","---------------------------------\n","Eval num_timesteps=3532800, episode_reward=3.40 +/- 0.11\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.4      |\n","| time/              |          |\n","|    total_timesteps | 3532800  |\n","| train/             |          |\n","|    actor_loss      | -0.878   |\n","|    critic_loss     | 0.0139   |\n","|    ent_coef        | 0.00147  |\n","|    ent_coef_loss   | 1.4      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 147195   |\n","---------------------------------\n","Eval num_timesteps=3535200, episode_reward=3.98 +/- 0.83\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.98     |\n","| time/              |          |\n","|    total_timesteps | 3535200  |\n","| train/             |          |\n","|    actor_loss      | -0.81    |\n","|    critic_loss     | 0.0234   |\n","|    ent_coef        | 0.00148  |\n","|    ent_coef_loss   | -3.66    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 147295   |\n","---------------------------------\n","Eval num_timesteps=3537600, episode_reward=3.80 +/- 0.53\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.8      |\n","| time/              |          |\n","|    total_timesteps | 3537600  |\n","| train/             |          |\n","|    actor_loss      | -0.76    |\n","|    critic_loss     | 0.0347   |\n","|    ent_coef        | 0.00146  |\n","|    ent_coef_loss   | -2.75    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 147395   |\n","---------------------------------\n","Eval num_timesteps=3540000, episode_reward=-0.87 +/- 0.39\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.87    |\n","| time/              |          |\n","|    total_timesteps | 3540000  |\n","| train/             |          |\n","|    actor_loss      | -0.746   |\n","|    critic_loss     | 0.0627   |\n","|    ent_coef        | 0.00148  |\n","|    ent_coef_loss   | 1.97     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 147495   |\n","---------------------------------\n","Eval num_timesteps=3542400, episode_reward=3.63 +/- 0.84\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.63     |\n","| time/              |          |\n","|    total_timesteps | 3542400  |\n","| train/             |          |\n","|    actor_loss      | -0.902   |\n","|    critic_loss     | 0.0131   |\n","|    ent_coef        | 0.00144  |\n","|    ent_coef_loss   | -3.95    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 147595   |\n","---------------------------------\n","Eval num_timesteps=3544800, episode_reward=3.11 +/- 1.06\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.11     |\n","| time/              |          |\n","|    total_timesteps | 3544800  |\n","| train/             |          |\n","|    actor_loss      | -0.818   |\n","|    critic_loss     | 0.0239   |\n","|    ent_coef        | 0.00141  |\n","|    ent_coef_loss   | -3.02    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 147695   |\n","---------------------------------\n","Eval num_timesteps=3547200, episode_reward=5.02 +/- 0.78\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.02     |\n","| time/              |          |\n","|    total_timesteps | 3547200  |\n","| train/             |          |\n","|    actor_loss      | -0.776   |\n","|    critic_loss     | 0.0174   |\n","|    ent_coef        | 0.00141  |\n","|    ent_coef_loss   | -3.64    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 147795   |\n","---------------------------------\n","Eval num_timesteps=3549600, episode_reward=0.66 +/- 1.85\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.662    |\n","| time/              |          |\n","|    total_timesteps | 3549600  |\n","| train/             |          |\n","|    actor_loss      | -0.986   |\n","|    critic_loss     | 0.0529   |\n","|    ent_coef        | 0.00144  |\n","|    ent_coef_loss   | 3.72     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 147895   |\n","---------------------------------\n","Eval num_timesteps=3552000, episode_reward=3.70 +/- 0.67\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.7      |\n","| time/              |          |\n","|    total_timesteps | 3552000  |\n","| train/             |          |\n","|    actor_loss      | -0.841   |\n","|    critic_loss     | 0.0166   |\n","|    ent_coef        | 0.00149  |\n","|    ent_coef_loss   | -0.432   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 147995   |\n","---------------------------------\n","Eval num_timesteps=3554400, episode_reward=5.62 +/- 0.00\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.62     |\n","| time/              |          |\n","|    total_timesteps | 3554400  |\n","| train/             |          |\n","|    actor_loss      | -0.898   |\n","|    critic_loss     | 0.0316   |\n","|    ent_coef        | 0.00152  |\n","|    ent_coef_loss   | 3.2      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148095   |\n","---------------------------------\n","Eval num_timesteps=3556800, episode_reward=3.45 +/- 0.67\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.45     |\n","| time/              |          |\n","|    total_timesteps | 3556800  |\n","| train/             |          |\n","|    actor_loss      | -0.848   |\n","|    critic_loss     | 0.0192   |\n","|    ent_coef        | 0.00154  |\n","|    ent_coef_loss   | 4.39     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148195   |\n","---------------------------------\n","Eval num_timesteps=3559200, episode_reward=3.93 +/- 0.08\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.93     |\n","| time/              |          |\n","|    total_timesteps | 3559200  |\n","| train/             |          |\n","|    actor_loss      | -0.939   |\n","|    critic_loss     | 0.0208   |\n","|    ent_coef        | 0.00149  |\n","|    ent_coef_loss   | -5.71    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148295   |\n","---------------------------------\n","Eval num_timesteps=3561600, episode_reward=2.52 +/- 0.35\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.52     |\n","| time/              |          |\n","|    total_timesteps | 3561600  |\n","| train/             |          |\n","|    actor_loss      | -0.611   |\n","|    critic_loss     | 0.0448   |\n","|    ent_coef        | 0.00149  |\n","|    ent_coef_loss   | 1.8      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148395   |\n","---------------------------------\n","Eval num_timesteps=3564000, episode_reward=2.91 +/- 2.04\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.91     |\n","| time/              |          |\n","|    total_timesteps | 3564000  |\n","| train/             |          |\n","|    actor_loss      | -0.578   |\n","|    critic_loss     | 0.0679   |\n","|    ent_coef        | 0.00149  |\n","|    ent_coef_loss   | -1.99    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148495   |\n","---------------------------------\n","Eval num_timesteps=3566400, episode_reward=2.07 +/- 0.97\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.07     |\n","| time/              |          |\n","|    total_timesteps | 3566400  |\n","| train/             |          |\n","|    actor_loss      | -0.84    |\n","|    critic_loss     | 0.0254   |\n","|    ent_coef        | 0.00149  |\n","|    ent_coef_loss   | 3.54     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148595   |\n","---------------------------------\n","Eval num_timesteps=3568800, episode_reward=-1.01 +/- 0.96\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.01    |\n","| time/              |          |\n","|    total_timesteps | 3568800  |\n","| train/             |          |\n","|    actor_loss      | -0.775   |\n","|    critic_loss     | 0.0744   |\n","|    ent_coef        | 0.00147  |\n","|    ent_coef_loss   | 1.14     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148695   |\n","---------------------------------\n","Eval num_timesteps=3571200, episode_reward=5.93 +/- 1.27\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.93     |\n","| time/              |          |\n","|    total_timesteps | 3571200  |\n","| train/             |          |\n","|    actor_loss      | -0.809   |\n","|    critic_loss     | 0.0203   |\n","|    ent_coef        | 0.00152  |\n","|    ent_coef_loss   | -7.69    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148795   |\n","---------------------------------\n","Eval num_timesteps=3573600, episode_reward=1.73 +/- 0.04\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.73     |\n","| time/              |          |\n","|    total_timesteps | 3573600  |\n","| train/             |          |\n","|    actor_loss      | -0.828   |\n","|    critic_loss     | 2.96     |\n","|    ent_coef        | 0.00154  |\n","|    ent_coef_loss   | 5.37     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148895   |\n","---------------------------------\n","Eval num_timesteps=3576000, episode_reward=3.59 +/- 0.58\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.59     |\n","| time/              |          |\n","|    total_timesteps | 3576000  |\n","| train/             |          |\n","|    actor_loss      | -0.766   |\n","|    critic_loss     | 0.036    |\n","|    ent_coef        | 0.0016   |\n","|    ent_coef_loss   | -1.82    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148995   |\n","---------------------------------\n","Eval num_timesteps=3578400, episode_reward=2.39 +/- 1.83\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.39     |\n","| time/              |          |\n","|    total_timesteps | 3578400  |\n","| train/             |          |\n","|    actor_loss      | -0.811   |\n","|    critic_loss     | 0.0188   |\n","|    ent_coef        | 0.0016   |\n","|    ent_coef_loss   | -7.09    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149095   |\n","---------------------------------\n","Eval num_timesteps=3580800, episode_reward=3.72 +/- 0.06\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.72     |\n","| time/              |          |\n","|    total_timesteps | 3580800  |\n","| train/             |          |\n","|    actor_loss      | -0.844   |\n","|    critic_loss     | 0.0225   |\n","|    ent_coef        | 0.00158  |\n","|    ent_coef_loss   | -1.99    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149195   |\n","---------------------------------\n","Eval num_timesteps=3583200, episode_reward=4.01 +/- 1.12\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.01     |\n","| time/              |          |\n","|    total_timesteps | 3583200  |\n","| train/             |          |\n","|    actor_loss      | -0.879   |\n","|    critic_loss     | 0.0247   |\n","|    ent_coef        | 0.00156  |\n","|    ent_coef_loss   | 0.494    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149295   |\n","---------------------------------\n","Eval num_timesteps=3585600, episode_reward=2.75 +/- 1.08\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.75     |\n","| time/              |          |\n","|    total_timesteps | 3585600  |\n","| train/             |          |\n","|    actor_loss      | -0.79    |\n","|    critic_loss     | 0.0364   |\n","|    ent_coef        | 0.00156  |\n","|    ent_coef_loss   | 2.79     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149395   |\n","---------------------------------\n","Eval num_timesteps=3588000, episode_reward=0.89 +/- 1.31\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.892    |\n","| time/              |          |\n","|    total_timesteps | 3588000  |\n","| train/             |          |\n","|    actor_loss      | -0.683   |\n","|    critic_loss     | 0.11     |\n","|    ent_coef        | 0.00159  |\n","|    ent_coef_loss   | -3.07    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149495   |\n","---------------------------------\n","Eval num_timesteps=3590400, episode_reward=5.17 +/- 0.03\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.17     |\n","| time/              |          |\n","|    total_timesteps | 3590400  |\n","| train/             |          |\n","|    actor_loss      | -0.833   |\n","|    critic_loss     | 0.0234   |\n","|    ent_coef        | 0.00155  |\n","|    ent_coef_loss   | -4.24    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149595   |\n","---------------------------------\n","Eval num_timesteps=3592800, episode_reward=1.49 +/- 1.53\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.49     |\n","| time/              |          |\n","|    total_timesteps | 3592800  |\n","| train/             |          |\n","|    actor_loss      | -1.13    |\n","|    critic_loss     | 0.043    |\n","|    ent_coef        | 0.00152  |\n","|    ent_coef_loss   | -7.12    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149695   |\n","---------------------------------\n","Eval num_timesteps=3595200, episode_reward=3.16 +/- 0.44\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.16     |\n","| time/              |          |\n","|    total_timesteps | 3595200  |\n","| train/             |          |\n","|    actor_loss      | -1.05    |\n","|    critic_loss     | 0.0491   |\n","|    ent_coef        | 0.0015   |\n","|    ent_coef_loss   | -1.63    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149795   |\n","---------------------------------\n","Eval num_timesteps=3597600, episode_reward=2.95 +/- 0.02\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.95     |\n","| time/              |          |\n","|    total_timesteps | 3597600  |\n","| train/             |          |\n","|    actor_loss      | -0.868   |\n","|    critic_loss     | 0.0242   |\n","|    ent_coef        | 0.00156  |\n","|    ent_coef_loss   | 2.36     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149895   |\n","---------------------------------\n","Eval num_timesteps=3600000, episode_reward=3.48 +/- 0.44\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.48     |\n","| time/              |          |\n","|    total_timesteps | 3600000  |\n","| train/             |          |\n","|    actor_loss      | -0.918   |\n","|    critic_loss     | 0.0163   |\n","|    ent_coef        | 0.00154  |\n","|    ent_coef_loss   | -4.42    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149995   |\n","---------------------------------\n","Eval num_timesteps=3602400, episode_reward=3.83 +/- 0.88\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.83     |\n","| time/              |          |\n","|    total_timesteps | 3602400  |\n","| train/             |          |\n","|    actor_loss      | -0.818   |\n","|    critic_loss     | 0.0242   |\n","|    ent_coef        | 0.0015   |\n","|    ent_coef_loss   | -6.53    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150095   |\n","---------------------------------\n","Eval num_timesteps=3604800, episode_reward=2.37 +/- 0.32\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.37     |\n","| time/              |          |\n","|    total_timesteps | 3604800  |\n","| train/             |          |\n","|    actor_loss      | -0.807   |\n","|    critic_loss     | 27       |\n","|    ent_coef        | 0.00147  |\n","|    ent_coef_loss   | -4.05    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150195   |\n","---------------------------------\n","Eval num_timesteps=3607200, episode_reward=5.69 +/- 0.22\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.69     |\n","| time/              |          |\n","|    total_timesteps | 3607200  |\n","| train/             |          |\n","|    actor_loss      | -0.965   |\n","|    critic_loss     | 0.0171   |\n","|    ent_coef        | 0.00145  |\n","|    ent_coef_loss   | -5.81    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150295   |\n","---------------------------------\n","Eval num_timesteps=3609600, episode_reward=2.34 +/- 3.20\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.34     |\n","| time/              |          |\n","|    total_timesteps | 3609600  |\n","| train/             |          |\n","|    actor_loss      | -0.869   |\n","|    critic_loss     | 0.0303   |\n","|    ent_coef        | 0.00142  |\n","|    ent_coef_loss   | -1.86    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150395   |\n","---------------------------------\n","Eval num_timesteps=3612000, episode_reward=4.33 +/- 0.30\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.33     |\n","| time/              |          |\n","|    total_timesteps | 3612000  |\n","| train/             |          |\n","|    actor_loss      | -0.81    |\n","|    critic_loss     | 0.0204   |\n","|    ent_coef        | 0.00142  |\n","|    ent_coef_loss   | 0.597    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150495   |\n","---------------------------------\n","Eval num_timesteps=3614400, episode_reward=4.46 +/- 0.34\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.46     |\n","| time/              |          |\n","|    total_timesteps | 3614400  |\n","| train/             |          |\n","|    actor_loss      | -0.837   |\n","|    critic_loss     | 0.0174   |\n","|    ent_coef        | 0.00145  |\n","|    ent_coef_loss   | 1.08     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150595   |\n","---------------------------------\n","Eval num_timesteps=3616800, episode_reward=5.21 +/- 0.32\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.21     |\n","| time/              |          |\n","|    total_timesteps | 3616800  |\n","| train/             |          |\n","|    actor_loss      | -0.855   |\n","|    critic_loss     | 0.0155   |\n","|    ent_coef        | 0.00143  |\n","|    ent_coef_loss   | 2.67     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150695   |\n","---------------------------------\n","Eval num_timesteps=3619200, episode_reward=3.65 +/- 0.52\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.65     |\n","| time/              |          |\n","|    total_timesteps | 3619200  |\n","| train/             |          |\n","|    actor_loss      | -0.829   |\n","|    critic_loss     | 0.0161   |\n","|    ent_coef        | 0.00145  |\n","|    ent_coef_loss   | 0.0808   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150795   |\n","---------------------------------\n","Eval num_timesteps=3621600, episode_reward=3.69 +/- 0.86\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.69     |\n","| time/              |          |\n","|    total_timesteps | 3621600  |\n","| train/             |          |\n","|    actor_loss      | -0.862   |\n","|    critic_loss     | 0.0278   |\n","|    ent_coef        | 0.00144  |\n","|    ent_coef_loss   | -3.53    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150895   |\n","---------------------------------\n","Eval num_timesteps=3624000, episode_reward=4.25 +/- 0.64\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.25     |\n","| time/              |          |\n","|    total_timesteps | 3624000  |\n","| train/             |          |\n","|    actor_loss      | -0.86    |\n","|    critic_loss     | 0.0287   |\n","|    ent_coef        | 0.00141  |\n","|    ent_coef_loss   | -4.29    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150995   |\n","---------------------------------\n","Eval num_timesteps=3626400, episode_reward=5.51 +/- 0.08\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.51     |\n","| time/              |          |\n","|    total_timesteps | 3626400  |\n","| train/             |          |\n","|    actor_loss      | -0.887   |\n","|    critic_loss     | 0.0175   |\n","|    ent_coef        | 0.00139  |\n","|    ent_coef_loss   | 0.509    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151095   |\n","---------------------------------\n","Eval num_timesteps=3628800, episode_reward=2.93 +/- 0.33\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.93     |\n","| time/              |          |\n","|    total_timesteps | 3628800  |\n","| train/             |          |\n","|    actor_loss      | -0.558   |\n","|    critic_loss     | 28.2     |\n","|    ent_coef        | 0.00138  |\n","|    ent_coef_loss   | -0.171   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151195   |\n","---------------------------------\n","Eval num_timesteps=3631200, episode_reward=3.95 +/- 0.70\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.95     |\n","| time/              |          |\n","|    total_timesteps | 3631200  |\n","| train/             |          |\n","|    actor_loss      | -0.905   |\n","|    critic_loss     | 0.0217   |\n","|    ent_coef        | 0.00137  |\n","|    ent_coef_loss   | 0.122    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151295   |\n","---------------------------------\n","Eval num_timesteps=3633600, episode_reward=4.06 +/- 0.24\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.06     |\n","| time/              |          |\n","|    total_timesteps | 3633600  |\n","| train/             |          |\n","|    actor_loss      | -0.83    |\n","|    critic_loss     | 0.0119   |\n","|    ent_coef        | 0.00138  |\n","|    ent_coef_loss   | -5.89    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151395   |\n","---------------------------------\n","Eval num_timesteps=3636000, episode_reward=2.63 +/- 0.44\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.63     |\n","| time/              |          |\n","|    total_timesteps | 3636000  |\n","| train/             |          |\n","|    actor_loss      | -0.881   |\n","|    critic_loss     | 0.0222   |\n","|    ent_coef        | 0.00137  |\n","|    ent_coef_loss   | -2.41    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151495   |\n","---------------------------------\n","Eval num_timesteps=3638400, episode_reward=4.46 +/- 0.09\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.46     |\n","| time/              |          |\n","|    total_timesteps | 3638400  |\n","| train/             |          |\n","|    actor_loss      | -0.8     |\n","|    critic_loss     | 0.0191   |\n","|    ent_coef        | 0.00134  |\n","|    ent_coef_loss   | -1.58    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151595   |\n","---------------------------------\n","Eval num_timesteps=3640800, episode_reward=3.56 +/- 0.37\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.56     |\n","| time/              |          |\n","|    total_timesteps | 3640800  |\n","| train/             |          |\n","|    actor_loss      | -0.816   |\n","|    critic_loss     | 0.0214   |\n","|    ent_coef        | 0.00133  |\n","|    ent_coef_loss   | -4.77    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151695   |\n","---------------------------------\n","Eval num_timesteps=3643200, episode_reward=2.21 +/- 0.10\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.21     |\n","| time/              |          |\n","|    total_timesteps | 3643200  |\n","| train/             |          |\n","|    actor_loss      | -0.815   |\n","|    critic_loss     | 0.0727   |\n","|    ent_coef        | 0.00131  |\n","|    ent_coef_loss   | 8.32     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151795   |\n","---------------------------------\n","Eval num_timesteps=3645600, episode_reward=4.22 +/- 0.44\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.22     |\n","| time/              |          |\n","|    total_timesteps | 3645600  |\n","| train/             |          |\n","|    actor_loss      | -0.907   |\n","|    critic_loss     | 0.0158   |\n","|    ent_coef        | 0.00137  |\n","|    ent_coef_loss   | 1.43     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151895   |\n","---------------------------------\n","Eval num_timesteps=3648000, episode_reward=3.04 +/- 0.86\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.04     |\n","| time/              |          |\n","|    total_timesteps | 3648000  |\n","| train/             |          |\n","|    actor_loss      | -0.871   |\n","|    critic_loss     | 0.0309   |\n","|    ent_coef        | 0.00135  |\n","|    ent_coef_loss   | -1.99    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151995   |\n","---------------------------------\n","Eval num_timesteps=3650400, episode_reward=2.95 +/- 0.53\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.95     |\n","| time/              |          |\n","|    total_timesteps | 3650400  |\n","| train/             |          |\n","|    actor_loss      | -0.857   |\n","|    critic_loss     | 0.0314   |\n","|    ent_coef        | 0.00135  |\n","|    ent_coef_loss   | -0.54    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152095   |\n","---------------------------------\n","Eval num_timesteps=3652800, episode_reward=1.83 +/- 2.53\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.83     |\n","| time/              |          |\n","|    total_timesteps | 3652800  |\n","| train/             |          |\n","|    actor_loss      | -0.839   |\n","|    critic_loss     | 0.017    |\n","|    ent_coef        | 0.00134  |\n","|    ent_coef_loss   | -7.06    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152195   |\n","---------------------------------\n","Eval num_timesteps=3655200, episode_reward=1.61 +/- 0.71\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.61     |\n","| time/              |          |\n","|    total_timesteps | 3655200  |\n","| train/             |          |\n","|    actor_loss      | -0.89    |\n","|    critic_loss     | 0.0563   |\n","|    ent_coef        | 0.00134  |\n","|    ent_coef_loss   | -3.45    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152295   |\n","---------------------------------\n","Eval num_timesteps=3657600, episode_reward=1.99 +/- 0.07\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.99     |\n","| time/              |          |\n","|    total_timesteps | 3657600  |\n","| train/             |          |\n","|    actor_loss      | -0.83    |\n","|    critic_loss     | 0.0206   |\n","|    ent_coef        | 0.00137  |\n","|    ent_coef_loss   | 8.55     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152395   |\n","---------------------------------\n","Eval num_timesteps=3660000, episode_reward=1.94 +/- 0.99\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.94     |\n","| time/              |          |\n","|    total_timesteps | 3660000  |\n","| train/             |          |\n","|    actor_loss      | -0.908   |\n","|    critic_loss     | 0.018    |\n","|    ent_coef        | 0.00137  |\n","|    ent_coef_loss   | -2       |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152495   |\n","---------------------------------\n","Eval num_timesteps=3662400, episode_reward=3.19 +/- 0.25\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.19     |\n","| time/              |          |\n","|    total_timesteps | 3662400  |\n","| train/             |          |\n","|    actor_loss      | -0.924   |\n","|    critic_loss     | 0.0142   |\n","|    ent_coef        | 0.00134  |\n","|    ent_coef_loss   | -2.27    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152595   |\n","---------------------------------\n","Eval num_timesteps=3664800, episode_reward=-2.34 +/- 0.27\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -2.34    |\n","| time/              |          |\n","|    total_timesteps | 3664800  |\n","| train/             |          |\n","|    actor_loss      | -0.835   |\n","|    critic_loss     | 0.0157   |\n","|    ent_coef        | 0.00133  |\n","|    ent_coef_loss   | -2.28    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152695   |\n","---------------------------------\n","Eval num_timesteps=3667200, episode_reward=4.05 +/- 0.36\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.05     |\n","| time/              |          |\n","|    total_timesteps | 3667200  |\n","| train/             |          |\n","|    actor_loss      | -0.779   |\n","|    critic_loss     | 0.0841   |\n","|    ent_coef        | 0.00133  |\n","|    ent_coef_loss   | -3.85    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152795   |\n","---------------------------------\n","Eval num_timesteps=3669600, episode_reward=1.68 +/- 0.33\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.68     |\n","| time/              |          |\n","|    total_timesteps | 3669600  |\n","| train/             |          |\n","|    actor_loss      | -0.932   |\n","|    critic_loss     | 0.0167   |\n","|    ent_coef        | 0.00134  |\n","|    ent_coef_loss   | 2.39     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152895   |\n","---------------------------------\n","Eval num_timesteps=3672000, episode_reward=1.44 +/- 1.24\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.44     |\n","| time/              |          |\n","|    total_timesteps | 3672000  |\n","| train/             |          |\n","|    actor_loss      | -0.875   |\n","|    critic_loss     | 1.04     |\n","|    ent_coef        | 0.00133  |\n","|    ent_coef_loss   | -0.204   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152995   |\n","---------------------------------\n","Eval num_timesteps=3674400, episode_reward=2.18 +/- 0.89\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.18     |\n","| time/              |          |\n","|    total_timesteps | 3674400  |\n","| train/             |          |\n","|    actor_loss      | -0.569   |\n","|    critic_loss     | 0.72     |\n","|    ent_coef        | 0.00133  |\n","|    ent_coef_loss   | 2.88     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153095   |\n","---------------------------------\n","Eval num_timesteps=3676800, episode_reward=1.68 +/- 2.01\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.68     |\n","| time/              |          |\n","|    total_timesteps | 3676800  |\n","| train/             |          |\n","|    actor_loss      | -0.822   |\n","|    critic_loss     | 0.0602   |\n","|    ent_coef        | 0.00137  |\n","|    ent_coef_loss   | 2.16     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153195   |\n","---------------------------------\n","Eval num_timesteps=3679200, episode_reward=2.12 +/- 0.63\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.12     |\n","| time/              |          |\n","|    total_timesteps | 3679200  |\n","| train/             |          |\n","|    actor_loss      | -0.929   |\n","|    critic_loss     | 0.0142   |\n","|    ent_coef        | 0.00138  |\n","|    ent_coef_loss   | 3.5      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153295   |\n","---------------------------------\n","Eval num_timesteps=3681600, episode_reward=2.42 +/- 1.12\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.42     |\n","| time/              |          |\n","|    total_timesteps | 3681600  |\n","| train/             |          |\n","|    actor_loss      | -0.962   |\n","|    critic_loss     | 0.0137   |\n","|    ent_coef        | 0.0014   |\n","|    ent_coef_loss   | 0.373    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153395   |\n","---------------------------------\n","Eval num_timesteps=3684000, episode_reward=2.81 +/- 0.28\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.81     |\n","| time/              |          |\n","|    total_timesteps | 3684000  |\n","| train/             |          |\n","|    actor_loss      | -0.994   |\n","|    critic_loss     | 0.0317   |\n","|    ent_coef        | 0.00141  |\n","|    ent_coef_loss   | -2.18    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153495   |\n","---------------------------------\n","Eval num_timesteps=3686400, episode_reward=2.60 +/- 1.02\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.6      |\n","| time/              |          |\n","|    total_timesteps | 3686400  |\n","| train/             |          |\n","|    actor_loss      | -0.867   |\n","|    critic_loss     | 0.0154   |\n","|    ent_coef        | 0.00139  |\n","|    ent_coef_loss   | -1.32    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153595   |\n","---------------------------------\n","Eval num_timesteps=3688800, episode_reward=1.79 +/- 1.01\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.79     |\n","| time/              |          |\n","|    total_timesteps | 3688800  |\n","| train/             |          |\n","|    actor_loss      | -0.763   |\n","|    critic_loss     | 0.0488   |\n","|    ent_coef        | 0.00138  |\n","|    ent_coef_loss   | 4.73     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153695   |\n","---------------------------------\n","Eval num_timesteps=3691200, episode_reward=4.06 +/- 1.57\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.06     |\n","| time/              |          |\n","|    total_timesteps | 3691200  |\n","| train/             |          |\n","|    actor_loss      | -1       |\n","|    critic_loss     | 0.033    |\n","|    ent_coef        | 0.00145  |\n","|    ent_coef_loss   | -2.57    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153795   |\n","---------------------------------\n","Eval num_timesteps=3693600, episode_reward=2.10 +/- 0.58\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.1      |\n","| time/              |          |\n","|    total_timesteps | 3693600  |\n","| train/             |          |\n","|    actor_loss      | -0.917   |\n","|    critic_loss     | 0.0197   |\n","|    ent_coef        | 0.0015   |\n","|    ent_coef_loss   | 5.18     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153895   |\n","---------------------------------\n","Eval num_timesteps=3696000, episode_reward=-1.35 +/- 5.36\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.35    |\n","| time/              |          |\n","|    total_timesteps | 3696000  |\n","| train/             |          |\n","|    actor_loss      | -0.876   |\n","|    critic_loss     | 0.0457   |\n","|    ent_coef        | 0.00147  |\n","|    ent_coef_loss   | -5.08    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153995   |\n","---------------------------------\n","Eval num_timesteps=3698400, episode_reward=4.46 +/- 0.31\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.46     |\n","| time/              |          |\n","|    total_timesteps | 3698400  |\n","| train/             |          |\n","|    actor_loss      | -1.03    |\n","|    critic_loss     | 0.0358   |\n","|    ent_coef        | 0.00144  |\n","|    ent_coef_loss   | -3.94    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154095   |\n","---------------------------------\n","Eval num_timesteps=3700800, episode_reward=2.08 +/- 0.01\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.08     |\n","| time/              |          |\n","|    total_timesteps | 3700800  |\n","| train/             |          |\n","|    actor_loss      | -0.85    |\n","|    critic_loss     | 0.0259   |\n","|    ent_coef        | 0.00146  |\n","|    ent_coef_loss   | 6.51     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154195   |\n","---------------------------------\n","Eval num_timesteps=3703200, episode_reward=1.57 +/- 0.75\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.57     |\n","| time/              |          |\n","|    total_timesteps | 3703200  |\n","| train/             |          |\n","|    actor_loss      | -1.01    |\n","|    critic_loss     | 0.0336   |\n","|    ent_coef        | 0.00151  |\n","|    ent_coef_loss   | -4.91    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154295   |\n","---------------------------------\n","Eval num_timesteps=3705600, episode_reward=-90.16 +/- 2.31\n","Episode length: 386.60 +/- 24.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 387      |\n","|    mean_reward     | -90.2    |\n","| time/              |          |\n","|    total_timesteps | 3705600  |\n","| train/             |          |\n","|    actor_loss      | -1.03    |\n","|    critic_loss     | 0.0477   |\n","|    ent_coef        | 0.00153  |\n","|    ent_coef_loss   | 4.15     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3210     |\n","|    fps             | 297      |\n","|    time_elapsed    | 12446    |\n","|    total_timesteps | 3707616  |\n","| train/             |          |\n","|    actor_loss      | -0.871   |\n","|    critic_loss     | 0.0302   |\n","|    ent_coef        | 0.00155  |\n","|    ent_coef_loss   | 2.81     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154479   |\n","---------------------------------\n","Eval num_timesteps=3708000, episode_reward=3.13 +/- 0.47\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.13     |\n","| time/              |          |\n","|    total_timesteps | 3708000  |\n","| train/             |          |\n","|    actor_loss      | -0.943   |\n","|    critic_loss     | 0.0143   |\n","|    ent_coef        | 0.00155  |\n","|    ent_coef_loss   | 1.24     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154495   |\n","---------------------------------\n","Eval num_timesteps=3710400, episode_reward=2.70 +/- 0.05\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.7      |\n","| time/              |          |\n","|    total_timesteps | 3710400  |\n","| train/             |          |\n","|    actor_loss      | -0.891   |\n","|    critic_loss     | 0.0396   |\n","|    ent_coef        | 0.00155  |\n","|    ent_coef_loss   | -1.14    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154595   |\n","---------------------------------\n","Eval num_timesteps=3712800, episode_reward=3.58 +/- 0.03\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.58     |\n","| time/              |          |\n","|    total_timesteps | 3712800  |\n","| train/             |          |\n","|    actor_loss      | -0.888   |\n","|    critic_loss     | 0.0319   |\n","|    ent_coef        | 0.00152  |\n","|    ent_coef_loss   | -2.59    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154695   |\n","---------------------------------\n","Eval num_timesteps=3715200, episode_reward=-93.13 +/- 4.40\n","Episode length: 351.40 +/- 20.09\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 351      |\n","|    mean_reward     | -93.1    |\n","| time/              |          |\n","|    total_timesteps | 3715200  |\n","| train/             |          |\n","|    actor_loss      | -0.871   |\n","|    critic_loss     | 0.0196   |\n","|    ent_coef        | 0.00151  |\n","|    ent_coef_loss   | 6.14     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3220     |\n","|    fps             | 297      |\n","|    time_elapsed    | 12475    |\n","|    total_timesteps | 3716064  |\n","| train/             |          |\n","|    actor_loss      | -0.859   |\n","|    critic_loss     | 0.0202   |\n","|    ent_coef        | 0.00151  |\n","|    ent_coef_loss   | -4.83    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154831   |\n","---------------------------------\n","Eval num_timesteps=3717600, episode_reward=4.58 +/- 0.65\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.58     |\n","| time/              |          |\n","|    total_timesteps | 3717600  |\n","| train/             |          |\n","|    actor_loss      | -0.935   |\n","|    critic_loss     | 0.02     |\n","|    ent_coef        | 0.00148  |\n","|    ent_coef_loss   | -9.47    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154895   |\n","---------------------------------\n","Eval num_timesteps=3720000, episode_reward=3.95 +/- 0.92\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.95     |\n","| time/              |          |\n","|    total_timesteps | 3720000  |\n","| train/             |          |\n","|    actor_loss      | -0.952   |\n","|    critic_loss     | 0.0289   |\n","|    ent_coef        | 0.00144  |\n","|    ent_coef_loss   | -4.72    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154995   |\n","---------------------------------\n","Eval num_timesteps=3722400, episode_reward=3.94 +/- 0.23\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.94     |\n","| time/              |          |\n","|    total_timesteps | 3722400  |\n","| train/             |          |\n","|    actor_loss      | -0.903   |\n","|    critic_loss     | 0.0315   |\n","|    ent_coef        | 0.0015   |\n","|    ent_coef_loss   | -0.475   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155095   |\n","---------------------------------\n","Eval num_timesteps=3724800, episode_reward=3.12 +/- 0.64\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.12     |\n","| time/              |          |\n","|    total_timesteps | 3724800  |\n","| train/             |          |\n","|    actor_loss      | -0.853   |\n","|    critic_loss     | 0.0194   |\n","|    ent_coef        | 0.00149  |\n","|    ent_coef_loss   | -5.34    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155195   |\n","---------------------------------\n","Eval num_timesteps=3727200, episode_reward=2.62 +/- 0.12\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.62     |\n","| time/              |          |\n","|    total_timesteps | 3727200  |\n","| train/             |          |\n","|    actor_loss      | -0.956   |\n","|    critic_loss     | 0.0114   |\n","|    ent_coef        | 0.00148  |\n","|    ent_coef_loss   | -1.26    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155295   |\n","---------------------------------\n","Eval num_timesteps=3729600, episode_reward=3.80 +/- 0.73\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.8      |\n","| time/              |          |\n","|    total_timesteps | 3729600  |\n","| train/             |          |\n","|    actor_loss      | -0.989   |\n","|    critic_loss     | 0.0304   |\n","|    ent_coef        | 0.00148  |\n","|    ent_coef_loss   | -2.9     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155395   |\n","---------------------------------\n","Eval num_timesteps=3732000, episode_reward=3.69 +/- 0.16\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.69     |\n","| time/              |          |\n","|    total_timesteps | 3732000  |\n","| train/             |          |\n","|    actor_loss      | -0.871   |\n","|    critic_loss     | 0.02     |\n","|    ent_coef        | 0.00145  |\n","|    ent_coef_loss   | -2.91    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155495   |\n","---------------------------------\n","Eval num_timesteps=3734400, episode_reward=6.57 +/- 3.14\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.57     |\n","| time/              |          |\n","|    total_timesteps | 3734400  |\n","| train/             |          |\n","|    actor_loss      | -0.934   |\n","|    critic_loss     | 0.0502   |\n","|    ent_coef        | 0.00143  |\n","|    ent_coef_loss   | -4.86    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155595   |\n","---------------------------------\n","Eval num_timesteps=3736800, episode_reward=-90.29 +/- 0.62\n","Episode length: 73.00 +/- 2.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 73       |\n","|    mean_reward     | -90.3    |\n","| time/              |          |\n","|    total_timesteps | 3736800  |\n","| train/             |          |\n","|    actor_loss      | -0.95    |\n","|    critic_loss     | 0.0255   |\n","|    ent_coef        | 0.00139  |\n","|    ent_coef_loss   | 5.48     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3230     |\n","|    fps             | 297      |\n","|    time_elapsed    | 12547    |\n","|    total_timesteps | 3736944  |\n","| train/             |          |\n","|    actor_loss      | -0.791   |\n","|    critic_loss     | 0.0327   |\n","|    ent_coef        | 0.0014   |\n","|    ent_coef_loss   | 6.69     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155701   |\n","---------------------------------\n","Eval num_timesteps=3739200, episode_reward=1.12 +/- 4.62\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.12     |\n","| time/              |          |\n","|    total_timesteps | 3739200  |\n","| train/             |          |\n","|    actor_loss      | -0.895   |\n","|    critic_loss     | 0.0826   |\n","|    ent_coef        | 0.00141  |\n","|    ent_coef_loss   | 7.77     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155795   |\n","---------------------------------\n","Eval num_timesteps=3741600, episode_reward=3.90 +/- 0.17\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.9      |\n","| time/              |          |\n","|    total_timesteps | 3741600  |\n","| train/             |          |\n","|    actor_loss      | -0.902   |\n","|    critic_loss     | 0.0218   |\n","|    ent_coef        | 0.00145  |\n","|    ent_coef_loss   | 1.69     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155895   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3240     |\n","|    fps             | 297      |\n","|    time_elapsed    | 12566    |\n","|    total_timesteps | 3743736  |\n","| train/             |          |\n","|    actor_loss      | -1       |\n","|    critic_loss     | 0.0161   |\n","|    ent_coef        | 0.00144  |\n","|    ent_coef_loss   | 0.503    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155984   |\n","---------------------------------\n","Eval num_timesteps=3744000, episode_reward=3.45 +/- 0.45\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.45     |\n","| time/              |          |\n","|    total_timesteps | 3744000  |\n","| train/             |          |\n","|    actor_loss      | -0.908   |\n","|    critic_loss     | 0.0213   |\n","|    ent_coef        | 0.00144  |\n","|    ent_coef_loss   | -0.393   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155995   |\n","---------------------------------\n","Eval num_timesteps=3746400, episode_reward=3.78 +/- 0.34\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.78     |\n","| time/              |          |\n","|    total_timesteps | 3746400  |\n","| train/             |          |\n","|    actor_loss      | -0.82    |\n","|    critic_loss     | 0.0362   |\n","|    ent_coef        | 0.0014   |\n","|    ent_coef_loss   | -4.83    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156095   |\n","---------------------------------\n","Eval num_timesteps=3748800, episode_reward=4.08 +/- 1.39\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.08     |\n","| time/              |          |\n","|    total_timesteps | 3748800  |\n","| train/             |          |\n","|    actor_loss      | -0.981   |\n","|    critic_loss     | 0.0194   |\n","|    ent_coef        | 0.00141  |\n","|    ent_coef_loss   | -4.53    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156195   |\n","---------------------------------\n","Eval num_timesteps=3751200, episode_reward=3.79 +/- 0.79\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.79     |\n","| time/              |          |\n","|    total_timesteps | 3751200  |\n","| train/             |          |\n","|    actor_loss      | -1.14    |\n","|    critic_loss     | 0.0415   |\n","|    ent_coef        | 0.0014   |\n","|    ent_coef_loss   | -4.9     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156295   |\n","---------------------------------\n","Eval num_timesteps=3753600, episode_reward=2.34 +/- 0.12\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.34     |\n","| time/              |          |\n","|    total_timesteps | 3753600  |\n","| train/             |          |\n","|    actor_loss      | -1.02    |\n","|    critic_loss     | 0.0188   |\n","|    ent_coef        | 0.00138  |\n","|    ent_coef_loss   | -6.03    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156395   |\n","---------------------------------\n","Eval num_timesteps=3756000, episode_reward=5.35 +/- 0.88\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.35     |\n","| time/              |          |\n","|    total_timesteps | 3756000  |\n","| train/             |          |\n","|    actor_loss      | -0.872   |\n","|    critic_loss     | 0.0298   |\n","|    ent_coef        | 0.00135  |\n","|    ent_coef_loss   | -0.605   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156495   |\n","---------------------------------\n","Eval num_timesteps=3758400, episode_reward=3.08 +/- 0.49\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.08     |\n","| time/              |          |\n","|    total_timesteps | 3758400  |\n","| train/             |          |\n","|    actor_loss      | -0.941   |\n","|    critic_loss     | 0.0202   |\n","|    ent_coef        | 0.00138  |\n","|    ent_coef_loss   | 3.05     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156595   |\n","---------------------------------\n","Eval num_timesteps=3760800, episode_reward=2.84 +/- 0.73\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.84     |\n","| time/              |          |\n","|    total_timesteps | 3760800  |\n","| train/             |          |\n","|    actor_loss      | -0.867   |\n","|    critic_loss     | 0.0485   |\n","|    ent_coef        | 0.00138  |\n","|    ent_coef_loss   | 1.82     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156695   |\n","---------------------------------\n","Eval num_timesteps=3763200, episode_reward=2.56 +/- 0.99\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.56     |\n","| time/              |          |\n","|    total_timesteps | 3763200  |\n","| train/             |          |\n","|    actor_loss      | -0.939   |\n","|    critic_loss     | 0.016    |\n","|    ent_coef        | 0.00139  |\n","|    ent_coef_loss   | 0.315    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156795   |\n","---------------------------------\n","Eval num_timesteps=3765600, episode_reward=3.54 +/- 0.79\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.54     |\n","| time/              |          |\n","|    total_timesteps | 3765600  |\n","| train/             |          |\n","|    actor_loss      | -0.884   |\n","|    critic_loss     | 0.0658   |\n","|    ent_coef        | 0.00137  |\n","|    ent_coef_loss   | -2.92    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156895   |\n","---------------------------------\n","Eval num_timesteps=3768000, episode_reward=3.32 +/- 0.57\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.32     |\n","| time/              |          |\n","|    total_timesteps | 3768000  |\n","| train/             |          |\n","|    actor_loss      | -0.9     |\n","|    critic_loss     | 0.0161   |\n","|    ent_coef        | 0.00136  |\n","|    ent_coef_loss   | -3.71    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156995   |\n","---------------------------------\n","Eval num_timesteps=3770400, episode_reward=5.05 +/- 0.31\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.05     |\n","| time/              |          |\n","|    total_timesteps | 3770400  |\n","| train/             |          |\n","|    actor_loss      | -0.86    |\n","|    critic_loss     | 0.0221   |\n","|    ent_coef        | 0.00135  |\n","|    ent_coef_loss   | 3.67     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157095   |\n","---------------------------------\n","Eval num_timesteps=3772800, episode_reward=5.63 +/- 0.56\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.63     |\n","| time/              |          |\n","|    total_timesteps | 3772800  |\n","| train/             |          |\n","|    actor_loss      | -0.882   |\n","|    critic_loss     | 0.0185   |\n","|    ent_coef        | 0.00136  |\n","|    ent_coef_loss   | -2.43    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157195   |\n","---------------------------------\n","Eval num_timesteps=3775200, episode_reward=1.66 +/- 0.86\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.66     |\n","| time/              |          |\n","|    total_timesteps | 3775200  |\n","| train/             |          |\n","|    actor_loss      | -0.943   |\n","|    critic_loss     | 0.0694   |\n","|    ent_coef        | 0.00135  |\n","|    ent_coef_loss   | 12.7     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157295   |\n","---------------------------------\n","Eval num_timesteps=3777600, episode_reward=2.27 +/- 0.76\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.27     |\n","| time/              |          |\n","|    total_timesteps | 3777600  |\n","| train/             |          |\n","|    actor_loss      | -0.848   |\n","|    critic_loss     | 0.0211   |\n","|    ent_coef        | 0.00143  |\n","|    ent_coef_loss   | -2.67    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157395   |\n","---------------------------------\n","Eval num_timesteps=3780000, episode_reward=2.22 +/- 0.21\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.22     |\n","| time/              |          |\n","|    total_timesteps | 3780000  |\n","| train/             |          |\n","|    actor_loss      | -0.854   |\n","|    critic_loss     | 0.0138   |\n","|    ent_coef        | 0.00142  |\n","|    ent_coef_loss   | -4.16    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157495   |\n","---------------------------------\n","Eval num_timesteps=3782400, episode_reward=2.57 +/- 0.21\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.57     |\n","| time/              |          |\n","|    total_timesteps | 3782400  |\n","| train/             |          |\n","|    actor_loss      | -0.995   |\n","|    critic_loss     | 0.017    |\n","|    ent_coef        | 0.00139  |\n","|    ent_coef_loss   | -5.5     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157595   |\n","---------------------------------\n","Eval num_timesteps=3784800, episode_reward=3.42 +/- 0.13\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.42     |\n","| time/              |          |\n","|    total_timesteps | 3784800  |\n","| train/             |          |\n","|    actor_loss      | -0.857   |\n","|    critic_loss     | 0.0157   |\n","|    ent_coef        | 0.00136  |\n","|    ent_coef_loss   | -2.62    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157695   |\n","---------------------------------\n","Eval num_timesteps=3787200, episode_reward=0.08 +/- 1.38\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.0818   |\n","| time/              |          |\n","|    total_timesteps | 3787200  |\n","| train/             |          |\n","|    actor_loss      | -0.924   |\n","|    critic_loss     | 0.017    |\n","|    ent_coef        | 0.00135  |\n","|    ent_coef_loss   | 4.32     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157795   |\n","---------------------------------\n","Eval num_timesteps=3789600, episode_reward=3.38 +/- 0.12\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.38     |\n","| time/              |          |\n","|    total_timesteps | 3789600  |\n","| train/             |          |\n","|    actor_loss      | -1.02    |\n","|    critic_loss     | 0.0181   |\n","|    ent_coef        | 0.00136  |\n","|    ent_coef_loss   | 0.773    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157895   |\n","---------------------------------\n","Eval num_timesteps=3792000, episode_reward=-95.66 +/- 0.49\n","Episode length: 76.20 +/- 0.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 76.2     |\n","|    mean_reward     | -95.7    |\n","| time/              |          |\n","|    total_timesteps | 3792000  |\n","| train/             |          |\n","|    actor_loss      | -0.896   |\n","|    critic_loss     | 0.0376   |\n","|    ent_coef        | 0.00136  |\n","|    ent_coef_loss   | -0.346   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3250     |\n","|    fps             | 297      |\n","|    time_elapsed    | 12735    |\n","|    total_timesteps | 3792288  |\n","| train/             |          |\n","|    actor_loss      | -0.914   |\n","|    critic_loss     | 0.0172   |\n","|    ent_coef        | 0.00136  |\n","|    ent_coef_loss   | -4.68    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158007   |\n","---------------------------------\n","Eval num_timesteps=3794400, episode_reward=4.08 +/- 0.05\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.08     |\n","| time/              |          |\n","|    total_timesteps | 3794400  |\n","| train/             |          |\n","|    actor_loss      | -0.825   |\n","|    critic_loss     | 0.0324   |\n","|    ent_coef        | 0.00132  |\n","|    ent_coef_loss   | -5.27    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158095   |\n","---------------------------------\n","Eval num_timesteps=3796800, episode_reward=-52.94 +/- 44.95\n","Episode length: 254.60 +/- 200.37\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 255      |\n","|    mean_reward     | -52.9    |\n","| time/              |          |\n","|    total_timesteps | 3796800  |\n","| train/             |          |\n","|    actor_loss      | -0.924   |\n","|    critic_loss     | 0.0146   |\n","|    ent_coef        | 0.00134  |\n","|    ent_coef_loss   | -0.00131 |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3260     |\n","|    fps             | 297      |\n","|    time_elapsed    | 12754    |\n","|    total_timesteps | 3798984  |\n","| train/             |          |\n","|    actor_loss      | -0.777   |\n","|    critic_loss     | 0.0343   |\n","|    ent_coef        | 0.00132  |\n","|    ent_coef_loss   | 19.1     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158286   |\n","---------------------------------\n","Eval num_timesteps=3799200, episode_reward=4.13 +/- 0.03\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.13     |\n","| time/              |          |\n","|    total_timesteps | 3799200  |\n","| train/             |          |\n","|    actor_loss      | -0.99    |\n","|    critic_loss     | 0.0187   |\n","|    ent_coef        | 0.00133  |\n","|    ent_coef_loss   | 23.3     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158295   |\n","---------------------------------\n","Eval num_timesteps=3801600, episode_reward=1.91 +/- 1.04\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.91     |\n","| time/              |          |\n","|    total_timesteps | 3801600  |\n","| train/             |          |\n","|    actor_loss      | -0.805   |\n","|    critic_loss     | 10.7     |\n","|    ent_coef        | 0.00143  |\n","|    ent_coef_loss   | -1.09    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158395   |\n","---------------------------------\n","Eval num_timesteps=3804000, episode_reward=1.56 +/- 0.40\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.56     |\n","| time/              |          |\n","|    total_timesteps | 3804000  |\n","| train/             |          |\n","|    actor_loss      | -0.705   |\n","|    critic_loss     | 30       |\n","|    ent_coef        | 0.0014   |\n","|    ent_coef_loss   | 24.3     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158495   |\n","---------------------------------\n","Eval num_timesteps=3806400, episode_reward=-91.18 +/- 0.98\n","Episode length: 90.80 +/- 8.33\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 90.8     |\n","|    mean_reward     | -91.2    |\n","| time/              |          |\n","|    total_timesteps | 3806400  |\n","| train/             |          |\n","|    actor_loss      | -0.832   |\n","|    critic_loss     | 28.3     |\n","|    ent_coef        | 0.00147  |\n","|    ent_coef_loss   | -1.02    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3270     |\n","|    fps             | 297      |\n","|    time_elapsed    | 12781    |\n","|    total_timesteps | 3807960  |\n","| train/             |          |\n","|    actor_loss      | -1.05    |\n","|    critic_loss     | 0.0147   |\n","|    ent_coef        | 0.00147  |\n","|    ent_coef_loss   | 4.24     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158660   |\n","---------------------------------\n","Eval num_timesteps=3808800, episode_reward=-92.88 +/- 1.71\n","Episode length: 119.60 +/- 20.09\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 120      |\n","|    mean_reward     | -92.9    |\n","| time/              |          |\n","|    total_timesteps | 3808800  |\n","| train/             |          |\n","|    actor_loss      | -0.979   |\n","|    critic_loss     | 0.0465   |\n","|    ent_coef        | 0.00148  |\n","|    ent_coef_loss   | -1.98    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3280     |\n","|    fps             | 297      |\n","|    time_elapsed    | 12785    |\n","|    total_timesteps | 3809880  |\n","| train/             |          |\n","|    actor_loss      | -1.01    |\n","|    critic_loss     | 0.0299   |\n","|    ent_coef        | 0.00149  |\n","|    ent_coef_loss   | -4.61    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158740   |\n","---------------------------------\n","Eval num_timesteps=3811200, episode_reward=-90.83 +/- 0.68\n","Episode length: 79.60 +/- 2.94\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 79.6     |\n","|    mean_reward     | -90.8    |\n","| time/              |          |\n","|    total_timesteps | 3811200  |\n","| train/             |          |\n","|    actor_loss      | -0.891   |\n","|    critic_loss     | 0.0192   |\n","|    ent_coef        | 0.00149  |\n","|    ent_coef_loss   | 28.7     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3290     |\n","|    fps             | 298      |\n","|    time_elapsed    | 12790    |\n","|    total_timesteps | 3812136  |\n","| train/             |          |\n","|    actor_loss      | -1.05    |\n","|    critic_loss     | 0.0248   |\n","|    ent_coef        | 0.00152  |\n","|    ent_coef_loss   | -0.361   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158834   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3300     |\n","|    fps             | 298      |\n","|    time_elapsed    | 12791    |\n","|    total_timesteps | 3813072  |\n","| train/             |          |\n","|    actor_loss      | -0.93    |\n","|    critic_loss     | 0.0305   |\n","|    ent_coef        | 0.00152  |\n","|    ent_coef_loss   | 7.67     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158873   |\n","---------------------------------\n","Eval num_timesteps=3813600, episode_reward=-35.28 +/- 46.15\n","Episode length: 346.00 +/- 188.61\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 346      |\n","|    mean_reward     | -35.3    |\n","| time/              |          |\n","|    total_timesteps | 3813600  |\n","| train/             |          |\n","|    actor_loss      | -1.03    |\n","|    critic_loss     | 0.0198   |\n","|    ent_coef        | 0.00153  |\n","|    ent_coef_loss   | 8.86     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158895   |\n","---------------------------------\n","Eval num_timesteps=3816000, episode_reward=3.01 +/- 0.88\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.01     |\n","| time/              |          |\n","|    total_timesteps | 3816000  |\n","| train/             |          |\n","|    actor_loss      | -1.07    |\n","|    critic_loss     | 0.0135   |\n","|    ent_coef        | 0.00154  |\n","|    ent_coef_loss   | -4.44    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3310     |\n","|    fps             | 298      |\n","|    time_elapsed    | 12807    |\n","|    total_timesteps | 3818112  |\n","| train/             |          |\n","|    actor_loss      | -0.951   |\n","|    critic_loss     | 0.0297   |\n","|    ent_coef        | 0.00152  |\n","|    ent_coef_loss   | -7.39    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159083   |\n","---------------------------------\n","Eval num_timesteps=3818400, episode_reward=-92.02 +/- 1.42\n","Episode length: 76.80 +/- 6.37\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 76.8     |\n","|    mean_reward     | -92      |\n","| time/              |          |\n","|    total_timesteps | 3818400  |\n","| train/             |          |\n","|    actor_loss      | -1.04    |\n","|    critic_loss     | 0.0265   |\n","|    ent_coef        | 0.00151  |\n","|    ent_coef_loss   | -4.37    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159095   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3320     |\n","|    fps             | 298      |\n","|    time_elapsed    | 12810    |\n","|    total_timesteps | 3819576  |\n","| train/             |          |\n","|    actor_loss      | -0.983   |\n","|    critic_loss     | 0.0398   |\n","|    ent_coef        | 0.00152  |\n","|    ent_coef_loss   | 7.18     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159144   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3330     |\n","|    fps             | 298      |\n","|    time_elapsed    | 12811    |\n","|    total_timesteps | 3819936  |\n","| train/             |          |\n","|    actor_loss      | -0.808   |\n","|    critic_loss     | 0.172    |\n","|    ent_coef        | 0.00152  |\n","|    ent_coef_loss   | 0.271    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159159   |\n","---------------------------------\n","Eval num_timesteps=3820800, episode_reward=-1.70 +/- 1.21\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.7     |\n","| time/              |          |\n","|    total_timesteps | 3820800  |\n","| train/             |          |\n","|    actor_loss      | -0.962   |\n","|    critic_loss     | 0.0187   |\n","|    ent_coef        | 0.00152  |\n","|    ent_coef_loss   | -3.64    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159195   |\n","---------------------------------\n","Eval num_timesteps=3823200, episode_reward=0.31 +/- 0.12\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.312    |\n","| time/              |          |\n","|    total_timesteps | 3823200  |\n","| train/             |          |\n","|    actor_loss      | -0.682   |\n","|    critic_loss     | 0.158    |\n","|    ent_coef        | 0.0015   |\n","|    ent_coef_loss   | -2.98    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159295   |\n","---------------------------------\n","Eval num_timesteps=3825600, episode_reward=-3.02 +/- 0.17\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -3.02    |\n","| time/              |          |\n","|    total_timesteps | 3825600  |\n","| train/             |          |\n","|    actor_loss      | -1.1     |\n","|    critic_loss     | 0.0145   |\n","|    ent_coef        | 0.00147  |\n","|    ent_coef_loss   | -4.1     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159395   |\n","---------------------------------\n","Eval num_timesteps=3828000, episode_reward=-95.42 +/- 1.91\n","Episode length: 77.60 +/- 4.41\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 77.6     |\n","|    mean_reward     | -95.4    |\n","| time/              |          |\n","|    total_timesteps | 3828000  |\n","| train/             |          |\n","|    actor_loss      | -1.01    |\n","|    critic_loss     | 0.0195   |\n","|    ent_coef        | 0.00144  |\n","|    ent_coef_loss   | 2.23     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159495   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3340     |\n","|    fps             | 298      |\n","|    time_elapsed    | 12837    |\n","|    total_timesteps | 3828144  |\n","| train/             |          |\n","|    actor_loss      | -0.941   |\n","|    critic_loss     | 0.0311   |\n","|    ent_coef        | 0.00144  |\n","|    ent_coef_loss   | 3.63     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159501   |\n","---------------------------------\n","Eval num_timesteps=3830400, episode_reward=2.93 +/- 0.81\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.93     |\n","| time/              |          |\n","|    total_timesteps | 3830400  |\n","| train/             |          |\n","|    actor_loss      | -0.991   |\n","|    critic_loss     | 0.0491   |\n","|    ent_coef        | 0.00145  |\n","|    ent_coef_loss   | -2.1     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159595   |\n","---------------------------------\n","Eval num_timesteps=3832800, episode_reward=3.04 +/- 0.55\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.04     |\n","| time/              |          |\n","|    total_timesteps | 3832800  |\n","| train/             |          |\n","|    actor_loss      | -0.712   |\n","|    critic_loss     | 0.0297   |\n","|    ent_coef        | 0.00143  |\n","|    ent_coef_loss   | -1.01    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159695   |\n","---------------------------------\n","Eval num_timesteps=3835200, episode_reward=-0.84 +/- 0.25\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.836   |\n","| time/              |          |\n","|    total_timesteps | 3835200  |\n","| train/             |          |\n","|    actor_loss      | -1.01    |\n","|    critic_loss     | 0.0522   |\n","|    ent_coef        | 0.0014   |\n","|    ent_coef_loss   | -0.886   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3350     |\n","|    fps             | 298      |\n","|    time_elapsed    | 12864    |\n","|    total_timesteps | 3837576  |\n","| train/             |          |\n","|    actor_loss      | -0.857   |\n","|    critic_loss     | 0.0309   |\n","|    ent_coef        | 0.00139  |\n","|    ent_coef_loss   | -1.8     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159894   |\n","---------------------------------\n","Eval num_timesteps=3837600, episode_reward=5.37 +/- 0.30\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.37     |\n","| time/              |          |\n","|    total_timesteps | 3837600  |\n","| train/             |          |\n","|    actor_loss      | -0.633   |\n","|    critic_loss     | 0.0212   |\n","|    ent_coef        | 0.00139  |\n","|    ent_coef_loss   | -1.32    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159895   |\n","---------------------------------\n","Eval num_timesteps=3840000, episode_reward=5.53 +/- 0.46\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.53     |\n","| time/              |          |\n","|    total_timesteps | 3840000  |\n","| train/             |          |\n","|    actor_loss      | -0.91    |\n","|    critic_loss     | 27.7     |\n","|    ent_coef        | 0.00141  |\n","|    ent_coef_loss   | 13.9     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159995   |\n","---------------------------------\n","Eval num_timesteps=3842400, episode_reward=3.37 +/- 0.10\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.37     |\n","| time/              |          |\n","|    total_timesteps | 3842400  |\n","| train/             |          |\n","|    actor_loss      | -0.632   |\n","|    critic_loss     | 0.0924   |\n","|    ent_coef        | 0.00145  |\n","|    ent_coef_loss   | -3.48    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160095   |\n","---------------------------------\n","Eval num_timesteps=3844800, episode_reward=1.53 +/- 0.48\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.53     |\n","| time/              |          |\n","|    total_timesteps | 3844800  |\n","| train/             |          |\n","|    actor_loss      | -0.937   |\n","|    critic_loss     | 0.0288   |\n","|    ent_coef        | 0.00147  |\n","|    ent_coef_loss   | 8.39     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160195   |\n","---------------------------------\n","Eval num_timesteps=3847200, episode_reward=1.14 +/- 0.28\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.14     |\n","| time/              |          |\n","|    total_timesteps | 3847200  |\n","| train/             |          |\n","|    actor_loss      | -0.978   |\n","|    critic_loss     | 0.108    |\n","|    ent_coef        | 0.00153  |\n","|    ent_coef_loss   | 14.4     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160295   |\n","---------------------------------\n","Eval num_timesteps=3849600, episode_reward=2.19 +/- 0.59\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.19     |\n","| time/              |          |\n","|    total_timesteps | 3849600  |\n","| train/             |          |\n","|    actor_loss      | -0.706   |\n","|    critic_loss     | 0.0237   |\n","|    ent_coef        | 0.0016   |\n","|    ent_coef_loss   | 16.6     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160395   |\n","---------------------------------\n","Eval num_timesteps=3852000, episode_reward=1.24 +/- 0.07\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.24     |\n","| time/              |          |\n","|    total_timesteps | 3852000  |\n","| train/             |          |\n","|    actor_loss      | -1.05    |\n","|    critic_loss     | 0.0194   |\n","|    ent_coef        | 0.00164  |\n","|    ent_coef_loss   | 2.6      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160495   |\n","---------------------------------\n","Eval num_timesteps=3854400, episode_reward=4.13 +/- 0.10\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.13     |\n","| time/              |          |\n","|    total_timesteps | 3854400  |\n","| train/             |          |\n","|    actor_loss      | -1.02    |\n","|    critic_loss     | 0.0184   |\n","|    ent_coef        | 0.00165  |\n","|    ent_coef_loss   | -0.22    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160595   |\n","---------------------------------\n","Eval num_timesteps=3856800, episode_reward=3.18 +/- 1.35\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.18     |\n","| time/              |          |\n","|    total_timesteps | 3856800  |\n","| train/             |          |\n","|    actor_loss      | -0.879   |\n","|    critic_loss     | 0.0784   |\n","|    ent_coef        | 0.00166  |\n","|    ent_coef_loss   | 9.71     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160695   |\n","---------------------------------\n","Eval num_timesteps=3859200, episode_reward=2.99 +/- 0.19\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.99     |\n","| time/              |          |\n","|    total_timesteps | 3859200  |\n","| train/             |          |\n","|    actor_loss      | -1.01    |\n","|    critic_loss     | 0.0147   |\n","|    ent_coef        | 0.00174  |\n","|    ent_coef_loss   | 2.36     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160795   |\n","---------------------------------\n","Eval num_timesteps=3861600, episode_reward=-98.88 +/- 2.78\n","Episode length: 145.00 +/- 31.84\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 145      |\n","|    mean_reward     | -98.9    |\n","| time/              |          |\n","|    total_timesteps | 3861600  |\n","| train/             |          |\n","|    actor_loss      | -0.778   |\n","|    critic_loss     | 0.0302   |\n","|    ent_coef        | 0.00175  |\n","|    ent_coef_loss   | 0.774    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160895   |\n","---------------------------------\n","Eval num_timesteps=3864000, episode_reward=-57.65 +/- 48.26\n","Episode length: 261.80 +/- 194.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 262      |\n","|    mean_reward     | -57.6    |\n","| time/              |          |\n","|    total_timesteps | 3864000  |\n","| train/             |          |\n","|    actor_loss      | -0.87    |\n","|    critic_loss     | 0.026    |\n","|    ent_coef        | 0.00176  |\n","|    ent_coef_loss   | 1.69     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3360     |\n","|    fps             | 298      |\n","|    time_elapsed    | 12961    |\n","|    total_timesteps | 3865848  |\n","| train/             |          |\n","|    actor_loss      | -0.6     |\n","|    critic_loss     | 25.6     |\n","|    ent_coef        | 0.00176  |\n","|    ent_coef_loss   | 1.66     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 161072   |\n","---------------------------------\n","Eval num_timesteps=3866400, episode_reward=-37.92 +/- 48.86\n","Episode length: 442.40 +/- 70.55\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 442      |\n","|    mean_reward     | -37.9    |\n","| time/              |          |\n","|    total_timesteps | 3866400  |\n","| train/             |          |\n","|    actor_loss      | -0.922   |\n","|    critic_loss     | 0.0458   |\n","|    ent_coef        | 0.00176  |\n","|    ent_coef_loss   | 11.2     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 161095   |\n","---------------------------------\n","Eval num_timesteps=3868800, episode_reward=2.88 +/- 0.81\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.88     |\n","| time/              |          |\n","|    total_timesteps | 3868800  |\n","| train/             |          |\n","|    actor_loss      | -0.608   |\n","|    critic_loss     | 0.0999   |\n","|    ent_coef        | 0.00178  |\n","|    ent_coef_loss   | 8.69     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 161195   |\n","---------------------------------\n","Eval num_timesteps=3871200, episode_reward=4.23 +/- 0.00\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.23     |\n","| time/              |          |\n","|    total_timesteps | 3871200  |\n","| train/             |          |\n","|    actor_loss      | -0.91    |\n","|    critic_loss     | 0.0171   |\n","|    ent_coef        | 0.00181  |\n","|    ent_coef_loss   | 0.618    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 161295   |\n","---------------------------------\n","Eval num_timesteps=3873600, episode_reward=2.99 +/- 0.30\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.99     |\n","| time/              |          |\n","|    total_timesteps | 3873600  |\n","| train/             |          |\n","|    actor_loss      | -1.02    |\n","|    critic_loss     | 0.0153   |\n","|    ent_coef        | 0.00179  |\n","|    ent_coef_loss   | -3.82    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 161395   |\n","---------------------------------\n","Eval num_timesteps=3876000, episode_reward=3.08 +/- 0.80\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.08     |\n","| time/              |          |\n","|    total_timesteps | 3876000  |\n","| train/             |          |\n","|    actor_loss      | -0.914   |\n","|    critic_loss     | 0.0352   |\n","|    ent_coef        | 0.0018   |\n","|    ent_coef_loss   | -5.86    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 161495   |\n","---------------------------------\n","Eval num_timesteps=3878400, episode_reward=4.87 +/- 1.31\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.87     |\n","| time/              |          |\n","|    total_timesteps | 3878400  |\n","| train/             |          |\n","|    actor_loss      | -0.732   |\n","|    critic_loss     | 0.435    |\n","|    ent_coef        | 0.00185  |\n","|    ent_coef_loss   | 6.01     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 161595   |\n","---------------------------------\n","Eval num_timesteps=3880800, episode_reward=2.50 +/- 0.35\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.5      |\n","| time/              |          |\n","|    total_timesteps | 3880800  |\n","| train/             |          |\n","|    actor_loss      | -0.918   |\n","|    critic_loss     | 0.0532   |\n","|    ent_coef        | 0.00187  |\n","|    ent_coef_loss   | 1.87     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 161695   |\n","---------------------------------\n","Eval num_timesteps=3883200, episode_reward=2.86 +/- 1.14\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.86     |\n","| time/              |          |\n","|    total_timesteps | 3883200  |\n","| train/             |          |\n","|    actor_loss      | -0.573   |\n","|    critic_loss     | 0.789    |\n","|    ent_coef        | 0.00186  |\n","|    ent_coef_loss   | 2.92     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 161795   |\n","---------------------------------\n","Eval num_timesteps=3885600, episode_reward=-1.53 +/- 3.42\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.53    |\n","| time/              |          |\n","|    total_timesteps | 3885600  |\n","| train/             |          |\n","|    actor_loss      | -0.716   |\n","|    critic_loss     | 0.0343   |\n","|    ent_coef        | 0.00192  |\n","|    ent_coef_loss   | -1.79    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 161895   |\n","---------------------------------\n","Eval num_timesteps=3888000, episode_reward=1.82 +/- 1.29\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.82     |\n","| time/              |          |\n","|    total_timesteps | 3888000  |\n","| train/             |          |\n","|    actor_loss      | -0.829   |\n","|    critic_loss     | 0.0255   |\n","|    ent_coef        | 0.00191  |\n","|    ent_coef_loss   | 1.25     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 161995   |\n","---------------------------------\n","Eval num_timesteps=3890400, episode_reward=3.30 +/- 1.07\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.3      |\n","| time/              |          |\n","|    total_timesteps | 3890400  |\n","| train/             |          |\n","|    actor_loss      | -0.897   |\n","|    critic_loss     | 0.0413   |\n","|    ent_coef        | 0.0019   |\n","|    ent_coef_loss   | 2.41     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162095   |\n","---------------------------------\n","Eval num_timesteps=3892800, episode_reward=2.19 +/- 3.05\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.19     |\n","| time/              |          |\n","|    total_timesteps | 3892800  |\n","| train/             |          |\n","|    actor_loss      | -1.09    |\n","|    critic_loss     | 0.0279   |\n","|    ent_coef        | 0.00188  |\n","|    ent_coef_loss   | -5.52    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3370     |\n","|    fps             | 298      |\n","|    time_elapsed    | 13062    |\n","|    total_timesteps | 3894528  |\n","| train/             |          |\n","|    actor_loss      | -1.05    |\n","|    critic_loss     | 0.0234   |\n","|    ent_coef        | 0.00188  |\n","|    ent_coef_loss   | 8.08     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162267   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3380     |\n","|    fps             | 298      |\n","|    time_elapsed    | 13062    |\n","|    total_timesteps | 3894768  |\n","| train/             |          |\n","|    actor_loss      | -0.96    |\n","|    critic_loss     | 0.033    |\n","|    ent_coef        | 0.00189  |\n","|    ent_coef_loss   | 7.86     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162277   |\n","---------------------------------\n","Eval num_timesteps=3895200, episode_reward=-90.73 +/- 0.44\n","Episode length: 69.20 +/- 3.92\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 69.2     |\n","|    mean_reward     | -90.7    |\n","| time/              |          |\n","|    total_timesteps | 3895200  |\n","| train/             |          |\n","|    actor_loss      | -0.921   |\n","|    critic_loss     | 0.101    |\n","|    ent_coef        | 0.0019   |\n","|    ent_coef_loss   | 12.8     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3390     |\n","|    fps             | 298      |\n","|    time_elapsed    | 13063    |\n","|    total_timesteps | 3895224  |\n","| train/             |          |\n","|    actor_loss      | -0.918   |\n","|    critic_loss     | 0.0759   |\n","|    ent_coef        | 0.0019   |\n","|    ent_coef_loss   | 11.9     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162296   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3400     |\n","|    fps             | 298      |\n","|    time_elapsed    | 13064    |\n","|    total_timesteps | 3896376  |\n","| train/             |          |\n","|    actor_loss      | -0.634   |\n","|    critic_loss     | 0.0385   |\n","|    ent_coef        | 0.00194  |\n","|    ent_coef_loss   | -1.42    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162344   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3410     |\n","|    fps             | 298      |\n","|    time_elapsed    | 13065    |\n","|    total_timesteps | 3896904  |\n","| train/             |          |\n","|    actor_loss      | -0.938   |\n","|    critic_loss     | 0.0332   |\n","|    ent_coef        | 0.00194  |\n","|    ent_coef_loss   | -5.54    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162366   |\n","---------------------------------\n","Eval num_timesteps=3897600, episode_reward=4.02 +/- 0.16\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.02     |\n","| time/              |          |\n","|    total_timesteps | 3897600  |\n","| train/             |          |\n","|    actor_loss      | -0.718   |\n","|    critic_loss     | 0.178    |\n","|    ent_coef        | 0.00193  |\n","|    ent_coef_loss   | -5.82    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162395   |\n","---------------------------------\n","Eval num_timesteps=3900000, episode_reward=2.00 +/- 0.22\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2        |\n","| time/              |          |\n","|    total_timesteps | 3900000  |\n","| train/             |          |\n","|    actor_loss      | -0.965   |\n","|    critic_loss     | 0.0288   |\n","|    ent_coef        | 0.00188  |\n","|    ent_coef_loss   | -4.39    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162495   |\n","---------------------------------\n","Eval num_timesteps=3902400, episode_reward=2.62 +/- 0.48\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.62     |\n","| time/              |          |\n","|    total_timesteps | 3902400  |\n","| train/             |          |\n","|    actor_loss      | -0.872   |\n","|    critic_loss     | 0.0262   |\n","|    ent_coef        | 0.00185  |\n","|    ent_coef_loss   | 0.95     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162595   |\n","---------------------------------\n","Eval num_timesteps=3904800, episode_reward=2.77 +/- 0.18\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.77     |\n","| time/              |          |\n","|    total_timesteps | 3904800  |\n","| train/             |          |\n","|    actor_loss      | -0.679   |\n","|    critic_loss     | 0.112    |\n","|    ent_coef        | 0.00182  |\n","|    ent_coef_loss   | -0.283   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162695   |\n","---------------------------------\n","Eval num_timesteps=3907200, episode_reward=2.50 +/- 0.60\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.5      |\n","| time/              |          |\n","|    total_timesteps | 3907200  |\n","| train/             |          |\n","|    actor_loss      | -0.604   |\n","|    critic_loss     | 0.0394   |\n","|    ent_coef        | 0.0019   |\n","|    ent_coef_loss   | 12       |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162795   |\n","---------------------------------\n","Eval num_timesteps=3909600, episode_reward=2.55 +/- 0.71\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.55     |\n","| time/              |          |\n","|    total_timesteps | 3909600  |\n","| train/             |          |\n","|    actor_loss      | -0.971   |\n","|    critic_loss     | 0.0324   |\n","|    ent_coef        | 0.00198  |\n","|    ent_coef_loss   | 10.8     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162895   |\n","---------------------------------\n","Eval num_timesteps=3912000, episode_reward=2.80 +/- 0.26\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.8      |\n","| time/              |          |\n","|    total_timesteps | 3912000  |\n","| train/             |          |\n","|    actor_loss      | -0.838   |\n","|    critic_loss     | 0.0859   |\n","|    ent_coef        | 0.00201  |\n","|    ent_coef_loss   | 0.467    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162995   |\n","---------------------------------\n","Eval num_timesteps=3914400, episode_reward=3.14 +/- 0.31\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.14     |\n","| time/              |          |\n","|    total_timesteps | 3914400  |\n","| train/             |          |\n","|    actor_loss      | -0.973   |\n","|    critic_loss     | 0.0188   |\n","|    ent_coef        | 0.00206  |\n","|    ent_coef_loss   | 0.155    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 163095   |\n","---------------------------------\n","Eval num_timesteps=3916800, episode_reward=-92.52 +/- 2.90\n","Episode length: 87.00 +/- 2.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 87       |\n","|    mean_reward     | -92.5    |\n","| time/              |          |\n","|    total_timesteps | 3916800  |\n","| train/             |          |\n","|    actor_loss      | -1.01    |\n","|    critic_loss     | 0.126    |\n","|    ent_coef        | 0.00211  |\n","|    ent_coef_loss   | 5.05     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 163195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3420     |\n","|    fps             | 298      |\n","|    time_elapsed    | 13136    |\n","|    total_timesteps | 3918288  |\n","| train/             |          |\n","|    actor_loss      | -0.893   |\n","|    critic_loss     | 1.23     |\n","|    ent_coef        | 0.00212  |\n","|    ent_coef_loss   | 0.348    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 163257   |\n","---------------------------------\n","Eval num_timesteps=3919200, episode_reward=0.51 +/- 1.03\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.507    |\n","| time/              |          |\n","|    total_timesteps | 3919200  |\n","| train/             |          |\n","|    actor_loss      | -0.994   |\n","|    critic_loss     | 0.0452   |\n","|    ent_coef        | 0.00214  |\n","|    ent_coef_loss   | 8.71     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 163295   |\n","---------------------------------\n","Eval num_timesteps=3921600, episode_reward=3.49 +/- 0.50\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.49     |\n","| time/              |          |\n","|    total_timesteps | 3921600  |\n","| train/             |          |\n","|    actor_loss      | -0.899   |\n","|    critic_loss     | 0.0403   |\n","|    ent_coef        | 0.00211  |\n","|    ent_coef_loss   | -4.57    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 163395   |\n","---------------------------------\n","Eval num_timesteps=3924000, episode_reward=3.44 +/- 1.19\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.44     |\n","| time/              |          |\n","|    total_timesteps | 3924000  |\n","| train/             |          |\n","|    actor_loss      | -0.714   |\n","|    critic_loss     | 0.0825   |\n","|    ent_coef        | 0.00211  |\n","|    ent_coef_loss   | 7.94     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 163495   |\n","---------------------------------\n","Eval num_timesteps=3926400, episode_reward=4.65 +/- 0.93\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.65     |\n","| time/              |          |\n","|    total_timesteps | 3926400  |\n","| train/             |          |\n","|    actor_loss      | -0.74    |\n","|    critic_loss     | 0.0818   |\n","|    ent_coef        | 0.00214  |\n","|    ent_coef_loss   | -2.33    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 163595   |\n","---------------------------------\n","Eval num_timesteps=3928800, episode_reward=-48.92 +/- 42.83\n","Episode length: 306.20 +/- 158.24\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 306      |\n","|    mean_reward     | -48.9    |\n","| time/              |          |\n","|    total_timesteps | 3928800  |\n","| train/             |          |\n","|    actor_loss      | -0.984   |\n","|    critic_loss     | 0.0318   |\n","|    ent_coef        | 0.00212  |\n","|    ent_coef_loss   | -4.01    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 163695   |\n","---------------------------------\n","Eval num_timesteps=3931200, episode_reward=3.26 +/- 1.62\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.26     |\n","| time/              |          |\n","|    total_timesteps | 3931200  |\n","| train/             |          |\n","|    actor_loss      | -0.922   |\n","|    critic_loss     | 0.0432   |\n","|    ent_coef        | 0.0021   |\n","|    ent_coef_loss   | -1.93    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 163795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3430     |\n","|    fps             | 298      |\n","|    time_elapsed    | 13186    |\n","|    total_timesteps | 3932880  |\n","| train/             |          |\n","|    actor_loss      | -0.938   |\n","|    critic_loss     | 0.0205   |\n","|    ent_coef        | 0.00209  |\n","|    ent_coef_loss   | 0.258    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 163865   |\n","---------------------------------\n","Eval num_timesteps=3933600, episode_reward=2.55 +/- 0.07\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.55     |\n","| time/              |          |\n","|    total_timesteps | 3933600  |\n","| train/             |          |\n","|    actor_loss      | -0.535   |\n","|    critic_loss     | 4.13     |\n","|    ent_coef        | 0.00209  |\n","|    ent_coef_loss   | 6.89     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 163895   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3440     |\n","|    fps             | 298      |\n","|    time_elapsed    | 13193    |\n","|    total_timesteps | 3935664  |\n","| train/             |          |\n","|    actor_loss      | -0.546   |\n","|    critic_loss     | 0.127    |\n","|    ent_coef        | 0.00208  |\n","|    ent_coef_loss   | -4.37    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 163981   |\n","---------------------------------\n","Eval num_timesteps=3936000, episode_reward=3.46 +/- 0.50\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.46     |\n","| time/              |          |\n","|    total_timesteps | 3936000  |\n","| train/             |          |\n","|    actor_loss      | -0.854   |\n","|    critic_loss     | 0.0754   |\n","|    ent_coef        | 0.00207  |\n","|    ent_coef_loss   | -7.24    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 163995   |\n","---------------------------------\n","Eval num_timesteps=3938400, episode_reward=1.74 +/- 0.07\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.74     |\n","| time/              |          |\n","|    total_timesteps | 3938400  |\n","| train/             |          |\n","|    actor_loss      | -0.859   |\n","|    critic_loss     | 29.2     |\n","|    ent_coef        | 0.002    |\n","|    ent_coef_loss   | 0.657    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164095   |\n","---------------------------------\n","Eval num_timesteps=3940800, episode_reward=2.98 +/- 0.20\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.98     |\n","| time/              |          |\n","|    total_timesteps | 3940800  |\n","| train/             |          |\n","|    actor_loss      | -0.945   |\n","|    critic_loss     | 0.0378   |\n","|    ent_coef        | 0.00198  |\n","|    ent_coef_loss   | -5.8     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164195   |\n","---------------------------------\n","Eval num_timesteps=3943200, episode_reward=2.23 +/- 0.08\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.23     |\n","| time/              |          |\n","|    total_timesteps | 3943200  |\n","| train/             |          |\n","|    actor_loss      | -0.557   |\n","|    critic_loss     | 0.0732   |\n","|    ent_coef        | 0.00198  |\n","|    ent_coef_loss   | 2.1      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164295   |\n","---------------------------------\n","Eval num_timesteps=3945600, episode_reward=1.17 +/- 0.05\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.17     |\n","| time/              |          |\n","|    total_timesteps | 3945600  |\n","| train/             |          |\n","|    actor_loss      | -0.99    |\n","|    critic_loss     | 0.0442   |\n","|    ent_coef        | 0.00202  |\n","|    ent_coef_loss   | 6.53     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164395   |\n","---------------------------------\n","Eval num_timesteps=3948000, episode_reward=-94.83 +/- 1.43\n","Episode length: 64.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 64       |\n","|    mean_reward     | -94.8    |\n","| time/              |          |\n","|    total_timesteps | 3948000  |\n","| train/             |          |\n","|    actor_loss      | -0.955   |\n","|    critic_loss     | 0.109    |\n","|    ent_coef        | 0.00208  |\n","|    ent_coef_loss   | 0.568    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164495   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3450     |\n","|    fps             | 298      |\n","|    time_elapsed    | 13237    |\n","|    total_timesteps | 3948144  |\n","| train/             |          |\n","|    actor_loss      | -0.822   |\n","|    critic_loss     | 0.0433   |\n","|    ent_coef        | 0.00208  |\n","|    ent_coef_loss   | 1.7      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164501   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3460     |\n","|    fps             | 298      |\n","|    time_elapsed    | 13239    |\n","|    total_timesteps | 3949680  |\n","| train/             |          |\n","|    actor_loss      | -1.06    |\n","|    critic_loss     | 0.0276   |\n","|    ent_coef        | 0.00207  |\n","|    ent_coef_loss   | -2.75    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164565   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3470     |\n","|    fps             | 298      |\n","|    time_elapsed    | 13240    |\n","|    total_timesteps | 3950040  |\n","| train/             |          |\n","|    actor_loss      | -0.247   |\n","|    critic_loss     | 27.7     |\n","|    ent_coef        | 0.00207  |\n","|    ent_coef_loss   | -0.947   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164580   |\n","---------------------------------\n","Eval num_timesteps=3950400, episode_reward=1.13 +/- 0.25\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.13     |\n","| time/              |          |\n","|    total_timesteps | 3950400  |\n","| train/             |          |\n","|    actor_loss      | -0.161   |\n","|    critic_loss     | 0.0895   |\n","|    ent_coef        | 0.00206  |\n","|    ent_coef_loss   | -2.69    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3480     |\n","|    fps             | 298      |\n","|    time_elapsed    | 13248    |\n","|    total_timesteps | 3952296  |\n","| train/             |          |\n","|    actor_loss      | -0.639   |\n","|    critic_loss     | 0.123    |\n","|    ent_coef        | 0.00201  |\n","|    ent_coef_loss   | -1.78    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164674   |\n","---------------------------------\n","Eval num_timesteps=3952800, episode_reward=-97.03 +/- 1.07\n","Episode length: 63.80 +/- 3.43\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 63.8     |\n","|    mean_reward     | -97      |\n","| time/              |          |\n","|    total_timesteps | 3952800  |\n","| train/             |          |\n","|    actor_loss      | -0.929   |\n","|    critic_loss     | 0.0368   |\n","|    ent_coef        | 0.00201  |\n","|    ent_coef_loss   | -0.308   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3490     |\n","|    fps             | 298      |\n","|    time_elapsed    | 13249    |\n","|    total_timesteps | 3952848  |\n","| train/             |          |\n","|    actor_loss      | -0.918   |\n","|    critic_loss     | 0.0301   |\n","|    ent_coef        | 0.00201  |\n","|    ent_coef_loss   | 3.48     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164697   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3500     |\n","|    fps             | 298      |\n","|    time_elapsed    | 13250    |\n","|    total_timesteps | 3953784  |\n","| train/             |          |\n","|    actor_loss      | -0.435   |\n","|    critic_loss     | 0.191    |\n","|    ent_coef        | 0.00202  |\n","|    ent_coef_loss   | 2.22     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164736   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3510     |\n","|    fps             | 298      |\n","|    time_elapsed    | 13250    |\n","|    total_timesteps | 3954528  |\n","| train/             |          |\n","|    actor_loss      | -0.51    |\n","|    critic_loss     | 0.0912   |\n","|    ent_coef        | 0.00202  |\n","|    ent_coef_loss   | 0.267    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164767   |\n","---------------------------------\n","Eval num_timesteps=3955200, episode_reward=-90.91 +/- 0.53\n","Episode length: 45.60 +/- 4.41\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 45.6     |\n","|    mean_reward     | -90.9    |\n","| time/              |          |\n","|    total_timesteps | 3955200  |\n","| train/             |          |\n","|    actor_loss      | -0.793   |\n","|    critic_loss     | 0.0496   |\n","|    ent_coef        | 0.00203  |\n","|    ent_coef_loss   | 0.329    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3520     |\n","|    fps             | 298      |\n","|    time_elapsed    | 13252    |\n","|    total_timesteps | 3955584  |\n","| train/             |          |\n","|    actor_loss      | -0.786   |\n","|    critic_loss     | 0.06     |\n","|    ent_coef        | 0.00203  |\n","|    ent_coef_loss   | -3.58    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164811   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3530     |\n","|    fps             | 298      |\n","|    time_elapsed    | 13252    |\n","|    total_timesteps | 3956160  |\n","| train/             |          |\n","|    actor_loss      | -0.992   |\n","|    critic_loss     | 0.113    |\n","|    ent_coef        | 0.00203  |\n","|    ent_coef_loss   | -5.28    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164835   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 3540     |\n","|    fps             | 298      |\n","|    time_elapsed    | 13253    |\n","|    total_timesteps | 3956952  |\n","| train/             |          |\n","|    actor_loss      | -0.284   |\n","|    critic_loss     | 0.115    |\n","|    ent_coef        | 0.00201  |\n","|    ent_coef_loss   | 5.29     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164868   |\n","---------------------------------\n","Eval num_timesteps=3957600, episode_reward=1.24 +/- 0.86\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.24     |\n","| time/              |          |\n","|    total_timesteps | 3957600  |\n","| train/             |          |\n","|    actor_loss      | -0.871   |\n","|    critic_loss     | 0.192    |\n","|    ent_coef        | 0.00203  |\n","|    ent_coef_loss   | 13.2     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164895   |\n","---------------------------------\n","Eval num_timesteps=3960000, episode_reward=1.77 +/- 0.18\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.77     |\n","| time/              |          |\n","|    total_timesteps | 3960000  |\n","| train/             |          |\n","|    actor_loss      | -0.494   |\n","|    critic_loss     | 0.0807   |\n","|    ent_coef        | 0.00216  |\n","|    ent_coef_loss   | 0.0456   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164995   |\n","---------------------------------\n","Eval num_timesteps=3962400, episode_reward=3.63 +/- 0.53\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.63     |\n","| time/              |          |\n","|    total_timesteps | 3962400  |\n","| train/             |          |\n","|    actor_loss      | -0.416   |\n","|    critic_loss     | 0.198    |\n","|    ent_coef        | 0.00221  |\n","|    ent_coef_loss   | -6.51    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 165095   |\n","---------------------------------\n","Eval num_timesteps=3964800, episode_reward=3.34 +/- 0.46\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.34     |\n","| time/              |          |\n","|    total_timesteps | 3964800  |\n","| train/             |          |\n","|    actor_loss      | -0.449   |\n","|    critic_loss     | 0.668    |\n","|    ent_coef        | 0.00213  |\n","|    ent_coef_loss   | -3.85    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 165195   |\n","---------------------------------\n","Eval num_timesteps=3967200, episode_reward=3.64 +/- 1.30\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.64     |\n","| time/              |          |\n","|    total_timesteps | 3967200  |\n","| train/             |          |\n","|    actor_loss      | -0.587   |\n","|    critic_loss     | 0.124    |\n","|    ent_coef        | 0.00211  |\n","|    ent_coef_loss   | 3.04     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 165295   |\n","---------------------------------\n","Eval num_timesteps=3969600, episode_reward=1.71 +/- 0.47\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.71     |\n","| time/              |          |\n","|    total_timesteps | 3969600  |\n","| train/             |          |\n","|    actor_loss      | -0.727   |\n","|    critic_loss     | 24.7     |\n","|    ent_coef        | 0.00208  |\n","|    ent_coef_loss   | -2.86    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 165395   |\n","---------------------------------\n","Eval num_timesteps=3972000, episode_reward=-3.33 +/- 0.48\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -3.33    |\n","| time/              |          |\n","|    total_timesteps | 3972000  |\n","| train/             |          |\n","|    actor_loss      | -0.767   |\n","|    critic_loss     | 0.0595   |\n","|    ent_coef        | 0.00208  |\n","|    ent_coef_loss   | -1.13    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 165495   |\n","---------------------------------\n","Eval num_timesteps=3974400, episode_reward=4.38 +/- 1.04\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.38     |\n","| time/              |          |\n","|    total_timesteps | 3974400  |\n","| train/             |          |\n","|    actor_loss      | -0.888   |\n","|    critic_loss     | 0.0453   |\n","|    ent_coef        | 0.00204  |\n","|    ent_coef_loss   | -8.9     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 165595   |\n","---------------------------------\n","Eval num_timesteps=3976800, episode_reward=2.20 +/- 0.63\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.2      |\n","| time/              |          |\n","|    total_timesteps | 3976800  |\n","| train/             |          |\n","|    actor_loss      | -0.832   |\n","|    critic_loss     | 0.0527   |\n","|    ent_coef        | 0.00198  |\n","|    ent_coef_loss   | -0.4     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 165695   |\n","---------------------------------\n","Eval num_timesteps=3979200, episode_reward=1.42 +/- 0.43\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.42     |\n","| time/              |          |\n","|    total_timesteps | 3979200  |\n","| train/             |          |\n","|    actor_loss      | -0.595   |\n","|    critic_loss     | 0.131    |\n","|    ent_coef        | 0.002    |\n","|    ent_coef_loss   | -2.62    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 165795   |\n","---------------------------------\n","Eval num_timesteps=3981600, episode_reward=2.32 +/- 0.86\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.32     |\n","| time/              |          |\n","|    total_timesteps | 3981600  |\n","| train/             |          |\n","|    actor_loss      | -0.551   |\n","|    critic_loss     | 0.408    |\n","|    ent_coef        | 0.00198  |\n","|    ent_coef_loss   | -2.08    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 165895   |\n","---------------------------------\n","Eval num_timesteps=3984000, episode_reward=1.54 +/- 0.74\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.54     |\n","| time/              |          |\n","|    total_timesteps | 3984000  |\n","| train/             |          |\n","|    actor_loss      | -0.777   |\n","|    critic_loss     | 0.0418   |\n","|    ent_coef        | 0.00196  |\n","|    ent_coef_loss   | -4.98    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 165995   |\n","---------------------------------\n","Eval num_timesteps=3986400, episode_reward=1.81 +/- 0.06\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.81     |\n","| time/              |          |\n","|    total_timesteps | 3986400  |\n","| train/             |          |\n","|    actor_loss      | -0.419   |\n","|    critic_loss     | 0.186    |\n","|    ent_coef        | 0.00198  |\n","|    ent_coef_loss   | 13.9     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 166095   |\n","---------------------------------\n","Eval num_timesteps=3988800, episode_reward=0.55 +/- 0.32\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.554    |\n","| time/              |          |\n","|    total_timesteps | 3988800  |\n","| train/             |          |\n","|    actor_loss      | -0.896   |\n","|    critic_loss     | 0.0694   |\n","|    ent_coef        | 0.00207  |\n","|    ent_coef_loss   | 2.56     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 166195   |\n","---------------------------------\n","Eval num_timesteps=3991200, episode_reward=2.05 +/- 0.43\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.05     |\n","| time/              |          |\n","|    total_timesteps | 3991200  |\n","| train/             |          |\n","|    actor_loss      | -0.868   |\n","|    critic_loss     | 0.0718   |\n","|    ent_coef        | 0.00209  |\n","|    ent_coef_loss   | -3.99    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 166295   |\n","---------------------------------\n","Eval num_timesteps=3993600, episode_reward=4.47 +/- 0.25\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.47     |\n","| time/              |          |\n","|    total_timesteps | 3993600  |\n","| train/             |          |\n","|    actor_loss      | -0.452   |\n","|    critic_loss     | 0.0423   |\n","|    ent_coef        | 0.00207  |\n","|    ent_coef_loss   | -4.09    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 166395   |\n","---------------------------------\n","Eval num_timesteps=3996000, episode_reward=2.39 +/- 0.21\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.39     |\n","| time/              |          |\n","|    total_timesteps | 3996000  |\n","| train/             |          |\n","|    actor_loss      | -0.606   |\n","|    critic_loss     | 0.216    |\n","|    ent_coef        | 0.00207  |\n","|    ent_coef_loss   | 1.5      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 166495   |\n","---------------------------------\n","Eval num_timesteps=3998400, episode_reward=3.09 +/- 0.03\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.09     |\n","| time/              |          |\n","|    total_timesteps | 3998400  |\n","| train/             |          |\n","|    actor_loss      | -0.888   |\n","|    critic_loss     | 0.0522   |\n","|    ent_coef        | 0.00213  |\n","|    ent_coef_loss   | -2.53    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 166595   |\n","---------------------------------\n","Eval num_timesteps=4000800, episode_reward=2.27 +/- 0.17\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.27     |\n","| time/              |          |\n","|    total_timesteps | 4000800  |\n","| train/             |          |\n","|    actor_loss      | -0.449   |\n","|    critic_loss     | 0.264    |\n","|    ent_coef        | 0.00214  |\n","|    ent_coef_loss   | 1.56     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 166695   |\n","---------------------------------\n","Eval num_timesteps=4003200, episode_reward=2.41 +/- 0.53\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.41     |\n","| time/              |          |\n","|    total_timesteps | 4003200  |\n","| train/             |          |\n","|    actor_loss      | -0.933   |\n","|    critic_loss     | 0.0921   |\n","|    ent_coef        | 0.00212  |\n","|    ent_coef_loss   | -1.03    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 166795   |\n","---------------------------------\n","Eval num_timesteps=4005600, episode_reward=3.30 +/- 0.47\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.3      |\n","| time/              |          |\n","|    total_timesteps | 4005600  |\n","| train/             |          |\n","|    actor_loss      | -0.764   |\n","|    critic_loss     | 0.0513   |\n","|    ent_coef        | 0.00215  |\n","|    ent_coef_loss   | 0.689    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 166895   |\n","---------------------------------\n","Eval num_timesteps=4008000, episode_reward=3.69 +/- 0.66\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.69     |\n","| time/              |          |\n","|    total_timesteps | 4008000  |\n","| train/             |          |\n","|    actor_loss      | -0.231   |\n","|    critic_loss     | 27.1     |\n","|    ent_coef        | 0.00216  |\n","|    ent_coef_loss   | 1.14     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 166995   |\n","---------------------------------\n","Eval num_timesteps=4010400, episode_reward=2.77 +/- 0.68\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.77     |\n","| time/              |          |\n","|    total_timesteps | 4010400  |\n","| train/             |          |\n","|    actor_loss      | 0.00506  |\n","|    critic_loss     | 0.481    |\n","|    ent_coef        | 0.00213  |\n","|    ent_coef_loss   | -4.31    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 167095   |\n","---------------------------------\n","Eval num_timesteps=4012800, episode_reward=-0.01 +/- 0.83\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.0103  |\n","| time/              |          |\n","|    total_timesteps | 4012800  |\n","| train/             |          |\n","|    actor_loss      | -0.298   |\n","|    critic_loss     | 36.2     |\n","|    ent_coef        | 0.00205  |\n","|    ent_coef_loss   | 0.69     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 167195   |\n","---------------------------------\n","Eval num_timesteps=4015200, episode_reward=-96.92 +/- 5.02\n","Episode length: 293.00 +/- 61.24\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 293      |\n","|    mean_reward     | -96.9    |\n","| time/              |          |\n","|    total_timesteps | 4015200  |\n","| train/             |          |\n","|    actor_loss      | -0.674   |\n","|    critic_loss     | 30.6     |\n","|    ent_coef        | 0.002    |\n","|    ent_coef_loss   | -3.99    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 167295   |\n","---------------------------------\n","Eval num_timesteps=4017600, episode_reward=2.80 +/- 0.03\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.8      |\n","| time/              |          |\n","|    total_timesteps | 4017600  |\n","| train/             |          |\n","|    actor_loss      | -0.378   |\n","|    critic_loss     | 0.245    |\n","|    ent_coef        | 0.00198  |\n","|    ent_coef_loss   | -4.49    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 167395   |\n","---------------------------------\n","Eval num_timesteps=4020000, episode_reward=0.52 +/- 0.07\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.517    |\n","| time/              |          |\n","|    total_timesteps | 4020000  |\n","| train/             |          |\n","|    actor_loss      | -0.458   |\n","|    critic_loss     | 0.264    |\n","|    ent_coef        | 0.00193  |\n","|    ent_coef_loss   | -3.73    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 167495   |\n","---------------------------------\n","Eval num_timesteps=4022400, episode_reward=1.68 +/- 1.04\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.68     |\n","| time/              |          |\n","|    total_timesteps | 4022400  |\n","| train/             |          |\n","|    actor_loss      | -0.549   |\n","|    critic_loss     | 0.0691   |\n","|    ent_coef        | 0.00192  |\n","|    ent_coef_loss   | 2.86     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 167595   |\n","---------------------------------\n","Eval num_timesteps=4024800, episode_reward=0.80 +/- 0.38\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.802    |\n","| time/              |          |\n","|    total_timesteps | 4024800  |\n","| train/             |          |\n","|    actor_loss      | -0.844   |\n","|    critic_loss     | 33.8     |\n","|    ent_coef        | 0.00192  |\n","|    ent_coef_loss   | -0.201   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 167695   |\n","---------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n"]},{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-22-fdb182ee4e64>\", line 10, in <cell line: 10>\n","    model.learn(total_timesteps=5000000, log_interval=10,callback=eval_callback)\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/sac/sac.py\", line 307, in learn\n","    return super().learn(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 312, in learn\n","    rollout = self.collect_rollouts(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 552, in collect_rollouts\n","    if callback.on_step() is False:\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 104, in on_step\n","    return self._on_step()\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 471, in _on_step\n","    np.savez(\n","  File \"<__array_function__ internals>\", line 180, in savez\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 615, in savez\n","    _savez(file, args, kwds, False)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 712, in _savez\n","    zipf = zipfile_factory(file, mode=\"w\", compression=compression)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 103, in zipfile_factory\n","    return zipfile.ZipFile(file, *args, **kwargs)\n","  File \"/usr/lib/python3.10/zipfile.py\", line 1251, in __init__\n","    self.fp = io.open(file, filemode)\n","OSError: [Errno 107] Transport endpoint is not connected: './multiwalker_sac_08_log_eval/evaluations.npz'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'OSError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-22-fdb182ee4e64>\", line 10, in <cell line: 10>\n","    model.learn(total_timesteps=5000000, log_interval=10,callback=eval_callback)\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/sac/sac.py\", line 307, in learn\n","    return super().learn(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 312, in learn\n","    rollout = self.collect_rollouts(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 552, in collect_rollouts\n","    if callback.on_step() is False:\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 104, in on_step\n","    return self._on_step()\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 471, in _on_step\n","    np.savez(\n","  File \"<__array_function__ internals>\", line 180, in savez\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 615, in savez\n","    _savez(file, args, kwds, False)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 712, in _savez\n","    zipf = zipfile_factory(file, mode=\"w\", compression=compression)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 103, in zipfile_factory\n","    return zipfile.ZipFile(file, *args, **kwargs)\n","  File \"/usr/lib/python3.10/zipfile.py\", line 1251, in __init__\n","    self.fp = io.open(file, filemode)\n","OSError: [Errno 107] Transport endpoint is not connected: './multiwalker_sac_08_log_eval/evaluations.npz'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'OSError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n","    if (await self.run_code(code, result,  async_=asy)):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n","    self.showtraceback(running_compiled_code=True)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n","    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-22-fdb182ee4e64>\", line 10, in <cell line: 10>\n","    model.learn(total_timesteps=5000000, log_interval=10,callback=eval_callback)\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/sac/sac.py\", line 307, in learn\n","    return super().learn(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 312, in learn\n","    rollout = self.collect_rollouts(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 552, in collect_rollouts\n","    if callback.on_step() is False:\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 104, in on_step\n","    return self._on_step()\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 471, in _on_step\n","    np.savez(\n","  File \"<__array_function__ internals>\", line 180, in savez\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 615, in savez\n","    _savez(file, args, kwds, False)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 712, in _savez\n","    zipf = zipfile_factory(file, mode=\"w\", compression=compression)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 103, in zipfile_factory\n","    return zipfile.ZipFile(file, *args, **kwargs)\n","  File \"/usr/lib/python3.10/zipfile.py\", line 1251, in __init__\n","    self.fp = io.open(file, filemode)\n","OSError: [Errno 107] Transport endpoint is not connected: './multiwalker_sac_08_log_eval/evaluations.npz'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'OSError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n","    if (await self.run_code(code, result,  async_=asy)):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n","    self.showtraceback(running_compiled_code=True)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n","    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n","    return runner(coro)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n","    coro.send(None)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n","    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3492, in run_ast_nodes\n","    self.showtraceback()\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1142, in structured_traceback\n","    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n"]}],"source":["from stable_baselines3.common.logger import configure\n","\n","tmp_path = \"multiwalker_sac_08_log_eval/\"\n","# set up logger\n","new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n","\n","model = SAC(\"MlpPolicy\", env, verbose=3) #,train_freq=1\n","\n","model.set_logger(new_logger)\n","model.learn(total_timesteps=5000000, log_interval=10,callback=eval_callback)\n","model.save(\"multiwalker_sac_08\")"]},{"cell_type":"code","source":["from stable_baselines3 import SAC\n","from pettingzoo.sisl import multiwalker_v9\n","import supersuit as ss\n","from stable_baselines3.common.monitor import Monitor"],"metadata":{"id":"-6BKh_FuTZzN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HTOVAZKHTdeV","executionInfo":{"status":"ok","timestamp":1697725712497,"user_tz":-120,"elapsed":334,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"67c4f770-06db-4aed-c296-ff714445355d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["'=2.13'\t\t\t\t     multiwalker_sac_log_eval\n"," DQN_new_pettingzoo_gym_cap.ipynb    multiwalker_td3_log_eval\n"," DQN_policies\t\t\t     policy_log_eval\n","'Entrenamientos antiguos sin logs'   PPO_policies\n"," Entrenamientos_log_no_eval\t     results_rllib\n"," MCR_TFM.ipynb\t\t\t     TFM_Multiwalker_DDPG_PPO_gym_cap.ipynb\n"," multi_car_racing\t\t     TFM_Multiwalker_SAC_gym_cap.ipynb\n"," multiwalker_ddpg_log_eval\t     TFM_Multiwalker_TD3_gym_cap.ipynb\n"," multiwalker_ddpg.zip\t\t     TFM_PPO_new_pettingzoo_gym_cap.ipynb\n"," multiwalker_ppo_log_eval\t     TFM_PPO_pettingzoo_gym_cap.ipynb\n"," multiwalker_ppo.zip\n"]}]},{"cell_type":"code","source":["env = multiwalker_v9.parallel_env(render_mode=\"rgb_array\",shared_reward=0.8)\n","\n","# #---------------------------------\n","# log_dir = \"./policy_log\"\n","# os.makedirs(log_dir, exist_ok=True)\n","# env = Monitor(env, log_dir)\n","# #---------------------------------\n","# env = ss.color_reduction_v0(env, mode='B')\n","# env = ss.black_death_v3(env)\n","# env = ss.resize_v1(env, x_size=84, y_size=84)\n","env = ss.frame_stack_v1(env, 3)\n","env = ss.pettingzoo_env_to_vec_env_v1(env)\n","env = ss.concat_vec_envs_v1(env, 8, num_cpus=4, base_class='stable_baselines3')"],"metadata":{"id":"U6TtjGBcTedF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3.common.callbacks import EvalCallback\n","eval_callback = EvalCallback(env, best_model_save_path=\"./multiwalker_sac2_08_log_eval/\",\n","                             log_path=\"./multiwalker_sac2_08_log_eval/\", eval_freq=100,\n","                             deterministic=False, render=False)\n"],"metadata":{"id":"1ah3u9HuTga_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3.common.logger import configure\n","\n","tmp_path = \"multiwalker_sac2_08_log_eval/\"\n","# set up logger\n","new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n","\n","model = SAC(\"MlpPolicy\", env, verbose=3,batch_size=512, learning_rate=0.0005) #,train_freq=1\n","\n","model.set_logger(new_logger)\n","model.learn(total_timesteps=5000000, log_interval=10,callback=eval_callback)\n","model.save(\"multiwalker_sac2_08\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"muMFxxDZTg_Z","outputId":"4158c12f-6247-4a53-f160-75a032b00543"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Logging to multiwalker_sac2_08_log_eval/\n","Using cuda device\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 10       |\n","|    fps             | 182      |\n","|    time_elapsed    | 9        |\n","|    total_timesteps | 1728     |\n","| train/             |          |\n","|    actor_loss      | -5.14    |\n","|    critic_loss     | 15.5     |\n","|    ent_coef        | 0.968    |\n","|    ent_coef_loss   | -0.222   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 67       |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 20       |\n","|    fps             | 213      |\n","|    time_elapsed    | 10       |\n","|    total_timesteps | 2208     |\n","| train/             |          |\n","|    actor_loss      | -4.99    |\n","|    critic_loss     | 91.9     |\n","|    ent_coef        | 0.958    |\n","|    ent_coef_loss   | -0.288   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 87       |\n","---------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n","|    total_timesteps | 2724000  |\n","| train/             |          |\n","|    actor_loss      | 5.79     |\n","|    critic_loss     | 0.982    |\n","|    ent_coef        | 0.00389  |\n","|    ent_coef_loss   | 0.793    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 113495   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4610     |\n","|    fps             | 251      |\n","|    time_elapsed    | 10836    |\n","|    total_timesteps | 2725392  |\n","| train/             |          |\n","|    actor_loss      | 5.13     |\n","|    critic_loss     | 14.1     |\n","|    ent_coef        | 0.00392  |\n","|    ent_coef_loss   | 1.84     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 113553   |\n","---------------------------------\n","Eval num_timesteps=2726400, episode_reward=13.35 +/- 3.28\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 13.4     |\n","| time/              |          |\n","|    total_timesteps | 2726400  |\n","| train/             |          |\n","|    actor_loss      | 5.89     |\n","|    critic_loss     | 0.652    |\n","|    ent_coef        | 0.004    |\n","|    ent_coef_loss   | 0.319    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 113595   |\n","---------------------------------\n","Eval num_timesteps=2728800, episode_reward=-42.10 +/- 46.33\n","Episode length: 296.60 +/- 166.08\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 297      |\n","|    mean_reward     | -42.1    |\n","| time/              |          |\n","|    total_timesteps | 2728800  |\n","| train/             |          |\n","|    actor_loss      | 5.59     |\n","|    critic_loss     | 0.589    |\n","|    ent_coef        | 0.00395  |\n","|    ent_coef_loss   | 0.0372   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 113695   |\n","---------------------------------\n","Eval num_timesteps=2731200, episode_reward=10.57 +/- 1.96\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 10.6     |\n","| time/              |          |\n","|    total_timesteps | 2731200  |\n","| train/             |          |\n","|    actor_loss      | 5.89     |\n","|    critic_loss     | 10.4     |\n","|    ent_coef        | 0.00393  |\n","|    ent_coef_loss   | 2.69     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 113795   |\n","---------------------------------\n","Eval num_timesteps=2733600, episode_reward=-0.29 +/- 0.97\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.288   |\n","| time/              |          |\n","|    total_timesteps | 2733600  |\n","| train/             |          |\n","|    actor_loss      | 5.57     |\n","|    critic_loss     | 0.486    |\n","|    ent_coef        | 0.00408  |\n","|    ent_coef_loss   | -1.28    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 113895   |\n","---------------------------------\n","Eval num_timesteps=2736000, episode_reward=-36.97 +/- 29.79\n","Episode length: 321.20 +/- 145.99\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 321      |\n","|    mean_reward     | -37      |\n","| time/              |          |\n","|    total_timesteps | 2736000  |\n","| train/             |          |\n","|    actor_loss      | 5.54     |\n","|    critic_loss     | 0.975    |\n","|    ent_coef        | 0.00404  |\n","|    ent_coef_loss   | 0.343    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 113995   |\n","---------------------------------\n","Eval num_timesteps=2738400, episode_reward=4.83 +/- 5.43\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.83     |\n","| time/              |          |\n","|    total_timesteps | 2738400  |\n","| train/             |          |\n","|    actor_loss      | 5.45     |\n","|    critic_loss     | 0.931    |\n","|    ent_coef        | 0.00406  |\n","|    ent_coef_loss   | -0.15    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 114095   |\n","---------------------------------\n","Eval num_timesteps=2740800, episode_reward=9.93 +/- 1.27\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 9.93     |\n","| time/              |          |\n","|    total_timesteps | 2740800  |\n","| train/             |          |\n","|    actor_loss      | 5.54     |\n","|    critic_loss     | 1.12     |\n","|    ent_coef        | 0.00408  |\n","|    ent_coef_loss   | 2.18     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 114195   |\n","---------------------------------\n","Eval num_timesteps=2743200, episode_reward=-64.96 +/- 10.78\n","Episode length: 285.40 +/- 80.34\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 285      |\n","|    mean_reward     | -65      |\n","| time/              |          |\n","|    total_timesteps | 2743200  |\n","| train/             |          |\n","|    actor_loss      | 5.41     |\n","|    critic_loss     | 0.352    |\n","|    ent_coef        | 0.00423  |\n","|    ent_coef_loss   | -0.219   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 114295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4620     |\n","|    fps             | 251      |\n","|    time_elapsed    | 10916    |\n","|    total_timesteps | 2744328  |\n","| train/             |          |\n","|    actor_loss      | 5.65     |\n","|    critic_loss     | 13.6     |\n","|    ent_coef        | 0.00423  |\n","|    ent_coef_loss   | 1.61     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 114342   |\n","---------------------------------\n","Eval num_timesteps=2745600, episode_reward=-67.71 +/- 4.77\n","Episode length: 265.40 +/- 54.38\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 265      |\n","|    mean_reward     | -67.7    |\n","| time/              |          |\n","|    total_timesteps | 2745600  |\n","| train/             |          |\n","|    actor_loss      | 5.44     |\n","|    critic_loss     | 0.645    |\n","|    ent_coef        | 0.00421  |\n","|    ent_coef_loss   | -2.11    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 114395   |\n","---------------------------------\n","Eval num_timesteps=2748000, episode_reward=15.18 +/- 3.73\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 15.2     |\n","| time/              |          |\n","|    total_timesteps | 2748000  |\n","| train/             |          |\n","|    actor_loss      | 5.25     |\n","|    critic_loss     | 0.279    |\n","|    ent_coef        | 0.00402  |\n","|    ent_coef_loss   | -1.86    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 114495   |\n","---------------------------------\n","Eval num_timesteps=2750400, episode_reward=-47.57 +/- 44.60\n","Episode length: 390.20 +/- 89.65\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 390      |\n","|    mean_reward     | -47.6    |\n","| time/              |          |\n","|    total_timesteps | 2750400  |\n","| train/             |          |\n","|    actor_loss      | 5.73     |\n","|    critic_loss     | 0.997    |\n","|    ent_coef        | 0.00397  |\n","|    ent_coef_loss   | 0.0172   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 114595   |\n","---------------------------------\n","Eval num_timesteps=2752800, episode_reward=8.07 +/- 5.94\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 8.07     |\n","| time/              |          |\n","|    total_timesteps | 2752800  |\n","| train/             |          |\n","|    actor_loss      | 5.4      |\n","|    critic_loss     | 0.563    |\n","|    ent_coef        | 0.00396  |\n","|    ent_coef_loss   | -0.0305  |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 114695   |\n","---------------------------------\n","Eval num_timesteps=2755200, episode_reward=-35.14 +/- 55.28\n","Episode length: 384.40 +/- 141.58\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 384      |\n","|    mean_reward     | -35.1    |\n","| time/              |          |\n","|    total_timesteps | 2755200  |\n","| train/             |          |\n","|    actor_loss      | 5.29     |\n","|    critic_loss     | 0.574    |\n","|    ent_coef        | 0.00384  |\n","|    ent_coef_loss   | -1.53    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 114795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4630     |\n","|    fps             | 251      |\n","|    time_elapsed    | 10964    |\n","|    total_timesteps | 2756568  |\n","| train/             |          |\n","|    actor_loss      | 5.22     |\n","|    critic_loss     | 0.421    |\n","|    ent_coef        | 0.00391  |\n","|    ent_coef_loss   | 0.316    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 114852   |\n","---------------------------------\n","Eval num_timesteps=2757600, episode_reward=-79.64 +/- 0.19\n","Episode length: 273.00 +/- 51.44\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 273      |\n","|    mean_reward     | -79.6    |\n","| time/              |          |\n","|    total_timesteps | 2757600  |\n","| train/             |          |\n","|    actor_loss      | 5.82     |\n","|    critic_loss     | 13.5     |\n","|    ent_coef        | 0.00396  |\n","|    ent_coef_loss   | -0.849   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 114895   |\n","---------------------------------\n","Eval num_timesteps=2760000, episode_reward=6.90 +/- 3.55\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.9      |\n","| time/              |          |\n","|    total_timesteps | 2760000  |\n","| train/             |          |\n","|    actor_loss      | 5.6      |\n","|    critic_loss     | 5.79     |\n","|    ent_coef        | 0.00393  |\n","|    ent_coef_loss   | 1.13     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 114995   |\n","---------------------------------\n","Eval num_timesteps=2762400, episode_reward=5.76 +/- 6.98\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.76     |\n","| time/              |          |\n","|    total_timesteps | 2762400  |\n","| train/             |          |\n","|    actor_loss      | 5.54     |\n","|    critic_loss     | 0.396    |\n","|    ent_coef        | 0.00404  |\n","|    ent_coef_loss   | 2.9      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 115095   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4640     |\n","|    fps             | 251      |\n","|    time_elapsed    | 10992    |\n","|    total_timesteps | 2764728  |\n","| train/             |          |\n","|    actor_loss      | 5.45     |\n","|    critic_loss     | 0.602    |\n","|    ent_coef        | 0.00419  |\n","|    ent_coef_loss   | 0.334    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 115192   |\n","---------------------------------\n","Eval num_timesteps=2764800, episode_reward=-40.42 +/- 38.66\n","Episode length: 397.40 +/- 83.77\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 397      |\n","|    mean_reward     | -40.4    |\n","| time/              |          |\n","|    total_timesteps | 2764800  |\n","| train/             |          |\n","|    actor_loss      | 5.94     |\n","|    critic_loss     | 1.55     |\n","|    ent_coef        | 0.00419  |\n","|    ent_coef_loss   | 0.719    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 115195   |\n","---------------------------------\n","Eval num_timesteps=2767200, episode_reward=-21.16 +/- 35.33\n","Episode length: 422.80 +/- 94.55\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 423      |\n","|    mean_reward     | -21.2    |\n","| time/              |          |\n","|    total_timesteps | 2767200  |\n","| train/             |          |\n","|    actor_loss      | 5.67     |\n","|    critic_loss     | 22.5     |\n","|    ent_coef        | 0.00435  |\n","|    ent_coef_loss   | -1.56    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 115295   |\n","---------------------------------\n","Eval num_timesteps=2769600, episode_reward=4.87 +/- 0.24\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.87     |\n","| time/              |          |\n","|    total_timesteps | 2769600  |\n","| train/             |          |\n","|    actor_loss      | 5.71     |\n","|    critic_loss     | 0.824    |\n","|    ent_coef        | 0.00425  |\n","|    ent_coef_loss   | 0.439    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 115395   |\n","---------------------------------\n","Eval num_timesteps=2772000, episode_reward=-2.72 +/- 16.95\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -2.72    |\n","| time/              |          |\n","|    total_timesteps | 2772000  |\n","| train/             |          |\n","|    actor_loss      | 5.6      |\n","|    critic_loss     | 12.6     |\n","|    ent_coef        | 0.00424  |\n","|    ent_coef_loss   | -0.889   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 115495   |\n","---------------------------------\n","Eval num_timesteps=2774400, episode_reward=-0.13 +/- 0.62\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.131   |\n","| time/              |          |\n","|    total_timesteps | 2774400  |\n","| train/             |          |\n","|    actor_loss      | 5.57     |\n","|    critic_loss     | 2.15     |\n","|    ent_coef        | 0.00414  |\n","|    ent_coef_loss   | 0.323    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 115595   |\n","---------------------------------\n","Eval num_timesteps=2776800, episode_reward=3.07 +/- 0.34\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.07     |\n","| time/              |          |\n","|    total_timesteps | 2776800  |\n","| train/             |          |\n","|    actor_loss      | 5.22     |\n","|    critic_loss     | 15.6     |\n","|    ent_coef        | 0.00415  |\n","|    ent_coef_loss   | 3.23     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 115695   |\n","---------------------------------\n","Eval num_timesteps=2779200, episode_reward=-31.50 +/- 44.14\n","Episode length: 379.60 +/- 147.46\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 380      |\n","|    mean_reward     | -31.5    |\n","| time/              |          |\n","|    total_timesteps | 2779200  |\n","| train/             |          |\n","|    actor_loss      | 5.94     |\n","|    critic_loss     | 0.489    |\n","|    ent_coef        | 0.00439  |\n","|    ent_coef_loss   | -0.857   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 115795   |\n","---------------------------------\n","Eval num_timesteps=2781600, episode_reward=4.00 +/- 1.93\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4        |\n","| time/              |          |\n","|    total_timesteps | 2781600  |\n","| train/             |          |\n","|    actor_loss      | 5.55     |\n","|    critic_loss     | 0.513    |\n","|    ent_coef        | 0.0042   |\n","|    ent_coef_loss   | -0.453   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 115895   |\n","---------------------------------\n","Eval num_timesteps=2784000, episode_reward=-48.03 +/- 39.39\n","Episode length: 326.00 +/- 142.07\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 326      |\n","|    mean_reward     | -48      |\n","| time/              |          |\n","|    total_timesteps | 2784000  |\n","| train/             |          |\n","|    actor_loss      | 5.41     |\n","|    critic_loss     | 0.487    |\n","|    ent_coef        | 0.00411  |\n","|    ent_coef_loss   | 0.379    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 115995   |\n","---------------------------------\n","Eval num_timesteps=2786400, episode_reward=-31.84 +/- 38.26\n","Episode length: 359.20 +/- 172.44\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 359      |\n","|    mean_reward     | -31.8    |\n","| time/              |          |\n","|    total_timesteps | 2786400  |\n","| train/             |          |\n","|    actor_loss      | 5.44     |\n","|    critic_loss     | 0.426    |\n","|    ent_coef        | 0.00402  |\n","|    ent_coef_loss   | -1.68    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 116095   |\n","---------------------------------\n","Eval num_timesteps=2788800, episode_reward=-44.84 +/- 37.14\n","Episode length: 323.00 +/- 144.52\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 323      |\n","|    mean_reward     | -44.8    |\n","| time/              |          |\n","|    total_timesteps | 2788800  |\n","| train/             |          |\n","|    actor_loss      | 5.83     |\n","|    critic_loss     | 1.5      |\n","|    ent_coef        | 0.00389  |\n","|    ent_coef_loss   | -0.706   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 116195   |\n","---------------------------------\n","Eval num_timesteps=2791200, episode_reward=-44.94 +/- 38.93\n","Episode length: 352.40 +/- 120.51\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 352      |\n","|    mean_reward     | -44.9    |\n","| time/              |          |\n","|    total_timesteps | 2791200  |\n","| train/             |          |\n","|    actor_loss      | 5.73     |\n","|    critic_loss     | 0.561    |\n","|    ent_coef        | 0.00376  |\n","|    ent_coef_loss   | -0.162   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 116295   |\n","---------------------------------\n","Eval num_timesteps=2793600, episode_reward=-54.62 +/- 45.18\n","Episode length: 326.00 +/- 142.07\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 326      |\n","|    mean_reward     | -54.6    |\n","| time/              |          |\n","|    total_timesteps | 2793600  |\n","| train/             |          |\n","|    actor_loss      | 5.57     |\n","|    critic_loss     | 1.34     |\n","|    ent_coef        | 0.0037   |\n","|    ent_coef_loss   | 1.46     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 116395   |\n","---------------------------------\n","Eval num_timesteps=2796000, episode_reward=-37.75 +/- 39.42\n","Episode length: 393.20 +/- 87.20\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 393      |\n","|    mean_reward     | -37.7    |\n","| time/              |          |\n","|    total_timesteps | 2796000  |\n","| train/             |          |\n","|    actor_loss      | 5.54     |\n","|    critic_loss     | 0.599    |\n","|    ent_coef        | 0.0037   |\n","|    ent_coef_loss   | -1.06    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 116495   |\n","---------------------------------\n","Eval num_timesteps=2798400, episode_reward=-42.39 +/- 45.76\n","Episode length: 360.20 +/- 114.15\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 360      |\n","|    mean_reward     | -42.4    |\n","| time/              |          |\n","|    total_timesteps | 2798400  |\n","| train/             |          |\n","|    actor_loss      | 5.6      |\n","|    critic_loss     | 0.334    |\n","|    ent_coef        | 0.00378  |\n","|    ent_coef_loss   | 1.86     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 116595   |\n","---------------------------------\n","Eval num_timesteps=2800800, episode_reward=8.30 +/- 0.30\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 8.3      |\n","| time/              |          |\n","|    total_timesteps | 2800800  |\n","| train/             |          |\n","|    actor_loss      | 5.66     |\n","|    critic_loss     | 0.728    |\n","|    ent_coef        | 0.00382  |\n","|    ent_coef_loss   | 0.741    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 116695   |\n","---------------------------------\n","Eval num_timesteps=2803200, episode_reward=-45.25 +/- 49.00\n","Episode length: 318.20 +/- 148.44\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 318      |\n","|    mean_reward     | -45.3    |\n","| time/              |          |\n","|    total_timesteps | 2803200  |\n","| train/             |          |\n","|    actor_loss      | 5.4      |\n","|    critic_loss     | 2.06     |\n","|    ent_coef        | 0.00387  |\n","|    ent_coef_loss   | -0.0545  |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 116795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4650     |\n","|    fps             | 250      |\n","|    time_elapsed    | 11174    |\n","|    total_timesteps | 2804616  |\n","| train/             |          |\n","|    actor_loss      | 5.55     |\n","|    critic_loss     | 0.7      |\n","|    ent_coef        | 0.00389  |\n","|    ent_coef_loss   | 1.22     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 116854   |\n","---------------------------------\n","Eval num_timesteps=2805600, episode_reward=14.23 +/- 6.13\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 14.2     |\n","| time/              |          |\n","|    total_timesteps | 2805600  |\n","| train/             |          |\n","|    actor_loss      | 5.76     |\n","|    critic_loss     | 0.854    |\n","|    ent_coef        | 0.00395  |\n","|    ent_coef_loss   | 0.841    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 116895   |\n","---------------------------------\n","Eval num_timesteps=2808000, episode_reward=18.96 +/- 2.39\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 19       |\n","| time/              |          |\n","|    total_timesteps | 2808000  |\n","| train/             |          |\n","|    actor_loss      | 5.58     |\n","|    critic_loss     | 4.83     |\n","|    ent_coef        | 0.00398  |\n","|    ent_coef_loss   | 1.14     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 116995   |\n","---------------------------------\n","Eval num_timesteps=2810400, episode_reward=-12.24 +/- 41.78\n","Episode length: 479.60 +/- 24.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 480      |\n","|    mean_reward     | -12.2    |\n","| time/              |          |\n","|    total_timesteps | 2810400  |\n","| train/             |          |\n","|    actor_loss      | 5.86     |\n","|    critic_loss     | 1.58     |\n","|    ent_coef        | 0.00407  |\n","|    ent_coef_loss   | -0.275   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 117095   |\n","---------------------------------\n","Eval num_timesteps=2812800, episode_reward=-22.73 +/- 39.87\n","Episode length: 411.20 +/- 108.76\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 411      |\n","|    mean_reward     | -22.7    |\n","| time/              |          |\n","|    total_timesteps | 2812800  |\n","| train/             |          |\n","|    actor_loss      | 5.61     |\n","|    critic_loss     | 0.615    |\n","|    ent_coef        | 0.00396  |\n","|    ent_coef_loss   | -0.83    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 117195   |\n","---------------------------------\n","Eval num_timesteps=2815200, episode_reward=-22.30 +/- 46.68\n","Episode length: 378.80 +/- 148.44\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 379      |\n","|    mean_reward     | -22.3    |\n","| time/              |          |\n","|    total_timesteps | 2815200  |\n","| train/             |          |\n","|    actor_loss      | 5.31     |\n","|    critic_loss     | 0.502    |\n","|    ent_coef        | 0.00407  |\n","|    ent_coef_loss   | 1.96     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 117295   |\n","---------------------------------\n","Eval num_timesteps=2817600, episode_reward=-66.05 +/- 2.62\n","Episode length: 413.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 413      |\n","|    mean_reward     | -66.1    |\n","| time/              |          |\n","|    total_timesteps | 2817600  |\n","| train/             |          |\n","|    actor_loss      | 5.17     |\n","|    critic_loss     | 0.307    |\n","|    ent_coef        | 0.00423  |\n","|    ent_coef_loss   | -0.567   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 117395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4660     |\n","|    fps             | 250      |\n","|    time_elapsed    | 11235    |\n","|    total_timesteps | 2817984  |\n","| train/             |          |\n","|    actor_loss      | 5.29     |\n","|    critic_loss     | 0.443    |\n","|    ent_coef        | 0.00427  |\n","|    ent_coef_loss   | 0.535    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 117411   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4670     |\n","|    fps             | 250      |\n","|    time_elapsed    | 11238    |\n","|    total_timesteps | 2819688  |\n","| train/             |          |\n","|    actor_loss      | 5.57     |\n","|    critic_loss     | 0.533    |\n","|    ent_coef        | 0.00428  |\n","|    ent_coef_loss   | -0.983   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 117482   |\n","---------------------------------\n","Eval num_timesteps=2820000, episode_reward=-45.92 +/- 45.43\n","Episode length: 349.40 +/- 122.96\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 349      |\n","|    mean_reward     | -45.9    |\n","| time/              |          |\n","|    total_timesteps | 2820000  |\n","| train/             |          |\n","|    actor_loss      | 5.48     |\n","|    critic_loss     | 7.64     |\n","|    ent_coef        | 0.00427  |\n","|    ent_coef_loss   | -0.951   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 117495   |\n","---------------------------------\n","Eval num_timesteps=2822400, episode_reward=9.82 +/- 10.82\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 9.82     |\n","| time/              |          |\n","|    total_timesteps | 2822400  |\n","| train/             |          |\n","|    actor_loss      | 5.4      |\n","|    critic_loss     | 0.4      |\n","|    ent_coef        | 0.00408  |\n","|    ent_coef_loss   | -2.84    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 117595   |\n","---------------------------------\n","Eval num_timesteps=2824800, episode_reward=4.82 +/- 3.90\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.82     |\n","| time/              |          |\n","|    total_timesteps | 2824800  |\n","| train/             |          |\n","|    actor_loss      | 5.74     |\n","|    critic_loss     | 1.76     |\n","|    ent_coef        | 0.00385  |\n","|    ent_coef_loss   | -0.288   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 117695   |\n","---------------------------------\n","Eval num_timesteps=2827200, episode_reward=-41.83 +/- 48.59\n","Episode length: 311.60 +/- 153.83\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 312      |\n","|    mean_reward     | -41.8    |\n","| time/              |          |\n","|    total_timesteps | 2827200  |\n","| train/             |          |\n","|    actor_loss      | 5.53     |\n","|    critic_loss     | 0.615    |\n","|    ent_coef        | 0.0037   |\n","|    ent_coef_loss   | -0.204   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 117795   |\n","---------------------------------\n","Eval num_timesteps=2829600, episode_reward=-84.52 +/- 1.80\n","Episode length: 266.60 +/- 120.51\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 267      |\n","|    mean_reward     | -84.5    |\n","| time/              |          |\n","|    total_timesteps | 2829600  |\n","| train/             |          |\n","|    actor_loss      | 5.52     |\n","|    critic_loss     | 0.501    |\n","|    ent_coef        | 0.00362  |\n","|    ent_coef_loss   | 0.849    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 117895   |\n","---------------------------------\n","Eval num_timesteps=2832000, episode_reward=0.63 +/- 1.96\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.63     |\n","| time/              |          |\n","|    total_timesteps | 2832000  |\n","| train/             |          |\n","|    actor_loss      | 5.84     |\n","|    critic_loss     | 4.12     |\n","|    ent_coef        | 0.00383  |\n","|    ent_coef_loss   | 0.512    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 117995   |\n","---------------------------------\n","Eval num_timesteps=2834400, episode_reward=-29.91 +/- 40.11\n","Episode length: 419.60 +/- 98.47\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 420      |\n","|    mean_reward     | -29.9    |\n","| time/              |          |\n","|    total_timesteps | 2834400  |\n","| train/             |          |\n","|    actor_loss      | 5.7      |\n","|    critic_loss     | 0.592    |\n","|    ent_coef        | 0.00378  |\n","|    ent_coef_loss   | 0.185    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 118095   |\n","---------------------------------\n","Eval num_timesteps=2836800, episode_reward=-20.41 +/- 42.43\n","Episode length: 397.20 +/- 125.90\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 397      |\n","|    mean_reward     | -20.4    |\n","| time/              |          |\n","|    total_timesteps | 2836800  |\n","| train/             |          |\n","|    actor_loss      | 5.33     |\n","|    critic_loss     | 24.7     |\n","|    ent_coef        | 0.00376  |\n","|    ent_coef_loss   | -2.2     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 118195   |\n","---------------------------------\n","Eval num_timesteps=2839200, episode_reward=-1.17 +/- 3.20\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.17    |\n","| time/              |          |\n","|    total_timesteps | 2839200  |\n","| train/             |          |\n","|    actor_loss      | 5.55     |\n","|    critic_loss     | 13.6     |\n","|    ent_coef        | 0.00368  |\n","|    ent_coef_loss   | 1.83     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 118295   |\n","---------------------------------\n","Eval num_timesteps=2841600, episode_reward=5.12 +/- 0.68\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.12     |\n","| time/              |          |\n","|    total_timesteps | 2841600  |\n","| train/             |          |\n","|    actor_loss      | 5.56     |\n","|    critic_loss     | 0.312    |\n","|    ent_coef        | 0.00367  |\n","|    ent_coef_loss   | -0.037   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 118395   |\n","---------------------------------\n","Eval num_timesteps=2844000, episode_reward=5.36 +/- 0.53\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.36     |\n","| time/              |          |\n","|    total_timesteps | 2844000  |\n","| train/             |          |\n","|    actor_loss      | 5.2      |\n","|    critic_loss     | 0.306    |\n","|    ent_coef        | 0.00371  |\n","|    ent_coef_loss   | -1.09    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 118495   |\n","---------------------------------\n","Eval num_timesteps=2846400, episode_reward=5.41 +/- 0.56\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.41     |\n","| time/              |          |\n","|    total_timesteps | 2846400  |\n","| train/             |          |\n","|    actor_loss      | 5.46     |\n","|    critic_loss     | 0.424    |\n","|    ent_coef        | 0.00367  |\n","|    ent_coef_loss   | 0.0514   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 118595   |\n","---------------------------------\n","Eval num_timesteps=2848800, episode_reward=3.42 +/- 0.84\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.42     |\n","| time/              |          |\n","|    total_timesteps | 2848800  |\n","| train/             |          |\n","|    actor_loss      | 5.7      |\n","|    critic_loss     | 14.2     |\n","|    ent_coef        | 0.00371  |\n","|    ent_coef_loss   | 1.23     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 118695   |\n","---------------------------------\n","Eval num_timesteps=2851200, episode_reward=-48.61 +/- 43.37\n","Episode length: 270.80 +/- 187.14\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 271      |\n","|    mean_reward     | -48.6    |\n","| time/              |          |\n","|    total_timesteps | 2851200  |\n","| train/             |          |\n","|    actor_loss      | 5.49     |\n","|    critic_loss     | 0.311    |\n","|    ent_coef        | 0.0039   |\n","|    ent_coef_loss   | -0.986   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 118795   |\n","---------------------------------\n","Eval num_timesteps=2853600, episode_reward=6.56 +/- 6.27\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.56     |\n","| time/              |          |\n","|    total_timesteps | 2853600  |\n","| train/             |          |\n","|    actor_loss      | 5.63     |\n","|    critic_loss     | 0.609    |\n","|    ent_coef        | 0.00384  |\n","|    ent_coef_loss   | 0.76     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 118895   |\n","---------------------------------\n","Eval num_timesteps=2856000, episode_reward=5.65 +/- 5.08\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.65     |\n","| time/              |          |\n","|    total_timesteps | 2856000  |\n","| train/             |          |\n","|    actor_loss      | 5.34     |\n","|    critic_loss     | 0.479    |\n","|    ent_coef        | 0.00393  |\n","|    ent_coef_loss   | 0.342    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 118995   |\n","---------------------------------\n","Eval num_timesteps=2858400, episode_reward=-81.11 +/- 0.57\n","Episode length: 155.00 +/- 9.80\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 155      |\n","|    mean_reward     | -81.1    |\n","| time/              |          |\n","|    total_timesteps | 2858400  |\n","| train/             |          |\n","|    actor_loss      | 5.53     |\n","|    critic_loss     | 0.533    |\n","|    ent_coef        | 0.00399  |\n","|    ent_coef_loss   | 1.18     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 119095   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4680     |\n","|    fps             | 250      |\n","|    time_elapsed    | 11410    |\n","|    total_timesteps | 2860656  |\n","| train/             |          |\n","|    actor_loss      | 5.74     |\n","|    critic_loss     | 24.4     |\n","|    ent_coef        | 0.00421  |\n","|    ent_coef_loss   | 0.754    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 119189   |\n","---------------------------------\n","Eval num_timesteps=2860800, episode_reward=-32.01 +/- 53.76\n","Episode length: 354.00 +/- 178.81\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 354      |\n","|    mean_reward     | -32      |\n","| time/              |          |\n","|    total_timesteps | 2860800  |\n","| train/             |          |\n","|    actor_loss      | 5.54     |\n","|    critic_loss     | 0.435    |\n","|    ent_coef        | 0.00422  |\n","|    ent_coef_loss   | 0.258    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 119195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4690     |\n","|    fps             | 250      |\n","|    time_elapsed    | 11423    |\n","|    total_timesteps | 2862480  |\n","| train/             |          |\n","|    actor_loss      | 5.17     |\n","|    critic_loss     | 0.288    |\n","|    ent_coef        | 0.00425  |\n","|    ent_coef_loss   | -1.48    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 119265   |\n","---------------------------------\n","Eval num_timesteps=2863200, episode_reward=6.62 +/- 1.03\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.62     |\n","| time/              |          |\n","|    total_timesteps | 2863200  |\n","| train/             |          |\n","|    actor_loss      | 5.92     |\n","|    critic_loss     | 10.6     |\n","|    ent_coef        | 0.00419  |\n","|    ent_coef_loss   | -0.233   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 119295   |\n","---------------------------------\n","Eval num_timesteps=2865600, episode_reward=6.17 +/- 0.32\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.17     |\n","| time/              |          |\n","|    total_timesteps | 2865600  |\n","| train/             |          |\n","|    actor_loss      | 5.54     |\n","|    critic_loss     | 0.551    |\n","|    ent_coef        | 0.00407  |\n","|    ent_coef_loss   | 0.823    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 119395   |\n","---------------------------------\n","Eval num_timesteps=2868000, episode_reward=-33.92 +/- 43.82\n","Episode length: 361.20 +/- 169.99\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 361      |\n","|    mean_reward     | -33.9    |\n","| time/              |          |\n","|    total_timesteps | 2868000  |\n","| train/             |          |\n","|    actor_loss      | 5.66     |\n","|    critic_loss     | 0.949    |\n","|    ent_coef        | 0.00426  |\n","|    ent_coef_loss   | 1.06     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 119495   |\n","---------------------------------\n","Eval num_timesteps=2870400, episode_reward=11.40 +/- 7.13\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 11.4     |\n","| time/              |          |\n","|    total_timesteps | 2870400  |\n","| train/             |          |\n","|    actor_loss      | 5.46     |\n","|    critic_loss     | 0.294    |\n","|    ent_coef        | 0.00414  |\n","|    ent_coef_loss   | -1.56    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 119595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4700     |\n","|    fps             | 250      |\n","|    time_elapsed    | 11466    |\n","|    total_timesteps | 2872632  |\n","| train/             |          |\n","|    actor_loss      | 5.52     |\n","|    critic_loss     | 4.5      |\n","|    ent_coef        | 0.00397  |\n","|    ent_coef_loss   | -2.78    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 119688   |\n","---------------------------------\n","Eval num_timesteps=2872800, episode_reward=-20.38 +/- 39.49\n","Episode length: 430.00 +/- 85.73\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 430      |\n","|    mean_reward     | -20.4    |\n","| time/              |          |\n","|    total_timesteps | 2872800  |\n","| train/             |          |\n","|    actor_loss      | 5.54     |\n","|    critic_loss     | 18.9     |\n","|    ent_coef        | 0.00396  |\n","|    ent_coef_loss   | -1.07    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 119695   |\n","---------------------------------\n","Eval num_timesteps=2875200, episode_reward=6.47 +/- 0.44\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.47     |\n","| time/              |          |\n","|    total_timesteps | 2875200  |\n","| train/             |          |\n","|    actor_loss      | 5.6      |\n","|    critic_loss     | 0.474    |\n","|    ent_coef        | 0.00379  |\n","|    ent_coef_loss   | -2.98    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 119795   |\n","---------------------------------\n","Eval num_timesteps=2877600, episode_reward=-52.46 +/- 46.20\n","Episode length: 285.80 +/- 174.89\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 286      |\n","|    mean_reward     | -52.5    |\n","| time/              |          |\n","|    total_timesteps | 2877600  |\n","| train/             |          |\n","|    actor_loss      | 6.11     |\n","|    critic_loss     | 0.699    |\n","|    ent_coef        | 0.00356  |\n","|    ent_coef_loss   | -1.27    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 119895   |\n","---------------------------------\n","Eval num_timesteps=2880000, episode_reward=3.05 +/- 5.66\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.05     |\n","| time/              |          |\n","|    total_timesteps | 2880000  |\n","| train/             |          |\n","|    actor_loss      | 5.76     |\n","|    critic_loss     | 2.62     |\n","|    ent_coef        | 0.00352  |\n","|    ent_coef_loss   | 1.1      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 119995   |\n","---------------------------------\n","Eval num_timesteps=2882400, episode_reward=5.51 +/- 1.37\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.51     |\n","| time/              |          |\n","|    total_timesteps | 2882400  |\n","| train/             |          |\n","|    actor_loss      | 5.4      |\n","|    critic_loss     | 0.655    |\n","|    ent_coef        | 0.00355  |\n","|    ent_coef_loss   | -0.828   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 120095   |\n","---------------------------------\n","Eval num_timesteps=2884800, episode_reward=-49.36 +/- 42.06\n","Episode length: 288.80 +/- 172.44\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 289      |\n","|    mean_reward     | -49.4    |\n","| time/              |          |\n","|    total_timesteps | 2884800  |\n","| train/             |          |\n","|    actor_loss      | 5.55     |\n","|    critic_loss     | 0.657    |\n","|    ent_coef        | 0.00359  |\n","|    ent_coef_loss   | -0.00328 |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 120195   |\n","---------------------------------\n","Eval num_timesteps=2887200, episode_reward=3.70 +/- 2.00\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.7      |\n","| time/              |          |\n","|    total_timesteps | 2887200  |\n","| train/             |          |\n","|    actor_loss      | 5.9      |\n","|    critic_loss     | 8.8      |\n","|    ent_coef        | 0.00361  |\n","|    ent_coef_loss   | 1.55     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 120295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4710     |\n","|    fps             | 250      |\n","|    time_elapsed    | 11545    |\n","|    total_timesteps | 2888976  |\n","| train/             |          |\n","|    actor_loss      | 5.65     |\n","|    critic_loss     | 0.705    |\n","|    ent_coef        | 0.00363  |\n","|    ent_coef_loss   | 1.01     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 120369   |\n","---------------------------------\n","Eval num_timesteps=2889600, episode_reward=-78.95 +/- 4.49\n","Episode length: 188.00 +/- 24.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 188      |\n","|    mean_reward     | -79      |\n","| time/              |          |\n","|    total_timesteps | 2889600  |\n","| train/             |          |\n","|    actor_loss      | 5.64     |\n","|    critic_loss     | 9.84     |\n","|    ent_coef        | 0.00363  |\n","|    ent_coef_loss   | 0.706    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 120395   |\n","---------------------------------\n","Eval num_timesteps=2892000, episode_reward=-43.39 +/- 41.94\n","Episode length: 375.20 +/- 101.90\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 375      |\n","|    mean_reward     | -43.4    |\n","| time/              |          |\n","|    total_timesteps | 2892000  |\n","| train/             |          |\n","|    actor_loss      | 5.3      |\n","|    critic_loss     | 0.678    |\n","|    ent_coef        | 0.00361  |\n","|    ent_coef_loss   | -1.65    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 120495   |\n","---------------------------------\n","Eval num_timesteps=2894400, episode_reward=-85.02 +/- 0.27\n","Episode length: 164.20 +/- 1.47\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 164      |\n","|    mean_reward     | -85      |\n","| time/              |          |\n","|    total_timesteps | 2894400  |\n","| train/             |          |\n","|    actor_loss      | 5.8      |\n","|    critic_loss     | 12.9     |\n","|    ent_coef        | 0.00372  |\n","|    ent_coef_loss   | -0.739   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 120595   |\n","---------------------------------\n","Eval num_timesteps=2896800, episode_reward=4.21 +/- 2.64\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.21     |\n","| time/              |          |\n","|    total_timesteps | 2896800  |\n","| train/             |          |\n","|    actor_loss      | 5.4      |\n","|    critic_loss     | 0.423    |\n","|    ent_coef        | 0.0037   |\n","|    ent_coef_loss   | 0.646    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 120695   |\n","---------------------------------\n","Eval num_timesteps=2899200, episode_reward=-20.07 +/- 38.96\n","Episode length: 426.80 +/- 89.65\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 427      |\n","|    mean_reward     | -20.1    |\n","| time/              |          |\n","|    total_timesteps | 2899200  |\n","| train/             |          |\n","|    actor_loss      | 5.6      |\n","|    critic_loss     | 12.2     |\n","|    ent_coef        | 0.00371  |\n","|    ent_coef_loss   | -0.172   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 120795   |\n","---------------------------------\n","Eval num_timesteps=2901600, episode_reward=-84.30 +/- 3.81\n","Episode length: 218.40 +/- 24.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 218      |\n","|    mean_reward     | -84.3    |\n","| time/              |          |\n","|    total_timesteps | 2901600  |\n","| train/             |          |\n","|    actor_loss      | 5.74     |\n","|    critic_loss     | 0.622    |\n","|    ent_coef        | 0.0037   |\n","|    ent_coef_loss   | 0.427    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 120895   |\n","---------------------------------\n","Eval num_timesteps=2904000, episode_reward=-35.76 +/- 43.92\n","Episode length: 386.80 +/- 138.64\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 387      |\n","|    mean_reward     | -35.8    |\n","| time/              |          |\n","|    total_timesteps | 2904000  |\n","| train/             |          |\n","|    actor_loss      | 5.47     |\n","|    critic_loss     | 0.475    |\n","|    ent_coef        | 0.00371  |\n","|    ent_coef_loss   | -0.431   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 120995   |\n","---------------------------------\n","Eval num_timesteps=2906400, episode_reward=-73.98 +/- 12.88\n","Episode length: 280.60 +/- 141.58\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 281      |\n","|    mean_reward     | -74      |\n","| time/              |          |\n","|    total_timesteps | 2906400  |\n","| train/             |          |\n","|    actor_loss      | 5.37     |\n","|    critic_loss     | 0.44     |\n","|    ent_coef        | 0.00379  |\n","|    ent_coef_loss   | -1.73    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 121095   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4720     |\n","|    fps             | 250      |\n","|    time_elapsed    | 11614    |\n","|    total_timesteps | 2907504  |\n","| train/             |          |\n","|    actor_loss      | 5.76     |\n","|    critic_loss     | 5.71     |\n","|    ent_coef        | 0.00384  |\n","|    ent_coef_loss   | 2.95     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 121141   |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    episodes        | 4730    |\n","|    fps             | 250     |\n","|    time_elapsed    | 11614   |\n","|    total_timesteps | 2907504 |\n","--------------------------------\n","Eval num_timesteps=2908800, episode_reward=-33.84 +/- 49.00\n","Episode length: 318.80 +/- 147.95\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 319      |\n","|    mean_reward     | -33.8    |\n","| time/              |          |\n","|    total_timesteps | 2908800  |\n","| train/             |          |\n","|    actor_loss      | 5.51     |\n","|    critic_loss     | 0.426    |\n","|    ent_coef        | 0.00399  |\n","|    ent_coef_loss   | 0.982    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 121195   |\n","---------------------------------\n","Eval num_timesteps=2911200, episode_reward=1.71 +/- 2.78\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.71     |\n","| time/              |          |\n","|    total_timesteps | 2911200  |\n","| train/             |          |\n","|    actor_loss      | 5.61     |\n","|    critic_loss     | 0.69     |\n","|    ent_coef        | 0.00395  |\n","|    ent_coef_loss   | -0.811   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 121295   |\n","---------------------------------\n","Eval num_timesteps=2913600, episode_reward=-79.18 +/- 2.03\n","Episode length: 176.60 +/- 42.13\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 177      |\n","|    mean_reward     | -79.2    |\n","| time/              |          |\n","|    total_timesteps | 2913600  |\n","| train/             |          |\n","|    actor_loss      | 5.46     |\n","|    critic_loss     | 0.357    |\n","|    ent_coef        | 0.00391  |\n","|    ent_coef_loss   | -0.21    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 121395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4740     |\n","|    fps             | 250      |\n","|    time_elapsed    | 11642    |\n","|    total_timesteps | 2914992  |\n","| train/             |          |\n","|    actor_loss      | 5.74     |\n","|    critic_loss     | 13.9     |\n","|    ent_coef        | 0.00386  |\n","|    ent_coef_loss   | 0.727    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 121453   |\n","---------------------------------\n","Eval num_timesteps=2916000, episode_reward=-45.87 +/- 37.02\n","Episode length: 411.20 +/- 72.50\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 411      |\n","|    mean_reward     | -45.9    |\n","| time/              |          |\n","|    total_timesteps | 2916000  |\n","| train/             |          |\n","|    actor_loss      | 5.54     |\n","|    critic_loss     | 0.499    |\n","|    ent_coef        | 0.00389  |\n","|    ent_coef_loss   | 1.52     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 121495   |\n","---------------------------------\n","Eval num_timesteps=2918400, episode_reward=-67.12 +/- 4.14\n","Episode length: 376.40 +/- 55.85\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 376      |\n","|    mean_reward     | -67.1    |\n","| time/              |          |\n","|    total_timesteps | 2918400  |\n","| train/             |          |\n","|    actor_loss      | 5.64     |\n","|    critic_loss     | 14       |\n","|    ent_coef        | 0.00385  |\n","|    ent_coef_loss   | -1.03    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 121595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4750     |\n","|    fps             | 250      |\n","|    time_elapsed    | 11661    |\n","|    total_timesteps | 2920272  |\n","| train/             |          |\n","|    actor_loss      | 5.45     |\n","|    critic_loss     | 0.248    |\n","|    ent_coef        | 0.00375  |\n","|    ent_coef_loss   | -0.0506  |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 121673   |\n","---------------------------------\n","Eval num_timesteps=2920800, episode_reward=5.76 +/- 1.08\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.76     |\n","| time/              |          |\n","|    total_timesteps | 2920800  |\n","| train/             |          |\n","|    actor_loss      | 5.97     |\n","|    critic_loss     | 10.4     |\n","|    ent_coef        | 0.00374  |\n","|    ent_coef_loss   | 1.65     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 121695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4760     |\n","|    fps             | 250      |\n","|    time_elapsed    | 11675    |\n","|    total_timesteps | 2923080  |\n","| train/             |          |\n","|    actor_loss      | 5.2      |\n","|    critic_loss     | 0.874    |\n","|    ent_coef        | 0.00386  |\n","|    ent_coef_loss   | 0.914    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 121790   |\n","---------------------------------\n","Eval num_timesteps=2923200, episode_reward=-72.69 +/- 1.81\n","Episode length: 291.80 +/- 57.32\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 292      |\n","|    mean_reward     | -72.7    |\n","| time/              |          |\n","|    total_timesteps | 2923200  |\n","| train/             |          |\n","|    actor_loss      | 5.53     |\n","|    critic_loss     | 0.274    |\n","|    ent_coef        | 0.00387  |\n","|    ent_coef_loss   | -0.991   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 121795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4770     |\n","|    fps             | 250      |\n","|    time_elapsed    | 11681    |\n","|    total_timesteps | 2924808  |\n","| train/             |          |\n","|    actor_loss      | 5.25     |\n","|    critic_loss     | 0.874    |\n","|    ent_coef        | 0.00392  |\n","|    ent_coef_loss   | 1.41     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 121862   |\n","---------------------------------\n","Eval num_timesteps=2925600, episode_reward=10.74 +/- 4.91\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 10.7     |\n","| time/              |          |\n","|    total_timesteps | 2925600  |\n","| train/             |          |\n","|    actor_loss      | 5.41     |\n","|    critic_loss     | 14.1     |\n","|    ent_coef        | 0.00405  |\n","|    ent_coef_loss   | 2.33     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 121895   |\n","---------------------------------\n","Eval num_timesteps=2928000, episode_reward=-25.47 +/- 40.86\n","Episode length: 376.00 +/- 151.87\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 376      |\n","|    mean_reward     | -25.5    |\n","| time/              |          |\n","|    total_timesteps | 2928000  |\n","| train/             |          |\n","|    actor_loss      | 5.37     |\n","|    critic_loss     | 0.318    |\n","|    ent_coef        | 0.00424  |\n","|    ent_coef_loss   | 4.84     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 121995   |\n","---------------------------------\n","Eval num_timesteps=2930400, episode_reward=-32.44 +/- 41.59\n","Episode length: 480.00 +/- 24.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 480      |\n","|    mean_reward     | -32.4    |\n","| time/              |          |\n","|    total_timesteps | 2930400  |\n","| train/             |          |\n","|    actor_loss      | 5.28     |\n","|    critic_loss     | 0.303    |\n","|    ent_coef        | 0.0045   |\n","|    ent_coef_loss   | 1.39     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 122095   |\n","---------------------------------\n","Eval num_timesteps=2932800, episode_reward=-22.87 +/- 50.42\n","Episode length: 373.60 +/- 154.81\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 374      |\n","|    mean_reward     | -22.9    |\n","| time/              |          |\n","|    total_timesteps | 2932800  |\n","| train/             |          |\n","|    actor_loss      | 5.49     |\n","|    critic_loss     | 0.412    |\n","|    ent_coef        | 0.00457  |\n","|    ent_coef_loss   | -0.538   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 122195   |\n","---------------------------------\n","Eval num_timesteps=2935200, episode_reward=14.77 +/- 0.32\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 14.8     |\n","| time/              |          |\n","|    total_timesteps | 2935200  |\n","| train/             |          |\n","|    actor_loss      | 5.86     |\n","|    critic_loss     | 0.778    |\n","|    ent_coef        | 0.00453  |\n","|    ent_coef_loss   | 0.0853   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 122295   |\n","---------------------------------\n","Eval num_timesteps=2937600, episode_reward=-19.32 +/- 36.38\n","Episode length: 442.40 +/- 70.55\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 442      |\n","|    mean_reward     | -19.3    |\n","| time/              |          |\n","|    total_timesteps | 2937600  |\n","| train/             |          |\n","|    actor_loss      | 5.89     |\n","|    critic_loss     | 1.02     |\n","|    ent_coef        | 0.00467  |\n","|    ent_coef_loss   | 1.97     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 122395   |\n","---------------------------------\n","Eval num_timesteps=2940000, episode_reward=-80.18 +/- 1.27\n","Episode length: 182.20 +/- 52.91\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 182      |\n","|    mean_reward     | -80.2    |\n","| time/              |          |\n","|    total_timesteps | 2940000  |\n","| train/             |          |\n","|    actor_loss      | 5.58     |\n","|    critic_loss     | 12.9     |\n","|    ent_coef        | 0.00457  |\n","|    ent_coef_loss   | 0.244    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 122495   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4780     |\n","|    fps             | 250      |\n","|    time_elapsed    | 11749    |\n","|    total_timesteps | 2940360  |\n","| train/             |          |\n","|    actor_loss      | 5.54     |\n","|    critic_loss     | 0.496    |\n","|    ent_coef        | 0.00456  |\n","|    ent_coef_loss   | 0.855    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 122510   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4790     |\n","|    fps             | 250      |\n","|    time_elapsed    | 11751    |\n","|    total_timesteps | 2942304  |\n","| train/             |          |\n","|    actor_loss      | 5.18     |\n","|    critic_loss     | 13.2     |\n","|    ent_coef        | 0.00444  |\n","|    ent_coef_loss   | -3.36    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 122591   |\n","---------------------------------\n","Eval num_timesteps=2942400, episode_reward=10.01 +/- 0.97\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 10       |\n","| time/              |          |\n","|    total_timesteps | 2942400  |\n","| train/             |          |\n","|    actor_loss      | 5.65     |\n","|    critic_loss     | 19.8     |\n","|    ent_coef        | 0.00444  |\n","|    ent_coef_loss   | -0.292   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 122595   |\n","---------------------------------\n","Eval num_timesteps=2944800, episode_reward=-81.15 +/- 3.61\n","Episode length: 336.60 +/- 0.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 337      |\n","|    mean_reward     | -81.1    |\n","| time/              |          |\n","|    total_timesteps | 2944800  |\n","| train/             |          |\n","|    actor_loss      | 5.36     |\n","|    critic_loss     | 0.264    |\n","|    ent_coef        | 0.00426  |\n","|    ent_coef_loss   | -2.59    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 122695   |\n","---------------------------------\n","Eval num_timesteps=2947200, episode_reward=-44.51 +/- 40.29\n","Episode length: 392.60 +/- 87.69\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 393      |\n","|    mean_reward     | -44.5    |\n","| time/              |          |\n","|    total_timesteps | 2947200  |\n","| train/             |          |\n","|    actor_loss      | 5.01     |\n","|    critic_loss     | 0.566    |\n","|    ent_coef        | 0.0042   |\n","|    ent_coef_loss   | 2.27     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 122795   |\n","---------------------------------\n","Eval num_timesteps=2949600, episode_reward=21.81 +/- 0.62\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 21.8     |\n","| time/              |          |\n","|    total_timesteps | 2949600  |\n","| train/             |          |\n","|    actor_loss      | 5.31     |\n","|    critic_loss     | 0.622    |\n","|    ent_coef        | 0.00438  |\n","|    ent_coef_loss   | -1.55    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 122895   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4800     |\n","|    fps             | 250      |\n","|    time_elapsed    | 11795    |\n","|    total_timesteps | 2950344  |\n","| train/             |          |\n","|    actor_loss      | 6.02     |\n","|    critic_loss     | 0.738    |\n","|    ent_coef        | 0.00432  |\n","|    ent_coef_loss   | -0.153   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 122926   |\n","---------------------------------\n","Eval num_timesteps=2952000, episode_reward=2.50 +/- 6.90\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.5      |\n","| time/              |          |\n","|    total_timesteps | 2952000  |\n","| train/             |          |\n","|    actor_loss      | 5.62     |\n","|    critic_loss     | 0.409    |\n","|    ent_coef        | 0.00421  |\n","|    ent_coef_loss   | -1.85    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 122995   |\n","---------------------------------\n","Eval num_timesteps=2954400, episode_reward=-35.03 +/- 42.22\n","Episode length: 348.80 +/- 185.18\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 349      |\n","|    mean_reward     | -35      |\n","| time/              |          |\n","|    total_timesteps | 2954400  |\n","| train/             |          |\n","|    actor_loss      | 5.38     |\n","|    critic_loss     | 0.253    |\n","|    ent_coef        | 0.00402  |\n","|    ent_coef_loss   | -1.47    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 123095   |\n","---------------------------------\n","Eval num_timesteps=2956800, episode_reward=-47.86 +/- 45.78\n","Episode length: 348.20 +/- 123.94\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 348      |\n","|    mean_reward     | -47.9    |\n","| time/              |          |\n","|    total_timesteps | 2956800  |\n","| train/             |          |\n","|    actor_loss      | 5.74     |\n","|    critic_loss     | 3.25     |\n","|    ent_coef        | 0.0039   |\n","|    ent_coef_loss   | 0.235    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 123195   |\n","---------------------------------\n","Eval num_timesteps=2959200, episode_reward=-24.56 +/- 38.82\n","Episode length: 364.40 +/- 166.08\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 364      |\n","|    mean_reward     | -24.6    |\n","| time/              |          |\n","|    total_timesteps | 2959200  |\n","| train/             |          |\n","|    actor_loss      | 5.46     |\n","|    critic_loss     | 25.5     |\n","|    ent_coef        | 0.00386  |\n","|    ent_coef_loss   | 0.21     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 123295   |\n","---------------------------------\n","Eval num_timesteps=2961600, episode_reward=-82.04 +/- 0.37\n","Episode length: 203.00 +/- 31.84\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 203      |\n","|    mean_reward     | -82      |\n","| time/              |          |\n","|    total_timesteps | 2961600  |\n","| train/             |          |\n","|    actor_loss      | 5.25     |\n","|    critic_loss     | 1.97     |\n","|    ent_coef        | 0.00386  |\n","|    ent_coef_loss   | -0.189   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 123395   |\n","---------------------------------\n","Eval num_timesteps=2964000, episode_reward=-27.73 +/- 39.77\n","Episode length: 375.20 +/- 152.85\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 375      |\n","|    mean_reward     | -27.7    |\n","| time/              |          |\n","|    total_timesteps | 2964000  |\n","| train/             |          |\n","|    actor_loss      | 5.21     |\n","|    critic_loss     | 0.356    |\n","|    ent_coef        | 0.0039   |\n","|    ent_coef_loss   | -0.11    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 123495   |\n","---------------------------------\n","Eval num_timesteps=2966400, episode_reward=4.69 +/- 2.92\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.69     |\n","| time/              |          |\n","|    total_timesteps | 2966400  |\n","| train/             |          |\n","|    actor_loss      | 5.72     |\n","|    critic_loss     | 0.726    |\n","|    ent_coef        | 0.004    |\n","|    ent_coef_loss   | -1.05    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 123595   |\n","---------------------------------\n","Eval num_timesteps=2968800, episode_reward=2.08 +/- 0.48\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.08     |\n","| time/              |          |\n","|    total_timesteps | 2968800  |\n","| train/             |          |\n","|    actor_loss      | 5.43     |\n","|    critic_loss     | 0.358    |\n","|    ent_coef        | 0.00411  |\n","|    ent_coef_loss   | 2.24     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 123695   |\n","---------------------------------\n","Eval num_timesteps=2971200, episode_reward=-42.29 +/- 39.77\n","Episode length: 286.40 +/- 174.40\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 286      |\n","|    mean_reward     | -42.3    |\n","| time/              |          |\n","|    total_timesteps | 2971200  |\n","| train/             |          |\n","|    actor_loss      | 5.6      |\n","|    critic_loss     | 12.4     |\n","|    ent_coef        | 0.00417  |\n","|    ent_coef_loss   | 0.996    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 123795   |\n","---------------------------------\n","Eval num_timesteps=2973600, episode_reward=-93.08 +/- 0.39\n","Episode length: 132.60 +/- 10.29\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 133      |\n","|    mean_reward     | -93.1    |\n","| time/              |          |\n","|    total_timesteps | 2973600  |\n","| train/             |          |\n","|    actor_loss      | 5.09     |\n","|    critic_loss     | 0.235    |\n","|    ent_coef        | 0.00412  |\n","|    ent_coef_loss   | -2.46    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 123895   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4810     |\n","|    fps             | 250      |\n","|    time_elapsed    | 11891    |\n","|    total_timesteps | 2975592  |\n","| train/             |          |\n","|    actor_loss      | 5.36     |\n","|    critic_loss     | 15.4     |\n","|    ent_coef        | 0.00403  |\n","|    ent_coef_loss   | -0.613   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 123978   |\n","---------------------------------\n","Eval num_timesteps=2976000, episode_reward=-1.24 +/- 1.09\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.24    |\n","| time/              |          |\n","|    total_timesteps | 2976000  |\n","| train/             |          |\n","|    actor_loss      | 5.83     |\n","|    critic_loss     | 6.38     |\n","|    ent_coef        | 0.00404  |\n","|    ent_coef_loss   | -1.11    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 123995   |\n","---------------------------------\n","Eval num_timesteps=2978400, episode_reward=-98.94 +/- 7.97\n","Episode length: 202.80 +/- 32.82\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 203      |\n","|    mean_reward     | -98.9    |\n","| time/              |          |\n","|    total_timesteps | 2978400  |\n","| train/             |          |\n","|    actor_loss      | 5.74     |\n","|    critic_loss     | 1.09     |\n","|    ent_coef        | 0.00412  |\n","|    ent_coef_loss   | 1.28     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 124095   |\n","---------------------------------\n","Eval num_timesteps=2980800, episode_reward=-45.23 +/- 41.72\n","Episode length: 301.40 +/- 162.16\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 301      |\n","|    mean_reward     | -45.2    |\n","| time/              |          |\n","|    total_timesteps | 2980800  |\n","| train/             |          |\n","|    actor_loss      | 5.61     |\n","|    critic_loss     | 0.434    |\n","|    ent_coef        | 0.0042   |\n","|    ent_coef_loss   | 2.71     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 124195   |\n","---------------------------------\n","Eval num_timesteps=2983200, episode_reward=-39.32 +/- 39.03\n","Episode length: 372.20 +/- 104.35\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 372      |\n","|    mean_reward     | -39.3    |\n","| time/              |          |\n","|    total_timesteps | 2983200  |\n","| train/             |          |\n","|    actor_loss      | 5.22     |\n","|    critic_loss     | 0.728    |\n","|    ent_coef        | 0.00447  |\n","|    ent_coef_loss   | -0.838   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 124295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4820     |\n","|    fps             | 250      |\n","|    time_elapsed    | 11928    |\n","|    total_timesteps | 2984184  |\n","| train/             |          |\n","|    actor_loss      | 5.66     |\n","|    critic_loss     | 0.324    |\n","|    ent_coef        | 0.00445  |\n","|    ent_coef_loss   | -1.99    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 124336   |\n","---------------------------------\n","Eval num_timesteps=2985600, episode_reward=-52.45 +/- 52.91\n","Episode length: 416.60 +/- 68.10\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 417      |\n","|    mean_reward     | -52.5    |\n","| time/              |          |\n","|    total_timesteps | 2985600  |\n","| train/             |          |\n","|    actor_loss      | 5.7      |\n","|    critic_loss     | 3.74     |\n","|    ent_coef        | 0.00436  |\n","|    ent_coef_loss   | -0.962   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 124395   |\n","---------------------------------\n","Eval num_timesteps=2988000, episode_reward=-79.65 +/- 6.74\n","Episode length: 322.40 +/- 83.77\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 322      |\n","|    mean_reward     | -79.7    |\n","| time/              |          |\n","|    total_timesteps | 2988000  |\n","| train/             |          |\n","|    actor_loss      | 5.5      |\n","|    critic_loss     | 0.409    |\n","|    ent_coef        | 0.00438  |\n","|    ent_coef_loss   | -1.85    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 124495   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4830     |\n","|    fps             | 250      |\n","|    time_elapsed    | 11948    |\n","|    total_timesteps | 2988576  |\n","| train/             |          |\n","|    actor_loss      | 5.51     |\n","|    critic_loss     | 13.4     |\n","|    ent_coef        | 0.00439  |\n","|    ent_coef_loss   | 1.79     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 124519   |\n","---------------------------------\n","Eval num_timesteps=2990400, episode_reward=-29.82 +/- 37.20\n","Episode length: 361.60 +/- 169.50\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 362      |\n","|    mean_reward     | -29.8    |\n","| time/              |          |\n","|    total_timesteps | 2990400  |\n","| train/             |          |\n","|    actor_loss      | 5.74     |\n","|    critic_loss     | 25.7     |\n","|    ent_coef        | 0.00444  |\n","|    ent_coef_loss   | 0.0584   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 124595   |\n","---------------------------------\n","Eval num_timesteps=2992800, episode_reward=-79.50 +/- 2.88\n","Episode length: 249.00 +/- 110.23\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 249      |\n","|    mean_reward     | -79.5    |\n","| time/              |          |\n","|    total_timesteps | 2992800  |\n","| train/             |          |\n","|    actor_loss      | 5.44     |\n","|    critic_loss     | 0.446    |\n","|    ent_coef        | 0.00432  |\n","|    ent_coef_loss   | 0.183    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 124695   |\n","---------------------------------\n","Eval num_timesteps=2995200, episode_reward=-44.07 +/- 33.40\n","Episode length: 408.20 +/- 74.95\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 408      |\n","|    mean_reward     | -44.1    |\n","| time/              |          |\n","|    total_timesteps | 2995200  |\n","| train/             |          |\n","|    actor_loss      | 5.62     |\n","|    critic_loss     | 0.908    |\n","|    ent_coef        | 0.00422  |\n","|    ent_coef_loss   | -1.82    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 124795   |\n","---------------------------------\n","Eval num_timesteps=2997600, episode_reward=-32.43 +/- 40.06\n","Episode length: 357.20 +/- 174.89\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 357      |\n","|    mean_reward     | -32.4    |\n","| time/              |          |\n","|    total_timesteps | 2997600  |\n","| train/             |          |\n","|    actor_loss      | 5.67     |\n","|    critic_loss     | 13.4     |\n","|    ent_coef        | 0.00399  |\n","|    ent_coef_loss   | -1.28    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 124895   |\n","---------------------------------\n","Eval num_timesteps=3000000, episode_reward=-29.28 +/- 39.75\n","Episode length: 377.60 +/- 149.91\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 378      |\n","|    mean_reward     | -29.3    |\n","| time/              |          |\n","|    total_timesteps | 3000000  |\n","| train/             |          |\n","|    actor_loss      | 5.34     |\n","|    critic_loss     | 7.16     |\n","|    ent_coef        | 0.00387  |\n","|    ent_coef_loss   | -0.974   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 124995   |\n","---------------------------------\n","Eval num_timesteps=3002400, episode_reward=-84.63 +/- 4.17\n","Episode length: 190.40 +/- 24.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 190      |\n","|    mean_reward     | -84.6    |\n","| time/              |          |\n","|    total_timesteps | 3002400  |\n","| train/             |          |\n","|    actor_loss      | 5.81     |\n","|    critic_loss     | 0.46     |\n","|    ent_coef        | 0.00379  |\n","|    ent_coef_loss   | 2.3      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 125095   |\n","---------------------------------\n","Eval num_timesteps=3004800, episode_reward=-43.32 +/- 37.27\n","Episode length: 357.20 +/- 116.60\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 357      |\n","|    mean_reward     | -43.3    |\n","| time/              |          |\n","|    total_timesteps | 3004800  |\n","| train/             |          |\n","|    actor_loss      | 5.52     |\n","|    critic_loss     | 0.502    |\n","|    ent_coef        | 0.00376  |\n","|    ent_coef_loss   | 0.321    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 125195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4840     |\n","|    fps             | 250      |\n","|    time_elapsed    | 12013    |\n","|    total_timesteps | 3005304  |\n","| train/             |          |\n","|    actor_loss      | 5.65     |\n","|    critic_loss     | 1.53     |\n","|    ent_coef        | 0.00377  |\n","|    ent_coef_loss   | 1.51     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 125216   |\n","---------------------------------\n","Eval num_timesteps=3007200, episode_reward=-87.59 +/- 1.59\n","Episode length: 183.00 +/- 36.74\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 183      |\n","|    mean_reward     | -87.6    |\n","| time/              |          |\n","|    total_timesteps | 3007200  |\n","| train/             |          |\n","|    actor_loss      | 5.66     |\n","|    critic_loss     | 14.1     |\n","|    ent_coef        | 0.00383  |\n","|    ent_coef_loss   | 2.02     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 125295   |\n","---------------------------------\n","Eval num_timesteps=3009600, episode_reward=0.82 +/- 3.26\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.817    |\n","| time/              |          |\n","|    total_timesteps | 3009600  |\n","| train/             |          |\n","|    actor_loss      | 5.91     |\n","|    critic_loss     | 0.634    |\n","|    ent_coef        | 0.00397  |\n","|    ent_coef_loss   | 2.09     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 125395   |\n","---------------------------------\n","Eval num_timesteps=3012000, episode_reward=-89.77 +/- 0.40\n","Episode length: 105.60 +/- 26.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 106      |\n","|    mean_reward     | -89.8    |\n","| time/              |          |\n","|    total_timesteps | 3012000  |\n","| train/             |          |\n","|    actor_loss      | 5.82     |\n","|    critic_loss     | 0.911    |\n","|    ent_coef        | 0.00398  |\n","|    ent_coef_loss   | 2.21     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 125495   |\n","---------------------------------\n","Eval num_timesteps=3014400, episode_reward=0.10 +/- 4.72\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.0964   |\n","| time/              |          |\n","|    total_timesteps | 3014400  |\n","| train/             |          |\n","|    actor_loss      | 5.33     |\n","|    critic_loss     | 0.264    |\n","|    ent_coef        | 0.00407  |\n","|    ent_coef_loss   | 0.555    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 125595   |\n","---------------------------------\n","Eval num_timesteps=3016800, episode_reward=-90.88 +/- 1.82\n","Episode length: 312.20 +/- 101.90\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 312      |\n","|    mean_reward     | -90.9    |\n","| time/              |          |\n","|    total_timesteps | 3016800  |\n","| train/             |          |\n","|    actor_loss      | 5.53     |\n","|    critic_loss     | 0.413    |\n","|    ent_coef        | 0.00416  |\n","|    ent_coef_loss   | 1.06     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 125695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4850     |\n","|    fps             | 250      |\n","|    time_elapsed    | 12061    |\n","|    total_timesteps | 3018312  |\n","| train/             |          |\n","|    actor_loss      | 5.75     |\n","|    critic_loss     | 0.888    |\n","|    ent_coef        | 0.0042   |\n","|    ent_coef_loss   | 1.14     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 125758   |\n","---------------------------------\n","Eval num_timesteps=3019200, episode_reward=15.23 +/- 3.65\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 15.2     |\n","| time/              |          |\n","|    total_timesteps | 3019200  |\n","| train/             |          |\n","|    actor_loss      | 5.41     |\n","|    critic_loss     | 0.693    |\n","|    ent_coef        | 0.00423  |\n","|    ent_coef_loss   | -1.25    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 125795   |\n","---------------------------------\n","Eval num_timesteps=3021600, episode_reward=-33.25 +/- 38.06\n","Episode length: 384.20 +/- 94.55\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 384      |\n","|    mean_reward     | -33.2    |\n","| time/              |          |\n","|    total_timesteps | 3021600  |\n","| train/             |          |\n","|    actor_loss      | 5.39     |\n","|    critic_loss     | 11.9     |\n","|    ent_coef        | 0.00409  |\n","|    ent_coef_loss   | -3.01    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 125895   |\n","---------------------------------\n","Eval num_timesteps=3024000, episode_reward=7.96 +/- 1.57\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.96     |\n","| time/              |          |\n","|    total_timesteps | 3024000  |\n","| train/             |          |\n","|    actor_loss      | 5.19     |\n","|    critic_loss     | 0.429    |\n","|    ent_coef        | 0.00393  |\n","|    ent_coef_loss   | -0.789   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 125995   |\n","---------------------------------\n","Eval num_timesteps=3026400, episode_reward=4.53 +/- 2.99\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.53     |\n","| time/              |          |\n","|    total_timesteps | 3026400  |\n","| train/             |          |\n","|    actor_loss      | 5.78     |\n","|    critic_loss     | 0.592    |\n","|    ent_coef        | 0.00393  |\n","|    ent_coef_loss   | 1.37     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 126095   |\n","---------------------------------\n","Eval num_timesteps=3028800, episode_reward=-75.96 +/- 2.62\n","Episode length: 184.00 +/- 2.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 184      |\n","|    mean_reward     | -76      |\n","| time/              |          |\n","|    total_timesteps | 3028800  |\n","| train/             |          |\n","|    actor_loss      | 5.43     |\n","|    critic_loss     | 0.486    |\n","|    ent_coef        | 0.00389  |\n","|    ent_coef_loss   | -1.09    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 126195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4860     |\n","|    fps             | 250      |\n","|    time_elapsed    | 12105    |\n","|    total_timesteps | 3029280  |\n","| train/             |          |\n","|    actor_loss      | 5.73     |\n","|    critic_loss     | 0.376    |\n","|    ent_coef        | 0.00388  |\n","|    ent_coef_loss   | 1.57     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 126215   |\n","---------------------------------\n","Eval num_timesteps=3031200, episode_reward=-29.75 +/- 41.16\n","Episode length: 405.60 +/- 115.62\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 406      |\n","|    mean_reward     | -29.7    |\n","| time/              |          |\n","|    total_timesteps | 3031200  |\n","| train/             |          |\n","|    actor_loss      | 5.4      |\n","|    critic_loss     | 0.305    |\n","|    ent_coef        | 0.00384  |\n","|    ent_coef_loss   | -1.04    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 126295   |\n","---------------------------------\n","Eval num_timesteps=3033600, episode_reward=7.73 +/- 8.76\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.73     |\n","| time/              |          |\n","|    total_timesteps | 3033600  |\n","| train/             |          |\n","|    actor_loss      | 5.38     |\n","|    critic_loss     | 0.334    |\n","|    ent_coef        | 0.00366  |\n","|    ent_coef_loss   | -1.32    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 126395   |\n","---------------------------------\n","Eval num_timesteps=3036000, episode_reward=-24.00 +/- 33.90\n","Episode length: 482.80 +/- 21.07\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 483      |\n","|    mean_reward     | -24      |\n","| time/              |          |\n","|    total_timesteps | 3036000  |\n","| train/             |          |\n","|    actor_loss      | 5.51     |\n","|    critic_loss     | 13.9     |\n","|    ent_coef        | 0.00353  |\n","|    ent_coef_loss   | 1.33     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 126495   |\n","---------------------------------\n","Eval num_timesteps=3038400, episode_reward=3.71 +/- 0.84\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.71     |\n","| time/              |          |\n","|    total_timesteps | 3038400  |\n","| train/             |          |\n","|    actor_loss      | 4.92     |\n","|    critic_loss     | 2.92     |\n","|    ent_coef        | 0.00368  |\n","|    ent_coef_loss   | -1.98    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 126595   |\n","---------------------------------\n","Eval num_timesteps=3040800, episode_reward=3.79 +/- 2.47\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.79     |\n","| time/              |          |\n","|    total_timesteps | 3040800  |\n","| train/             |          |\n","|    actor_loss      | 5.41     |\n","|    critic_loss     | 0.37     |\n","|    ent_coef        | 0.00378  |\n","|    ent_coef_loss   | -1.76    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 126695   |\n","---------------------------------\n","Eval num_timesteps=3043200, episode_reward=-57.28 +/- 47.04\n","Episode length: 319.40 +/- 147.46\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 319      |\n","|    mean_reward     | -57.3    |\n","| time/              |          |\n","|    total_timesteps | 3043200  |\n","| train/             |          |\n","|    actor_loss      | 5.1      |\n","|    critic_loss     | 0.247    |\n","|    ent_coef        | 0.00358  |\n","|    ent_coef_loss   | -2.63    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 126795   |\n","---------------------------------\n","Eval num_timesteps=3045600, episode_reward=-2.63 +/- 1.76\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -2.63    |\n","| time/              |          |\n","|    total_timesteps | 3045600  |\n","| train/             |          |\n","|    actor_loss      | 5.79     |\n","|    critic_loss     | 0.525    |\n","|    ent_coef        | 0.00353  |\n","|    ent_coef_loss   | 1.92     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 126895   |\n","---------------------------------\n","Eval num_timesteps=3048000, episode_reward=-31.49 +/- 35.36\n","Episode length: 394.40 +/- 129.33\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 394      |\n","|    mean_reward     | -31.5    |\n","| time/              |          |\n","|    total_timesteps | 3048000  |\n","| train/             |          |\n","|    actor_loss      | 5.6      |\n","|    critic_loss     | 0.442    |\n","|    ent_coef        | 0.00353  |\n","|    ent_coef_loss   | 1.36     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 126995   |\n","---------------------------------\n","Eval num_timesteps=3050400, episode_reward=-31.84 +/- 37.03\n","Episode length: 369.20 +/- 160.20\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 369      |\n","|    mean_reward     | -31.8    |\n","| time/              |          |\n","|    total_timesteps | 3050400  |\n","| train/             |          |\n","|    actor_loss      | 5.45     |\n","|    critic_loss     | 0.412    |\n","|    ent_coef        | 0.00347  |\n","|    ent_coef_loss   | -0.511   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 127095   |\n","---------------------------------\n","Eval num_timesteps=3052800, episode_reward=-76.92 +/- 0.26\n","Episode length: 448.40 +/- 28.90\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 448      |\n","|    mean_reward     | -76.9    |\n","| time/              |          |\n","|    total_timesteps | 3052800  |\n","| train/             |          |\n","|    actor_loss      | 5.34     |\n","|    critic_loss     | 0.776    |\n","|    ent_coef        | 0.00342  |\n","|    ent_coef_loss   | -1       |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 127195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4870     |\n","|    fps             | 250      |\n","|    time_elapsed    | 12204    |\n","|    total_timesteps | 3053472  |\n","| train/             |          |\n","|    actor_loss      | 5.31     |\n","|    critic_loss     | 0.755    |\n","|    ent_coef        | 0.00341  |\n","|    ent_coef_loss   | 0.223    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 127223   |\n","---------------------------------\n","Eval num_timesteps=3055200, episode_reward=-56.95 +/- 44.90\n","Episode length: 285.80 +/- 174.89\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 286      |\n","|    mean_reward     | -57      |\n","| time/              |          |\n","|    total_timesteps | 3055200  |\n","| train/             |          |\n","|    actor_loss      | 5.11     |\n","|    critic_loss     | 0.424    |\n","|    ent_coef        | 0.00343  |\n","|    ent_coef_loss   | -0.796   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 127295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4880     |\n","|    fps             | 250      |\n","|    time_elapsed    | 12218    |\n","|    total_timesteps | 3057192  |\n","| train/             |          |\n","|    actor_loss      | 5.5      |\n","|    critic_loss     | 0.678    |\n","|    ent_coef        | 0.00348  |\n","|    ent_coef_loss   | 1.33     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 127378   |\n","---------------------------------\n","Eval num_timesteps=3057600, episode_reward=-45.92 +/- 45.83\n","Episode length: 401.00 +/- 80.83\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 401      |\n","|    mean_reward     | -45.9    |\n","| time/              |          |\n","|    total_timesteps | 3057600  |\n","| train/             |          |\n","|    actor_loss      | 5.45     |\n","|    critic_loss     | 10.3     |\n","|    ent_coef        | 0.00348  |\n","|    ent_coef_loss   | -0.526   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 127395   |\n","---------------------------------\n","Eval num_timesteps=3060000, episode_reward=-66.24 +/- 3.20\n","Episode length: 309.20 +/- 79.85\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 309      |\n","|    mean_reward     | -66.2    |\n","| time/              |          |\n","|    total_timesteps | 3060000  |\n","| train/             |          |\n","|    actor_loss      | 5.23     |\n","|    critic_loss     | 0.282    |\n","|    ent_coef        | 0.00344  |\n","|    ent_coef_loss   | -1.76    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 127495   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4890     |\n","|    fps             | 250      |\n","|    time_elapsed    | 12236    |\n","|    total_timesteps | 3062232  |\n","| train/             |          |\n","|    actor_loss      | 5.81     |\n","|    critic_loss     | 0.564    |\n","|    ent_coef        | 0.00351  |\n","|    ent_coef_loss   | 2.58     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 127588   |\n","---------------------------------\n","Eval num_timesteps=3062400, episode_reward=-73.97 +/- 1.96\n","Episode length: 162.40 +/- 4.41\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 162      |\n","|    mean_reward     | -74      |\n","| time/              |          |\n","|    total_timesteps | 3062400  |\n","| train/             |          |\n","|    actor_loss      | 5.59     |\n","|    critic_loss     | 0.511    |\n","|    ent_coef        | 0.00352  |\n","|    ent_coef_loss   | 3.62     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 127595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4900     |\n","|    fps             | 250      |\n","|    time_elapsed    | 12240    |\n","|    total_timesteps | 3062904  |\n","| train/             |          |\n","|    actor_loss      | 5.37     |\n","|    critic_loss     | 0.701    |\n","|    ent_coef        | 0.00361  |\n","|    ent_coef_loss   | 4.18     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 127616   |\n","---------------------------------\n","Eval num_timesteps=3064800, episode_reward=-25.51 +/- 45.67\n","Episode length: 341.20 +/- 194.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 341      |\n","|    mean_reward     | -25.5    |\n","| time/              |          |\n","|    total_timesteps | 3064800  |\n","| train/             |          |\n","|    actor_loss      | 5.72     |\n","|    critic_loss     | 13.2     |\n","|    ent_coef        | 0.00388  |\n","|    ent_coef_loss   | 0.62     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 127695   |\n","---------------------------------\n","Eval num_timesteps=3067200, episode_reward=10.74 +/- 0.05\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 10.7     |\n","| time/              |          |\n","|    total_timesteps | 3067200  |\n","| train/             |          |\n","|    actor_loss      | 5.47     |\n","|    critic_loss     | 0.339    |\n","|    ent_coef        | 0.00387  |\n","|    ent_coef_loss   | 1.55     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 127795   |\n","---------------------------------\n","Eval num_timesteps=3069600, episode_reward=8.20 +/- 0.35\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 8.2      |\n","| time/              |          |\n","|    total_timesteps | 3069600  |\n","| train/             |          |\n","|    actor_loss      | 5.65     |\n","|    critic_loss     | 16.3     |\n","|    ent_coef        | 0.00394  |\n","|    ent_coef_loss   | 1.6      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 127895   |\n","---------------------------------\n","Eval num_timesteps=3072000, episode_reward=-50.73 +/- 41.03\n","Episode length: 299.00 +/- 164.12\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 299      |\n","|    mean_reward     | -50.7    |\n","| time/              |          |\n","|    total_timesteps | 3072000  |\n","| train/             |          |\n","|    actor_loss      | 5.46     |\n","|    critic_loss     | 0.87     |\n","|    ent_coef        | 0.00401  |\n","|    ent_coef_loss   | 1.17     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 127995   |\n","---------------------------------\n","Eval num_timesteps=3074400, episode_reward=-26.33 +/- 38.34\n","Episode length: 427.60 +/- 88.67\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 428      |\n","|    mean_reward     | -26.3    |\n","| time/              |          |\n","|    total_timesteps | 3074400  |\n","| train/             |          |\n","|    actor_loss      | 5.24     |\n","|    critic_loss     | 0.512    |\n","|    ent_coef        | 0.00407  |\n","|    ent_coef_loss   | 1.8      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 128095   |\n","---------------------------------\n","Eval num_timesteps=3076800, episode_reward=-75.98 +/- 2.20\n","Episode length: 177.80 +/- 35.27\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 178      |\n","|    mean_reward     | -76      |\n","| time/              |          |\n","|    total_timesteps | 3076800  |\n","| train/             |          |\n","|    actor_loss      | 5.39     |\n","|    critic_loss     | 13.3     |\n","|    ent_coef        | 0.00401  |\n","|    ent_coef_loss   | 0.0393   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 128195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4910     |\n","|    fps             | 250      |\n","|    time_elapsed    | 12302    |\n","|    total_timesteps | 3077688  |\n","| train/             |          |\n","|    actor_loss      | 5.38     |\n","|    critic_loss     | 0.488    |\n","|    ent_coef        | 0.00397  |\n","|    ent_coef_loss   | 0.0685   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 128232   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4920     |\n","|    fps             | 250      |\n","|    time_elapsed    | 12304    |\n","|    total_timesteps | 3078768  |\n","| train/             |          |\n","|    actor_loss      | 5.54     |\n","|    critic_loss     | 0.457    |\n","|    ent_coef        | 0.00395  |\n","|    ent_coef_loss   | 0.665    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 128277   |\n","---------------------------------\n","Eval num_timesteps=3079200, episode_reward=-18.87 +/- 42.24\n","Episode length: 371.20 +/- 157.75\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 371      |\n","|    mean_reward     | -18.9    |\n","| time/              |          |\n","|    total_timesteps | 3079200  |\n","| train/             |          |\n","|    actor_loss      | 5.08     |\n","|    critic_loss     | 0.407    |\n","|    ent_coef        | 0.00393  |\n","|    ent_coef_loss   | -2.07    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 128295   |\n","---------------------------------\n","Eval num_timesteps=3081600, episode_reward=-23.77 +/- 46.25\n","Episode length: 364.00 +/- 166.57\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 364      |\n","|    mean_reward     | -23.8    |\n","| time/              |          |\n","|    total_timesteps | 3081600  |\n","| train/             |          |\n","|    actor_loss      | 5.38     |\n","|    critic_loss     | 0.363    |\n","|    ent_coef        | 0.00388  |\n","|    ent_coef_loss   | -0.612   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 128395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4930     |\n","|    fps             | 250      |\n","|    time_elapsed    | 12321    |\n","|    total_timesteps | 3082032  |\n","| train/             |          |\n","|    actor_loss      | 5.59     |\n","|    critic_loss     | 0.782    |\n","|    ent_coef        | 0.00388  |\n","|    ent_coef_loss   | 0.424    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 128413   |\n","---------------------------------\n","Eval num_timesteps=3084000, episode_reward=-73.44 +/- 3.11\n","Episode length: 210.00 +/- 9.80\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 210      |\n","|    mean_reward     | -73.4    |\n","| time/              |          |\n","|    total_timesteps | 3084000  |\n","| train/             |          |\n","|    actor_loss      | 5.61     |\n","|    critic_loss     | 0.361    |\n","|    ent_coef        | 0.00395  |\n","|    ent_coef_loss   | 2.57     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 128495   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4940     |\n","|    fps             | 250      |\n","|    time_elapsed    | 12329    |\n","|    total_timesteps | 3084912  |\n","| train/             |          |\n","|    actor_loss      | 5.55     |\n","|    critic_loss     | 0.536    |\n","|    ent_coef        | 0.00402  |\n","|    ent_coef_loss   | 0.717    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 128533   |\n","---------------------------------\n","Eval num_timesteps=3086400, episode_reward=-87.17 +/- 3.51\n","Episode length: 164.80 +/- 55.36\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 165      |\n","|    mean_reward     | -87.2    |\n","| time/              |          |\n","|    total_timesteps | 3086400  |\n","| train/             |          |\n","|    actor_loss      | 5.77     |\n","|    critic_loss     | 0.634    |\n","|    ent_coef        | 0.00401  |\n","|    ent_coef_loss   | -0.165   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 128595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4950     |\n","|    fps             | 250      |\n","|    time_elapsed    | 12336    |\n","|    total_timesteps | 3087120  |\n","| train/             |          |\n","|    actor_loss      | 5.08     |\n","|    critic_loss     | 0.321    |\n","|    ent_coef        | 0.00399  |\n","|    ent_coef_loss   | -2.53    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 128625   |\n","---------------------------------\n","Eval num_timesteps=3088800, episode_reward=-80.76 +/- 4.44\n","Episode length: 175.60 +/- 65.65\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 176      |\n","|    mean_reward     | -80.8    |\n","| time/              |          |\n","|    total_timesteps | 3088800  |\n","| train/             |          |\n","|    actor_loss      | 5.66     |\n","|    critic_loss     | 14.2     |\n","|    ent_coef        | 0.00391  |\n","|    ent_coef_loss   | -0.26    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 128695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4960     |\n","|    fps             | 250      |\n","|    time_elapsed    | 12342    |\n","|    total_timesteps | 3090576  |\n","| train/             |          |\n","|    actor_loss      | 5.71     |\n","|    critic_loss     | 0.44     |\n","|    ent_coef        | 0.00399  |\n","|    ent_coef_loss   | 1.45     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 128769   |\n","---------------------------------\n","Eval num_timesteps=3091200, episode_reward=-49.83 +/- 53.17\n","Episode length: 267.80 +/- 189.59\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 268      |\n","|    mean_reward     | -49.8    |\n","| time/              |          |\n","|    total_timesteps | 3091200  |\n","| train/             |          |\n","|    actor_loss      | 5.81     |\n","|    critic_loss     | 14.1     |\n","|    ent_coef        | 0.00402  |\n","|    ent_coef_loss   | 2.06     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 128795   |\n","---------------------------------\n","Eval num_timesteps=3093600, episode_reward=-78.89 +/- 1.06\n","Episode length: 270.80 +/- 72.50\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 271      |\n","|    mean_reward     | -78.9    |\n","| time/              |          |\n","|    total_timesteps | 3093600  |\n","| train/             |          |\n","|    actor_loss      | 5.45     |\n","|    critic_loss     | 0.344    |\n","|    ent_coef        | 0.00403  |\n","|    ent_coef_loss   | -0.873   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 128895   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4970     |\n","|    fps             | 250      |\n","|    time_elapsed    | 12360    |\n","|    total_timesteps | 3094128  |\n","| train/             |          |\n","|    actor_loss      | 5.41     |\n","|    critic_loss     | 0.498    |\n","|    ent_coef        | 0.00403  |\n","|    ent_coef_loss   | 1.52     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 128917   |\n","---------------------------------\n","Eval num_timesteps=3096000, episode_reward=-88.47 +/- 2.83\n","Episode length: 164.60 +/- 37.23\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 165      |\n","|    mean_reward     | -88.5    |\n","| time/              |          |\n","|    total_timesteps | 3096000  |\n","| train/             |          |\n","|    actor_loss      | 5.81     |\n","|    critic_loss     | 12.1     |\n","|    ent_coef        | 0.00406  |\n","|    ent_coef_loss   | 1.7      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 128995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4980     |\n","|    fps             | 250      |\n","|    time_elapsed    | 12366    |\n","|    total_timesteps | 3097968  |\n","| train/             |          |\n","|    actor_loss      | 5.55     |\n","|    critic_loss     | 1.29     |\n","|    ent_coef        | 0.00426  |\n","|    ent_coef_loss   | 1.63     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 129077   |\n","---------------------------------\n","Eval num_timesteps=3098400, episode_reward=-79.58 +/- 6.49\n","Episode length: 320.20 +/- 74.46\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 320      |\n","|    mean_reward     | -79.6    |\n","| time/              |          |\n","|    total_timesteps | 3098400  |\n","| train/             |          |\n","|    actor_loss      | 5.76     |\n","|    critic_loss     | 2.24     |\n","|    ent_coef        | 0.00428  |\n","|    ent_coef_loss   | 1.13     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 129095   |\n","---------------------------------\n","Eval num_timesteps=3100800, episode_reward=7.35 +/- 5.10\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.35     |\n","| time/              |          |\n","|    total_timesteps | 3100800  |\n","| train/             |          |\n","|    actor_loss      | 5.01     |\n","|    critic_loss     | 0.313    |\n","|    ent_coef        | 0.0044   |\n","|    ent_coef_loss   | -0.576   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 129195   |\n","---------------------------------\n","Eval num_timesteps=3103200, episode_reward=6.59 +/- 0.04\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.59     |\n","| time/              |          |\n","|    total_timesteps | 3103200  |\n","| train/             |          |\n","|    actor_loss      | 5.14     |\n","|    critic_loss     | 0.343    |\n","|    ent_coef        | 0.00433  |\n","|    ent_coef_loss   | -0.747   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 129295   |\n","---------------------------------\n","Eval num_timesteps=3105600, episode_reward=-72.77 +/- 4.72\n","Episode length: 242.20 +/- 30.37\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 242      |\n","|    mean_reward     | -72.8    |\n","| time/              |          |\n","|    total_timesteps | 3105600  |\n","| train/             |          |\n","|    actor_loss      | 5.25     |\n","|    critic_loss     | 8.93     |\n","|    ent_coef        | 0.00409  |\n","|    ent_coef_loss   | -1.03    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 129395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 4990     |\n","|    fps             | 250      |\n","|    time_elapsed    | 12403    |\n","|    total_timesteps | 3106560  |\n","| train/             |          |\n","|    actor_loss      | 5.5      |\n","|    critic_loss     | 12.7     |\n","|    ent_coef        | 0.00402  |\n","|    ent_coef_loss   | -2.86    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 129435   |\n","---------------------------------\n","Eval num_timesteps=3108000, episode_reward=-18.28 +/- 51.69\n","Episode length: 437.20 +/- 76.91\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 437      |\n","|    mean_reward     | -18.3    |\n","| time/              |          |\n","|    total_timesteps | 3108000  |\n","| train/             |          |\n","|    actor_loss      | 5.55     |\n","|    critic_loss     | 0.281    |\n","|    ent_coef        | 0.00384  |\n","|    ent_coef_loss   | -1.28    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 129495   |\n","---------------------------------\n","Eval num_timesteps=3110400, episode_reward=-73.07 +/- 1.38\n","Episode length: 351.60 +/- 97.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 352      |\n","|    mean_reward     | -73.1    |\n","| time/              |          |\n","|    total_timesteps | 3110400  |\n","| train/             |          |\n","|    actor_loss      | 5.27     |\n","|    critic_loss     | 7.09     |\n","|    ent_coef        | 0.00364  |\n","|    ent_coef_loss   | -1.01    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 129595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5000     |\n","|    fps             | 250      |\n","|    time_elapsed    | 12421    |\n","|    total_timesteps | 3110496  |\n","| train/             |          |\n","|    actor_loss      | 5.3      |\n","|    critic_loss     | 0.529    |\n","|    ent_coef        | 0.00363  |\n","|    ent_coef_loss   | -1.75    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 129599   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5010     |\n","|    fps             | 250      |\n","|    time_elapsed    | 12423    |\n","|    total_timesteps | 3111648  |\n","| train/             |          |\n","|    actor_loss      | 5.16     |\n","|    critic_loss     | 13.6     |\n","|    ent_coef        | 0.0036   |\n","|    ent_coef_loss   | -1.08    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 129647   |\n","---------------------------------\n","Eval num_timesteps=3112800, episode_reward=-31.08 +/- 42.16\n","Episode length: 338.00 +/- 198.41\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 338      |\n","|    mean_reward     | -31.1    |\n","| time/              |          |\n","|    total_timesteps | 3112800  |\n","| train/             |          |\n","|    actor_loss      | 5.72     |\n","|    critic_loss     | 0.487    |\n","|    ent_coef        | 0.00363  |\n","|    ent_coef_loss   | 1.21     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 129695   |\n","---------------------------------\n","Eval num_timesteps=3115200, episode_reward=-31.83 +/- 43.51\n","Episode length: 360.40 +/- 170.97\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 360      |\n","|    mean_reward     | -31.8    |\n","| time/              |          |\n","|    total_timesteps | 3115200  |\n","| train/             |          |\n","|    actor_loss      | 5.57     |\n","|    critic_loss     | 0.487    |\n","|    ent_coef        | 0.00364  |\n","|    ent_coef_loss   | 1.71     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 129795   |\n","---------------------------------\n","Eval num_timesteps=3117600, episode_reward=-35.86 +/- 47.58\n","Episode length: 385.60 +/- 140.11\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 386      |\n","|    mean_reward     | -35.9    |\n","| time/              |          |\n","|    total_timesteps | 3117600  |\n","| train/             |          |\n","|    actor_loss      | 5.51     |\n","|    critic_loss     | 9.47     |\n","|    ent_coef        | 0.00357  |\n","|    ent_coef_loss   | -0.8     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 129895   |\n","---------------------------------\n","Eval num_timesteps=3120000, episode_reward=-45.87 +/- 42.40\n","Episode length: 331.40 +/- 137.66\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 331      |\n","|    mean_reward     | -45.9    |\n","| time/              |          |\n","|    total_timesteps | 3120000  |\n","| train/             |          |\n","|    actor_loss      | 5.43     |\n","|    critic_loss     | 1.91     |\n","|    ent_coef        | 0.00351  |\n","|    ent_coef_loss   | -0.715   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 129995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5020     |\n","|    fps             | 250      |\n","|    time_elapsed    | 12462    |\n","|    total_timesteps | 3121320  |\n","| train/             |          |\n","|    actor_loss      | 5.28     |\n","|    critic_loss     | 0.461    |\n","|    ent_coef        | 0.00366  |\n","|    ent_coef_loss   | -2.5     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 130050   |\n","---------------------------------\n","Eval num_timesteps=3122400, episode_reward=-78.00 +/- 2.14\n","Episode length: 313.80 +/- 30.37\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 314      |\n","|    mean_reward     | -78      |\n","| time/              |          |\n","|    total_timesteps | 3122400  |\n","| train/             |          |\n","|    actor_loss      | 5.22     |\n","|    critic_loss     | 0.608    |\n","|    ent_coef        | 0.00365  |\n","|    ent_coef_loss   | -1.58    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 130095   |\n","---------------------------------\n","Eval num_timesteps=3124800, episode_reward=-38.06 +/- 44.53\n","Episode length: 384.40 +/- 141.58\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 384      |\n","|    mean_reward     | -38.1    |\n","| time/              |          |\n","|    total_timesteps | 3124800  |\n","| train/             |          |\n","|    actor_loss      | 5.64     |\n","|    critic_loss     | 0.73     |\n","|    ent_coef        | 0.00371  |\n","|    ent_coef_loss   | 0.944    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 130195   |\n","---------------------------------\n","Eval num_timesteps=3127200, episode_reward=-34.11 +/- 39.24\n","Episode length: 341.60 +/- 129.33\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 342      |\n","|    mean_reward     | -34.1    |\n","| time/              |          |\n","|    total_timesteps | 3127200  |\n","| train/             |          |\n","|    actor_loss      | 5.59     |\n","|    critic_loss     | 1.46     |\n","|    ent_coef        | 0.00382  |\n","|    ent_coef_loss   | 3.23     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 130295   |\n","---------------------------------\n","Eval num_timesteps=3129600, episode_reward=-84.09 +/- 1.50\n","Episode length: 178.60 +/- 11.76\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 179      |\n","|    mean_reward     | -84.1    |\n","| time/              |          |\n","|    total_timesteps | 3129600  |\n","| train/             |          |\n","|    actor_loss      | 5.61     |\n","|    critic_loss     | 6.81     |\n","|    ent_coef        | 0.00406  |\n","|    ent_coef_loss   | 1.51     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 130395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5030     |\n","|    fps             | 250      |\n","|    time_elapsed    | 12498    |\n","|    total_timesteps | 3131736  |\n","| train/             |          |\n","|    actor_loss      | 5.79     |\n","|    critic_loss     | 0.614    |\n","|    ent_coef        | 0.00417  |\n","|    ent_coef_loss   | 1.17     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 130484   |\n","---------------------------------\n","Eval num_timesteps=3132000, episode_reward=-44.59 +/- 37.88\n","Episode length: 399.20 +/- 82.30\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 399      |\n","|    mean_reward     | -44.6    |\n","| time/              |          |\n","|    total_timesteps | 3132000  |\n","| train/             |          |\n","|    actor_loss      | 5.31     |\n","|    critic_loss     | 7.03     |\n","|    ent_coef        | 0.00417  |\n","|    ent_coef_loss   | 0.0774   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 130495   |\n","---------------------------------\n","Eval num_timesteps=3134400, episode_reward=26.09 +/- 2.36\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 26.1     |\n","| time/              |          |\n","|    total_timesteps | 3134400  |\n","| train/             |          |\n","|    actor_loss      | 5.3      |\n","|    critic_loss     | 6.81     |\n","|    ent_coef        | 0.00408  |\n","|    ent_coef_loss   | -1.11    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 130595   |\n","---------------------------------\n","Eval num_timesteps=3136800, episode_reward=-80.43 +/- 1.76\n","Episode length: 197.00 +/- 58.79\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 197      |\n","|    mean_reward     | -80.4    |\n","| time/              |          |\n","|    total_timesteps | 3136800  |\n","| train/             |          |\n","|    actor_loss      | 5.37     |\n","|    critic_loss     | 1.5      |\n","|    ent_coef        | 0.00406  |\n","|    ent_coef_loss   | 2.29     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 130695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5040     |\n","|    fps             | 250      |\n","|    time_elapsed    | 12528    |\n","|    total_timesteps | 3138720  |\n","| train/             |          |\n","|    actor_loss      | 5.27     |\n","|    critic_loss     | 0.59     |\n","|    ent_coef        | 0.00411  |\n","|    ent_coef_loss   | -0.0728  |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 130775   |\n","---------------------------------\n","Eval num_timesteps=3139200, episode_reward=-21.00 +/- 53.53\n","Episode length: 369.20 +/- 160.20\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 369      |\n","|    mean_reward     | -21      |\n","| time/              |          |\n","|    total_timesteps | 3139200  |\n","| train/             |          |\n","|    actor_loss      | 5.32     |\n","|    critic_loss     | 0.493    |\n","|    ent_coef        | 0.00413  |\n","|    ent_coef_loss   | 0.0203   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 130795   |\n","---------------------------------\n","Eval num_timesteps=3141600, episode_reward=16.81 +/- 2.13\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 16.8     |\n","| time/              |          |\n","|    total_timesteps | 3141600  |\n","| train/             |          |\n","|    actor_loss      | 5.41     |\n","|    critic_loss     | 13.2     |\n","|    ent_coef        | 0.00398  |\n","|    ent_coef_loss   | -1.6     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 130895   |\n","---------------------------------\n","Eval num_timesteps=3144000, episode_reward=2.39 +/- 1.28\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.39     |\n","| time/              |          |\n","|    total_timesteps | 3144000  |\n","| train/             |          |\n","|    actor_loss      | 5.49     |\n","|    critic_loss     | 0.777    |\n","|    ent_coef        | 0.00383  |\n","|    ent_coef_loss   | 0.224    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 130995   |\n","---------------------------------\n","Eval num_timesteps=3146400, episode_reward=5.89 +/- 3.27\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.89     |\n","| time/              |          |\n","|    total_timesteps | 3146400  |\n","| train/             |          |\n","|    actor_loss      | 5.11     |\n","|    critic_loss     | 0.563    |\n","|    ent_coef        | 0.0037   |\n","|    ent_coef_loss   | -2.21    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 131095   |\n","---------------------------------\n","Eval num_timesteps=3148800, episode_reward=10.63 +/- 3.82\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 10.6     |\n","| time/              |          |\n","|    total_timesteps | 3148800  |\n","| train/             |          |\n","|    actor_loss      | 5.06     |\n","|    critic_loss     | 0.943    |\n","|    ent_coef        | 0.00368  |\n","|    ent_coef_loss   | -0.704   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 131195   |\n","---------------------------------\n","Eval num_timesteps=3151200, episode_reward=17.04 +/- 2.31\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 17       |\n","| time/              |          |\n","|    total_timesteps | 3151200  |\n","| train/             |          |\n","|    actor_loss      | 5.64     |\n","|    critic_loss     | 0.499    |\n","|    ent_coef        | 0.00366  |\n","|    ent_coef_loss   | 1.45     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 131295   |\n","---------------------------------\n","Eval num_timesteps=3153600, episode_reward=4.84 +/- 1.22\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.84     |\n","| time/              |          |\n","|    total_timesteps | 3153600  |\n","| train/             |          |\n","|    actor_loss      | 5.44     |\n","|    critic_loss     | 0.502    |\n","|    ent_coef        | 0.00378  |\n","|    ent_coef_loss   | 1.85     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 131395   |\n","---------------------------------\n","Eval num_timesteps=3156000, episode_reward=7.93 +/- 1.79\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.93     |\n","| time/              |          |\n","|    total_timesteps | 3156000  |\n","| train/             |          |\n","|    actor_loss      | 5.37     |\n","|    critic_loss     | 0.573    |\n","|    ent_coef        | 0.00375  |\n","|    ent_coef_loss   | 1.21     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 131495   |\n","---------------------------------\n","Eval num_timesteps=3158400, episode_reward=6.81 +/- 0.61\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.81     |\n","| time/              |          |\n","|    total_timesteps | 3158400  |\n","| train/             |          |\n","|    actor_loss      | 4.85     |\n","|    critic_loss     | 0.344    |\n","|    ent_coef        | 0.00385  |\n","|    ent_coef_loss   | 0.336    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 131595   |\n","---------------------------------\n","Eval num_timesteps=3160800, episode_reward=-13.02 +/- 48.42\n","Episode length: 404.00 +/- 117.58\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 404      |\n","|    mean_reward     | -13      |\n","| time/              |          |\n","|    total_timesteps | 3160800  |\n","| train/             |          |\n","|    actor_loss      | 5.17     |\n","|    critic_loss     | 0.88     |\n","|    ent_coef        | 0.00393  |\n","|    ent_coef_loss   | 0.605    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 131695   |\n","---------------------------------\n","Eval num_timesteps=3163200, episode_reward=6.07 +/- 3.31\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.07     |\n","| time/              |          |\n","|    total_timesteps | 3163200  |\n","| train/             |          |\n","|    actor_loss      | 5.22     |\n","|    critic_loss     | 0.291    |\n","|    ent_coef        | 0.00404  |\n","|    ent_coef_loss   | 0.819    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 131795   |\n","---------------------------------\n","Eval num_timesteps=3165600, episode_reward=-40.41 +/- 55.02\n","Episode length: 309.80 +/- 155.30\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 310      |\n","|    mean_reward     | -40.4    |\n","| time/              |          |\n","|    total_timesteps | 3165600  |\n","| train/             |          |\n","|    actor_loss      | 5.83     |\n","|    critic_loss     | 8.38     |\n","|    ent_coef        | 0.00431  |\n","|    ent_coef_loss   | 2.72     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 131895   |\n","---------------------------------\n","Eval num_timesteps=3168000, episode_reward=6.03 +/- 6.15\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.03     |\n","| time/              |          |\n","|    total_timesteps | 3168000  |\n","| train/             |          |\n","|    actor_loss      | 5.56     |\n","|    critic_loss     | 9.6      |\n","|    ent_coef        | 0.00434  |\n","|    ent_coef_loss   | -0.564   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 131995   |\n","---------------------------------\n","Eval num_timesteps=3170400, episode_reward=-42.07 +/- 45.10\n","Episode length: 324.20 +/- 143.54\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 324      |\n","|    mean_reward     | -42.1    |\n","| time/              |          |\n","|    total_timesteps | 3170400  |\n","| train/             |          |\n","|    actor_loss      | 5.65     |\n","|    critic_loss     | 0.868    |\n","|    ent_coef        | 0.00423  |\n","|    ent_coef_loss   | -2.55    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 132095   |\n","---------------------------------\n","Eval num_timesteps=3172800, episode_reward=23.34 +/- 8.30\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 23.3     |\n","| time/              |          |\n","|    total_timesteps | 3172800  |\n","| train/             |          |\n","|    actor_loss      | 5.39     |\n","|    critic_loss     | 0.506    |\n","|    ent_coef        | 0.00408  |\n","|    ent_coef_loss   | 1.28     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 132195   |\n","---------------------------------\n","Eval num_timesteps=3175200, episode_reward=-27.64 +/- 41.62\n","Episode length: 459.80 +/- 32.82\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 460      |\n","|    mean_reward     | -27.6    |\n","| time/              |          |\n","|    total_timesteps | 3175200  |\n","| train/             |          |\n","|    actor_loss      | 5.47     |\n","|    critic_loss     | 0.482    |\n","|    ent_coef        | 0.0041   |\n","|    ent_coef_loss   | -1.01    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 132295   |\n","---------------------------------\n","Eval num_timesteps=3177600, episode_reward=5.28 +/- 3.12\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.28     |\n","| time/              |          |\n","|    total_timesteps | 3177600  |\n","| train/             |          |\n","|    actor_loss      | 5.37     |\n","|    critic_loss     | 0.673    |\n","|    ent_coef        | 0.00401  |\n","|    ent_coef_loss   | 0.813    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 132395   |\n","---------------------------------\n","Eval num_timesteps=3180000, episode_reward=-86.96 +/- 1.27\n","Episode length: 154.20 +/- 13.72\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 154      |\n","|    mean_reward     | -87      |\n","| time/              |          |\n","|    total_timesteps | 3180000  |\n","| train/             |          |\n","|    actor_loss      | 6.04     |\n","|    critic_loss     | 1.49     |\n","|    ent_coef        | 0.00398  |\n","|    ent_coef_loss   | 0.000473 |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 132495   |\n","---------------------------------\n","Eval num_timesteps=3182400, episode_reward=-33.42 +/- 45.78\n","Episode length: 439.20 +/- 74.46\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 439      |\n","|    mean_reward     | -33.4    |\n","| time/              |          |\n","|    total_timesteps | 3182400  |\n","| train/             |          |\n","|    actor_loss      | 5.33     |\n","|    critic_loss     | 0.462    |\n","|    ent_coef        | 0.00397  |\n","|    ent_coef_loss   | -0.00985 |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 132595   |\n","---------------------------------\n","Eval num_timesteps=3184800, episode_reward=4.70 +/- 0.06\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.7      |\n","| time/              |          |\n","|    total_timesteps | 3184800  |\n","| train/             |          |\n","|    actor_loss      | 5.54     |\n","|    critic_loss     | 0.66     |\n","|    ent_coef        | 0.00384  |\n","|    ent_coef_loss   | -0.371   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 132695   |\n","---------------------------------\n","Eval num_timesteps=3187200, episode_reward=-87.55 +/- 1.95\n","Episode length: 205.20 +/- 40.66\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 205      |\n","|    mean_reward     | -87.5    |\n","| time/              |          |\n","|    total_timesteps | 3187200  |\n","| train/             |          |\n","|    actor_loss      | 5.4      |\n","|    critic_loss     | 2.59     |\n","|    ent_coef        | 0.00396  |\n","|    ent_coef_loss   | -0.99    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 132795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5050     |\n","|    fps             | 250      |\n","|    time_elapsed    | 12732    |\n","|    total_timesteps | 3187776  |\n","| train/             |          |\n","|    actor_loss      | 5.4      |\n","|    critic_loss     | 0.701    |\n","|    ent_coef        | 0.00396  |\n","|    ent_coef_loss   | -1.45    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 132819   |\n","---------------------------------\n","Eval num_timesteps=3189600, episode_reward=-3.35 +/- 0.04\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -3.35    |\n","| time/              |          |\n","|    total_timesteps | 3189600  |\n","| train/             |          |\n","|    actor_loss      | 5.75     |\n","|    critic_loss     | 0.611    |\n","|    ent_coef        | 0.00404  |\n","|    ent_coef_loss   | -2.37    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 132895   |\n","---------------------------------\n","Eval num_timesteps=3192000, episode_reward=-1.72 +/- 0.10\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.72    |\n","| time/              |          |\n","|    total_timesteps | 3192000  |\n","| train/             |          |\n","|    actor_loss      | 5.5      |\n","|    critic_loss     | 18.9     |\n","|    ent_coef        | 0.00398  |\n","|    ent_coef_loss   | 1.67     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 132995   |\n","---------------------------------\n","Eval num_timesteps=3194400, episode_reward=-2.82 +/- 0.82\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -2.82    |\n","| time/              |          |\n","|    total_timesteps | 3194400  |\n","| train/             |          |\n","|    actor_loss      | 5.16     |\n","|    critic_loss     | 0.468    |\n","|    ent_coef        | 0.00413  |\n","|    ent_coef_loss   | -1.41    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 133095   |\n","---------------------------------\n","Eval num_timesteps=3196800, episode_reward=-32.00 +/- 43.52\n","Episode length: 371.20 +/- 157.75\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 371      |\n","|    mean_reward     | -32      |\n","| time/              |          |\n","|    total_timesteps | 3196800  |\n","| train/             |          |\n","|    actor_loss      | 5.18     |\n","|    critic_loss     | 0.636    |\n","|    ent_coef        | 0.00423  |\n","|    ent_coef_loss   | -0.894   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 133195   |\n","---------------------------------\n","Eval num_timesteps=3199200, episode_reward=6.40 +/- 1.33\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.4      |\n","| time/              |          |\n","|    total_timesteps | 3199200  |\n","| train/             |          |\n","|    actor_loss      | 5.49     |\n","|    critic_loss     | 0.418    |\n","|    ent_coef        | 0.00421  |\n","|    ent_coef_loss   | 0.0564   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 133295   |\n","---------------------------------\n","Eval num_timesteps=3201600, episode_reward=-51.29 +/- 42.53\n","Episode length: 311.00 +/- 154.32\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 311      |\n","|    mean_reward     | -51.3    |\n","| time/              |          |\n","|    total_timesteps | 3201600  |\n","| train/             |          |\n","|    actor_loss      | 5.33     |\n","|    critic_loss     | 0.472    |\n","|    ent_coef        | 0.0042   |\n","|    ent_coef_loss   | -0.195   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 133395   |\n","---------------------------------\n","Eval num_timesteps=3204000, episode_reward=1.59 +/- 0.26\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.59     |\n","| time/              |          |\n","|    total_timesteps | 3204000  |\n","| train/             |          |\n","|    actor_loss      | 5.54     |\n","|    critic_loss     | 9.61     |\n","|    ent_coef        | 0.0041   |\n","|    ent_coef_loss   | -0.0922  |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 133495   |\n","---------------------------------\n","Eval num_timesteps=3206400, episode_reward=-21.95 +/- 42.84\n","Episode length: 405.20 +/- 116.11\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 405      |\n","|    mean_reward     | -21.9    |\n","| time/              |          |\n","|    total_timesteps | 3206400  |\n","| train/             |          |\n","|    actor_loss      | 5.27     |\n","|    critic_loss     | 0.943    |\n","|    ent_coef        | 0.00426  |\n","|    ent_coef_loss   | 1.4      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 133595   |\n","---------------------------------\n","Eval num_timesteps=3208800, episode_reward=-32.17 +/- 41.87\n","Episode length: 378.40 +/- 148.93\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 378      |\n","|    mean_reward     | -32.2    |\n","| time/              |          |\n","|    total_timesteps | 3208800  |\n","| train/             |          |\n","|    actor_loss      | 5.25     |\n","|    critic_loss     | 0.357    |\n","|    ent_coef        | 0.00438  |\n","|    ent_coef_loss   | -0.547   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 133695   |\n","---------------------------------\n","Eval num_timesteps=3211200, episode_reward=-77.35 +/- 1.06\n","Episode length: 362.80 +/- 48.01\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 363      |\n","|    mean_reward     | -77.3    |\n","| time/              |          |\n","|    total_timesteps | 3211200  |\n","| train/             |          |\n","|    actor_loss      | 5.59     |\n","|    critic_loss     | 1.75     |\n","|    ent_coef        | 0.00441  |\n","|    ent_coef_loss   | -0.108   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 133795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5060     |\n","|    fps             | 250      |\n","|    time_elapsed    | 12842    |\n","|    total_timesteps | 3211896  |\n","| train/             |          |\n","|    actor_loss      | 5.25     |\n","|    critic_loss     | 0.473    |\n","|    ent_coef        | 0.00441  |\n","|    ent_coef_loss   | -1.09    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 133824   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5070     |\n","|    fps             | 250      |\n","|    time_elapsed    | 12844    |\n","|    total_timesteps | 3213552  |\n","| train/             |          |\n","|    actor_loss      | 5.53     |\n","|    critic_loss     | 0.478    |\n","|    ent_coef        | 0.00439  |\n","|    ent_coef_loss   | 2.87     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 133893   |\n","---------------------------------\n","Eval num_timesteps=3213600, episode_reward=6.17 +/- 0.32\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.17     |\n","| time/              |          |\n","|    total_timesteps | 3213600  |\n","| train/             |          |\n","|    actor_loss      | 4.91     |\n","|    critic_loss     | 0.43     |\n","|    ent_coef        | 0.00439  |\n","|    ent_coef_loss   | 1.05     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 133895   |\n","---------------------------------\n","Eval num_timesteps=3216000, episode_reward=-39.83 +/- 30.25\n","Episode length: 499.40 +/- 0.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 499      |\n","|    mean_reward     | -39.8    |\n","| time/              |          |\n","|    total_timesteps | 3216000  |\n","| train/             |          |\n","|    actor_loss      | 5.04     |\n","|    critic_loss     | 0.302    |\n","|    ent_coef        | 0.00454  |\n","|    ent_coef_loss   | 0.825    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 133995   |\n","---------------------------------\n","Eval num_timesteps=3218400, episode_reward=-34.79 +/- 37.94\n","Episode length: 440.00 +/- 48.99\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 440      |\n","|    mean_reward     | -34.8    |\n","| time/              |          |\n","|    total_timesteps | 3218400  |\n","| train/             |          |\n","|    actor_loss      | 5.16     |\n","|    critic_loss     | 13.4     |\n","|    ent_coef        | 0.00471  |\n","|    ent_coef_loss   | -0.411   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 134095   |\n","---------------------------------\n","Eval num_timesteps=3220800, episode_reward=-79.19 +/- 6.55\n","Episode length: 156.80 +/- 96.51\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 157      |\n","|    mean_reward     | -79.2    |\n","| time/              |          |\n","|    total_timesteps | 3220800  |\n","| train/             |          |\n","|    actor_loss      | 5.16     |\n","|    critic_loss     | 0.72     |\n","|    ent_coef        | 0.00455  |\n","|    ent_coef_loss   | 0.376    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 134195   |\n","---------------------------------\n","Eval num_timesteps=3223200, episode_reward=10.01 +/- 0.67\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 10       |\n","| time/              |          |\n","|    total_timesteps | 3223200  |\n","| train/             |          |\n","|    actor_loss      | 5.34     |\n","|    critic_loss     | 11.4     |\n","|    ent_coef        | 0.00452  |\n","|    ent_coef_loss   | -2.74    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 134295   |\n","---------------------------------\n","Eval num_timesteps=3225600, episode_reward=14.95 +/- 2.59\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 15       |\n","| time/              |          |\n","|    total_timesteps | 3225600  |\n","| train/             |          |\n","|    actor_loss      | 5.11     |\n","|    critic_loss     | 0.467    |\n","|    ent_coef        | 0.00433  |\n","|    ent_coef_loss   | -0.535   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 134395   |\n","---------------------------------\n","Eval num_timesteps=3228000, episode_reward=12.38 +/- 1.08\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 12.4     |\n","| time/              |          |\n","|    total_timesteps | 3228000  |\n","| train/             |          |\n","|    actor_loss      | 5.29     |\n","|    critic_loss     | 0.817    |\n","|    ent_coef        | 0.00417  |\n","|    ent_coef_loss   | -1.2     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 134495   |\n","---------------------------------\n","Eval num_timesteps=3230400, episode_reward=-17.73 +/- 44.67\n","Episode length: 407.20 +/- 113.66\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 407      |\n","|    mean_reward     | -17.7    |\n","| time/              |          |\n","|    total_timesteps | 3230400  |\n","| train/             |          |\n","|    actor_loss      | 4.93     |\n","|    critic_loss     | 0.369    |\n","|    ent_coef        | 0.00421  |\n","|    ent_coef_loss   | 0.963    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 134595   |\n","---------------------------------\n","Eval num_timesteps=3232800, episode_reward=8.31 +/- 1.90\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 8.31     |\n","| time/              |          |\n","|    total_timesteps | 3232800  |\n","| train/             |          |\n","|    actor_loss      | 4.98     |\n","|    critic_loss     | 0.491    |\n","|    ent_coef        | 0.00421  |\n","|    ent_coef_loss   | -0.689   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 134695   |\n","---------------------------------\n","Eval num_timesteps=3235200, episode_reward=-31.80 +/- 44.07\n","Episode length: 422.00 +/- 95.53\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 422      |\n","|    mean_reward     | -31.8    |\n","| time/              |          |\n","|    total_timesteps | 3235200  |\n","| train/             |          |\n","|    actor_loss      | 5.49     |\n","|    critic_loss     | 0.508    |\n","|    ent_coef        | 0.00427  |\n","|    ent_coef_loss   | -0.441   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 134795   |\n","---------------------------------\n","Eval num_timesteps=3237600, episode_reward=-47.46 +/- 38.42\n","Episode length: 375.20 +/- 101.90\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 375      |\n","|    mean_reward     | -47.5    |\n","| time/              |          |\n","|    total_timesteps | 3237600  |\n","| train/             |          |\n","|    actor_loss      | 5.34     |\n","|    critic_loss     | 0.4      |\n","|    ent_coef        | 0.00425  |\n","|    ent_coef_loss   | 0.398    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 134895   |\n","---------------------------------\n","Eval num_timesteps=3240000, episode_reward=-0.02 +/- 0.75\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.0246  |\n","| time/              |          |\n","|    total_timesteps | 3240000  |\n","| train/             |          |\n","|    actor_loss      | 5.61     |\n","|    critic_loss     | 0.474    |\n","|    ent_coef        | 0.00425  |\n","|    ent_coef_loss   | 2.34     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 134995   |\n","---------------------------------\n","Eval num_timesteps=3242400, episode_reward=-38.75 +/- 45.11\n","Episode length: 340.40 +/- 130.31\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 340      |\n","|    mean_reward     | -38.8    |\n","| time/              |          |\n","|    total_timesteps | 3242400  |\n","| train/             |          |\n","|    actor_loss      | 5.16     |\n","|    critic_loss     | 0.46     |\n","|    ent_coef        | 0.00449  |\n","|    ent_coef_loss   | 2.85     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 135095   |\n","---------------------------------\n","Eval num_timesteps=3244800, episode_reward=-85.79 +/- 2.48\n","Episode length: 256.40 +/- 80.34\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 256      |\n","|    mean_reward     | -85.8    |\n","| time/              |          |\n","|    total_timesteps | 3244800  |\n","| train/             |          |\n","|    actor_loss      | 4.95     |\n","|    critic_loss     | 6.88     |\n","|    ent_coef        | 0.00465  |\n","|    ent_coef_loss   | -0.092   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 135195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5080     |\n","|    fps             | 250      |\n","|    time_elapsed    | 12977    |\n","|    total_timesteps | 3245400  |\n","| train/             |          |\n","|    actor_loss      | 5.38     |\n","|    critic_loss     | 0.389    |\n","|    ent_coef        | 0.00467  |\n","|    ent_coef_loss   | 0.0861   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 135220   |\n","---------------------------------\n","Eval num_timesteps=3247200, episode_reward=-19.24 +/- 50.07\n","Episode length: 401.60 +/- 120.51\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 402      |\n","|    mean_reward     | -19.2    |\n","| time/              |          |\n","|    total_timesteps | 3247200  |\n","| train/             |          |\n","|    actor_loss      | 5.19     |\n","|    critic_loss     | 0.86     |\n","|    ent_coef        | 0.00461  |\n","|    ent_coef_loss   | -0.425   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 135295   |\n","---------------------------------\n","Eval num_timesteps=3249600, episode_reward=-35.07 +/- 46.84\n","Episode length: 372.00 +/- 156.77\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 372      |\n","|    mean_reward     | -35.1    |\n","| time/              |          |\n","|    total_timesteps | 3249600  |\n","| train/             |          |\n","|    actor_loss      | 5.54     |\n","|    critic_loss     | 0.799    |\n","|    ent_coef        | 0.0045   |\n","|    ent_coef_loss   | -1.31    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 135395   |\n","---------------------------------\n","Eval num_timesteps=3252000, episode_reward=-37.66 +/- 40.62\n","Episode length: 388.40 +/- 136.68\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 388      |\n","|    mean_reward     | -37.7    |\n","| time/              |          |\n","|    total_timesteps | 3252000  |\n","| train/             |          |\n","|    actor_loss      | 5.41     |\n","|    critic_loss     | 0.582    |\n","|    ent_coef        | 0.00429  |\n","|    ent_coef_loss   | -0.218   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 135495   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5090     |\n","|    fps             | 250      |\n","|    time_elapsed    | 13007    |\n","|    total_timesteps | 3253728  |\n","| train/             |          |\n","|    actor_loss      | 5.46     |\n","|    critic_loss     | 14.1     |\n","|    ent_coef        | 0.00429  |\n","|    ent_coef_loss   | 0.866    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 135567   |\n","---------------------------------\n","Eval num_timesteps=3254400, episode_reward=-41.07 +/- 37.66\n","Episode length: 410.60 +/- 72.99\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 411      |\n","|    mean_reward     | -41.1    |\n","| time/              |          |\n","|    total_timesteps | 3254400  |\n","| train/             |          |\n","|    actor_loss      | 5.39     |\n","|    critic_loss     | 0.683    |\n","|    ent_coef        | 0.00429  |\n","|    ent_coef_loss   | -1.33    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 135595   |\n","---------------------------------\n","Eval num_timesteps=3256800, episode_reward=-67.95 +/- 5.68\n","Episode length: 343.00 +/- 63.69\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 343      |\n","|    mean_reward     | -68      |\n","| time/              |          |\n","|    total_timesteps | 3256800  |\n","| train/             |          |\n","|    actor_loss      | 5.1      |\n","|    critic_loss     | 0.619    |\n","|    ent_coef        | 0.00417  |\n","|    ent_coef_loss   | -0.445   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 135695   |\n","---------------------------------\n","Eval num_timesteps=3259200, episode_reward=-76.73 +/- 1.08\n","Episode length: 169.20 +/- 0.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 169      |\n","|    mean_reward     | -76.7    |\n","| time/              |          |\n","|    total_timesteps | 3259200  |\n","| train/             |          |\n","|    actor_loss      | 5.04     |\n","|    critic_loss     | 0.382    |\n","|    ent_coef        | 0.00432  |\n","|    ent_coef_loss   | 0.212    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 135795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5100     |\n","|    fps             | 250      |\n","|    time_elapsed    | 13037    |\n","|    total_timesteps | 3259296  |\n","| train/             |          |\n","|    actor_loss      | 5.17     |\n","|    critic_loss     | 0.347    |\n","|    ent_coef        | 0.00432  |\n","|    ent_coef_loss   | 0.298    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 135799   |\n","---------------------------------\n","Eval num_timesteps=3261600, episode_reward=-47.53 +/- 51.53\n","Episode length: 483.80 +/- 13.23\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 484      |\n","|    mean_reward     | -47.5    |\n","| time/              |          |\n","|    total_timesteps | 3261600  |\n","| train/             |          |\n","|    actor_loss      | 5.08     |\n","|    critic_loss     | 0.795    |\n","|    ent_coef        | 0.0045   |\n","|    ent_coef_loss   | 2.49     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 135895   |\n","---------------------------------\n","Eval num_timesteps=3264000, episode_reward=-32.38 +/- 42.06\n","Episode length: 486.00 +/- 17.15\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 486      |\n","|    mean_reward     | -32.4    |\n","| time/              |          |\n","|    total_timesteps | 3264000  |\n","| train/             |          |\n","|    actor_loss      | 5.29     |\n","|    critic_loss     | 0.718    |\n","|    ent_coef        | 0.00456  |\n","|    ent_coef_loss   | -0.32    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 135995   |\n","---------------------------------\n","Eval num_timesteps=3266400, episode_reward=19.41 +/- 7.39\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 19.4     |\n","| time/              |          |\n","|    total_timesteps | 3266400  |\n","| train/             |          |\n","|    actor_loss      | 4.95     |\n","|    critic_loss     | 13.3     |\n","|    ent_coef        | 0.00453  |\n","|    ent_coef_loss   | -0.234   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 136095   |\n","---------------------------------\n","Eval num_timesteps=3268800, episode_reward=13.29 +/- 5.57\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 13.3     |\n","| time/              |          |\n","|    total_timesteps | 3268800  |\n","| train/             |          |\n","|    actor_loss      | 5.24     |\n","|    critic_loss     | 12.3     |\n","|    ent_coef        | 0.00457  |\n","|    ent_coef_loss   | -0.888   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 136195   |\n","---------------------------------\n","Eval num_timesteps=3271200, episode_reward=-38.99 +/- 37.54\n","Episode length: 392.80 +/- 131.29\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 393      |\n","|    mean_reward     | -39      |\n","| time/              |          |\n","|    total_timesteps | 3271200  |\n","| train/             |          |\n","|    actor_loss      | 5.69     |\n","|    critic_loss     | 23       |\n","|    ent_coef        | 0.00435  |\n","|    ent_coef_loss   | -0.0825  |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 136295   |\n","---------------------------------\n","Eval num_timesteps=3273600, episode_reward=-89.69 +/- 5.97\n","Episode length: 433.00 +/- 51.44\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 433      |\n","|    mean_reward     | -89.7    |\n","| time/              |          |\n","|    total_timesteps | 3273600  |\n","| train/             |          |\n","|    actor_loss      | 5.29     |\n","|    critic_loss     | 0.254    |\n","|    ent_coef        | 0.00415  |\n","|    ent_coef_loss   | 0.0232   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 136395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5110     |\n","|    fps             | 249      |\n","|    time_elapsed    | 13097    |\n","|    total_timesteps | 3274200  |\n","| train/             |          |\n","|    actor_loss      | 5.17     |\n","|    critic_loss     | 4.67     |\n","|    ent_coef        | 0.00413  |\n","|    ent_coef_loss   | -1.13    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 136420   |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    episodes        | 5120    |\n","|    fps             | 249     |\n","|    time_elapsed    | 13097   |\n","|    total_timesteps | 3274200 |\n","--------------------------------\n","Eval num_timesteps=3276000, episode_reward=-40.91 +/- 37.28\n","Episode length: 410.60 +/- 72.99\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 411      |\n","|    mean_reward     | -40.9    |\n","| time/              |          |\n","|    total_timesteps | 3276000  |\n","| train/             |          |\n","|    actor_loss      | 5.4      |\n","|    critic_loss     | 0.695    |\n","|    ent_coef        | 0.00412  |\n","|    ent_coef_loss   | -0.135   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 136495   |\n","---------------------------------\n","Eval num_timesteps=3278400, episode_reward=-41.85 +/- 42.62\n","Episode length: 467.00 +/- 26.94\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 467      |\n","|    mean_reward     | -41.9    |\n","| time/              |          |\n","|    total_timesteps | 3278400  |\n","| train/             |          |\n","|    actor_loss      | 5.64     |\n","|    critic_loss     | 0.462    |\n","|    ent_coef        | 0.00412  |\n","|    ent_coef_loss   | 1.43     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 136595   |\n","---------------------------------\n","Eval num_timesteps=3280800, episode_reward=24.03 +/- 1.44\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 24       |\n","| time/              |          |\n","|    total_timesteps | 3280800  |\n","| train/             |          |\n","|    actor_loss      | 5.14     |\n","|    critic_loss     | 0.575    |\n","|    ent_coef        | 0.0043   |\n","|    ent_coef_loss   | -1.24    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 136695   |\n","---------------------------------\n","Eval num_timesteps=3283200, episode_reward=-26.95 +/- 48.44\n","Episode length: 358.40 +/- 115.62\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 358      |\n","|    mean_reward     | -27      |\n","| time/              |          |\n","|    total_timesteps | 3283200  |\n","| train/             |          |\n","|    actor_loss      | 5.43     |\n","|    critic_loss     | 1.12     |\n","|    ent_coef        | 0.00431  |\n","|    ent_coef_loss   | 3.34     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 136795   |\n","---------------------------------\n","Eval num_timesteps=3285600, episode_reward=34.76 +/- 1.39\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 34.8     |\n","| time/              |          |\n","|    total_timesteps | 3285600  |\n","| train/             |          |\n","|    actor_loss      | 5.28     |\n","|    critic_loss     | 0.696    |\n","|    ent_coef        | 0.00422  |\n","|    ent_coef_loss   | -2.85    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 136895   |\n","---------------------------------\n","New best mean reward!\n","Eval num_timesteps=3288000, episode_reward=3.00 +/- 1.20\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3        |\n","| time/              |          |\n","|    total_timesteps | 3288000  |\n","| train/             |          |\n","|    actor_loss      | 4.98     |\n","|    critic_loss     | 7.95     |\n","|    ent_coef        | 0.00415  |\n","|    ent_coef_loss   | 1.63     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 136995   |\n","---------------------------------\n","Eval num_timesteps=3290400, episode_reward=27.17 +/- 0.53\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 27.2     |\n","| time/              |          |\n","|    total_timesteps | 3290400  |\n","| train/             |          |\n","|    actor_loss      | 5.75     |\n","|    critic_loss     | 1.49     |\n","|    ent_coef        | 0.00424  |\n","|    ent_coef_loss   | -1.38    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 137095   |\n","---------------------------------\n","Eval num_timesteps=3292800, episode_reward=12.33 +/- 5.38\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 12.3     |\n","| time/              |          |\n","|    total_timesteps | 3292800  |\n","| train/             |          |\n","|    actor_loss      | 5.19     |\n","|    critic_loss     | 13.8     |\n","|    ent_coef        | 0.00399  |\n","|    ent_coef_loss   | -1.83    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 137195   |\n","---------------------------------\n","Eval num_timesteps=3295200, episode_reward=-10.79 +/- 48.18\n","Episode length: 412.00 +/- 107.78\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 412      |\n","|    mean_reward     | -10.8    |\n","| time/              |          |\n","|    total_timesteps | 3295200  |\n","| train/             |          |\n","|    actor_loss      | 4.75     |\n","|    critic_loss     | 0.473    |\n","|    ent_coef        | 0.0039   |\n","|    ent_coef_loss   | 1.17     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 137295   |\n","---------------------------------\n","Eval num_timesteps=3297600, episode_reward=-30.49 +/- 46.23\n","Episode length: 393.20 +/- 87.20\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 393      |\n","|    mean_reward     | -30.5    |\n","| time/              |          |\n","|    total_timesteps | 3297600  |\n","| train/             |          |\n","|    actor_loss      | 5.05     |\n","|    critic_loss     | 0.733    |\n","|    ent_coef        | 0.00401  |\n","|    ent_coef_loss   | -0.0124  |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 137395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5130     |\n","|    fps             | 249      |\n","|    time_elapsed    | 13199    |\n","|    total_timesteps | 3297936  |\n","| train/             |          |\n","|    actor_loss      | 5.12     |\n","|    critic_loss     | 0.443    |\n","|    ent_coef        | 0.00401  |\n","|    ent_coef_loss   | 0.108    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 137409   |\n","---------------------------------\n","Eval num_timesteps=3300000, episode_reward=7.04 +/- 0.98\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.04     |\n","| time/              |          |\n","|    total_timesteps | 3300000  |\n","| train/             |          |\n","|    actor_loss      | 5.6      |\n","|    critic_loss     | 1.77     |\n","|    ent_coef        | 0.00408  |\n","|    ent_coef_loss   | 2.76     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 137495   |\n","---------------------------------\n","Eval num_timesteps=3302400, episode_reward=29.55 +/- 1.78\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 29.5     |\n","| time/              |          |\n","|    total_timesteps | 3302400  |\n","| train/             |          |\n","|    actor_loss      | 5.6      |\n","|    critic_loss     | 0.475    |\n","|    ent_coef        | 0.00426  |\n","|    ent_coef_loss   | 0.595    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 137595   |\n","---------------------------------\n","Eval num_timesteps=3304800, episode_reward=-43.02 +/- 49.44\n","Episode length: 381.80 +/- 96.51\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 382      |\n","|    mean_reward     | -43      |\n","| time/              |          |\n","|    total_timesteps | 3304800  |\n","| train/             |          |\n","|    actor_loss      | 5.19     |\n","|    critic_loss     | 6.06     |\n","|    ent_coef        | 0.00429  |\n","|    ent_coef_loss   | 0.636    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 137695   |\n","---------------------------------\n","Eval num_timesteps=3307200, episode_reward=-75.74 +/- 8.72\n","Episode length: 440.00 +/- 41.64\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 440      |\n","|    mean_reward     | -75.7    |\n","| time/              |          |\n","|    total_timesteps | 3307200  |\n","| train/             |          |\n","|    actor_loss      | 5.01     |\n","|    critic_loss     | 0.544    |\n","|    ent_coef        | 0.00444  |\n","|    ent_coef_loss   | -2.08    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 137795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5140     |\n","|    fps             | 249      |\n","|    time_elapsed    | 13241    |\n","|    total_timesteps | 3309552  |\n","| train/             |          |\n","|    actor_loss      | 5.08     |\n","|    critic_loss     | 0.443    |\n","|    ent_coef        | 0.00461  |\n","|    ent_coef_loss   | 0.935    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 137893   |\n","---------------------------------\n","Eval num_timesteps=3309600, episode_reward=-81.54 +/- 1.05\n","Episode length: 105.80 +/- 8.82\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 106      |\n","|    mean_reward     | -81.5    |\n","| time/              |          |\n","|    total_timesteps | 3309600  |\n","| train/             |          |\n","|    actor_loss      | 5.71     |\n","|    critic_loss     | 0.839    |\n","|    ent_coef        | 0.00461  |\n","|    ent_coef_loss   | 1.29     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 137895   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5150     |\n","|    fps             | 249      |\n","|    time_elapsed    | 13244    |\n","|    total_timesteps | 3309792  |\n","| train/             |          |\n","|    actor_loss      | 5.13     |\n","|    critic_loss     | 1.49     |\n","|    ent_coef        | 0.00462  |\n","|    ent_coef_loss   | -0.772   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 137903   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5160     |\n","|    fps             | 249      |\n","|    time_elapsed    | 13247    |\n","|    total_timesteps | 3311736  |\n","| train/             |          |\n","|    actor_loss      | 4.99     |\n","|    critic_loss     | 0.388    |\n","|    ent_coef        | 0.00471  |\n","|    ent_coef_loss   | -0.792   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 137984   |\n","---------------------------------\n","Eval num_timesteps=3312000, episode_reward=13.73 +/- 1.42\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 13.7     |\n","| time/              |          |\n","|    total_timesteps | 3312000  |\n","| train/             |          |\n","|    actor_loss      | 5.82     |\n","|    critic_loss     | 0.852    |\n","|    ent_coef        | 0.00471  |\n","|    ent_coef_loss   | -1.76    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 137995   |\n","---------------------------------\n","Eval num_timesteps=3314400, episode_reward=5.26 +/- 1.11\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.26     |\n","| time/              |          |\n","|    total_timesteps | 3314400  |\n","| train/             |          |\n","|    actor_loss      | 5.08     |\n","|    critic_loss     | 0.336    |\n","|    ent_coef        | 0.00448  |\n","|    ent_coef_loss   | 1.04     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 138095   |\n","---------------------------------\n","Eval num_timesteps=3316800, episode_reward=4.51 +/- 1.09\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.51     |\n","| time/              |          |\n","|    total_timesteps | 3316800  |\n","| train/             |          |\n","|    actor_loss      | 5.35     |\n","|    critic_loss     | 0.597    |\n","|    ent_coef        | 0.00444  |\n","|    ent_coef_loss   | 2.88     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 138195   |\n","---------------------------------\n","Eval num_timesteps=3319200, episode_reward=-2.37 +/- 3.64\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -2.37    |\n","| time/              |          |\n","|    total_timesteps | 3319200  |\n","| train/             |          |\n","|    actor_loss      | 5.16     |\n","|    critic_loss     | 12.9     |\n","|    ent_coef        | 0.00462  |\n","|    ent_coef_loss   | 0.558    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 138295   |\n","---------------------------------\n","Eval num_timesteps=3321600, episode_reward=6.45 +/- 4.24\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.45     |\n","| time/              |          |\n","|    total_timesteps | 3321600  |\n","| train/             |          |\n","|    actor_loss      | 5.09     |\n","|    critic_loss     | 0.419    |\n","|    ent_coef        | 0.00475  |\n","|    ent_coef_loss   | 0.363    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 138395   |\n","---------------------------------\n","Eval num_timesteps=3324000, episode_reward=0.62 +/- 0.20\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.623    |\n","| time/              |          |\n","|    total_timesteps | 3324000  |\n","| train/             |          |\n","|    actor_loss      | 4.97     |\n","|    critic_loss     | 0.384    |\n","|    ent_coef        | 0.00478  |\n","|    ent_coef_loss   | -0.658   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 138495   |\n","---------------------------------\n","Eval num_timesteps=3326400, episode_reward=6.87 +/- 0.37\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.87     |\n","| time/              |          |\n","|    total_timesteps | 3326400  |\n","| train/             |          |\n","|    actor_loss      | 4.89     |\n","|    critic_loss     | 12       |\n","|    ent_coef        | 0.00484  |\n","|    ent_coef_loss   | -0.381   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 138595   |\n","---------------------------------\n","Eval num_timesteps=3328800, episode_reward=1.61 +/- 0.57\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.61     |\n","| time/              |          |\n","|    total_timesteps | 3328800  |\n","| train/             |          |\n","|    actor_loss      | 5.08     |\n","|    critic_loss     | 0.486    |\n","|    ent_coef        | 0.00497  |\n","|    ent_coef_loss   | -0.523   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 138695   |\n","---------------------------------\n","Eval num_timesteps=3331200, episode_reward=26.30 +/- 3.02\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 26.3     |\n","| time/              |          |\n","|    total_timesteps | 3331200  |\n","| train/             |          |\n","|    actor_loss      | 5.1      |\n","|    critic_loss     | 0.826    |\n","|    ent_coef        | 0.00515  |\n","|    ent_coef_loss   | -1.87    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 138795   |\n","---------------------------------\n","Eval num_timesteps=3333600, episode_reward=-79.65 +/- 0.55\n","Episode length: 300.80 +/- 20.58\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 301      |\n","|    mean_reward     | -79.7    |\n","| time/              |          |\n","|    total_timesteps | 3333600  |\n","| train/             |          |\n","|    actor_loss      | 5.52     |\n","|    critic_loss     | 0.656    |\n","|    ent_coef        | 0.00515  |\n","|    ent_coef_loss   | -0.515   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 138895   |\n","---------------------------------\n","Eval num_timesteps=3336000, episode_reward=17.35 +/- 0.77\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 17.3     |\n","| time/              |          |\n","|    total_timesteps | 3336000  |\n","| train/             |          |\n","|    actor_loss      | 5.59     |\n","|    critic_loss     | 0.462    |\n","|    ent_coef        | 0.00493  |\n","|    ent_coef_loss   | 0.948    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 138995   |\n","---------------------------------\n","Eval num_timesteps=3338400, episode_reward=15.80 +/- 0.12\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 15.8     |\n","| time/              |          |\n","|    total_timesteps | 3338400  |\n","| train/             |          |\n","|    actor_loss      | 5.65     |\n","|    critic_loss     | 0.765    |\n","|    ent_coef        | 0.00479  |\n","|    ent_coef_loss   | -0.202   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 139095   |\n","---------------------------------\n","Eval num_timesteps=3340800, episode_reward=-41.66 +/- 37.33\n","Episode length: 361.60 +/- 169.50\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 362      |\n","|    mean_reward     | -41.7    |\n","| time/              |          |\n","|    total_timesteps | 3340800  |\n","| train/             |          |\n","|    actor_loss      | 5.03     |\n","|    critic_loss     | 0.683    |\n","|    ent_coef        | 0.00478  |\n","|    ent_coef_loss   | 0.864    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 139195   |\n","---------------------------------\n","Eval num_timesteps=3343200, episode_reward=-24.43 +/- 41.62\n","Episode length: 424.40 +/- 92.59\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 424      |\n","|    mean_reward     | -24.4    |\n","| time/              |          |\n","|    total_timesteps | 3343200  |\n","| train/             |          |\n","|    actor_loss      | 5.08     |\n","|    critic_loss     | 8.8      |\n","|    ent_coef        | 0.00488  |\n","|    ent_coef_loss   | 0.643    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 139295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5170     |\n","|    fps             | 249      |\n","|    time_elapsed    | 13390    |\n","|    total_timesteps | 3344544  |\n","| train/             |          |\n","|    actor_loss      | 4.77     |\n","|    critic_loss     | 1.14     |\n","|    ent_coef        | 0.0049   |\n","|    ent_coef_loss   | -1.3     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 139351   |\n","---------------------------------\n","Eval num_timesteps=3345600, episode_reward=-37.92 +/- 37.90\n","Episode length: 369.20 +/- 106.80\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 369      |\n","|    mean_reward     | -37.9    |\n","| time/              |          |\n","|    total_timesteps | 3345600  |\n","| train/             |          |\n","|    actor_loss      | 5.21     |\n","|    critic_loss     | 4.28     |\n","|    ent_coef        | 0.00491  |\n","|    ent_coef_loss   | -0.875   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 139395   |\n","---------------------------------\n","Eval num_timesteps=3348000, episode_reward=-3.35 +/- 12.22\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -3.35    |\n","| time/              |          |\n","|    total_timesteps | 3348000  |\n","| train/             |          |\n","|    actor_loss      | 5.18     |\n","|    critic_loss     | 0.831    |\n","|    ent_coef        | 0.00486  |\n","|    ent_coef_loss   | -1.61    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 139495   |\n","---------------------------------\n","Eval num_timesteps=3350400, episode_reward=16.59 +/- 2.71\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 16.6     |\n","| time/              |          |\n","|    total_timesteps | 3350400  |\n","| train/             |          |\n","|    actor_loss      | 5.1      |\n","|    critic_loss     | 0.458    |\n","|    ent_coef        | 0.00462  |\n","|    ent_coef_loss   | -1.42    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 139595   |\n","---------------------------------\n","Eval num_timesteps=3352800, episode_reward=10.25 +/- 0.87\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 10.2     |\n","| time/              |          |\n","|    total_timesteps | 3352800  |\n","| train/             |          |\n","|    actor_loss      | 5        |\n","|    critic_loss     | 1.11     |\n","|    ent_coef        | 0.00439  |\n","|    ent_coef_loss   | -1.27    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 139695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5180     |\n","|    fps             | 249      |\n","|    time_elapsed    | 13431    |\n","|    total_timesteps | 3353688  |\n","| train/             |          |\n","|    actor_loss      | 5.35     |\n","|    critic_loss     | 13.5     |\n","|    ent_coef        | 0.00439  |\n","|    ent_coef_loss   | -0.69    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 139732   |\n","---------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n"]},{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-22-6784f180a7e5>\", line 10, in <cell line: 10>\n","    model.learn(total_timesteps=5000000, log_interval=10,callback=eval_callback)\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/sac/sac.py\", line 307, in learn\n","    return super().learn(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 312, in learn\n","    rollout = self.collect_rollouts(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 552, in collect_rollouts\n","    if callback.on_step() is False:\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 104, in on_step\n","    return self._on_step()\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 471, in _on_step\n","    np.savez(\n","  File \"<__array_function__ internals>\", line 180, in savez\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 615, in savez\n","    _savez(file, args, kwds, False)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 712, in _savez\n","    zipf = zipfile_factory(file, mode=\"w\", compression=compression)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 103, in zipfile_factory\n","    return zipfile.ZipFile(file, *args, **kwargs)\n","  File \"/usr/lib/python3.10/zipfile.py\", line 1251, in __init__\n","    self.fp = io.open(file, filemode)\n","OSError: [Errno 107] Transport endpoint is not connected: './multiwalker_sac2_08_log_eval/evaluations.npz'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'OSError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-22-6784f180a7e5>\", line 10, in <cell line: 10>\n","    model.learn(total_timesteps=5000000, log_interval=10,callback=eval_callback)\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/sac/sac.py\", line 307, in learn\n","    return super().learn(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 312, in learn\n","    rollout = self.collect_rollouts(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 552, in collect_rollouts\n","    if callback.on_step() is False:\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 104, in on_step\n","    return self._on_step()\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 471, in _on_step\n","    np.savez(\n","  File \"<__array_function__ internals>\", line 180, in savez\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 615, in savez\n","    _savez(file, args, kwds, False)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 712, in _savez\n","    zipf = zipfile_factory(file, mode=\"w\", compression=compression)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 103, in zipfile_factory\n","    return zipfile.ZipFile(file, *args, **kwargs)\n","  File \"/usr/lib/python3.10/zipfile.py\", line 1251, in __init__\n","    self.fp = io.open(file, filemode)\n","OSError: [Errno 107] Transport endpoint is not connected: './multiwalker_sac2_08_log_eval/evaluations.npz'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'OSError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n","    if (await self.run_code(code, result,  async_=asy)):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n","    self.showtraceback(running_compiled_code=True)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n","    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-22-6784f180a7e5>\", line 10, in <cell line: 10>\n","    model.learn(total_timesteps=5000000, log_interval=10,callback=eval_callback)\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/sac/sac.py\", line 307, in learn\n","    return super().learn(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 312, in learn\n","    rollout = self.collect_rollouts(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 552, in collect_rollouts\n","    if callback.on_step() is False:\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 104, in on_step\n","    return self._on_step()\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 471, in _on_step\n","    np.savez(\n","  File \"<__array_function__ internals>\", line 180, in savez\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 615, in savez\n","    _savez(file, args, kwds, False)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 712, in _savez\n","    zipf = zipfile_factory(file, mode=\"w\", compression=compression)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 103, in zipfile_factory\n","    return zipfile.ZipFile(file, *args, **kwargs)\n","  File \"/usr/lib/python3.10/zipfile.py\", line 1251, in __init__\n","    self.fp = io.open(file, filemode)\n","OSError: [Errno 107] Transport endpoint is not connected: './multiwalker_sac2_08_log_eval/evaluations.npz'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'OSError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n","    if (await self.run_code(code, result,  async_=asy)):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n","    self.showtraceback(running_compiled_code=True)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n","    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n","    return runner(coro)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n","    coro.send(None)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n","    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3492, in run_ast_nodes\n","    self.showtraceback()\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1142, in structured_traceback\n","    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"OooX9WwN9AGO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3 import SAC\n","from pettingzoo.sisl import multiwalker_v9\n","import supersuit as ss\n","from stable_baselines3.common.monitor import Monitor"],"metadata":{"id":"5ypcPewx9AxR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697887381542,"user_tz":-120,"elapsed":59,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"bcbc999c-9da8-47cf-d68d-f554b75a1956","id":"mqyCzx7d9AxS"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["'=2.13'\t\t\t\t     multiwalker_sac2_log_eval\n"," DQN_new_pettingzoo_gym_cap.ipynb    multiwalker_sac_log_eval\n"," DQN_policies\t\t\t     multiwalker_td3_log_eval\n","'Entrenamientos antiguos sin logs'   policy_log_eval\n"," Entrenamientos_log_no_eval\t     PPO_policies\n"," MCR_TFM.ipynb\t\t\t     results_rllib\n"," multi_car_racing\t\t     TFM_Multiwalker_DDPG_PPO_gym_cap.ipynb\n"," multiwalker_ddpg_log_eval\t     TFM_Multiwalker_SAC_gym_cap.ipynb\n"," multiwalker_ddpg.zip\t\t     TFM_Multiwalker_TD3_gym_cap.ipynb\n"," multiwalker_ppo_log_eval\t     TFM_PPO_new_pettingzoo_gym_cap.ipynb\n"," multiwalker_ppo.zip\t\t     TFM_PPO_pettingzoo_gym_cap.ipynb\n"]}]},{"cell_type":"code","source":["env = multiwalker_v9.parallel_env(render_mode=\"rgb_array\",shared_reward=0.8)\n","\n","env = ss.frame_stack_v1(env, 3)\n","env = ss.pettingzoo_env_to_vec_env_v1(env)\n","env = ss.concat_vec_envs_v1(env, 8, num_cpus=4, base_class='stable_baselines3')"],"metadata":{"id":"oBQ_pARy9AxT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3.common.callbacks import EvalCallback\n","eval_callback = EvalCallback(env, best_model_save_path=\"./multiwalker_sac3_08_log_eval/\",\n","                             log_path=\"./multiwalker_sac3_08_log_eval/\", eval_freq=200,\n","                             deterministic=False, render=False)\n"],"metadata":{"id":"iN3G9S1T9AxU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3.common.logger import configure\n","\n","tmp_path = \"multiwalker_sac3_08_log_eval/\"\n","# set up logger\n","new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n","\n","model = SAC(\"MlpPolicy\", env, verbose=3,batch_size=512, learning_rate=0.001,gamma=0.9,tau=0.01) #,train_freq=1\n","\n","model.set_logger(new_logger)\n","model.learn(total_timesteps=4000000, log_interval=10,callback=eval_callback)\n","model.save(\"multiwalker_sac3_08\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"23c5ab84-2d42-474c-d964-f44493b02858","id":"eyrgIrJl9AxV","executionInfo":{"status":"ok","timestamp":1698940383113,"user_tz":-60,"elapsed":5872204,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Logging to multiwalker_sac3_08_log_eval/\n","Using cuda device\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 10       |\n","|    fps             | 329      |\n","|    time_elapsed    | 4        |\n","|    total_timesteps | 1608     |\n","| train/             |          |\n","|    actor_loss      | -5.31    |\n","|    critic_loss     | 77.4     |\n","|    ent_coef        | 0.941    |\n","|    ent_coef_loss   | -0.411   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 62       |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 20       |\n","|    fps             | 347      |\n","|    time_elapsed    | 5        |\n","|    total_timesteps | 1752     |\n","| train/             |          |\n","|    actor_loss      | -4.9     |\n","|    critic_loss     | 91.9     |\n","|    ent_coef        | 0.935    |\n","|    ent_coef_loss   | -0.452   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 68       |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 30       |\n","|    fps             | 463      |\n","|    time_elapsed    | 6        |\n","|    total_timesteps | 2880     |\n","| train/             |          |\n","|    actor_loss      | -5.19    |\n","|    critic_loss     | 133      |\n","|    ent_coef        | 0.892    |\n","|    ent_coef_loss   | -0.767   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 115      |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40       |\n","|    fps             | 477      |\n","|    time_elapsed    | 6        |\n","|    total_timesteps | 3168     |\n","| train/             |          |\n","|    actor_loss      | -4.9     |\n","|    critic_loss     | 195      |\n","|    ent_coef        | 0.882    |\n","|    ent_coef_loss   | -0.842   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 127      |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 50       |\n","|    fps             | 532      |\n","|    time_elapsed    | 8        |\n","|    total_timesteps | 4416     |\n","| train/             |          |\n","|    actor_loss      | -5.47    |\n","|    critic_loss     | 92.9     |\n","|    ent_coef        | 0.837    |\n","|    ent_coef_loss   | -1.19    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 179      |\n","---------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n","|    ent_coef_loss   | -2.34    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33360    |\n","|    fps             | 678      |\n","|    time_elapsed    | 5456     |\n","|    total_timesteps | 3701184  |\n","| train/             |          |\n","|    actor_loss      | 10.4     |\n","|    critic_loss     | 3.63     |\n","|    ent_coef        | 0.00789  |\n","|    ent_coef_loss   | 0.521    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154211   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33370    |\n","|    fps             | 678      |\n","|    time_elapsed    | 5458     |\n","|    total_timesteps | 3702912  |\n","| train/             |          |\n","|    actor_loss      | 10.9     |\n","|    critic_loss     | 4.04     |\n","|    ent_coef        | 0.00803  |\n","|    ent_coef_loss   | 0.93     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154283   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33380    |\n","|    fps             | 678      |\n","|    time_elapsed    | 5459     |\n","|    total_timesteps | 3704040  |\n","| train/             |          |\n","|    actor_loss      | 11.7     |\n","|    critic_loss     | 13.3     |\n","|    ent_coef        | 0.00794  |\n","|    ent_coef_loss   | -0.167   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154330   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33390    |\n","|    fps             | 678      |\n","|    time_elapsed    | 5459     |\n","|    total_timesteps | 3704808  |\n","| train/             |          |\n","|    actor_loss      | 10.4     |\n","|    critic_loss     | 17.9     |\n","|    ent_coef        | 0.00807  |\n","|    ent_coef_loss   | 1.84     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154362   |\n","---------------------------------\n","Eval num_timesteps=3705600, episode_reward=-92.45 +/- 0.52\n","Episode length: 94.60 +/- 16.66\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 94.6     |\n","|    mean_reward     | -92.5    |\n","| time/              |          |\n","|    total_timesteps | 3705600  |\n","| train/             |          |\n","|    actor_loss      | 11.8     |\n","|    critic_loss     | 40.7     |\n","|    ent_coef        | 0.00836  |\n","|    ent_coef_loss   | 1.69     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33400    |\n","|    fps             | 678      |\n","|    time_elapsed    | 5462     |\n","|    total_timesteps | 3705840  |\n","| train/             |          |\n","|    actor_loss      | 12.3     |\n","|    critic_loss     | 14       |\n","|    ent_coef        | 0.00843  |\n","|    ent_coef_loss   | 1.37     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154405   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33410    |\n","|    fps             | 678      |\n","|    time_elapsed    | 5465     |\n","|    total_timesteps | 3707856  |\n","| train/             |          |\n","|    actor_loss      | 11.1     |\n","|    critic_loss     | 13       |\n","|    ent_coef        | 0.00836  |\n","|    ent_coef_loss   | 0.913    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154489   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33420    |\n","|    fps             | 678      |\n","|    time_elapsed    | 5466     |\n","|    total_timesteps | 3708624  |\n","| train/             |          |\n","|    actor_loss      | 11.3     |\n","|    critic_loss     | 24.5     |\n","|    ent_coef        | 0.00829  |\n","|    ent_coef_loss   | -0.733   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154521   |\n","---------------------------------\n","Eval num_timesteps=3710400, episode_reward=-91.52 +/- 0.48\n","Episode length: 82.20 +/- 3.92\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 82.2     |\n","|    mean_reward     | -91.5    |\n","| time/              |          |\n","|    total_timesteps | 3710400  |\n","| train/             |          |\n","|    actor_loss      | 11.3     |\n","|    critic_loss     | 11.9     |\n","|    ent_coef        | 0.0083   |\n","|    ent_coef_loss   | -0.15    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33430    |\n","|    fps             | 678      |\n","|    time_elapsed    | 5470     |\n","|    total_timesteps | 3710448  |\n","| train/             |          |\n","|    actor_loss      | 10.7     |\n","|    critic_loss     | 5.84     |\n","|    ent_coef        | 0.00831  |\n","|    ent_coef_loss   | 0.644    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154597   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33440    |\n","|    fps             | 678      |\n","|    time_elapsed    | 5471     |\n","|    total_timesteps | 3711360  |\n","| train/             |          |\n","|    actor_loss      | 11.4     |\n","|    critic_loss     | 22.8     |\n","|    ent_coef        | 0.00846  |\n","|    ent_coef_loss   | 2.08     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154635   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33450    |\n","|    fps             | 678      |\n","|    time_elapsed    | 5472     |\n","|    total_timesteps | 3712416  |\n","| train/             |          |\n","|    actor_loss      | 10.6     |\n","|    critic_loss     | 13.1     |\n","|    ent_coef        | 0.00872  |\n","|    ent_coef_loss   | -2.02    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154679   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33460    |\n","|    fps             | 678      |\n","|    time_elapsed    | 5473     |\n","|    total_timesteps | 3713280  |\n","| train/             |          |\n","|    actor_loss      | 10       |\n","|    critic_loss     | 4        |\n","|    ent_coef        | 0.00862  |\n","|    ent_coef_loss   | -0.896   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154715   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33470    |\n","|    fps             | 678      |\n","|    time_elapsed    | 5474     |\n","|    total_timesteps | 3714480  |\n","| train/             |          |\n","|    actor_loss      | 12.7     |\n","|    critic_loss     | 6.43     |\n","|    ent_coef        | 0.00871  |\n","|    ent_coef_loss   | 1.01     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154765   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33480    |\n","|    fps             | 678      |\n","|    time_elapsed    | 5474     |\n","|    total_timesteps | 3715176  |\n","| train/             |          |\n","|    actor_loss      | 10.7     |\n","|    critic_loss     | 28.3     |\n","|    ent_coef        | 0.00874  |\n","|    ent_coef_loss   | -2.22    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154794   |\n","---------------------------------\n","Eval num_timesteps=3715200, episode_reward=-90.72 +/- 0.49\n","Episode length: 76.60 +/- 10.29\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 76.6     |\n","|    mean_reward     | -90.7    |\n","| time/              |          |\n","|    total_timesteps | 3715200  |\n","| train/             |          |\n","|    actor_loss      | 12.1     |\n","|    critic_loss     | 17.7     |\n","|    ent_coef        | 0.00874  |\n","|    ent_coef_loss   | -0.368   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33490    |\n","|    fps             | 678      |\n","|    time_elapsed    | 5476     |\n","|    total_timesteps | 3716208  |\n","| train/             |          |\n","|    actor_loss      | 10.7     |\n","|    critic_loss     | 14       |\n","|    ent_coef        | 0.00863  |\n","|    ent_coef_loss   | 0.97     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154837   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33500    |\n","|    fps             | 678      |\n","|    time_elapsed    | 5477     |\n","|    total_timesteps | 3717336  |\n","| train/             |          |\n","|    actor_loss      | 12       |\n","|    critic_loss     | 14.6     |\n","|    ent_coef        | 0.00866  |\n","|    ent_coef_loss   | 0.141    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154884   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33510    |\n","|    fps             | 678      |\n","|    time_elapsed    | 5478     |\n","|    total_timesteps | 3718488  |\n","| train/             |          |\n","|    actor_loss      | 10.9     |\n","|    critic_loss     | 24.9     |\n","|    ent_coef        | 0.00889  |\n","|    ent_coef_loss   | 0.971    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154932   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33520    |\n","|    fps             | 678      |\n","|    time_elapsed    | 5479     |\n","|    total_timesteps | 3719184  |\n","| train/             |          |\n","|    actor_loss      | 11.5     |\n","|    critic_loss     | 38       |\n","|    ent_coef        | 0.00895  |\n","|    ent_coef_loss   | 0.675    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154961   |\n","---------------------------------\n","Eval num_timesteps=3720000, episode_reward=-90.00 +/- 2.59\n","Episode length: 108.00 +/- 17.15\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 108      |\n","|    mean_reward     | -90      |\n","| time/              |          |\n","|    total_timesteps | 3720000  |\n","| train/             |          |\n","|    actor_loss      | 10.5     |\n","|    critic_loss     | 12.8     |\n","|    ent_coef        | 0.00897  |\n","|    ent_coef_loss   | -0.752   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154995   |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    episodes        | 33530   |\n","|    fps             | 678     |\n","|    time_elapsed    | 5481    |\n","|    total_timesteps | 3720000 |\n","--------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33540    |\n","|    fps             | 678      |\n","|    time_elapsed    | 5481     |\n","|    total_timesteps | 3720576  |\n","| train/             |          |\n","|    actor_loss      | 10.3     |\n","|    critic_loss     | 12.6     |\n","|    ent_coef        | 0.00887  |\n","|    ent_coef_loss   | -1.61    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155019   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33550    |\n","|    fps             | 678      |\n","|    time_elapsed    | 5483     |\n","|    total_timesteps | 3722376  |\n","| train/             |          |\n","|    actor_loss      | 11       |\n","|    critic_loss     | 12       |\n","|    ent_coef        | 0.00872  |\n","|    ent_coef_loss   | -1.57    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155094   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33560    |\n","|    fps             | 678      |\n","|    time_elapsed    | 5485     |\n","|    total_timesteps | 3723528  |\n","| train/             |          |\n","|    actor_loss      | 12.6     |\n","|    critic_loss     | 11.8     |\n","|    ent_coef        | 0.00848  |\n","|    ent_coef_loss   | 2.37     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155142   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33570    |\n","|    fps             | 678      |\n","|    time_elapsed    | 5486     |\n","|    total_timesteps | 3724152  |\n","| train/             |          |\n","|    actor_loss      | 13.6     |\n","|    critic_loss     | 20.1     |\n","|    ent_coef        | 0.00838  |\n","|    ent_coef_loss   | 0.971    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155168   |\n","---------------------------------\n","Eval num_timesteps=3724800, episode_reward=-91.78 +/- 3.74\n","Episode length: 82.60 +/- 17.64\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 82.6     |\n","|    mean_reward     | -91.8    |\n","| time/              |          |\n","|    total_timesteps | 3724800  |\n","| train/             |          |\n","|    actor_loss      | 9.93     |\n","|    critic_loss     | 38.2     |\n","|    ent_coef        | 0.00822  |\n","|    ent_coef_loss   | -1.87    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33580    |\n","|    fps             | 678      |\n","|    time_elapsed    | 5490     |\n","|    total_timesteps | 3725952  |\n","| train/             |          |\n","|    actor_loss      | 12       |\n","|    critic_loss     | 33.2     |\n","|    ent_coef        | 0.00819  |\n","|    ent_coef_loss   | 1.58     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155243   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33590    |\n","|    fps             | 678      |\n","|    time_elapsed    | 5492     |\n","|    total_timesteps | 3727008  |\n","| train/             |          |\n","|    actor_loss      | 10.5     |\n","|    critic_loss     | 21.6     |\n","|    ent_coef        | 0.00826  |\n","|    ent_coef_loss   | -1.75    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155287   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33600    |\n","|    fps             | 678      |\n","|    time_elapsed    | 5492     |\n","|    total_timesteps | 3727368  |\n","| train/             |          |\n","|    actor_loss      | 12.3     |\n","|    critic_loss     | 7.32     |\n","|    ent_coef        | 0.00823  |\n","|    ent_coef_loss   | -0.217   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155302   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33610    |\n","|    fps             | 678      |\n","|    time_elapsed    | 5493     |\n","|    total_timesteps | 3728856  |\n","| train/             |          |\n","|    actor_loss      | 11.5     |\n","|    critic_loss     | 4.72     |\n","|    ent_coef        | 0.00779  |\n","|    ent_coef_loss   | -0.135   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155364   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33620    |\n","|    fps             | 678      |\n","|    time_elapsed    | 5494     |\n","|    total_timesteps | 3729456  |\n","| train/             |          |\n","|    actor_loss      | 11.3     |\n","|    critic_loss     | 9.5      |\n","|    ent_coef        | 0.00772  |\n","|    ent_coef_loss   | -0.0919  |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155389   |\n","---------------------------------\n","Eval num_timesteps=3729600, episode_reward=-86.01 +/- 1.25\n","Episode length: 66.00 +/- 2.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 66       |\n","|    mean_reward     | -86      |\n","| time/              |          |\n","|    total_timesteps | 3729600  |\n","| train/             |          |\n","|    actor_loss      | 10.9     |\n","|    critic_loss     | 3.4      |\n","|    ent_coef        | 0.00772  |\n","|    ent_coef_loss   | -1.41    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33630    |\n","|    fps             | 678      |\n","|    time_elapsed    | 5495     |\n","|    total_timesteps | 3730152  |\n","| train/             |          |\n","|    actor_loss      | 10.3     |\n","|    critic_loss     | 23.9     |\n","|    ent_coef        | 0.00775  |\n","|    ent_coef_loss   | -0.443   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155418   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33640    |\n","|    fps             | 678      |\n","|    time_elapsed    | 5496     |\n","|    total_timesteps | 3731448  |\n","| train/             |          |\n","|    actor_loss      | 11       |\n","|    critic_loss     | 9.59     |\n","|    ent_coef        | 0.00776  |\n","|    ent_coef_loss   | -0.565   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155472   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33650    |\n","|    fps             | 678      |\n","|    time_elapsed    | 5497     |\n","|    total_timesteps | 3732144  |\n","| train/             |          |\n","|    actor_loss      | 11.2     |\n","|    critic_loss     | 26.5     |\n","|    ent_coef        | 0.00777  |\n","|    ent_coef_loss   | 1.85     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155501   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33660    |\n","|    fps             | 678      |\n","|    time_elapsed    | 5498     |\n","|    total_timesteps | 3733032  |\n","| train/             |          |\n","|    actor_loss      | 11.3     |\n","|    critic_loss     | 30.3     |\n","|    ent_coef        | 0.00789  |\n","|    ent_coef_loss   | 0.121    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155538   |\n","---------------------------------\n","Eval num_timesteps=3734400, episode_reward=-89.30 +/- 0.89\n","Episode length: 62.00 +/- 4.90\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 62       |\n","|    mean_reward     | -89.3    |\n","| time/              |          |\n","|    total_timesteps | 3734400  |\n","| train/             |          |\n","|    actor_loss      | 11.9     |\n","|    critic_loss     | 12.1     |\n","|    ent_coef        | 0.00796  |\n","|    ent_coef_loss   | 0.57     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33670    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5499     |\n","|    total_timesteps | 3734424  |\n","| train/             |          |\n","|    actor_loss      | 12.1     |\n","|    critic_loss     | 18.1     |\n","|    ent_coef        | 0.00796  |\n","|    ent_coef_loss   | 0.952    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155596   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33680    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5499     |\n","|    total_timesteps | 3734568  |\n","| train/             |          |\n","|    actor_loss      | 10.8     |\n","|    critic_loss     | 17.2     |\n","|    ent_coef        | 0.00797  |\n","|    ent_coef_loss   | -1.23    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155602   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33690    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5501     |\n","|    total_timesteps | 3735936  |\n","| train/             |          |\n","|    actor_loss      | 12.1     |\n","|    critic_loss     | 12.8     |\n","|    ent_coef        | 0.00798  |\n","|    ent_coef_loss   | 1.73     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155659   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33700    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5501     |\n","|    total_timesteps | 3736608  |\n","| train/             |          |\n","|    actor_loss      | 11.2     |\n","|    critic_loss     | 21.9     |\n","|    ent_coef        | 0.00815  |\n","|    ent_coef_loss   | 0.475    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155687   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33710    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5502     |\n","|    total_timesteps | 3737328  |\n","| train/             |          |\n","|    actor_loss      | 11.5     |\n","|    critic_loss     | 27.6     |\n","|    ent_coef        | 0.0083   |\n","|    ent_coef_loss   | -0.984   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155717   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33720    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5503     |\n","|    total_timesteps | 3737856  |\n","| train/             |          |\n","|    actor_loss      | 11.3     |\n","|    critic_loss     | 9.38     |\n","|    ent_coef        | 0.00827  |\n","|    ent_coef_loss   | -3.17    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155739   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33730    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5504     |\n","|    total_timesteps | 3739056  |\n","| train/             |          |\n","|    actor_loss      | 10.5     |\n","|    critic_loss     | 14.6     |\n","|    ent_coef        | 0.00809  |\n","|    ent_coef_loss   | -0.84    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155789   |\n","---------------------------------\n","Eval num_timesteps=3739200, episode_reward=-91.42 +/- 2.93\n","Episode length: 61.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 61       |\n","|    mean_reward     | -91.4    |\n","| time/              |          |\n","|    total_timesteps | 3739200  |\n","| train/             |          |\n","|    actor_loss      | 13.1     |\n","|    critic_loss     | 43       |\n","|    ent_coef        | 0.0081   |\n","|    ent_coef_loss   | 1.71     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33740    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5506     |\n","|    total_timesteps | 3739512  |\n","| train/             |          |\n","|    actor_loss      | 11.4     |\n","|    critic_loss     | 60.3     |\n","|    ent_coef        | 0.00812  |\n","|    ent_coef_loss   | -1.51    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155808   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33750    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5507     |\n","|    total_timesteps | 3739920  |\n","| train/             |          |\n","|    actor_loss      | 10.5     |\n","|    critic_loss     | 31.9     |\n","|    ent_coef        | 0.00809  |\n","|    ent_coef_loss   | -1.06    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155825   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33760    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5508     |\n","|    total_timesteps | 3741120  |\n","| train/             |          |\n","|    actor_loss      | 10.9     |\n","|    critic_loss     | 11       |\n","|    ent_coef        | 0.00798  |\n","|    ent_coef_loss   | -1.3     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155875   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33770    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5509     |\n","|    total_timesteps | 3741792  |\n","| train/             |          |\n","|    actor_loss      | 11.6     |\n","|    critic_loss     | 41.3     |\n","|    ent_coef        | 0.00788  |\n","|    ent_coef_loss   | -1.58    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155903   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33780    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5511     |\n","|    total_timesteps | 3742632  |\n","| train/             |          |\n","|    actor_loss      | 10.4     |\n","|    critic_loss     | 3.65     |\n","|    ent_coef        | 0.00781  |\n","|    ent_coef_loss   | -2.82    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155938   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33790    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5512     |\n","|    total_timesteps | 3743520  |\n","| train/             |          |\n","|    actor_loss      | 11.4     |\n","|    critic_loss     | 5.32     |\n","|    ent_coef        | 0.00768  |\n","|    ent_coef_loss   | -1.24    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155975   |\n","---------------------------------\n","Eval num_timesteps=3744000, episode_reward=-93.35 +/- 4.11\n","Episode length: 96.60 +/- 11.76\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 96.6     |\n","|    mean_reward     | -93.3    |\n","| time/              |          |\n","|    total_timesteps | 3744000  |\n","| train/             |          |\n","|    actor_loss      | 11.1     |\n","|    critic_loss     | 11.1     |\n","|    ent_coef        | 0.00753  |\n","|    ent_coef_loss   | -0.135   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33800    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5515     |\n","|    total_timesteps | 3745512  |\n","| train/             |          |\n","|    actor_loss      | 10.4     |\n","|    critic_loss     | 41.8     |\n","|    ent_coef        | 0.00762  |\n","|    ent_coef_loss   | 2.1      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156058   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33810    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5515     |\n","|    total_timesteps | 3746184  |\n","| train/             |          |\n","|    actor_loss      | 10       |\n","|    critic_loss     | 18.3     |\n","|    ent_coef        | 0.00781  |\n","|    ent_coef_loss   | -0.639   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156086   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33820    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5516     |\n","|    total_timesteps | 3747504  |\n","| train/             |          |\n","|    actor_loss      | 11.9     |\n","|    critic_loss     | 23.6     |\n","|    ent_coef        | 0.0078   |\n","|    ent_coef_loss   | -0.761   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156141   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33830    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5517     |\n","|    total_timesteps | 3748080  |\n","| train/             |          |\n","|    actor_loss      | 11.8     |\n","|    critic_loss     | 38.2     |\n","|    ent_coef        | 0.00768  |\n","|    ent_coef_loss   | 0.0722   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156165   |\n","---------------------------------\n","Eval num_timesteps=3748800, episode_reward=-86.80 +/- 2.30\n","Episode length: 84.00 +/- 4.90\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 84       |\n","|    mean_reward     | -86.8    |\n","| time/              |          |\n","|    total_timesteps | 3748800  |\n","| train/             |          |\n","|    actor_loss      | 11.1     |\n","|    critic_loss     | 4.97     |\n","|    ent_coef        | 0.0076   |\n","|    ent_coef_loss   | -0.238   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33840    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5519     |\n","|    total_timesteps | 3749400  |\n","| train/             |          |\n","|    actor_loss      | 10.9     |\n","|    critic_loss     | 4.07     |\n","|    ent_coef        | 0.00762  |\n","|    ent_coef_loss   | -1.04    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156220   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33850    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5520     |\n","|    total_timesteps | 3750192  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 10.9     |\n","|    ent_coef        | 0.00752  |\n","|    ent_coef_loss   | -0.217   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156253   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33860    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5521     |\n","|    total_timesteps | 3751272  |\n","| train/             |          |\n","|    actor_loss      | 12.6     |\n","|    critic_loss     | 10.5     |\n","|    ent_coef        | 0.00743  |\n","|    ent_coef_loss   | 2.33     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156298   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33870    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5521     |\n","|    total_timesteps | 3751920  |\n","| train/             |          |\n","|    actor_loss      | 11       |\n","|    critic_loss     | 9.63     |\n","|    ent_coef        | 0.00748  |\n","|    ent_coef_loss   | -0.00819 |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156325   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33880    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5522     |\n","|    total_timesteps | 3753096  |\n","| train/             |          |\n","|    actor_loss      | 11.9     |\n","|    critic_loss     | 16.1     |\n","|    ent_coef        | 0.00781  |\n","|    ent_coef_loss   | 3.28     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156374   |\n","---------------------------------\n","Eval num_timesteps=3753600, episode_reward=-92.33 +/- 2.74\n","Episode length: 96.80 +/- 1.47\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 96.8     |\n","|    mean_reward     | -92.3    |\n","| time/              |          |\n","|    total_timesteps | 3753600  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 26.6     |\n","|    ent_coef        | 0.00795  |\n","|    ent_coef_loss   | -0.701   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33890    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5526     |\n","|    total_timesteps | 3754704  |\n","| train/             |          |\n","|    actor_loss      | 10.4     |\n","|    critic_loss     | 37.1     |\n","|    ent_coef        | 0.0082   |\n","|    ent_coef_loss   | 1.98     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156441   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33900    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5528     |\n","|    total_timesteps | 3755976  |\n","| train/             |          |\n","|    actor_loss      | 10.6     |\n","|    critic_loss     | 8.09     |\n","|    ent_coef        | 0.00829  |\n","|    ent_coef_loss   | 0.692    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156494   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33910    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5530     |\n","|    total_timesteps | 3757248  |\n","| train/             |          |\n","|    actor_loss      | 11.6     |\n","|    critic_loss     | 21.7     |\n","|    ent_coef        | 0.00854  |\n","|    ent_coef_loss   | 0.536    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156547   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33920    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5530     |\n","|    total_timesteps | 3757560  |\n","| train/             |          |\n","|    actor_loss      | 11.7     |\n","|    critic_loss     | 14.9     |\n","|    ent_coef        | 0.00854  |\n","|    ent_coef_loss   | -0.709   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156560   |\n","---------------------------------\n","Eval num_timesteps=3758400, episode_reward=-88.79 +/- 3.41\n","Episode length: 95.20 +/- 8.82\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 95.2     |\n","|    mean_reward     | -88.8    |\n","| time/              |          |\n","|    total_timesteps | 3758400  |\n","| train/             |          |\n","|    actor_loss      | 11.4     |\n","|    critic_loss     | 23.4     |\n","|    ent_coef        | 0.0085   |\n","|    ent_coef_loss   | 0.0561   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33930    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5534     |\n","|    total_timesteps | 3759240  |\n","| train/             |          |\n","|    actor_loss      | 11.8     |\n","|    critic_loss     | 21.8     |\n","|    ent_coef        | 0.00835  |\n","|    ent_coef_loss   | -0.272   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156630   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33940    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5534     |\n","|    total_timesteps | 3759720  |\n","| train/             |          |\n","|    actor_loss      | 10.9     |\n","|    critic_loss     | 2.66     |\n","|    ent_coef        | 0.00834  |\n","|    ent_coef_loss   | -0.0391  |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156650   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33950    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5535     |\n","|    total_timesteps | 3761016  |\n","| train/             |          |\n","|    actor_loss      | 10.7     |\n","|    critic_loss     | 3.36     |\n","|    ent_coef        | 0.00825  |\n","|    ent_coef_loss   | -1.61    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156704   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33960    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5535     |\n","|    total_timesteps | 3761184  |\n","| train/             |          |\n","|    actor_loss      | 11.3     |\n","|    critic_loss     | 21.6     |\n","|    ent_coef        | 0.00824  |\n","|    ent_coef_loss   | -0.658   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156711   |\n","---------------------------------\n","Eval num_timesteps=3763200, episode_reward=-90.83 +/- 0.78\n","Episode length: 69.80 +/- 13.23\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 69.8     |\n","|    mean_reward     | -90.8    |\n","| time/              |          |\n","|    total_timesteps | 3763200  |\n","| train/             |          |\n","|    actor_loss      | 11.4     |\n","|    critic_loss     | 27.8     |\n","|    ent_coef        | 0.0078   |\n","|    ent_coef_loss   | 0.289    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33970    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5538     |\n","|    total_timesteps | 3763920  |\n","| train/             |          |\n","|    actor_loss      | 11.8     |\n","|    critic_loss     | 4.87     |\n","|    ent_coef        | 0.00773  |\n","|    ent_coef_loss   | 0.432    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156825   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33980    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5539     |\n","|    total_timesteps | 3764760  |\n","| train/             |          |\n","|    actor_loss      | 11.1     |\n","|    critic_loss     | 20.1     |\n","|    ent_coef        | 0.0077   |\n","|    ent_coef_loss   | -0.801   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156860   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 33990    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5540     |\n","|    total_timesteps | 3765576  |\n","| train/             |          |\n","|    actor_loss      | 11       |\n","|    critic_loss     | 5.57     |\n","|    ent_coef        | 0.0077   |\n","|    ent_coef_loss   | 0.193    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156894   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34000    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5541     |\n","|    total_timesteps | 3766656  |\n","| train/             |          |\n","|    actor_loss      | 10.5     |\n","|    critic_loss     | 6.69     |\n","|    ent_coef        | 0.00789  |\n","|    ent_coef_loss   | 1.07     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156939   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34010    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5541     |\n","|    total_timesteps | 3767040  |\n","| train/             |          |\n","|    actor_loss      | 12.2     |\n","|    critic_loss     | 8.39     |\n","|    ent_coef        | 0.00794  |\n","|    ent_coef_loss   | 0.094    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156955   |\n","---------------------------------\n","Eval num_timesteps=3768000, episode_reward=-93.71 +/- 1.36\n","Episode length: 82.80 +/- 8.33\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 82.8     |\n","|    mean_reward     | -93.7    |\n","| time/              |          |\n","|    total_timesteps | 3768000  |\n","| train/             |          |\n","|    actor_loss      | 10.4     |\n","|    critic_loss     | 17       |\n","|    ent_coef        | 0.00792  |\n","|    ent_coef_loss   | -0.643   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34020    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5544     |\n","|    total_timesteps | 3768960  |\n","| train/             |          |\n","|    actor_loss      | 11.3     |\n","|    critic_loss     | 24.4     |\n","|    ent_coef        | 0.00783  |\n","|    ent_coef_loss   | -0.451   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157035   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34030    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5547     |\n","|    total_timesteps | 3770760  |\n","| train/             |          |\n","|    actor_loss      | 11.4     |\n","|    critic_loss     | 45.7     |\n","|    ent_coef        | 0.0078   |\n","|    ent_coef_loss   | 1.42     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157110   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34040    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5548     |\n","|    total_timesteps | 3771888  |\n","| train/             |          |\n","|    actor_loss      | 12.5     |\n","|    critic_loss     | 18.5     |\n","|    ent_coef        | 0.00788  |\n","|    ent_coef_loss   | 2.33     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157157   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34050    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5549     |\n","|    total_timesteps | 3772344  |\n","| train/             |          |\n","|    actor_loss      | 11.6     |\n","|    critic_loss     | 16.6     |\n","|    ent_coef        | 0.00806  |\n","|    ent_coef_loss   | 2.5      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157176   |\n","---------------------------------\n","Eval num_timesteps=3772800, episode_reward=-91.11 +/- 0.46\n","Episode length: 101.00 +/- 22.05\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 101      |\n","|    mean_reward     | -91.1    |\n","| time/              |          |\n","|    total_timesteps | 3772800  |\n","| train/             |          |\n","|    actor_loss      | 12.6     |\n","|    critic_loss     | 18.2     |\n","|    ent_coef        | 0.00811  |\n","|    ent_coef_loss   | -1.03    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34060    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5553     |\n","|    total_timesteps | 3773640  |\n","| train/             |          |\n","|    actor_loss      | 10.9     |\n","|    critic_loss     | 25.4     |\n","|    ent_coef        | 0.00802  |\n","|    ent_coef_loss   | -0.268   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157230   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34070    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5554     |\n","|    total_timesteps | 3774672  |\n","| train/             |          |\n","|    actor_loss      | 11.4     |\n","|    critic_loss     | 8.48     |\n","|    ent_coef        | 0.00809  |\n","|    ent_coef_loss   | 1.19     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157273   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34080    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5554     |\n","|    total_timesteps | 3775128  |\n","| train/             |          |\n","|    actor_loss      | 10.9     |\n","|    critic_loss     | 21.9     |\n","|    ent_coef        | 0.00816  |\n","|    ent_coef_loss   | 1.07     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157292   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34090    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5556     |\n","|    total_timesteps | 3776928  |\n","| train/             |          |\n","|    actor_loss      | 11.5     |\n","|    critic_loss     | 19.4     |\n","|    ent_coef        | 0.00842  |\n","|    ent_coef_loss   | 1.39     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157367   |\n","---------------------------------\n","Eval num_timesteps=3777600, episode_reward=-89.74 +/- 1.03\n","Episode length: 71.60 +/- 1.96\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 71.6     |\n","|    mean_reward     | -89.7    |\n","| time/              |          |\n","|    total_timesteps | 3777600  |\n","| train/             |          |\n","|    actor_loss      | 10.8     |\n","|    critic_loss     | 17.5     |\n","|    ent_coef        | 0.00851  |\n","|    ent_coef_loss   | -3.08    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157395   |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    episodes        | 34100   |\n","|    fps             | 679     |\n","|    time_elapsed    | 5557    |\n","|    total_timesteps | 3777600 |\n","--------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34110    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5558     |\n","|    total_timesteps | 3778752  |\n","| train/             |          |\n","|    actor_loss      | 10.7     |\n","|    critic_loss     | 37.6     |\n","|    ent_coef        | 0.00801  |\n","|    ent_coef_loss   | -4.01    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157443   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34120    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5559     |\n","|    total_timesteps | 3779376  |\n","| train/             |          |\n","|    actor_loss      | 10.1     |\n","|    critic_loss     | 16.1     |\n","|    ent_coef        | 0.00782  |\n","|    ent_coef_loss   | -0.292   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157469   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34130    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5559     |\n","|    total_timesteps | 3780120  |\n","| train/             |          |\n","|    actor_loss      | 11.6     |\n","|    critic_loss     | 30.6     |\n","|    ent_coef        | 0.00783  |\n","|    ent_coef_loss   | -0.522   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157500   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34140    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5560     |\n","|    total_timesteps | 3780552  |\n","| train/             |          |\n","|    actor_loss      | 10.1     |\n","|    critic_loss     | 3.53     |\n","|    ent_coef        | 0.00779  |\n","|    ent_coef_loss   | -2.75    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157518   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34150    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5560     |\n","|    total_timesteps | 3781272  |\n","| train/             |          |\n","|    actor_loss      | 11.9     |\n","|    critic_loss     | 14.8     |\n","|    ent_coef        | 0.00759  |\n","|    ent_coef_loss   | -1.5     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157548   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34160    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5561     |\n","|    total_timesteps | 3782352  |\n","| train/             |          |\n","|    actor_loss      | 11.3     |\n","|    critic_loss     | 17.7     |\n","|    ent_coef        | 0.00761  |\n","|    ent_coef_loss   | -0.592   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157593   |\n","---------------------------------\n","Eval num_timesteps=3782400, episode_reward=-88.48 +/- 0.51\n","Episode length: 69.00 +/- 7.35\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 69       |\n","|    mean_reward     | -88.5    |\n","| time/              |          |\n","|    total_timesteps | 3782400  |\n","| train/             |          |\n","|    actor_loss      | 10.6     |\n","|    critic_loss     | 29       |\n","|    ent_coef        | 0.00761  |\n","|    ent_coef_loss   | -2.18    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34170    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5562     |\n","|    total_timesteps | 3782712  |\n","| train/             |          |\n","|    actor_loss      | 11.9     |\n","|    critic_loss     | 14       |\n","|    ent_coef        | 0.00757  |\n","|    ent_coef_loss   | -0.946   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157608   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34180    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5564     |\n","|    total_timesteps | 3784080  |\n","| train/             |          |\n","|    actor_loss      | 10.9     |\n","|    critic_loss     | 11.5     |\n","|    ent_coef        | 0.00781  |\n","|    ent_coef_loss   | -0.425   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157665   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34190    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5565     |\n","|    total_timesteps | 3784680  |\n","| train/             |          |\n","|    actor_loss      | 11.6     |\n","|    critic_loss     | 3.66     |\n","|    ent_coef        | 0.00787  |\n","|    ent_coef_loss   | 0.538    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157690   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34200    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5566     |\n","|    total_timesteps | 3785760  |\n","| train/             |          |\n","|    actor_loss      | 11       |\n","|    critic_loss     | 3.15     |\n","|    ent_coef        | 0.00784  |\n","|    ent_coef_loss   | -0.896   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157735   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34210    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5567     |\n","|    total_timesteps | 3786432  |\n","| train/             |          |\n","|    actor_loss      | 11.8     |\n","|    critic_loss     | 31.1     |\n","|    ent_coef        | 0.00779  |\n","|    ent_coef_loss   | 0.782    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157763   |\n","---------------------------------\n","Eval num_timesteps=3787200, episode_reward=-87.53 +/- 0.49\n","Episode length: 70.80 +/- 3.43\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 70.8     |\n","|    mean_reward     | -87.5    |\n","| time/              |          |\n","|    total_timesteps | 3787200  |\n","| train/             |          |\n","|    actor_loss      | 11.5     |\n","|    critic_loss     | 5.17     |\n","|    ent_coef        | 0.00769  |\n","|    ent_coef_loss   | -0.479   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34220    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5572     |\n","|    total_timesteps | 3788664  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 3.49     |\n","|    ent_coef        | 0.00777  |\n","|    ent_coef_loss   | -1.78    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157856   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34230    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5572     |\n","|    total_timesteps | 3788904  |\n","| train/             |          |\n","|    actor_loss      | 10.5     |\n","|    critic_loss     | 16.4     |\n","|    ent_coef        | 0.00776  |\n","|    ent_coef_loss   | 0.408    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157866   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34240    |\n","|    fps             | 679      |\n","|    time_elapsed    | 5573     |\n","|    total_timesteps | 3790080  |\n","| train/             |          |\n","|    actor_loss      | 11.3     |\n","|    critic_loss     | 24.5     |\n","|    ent_coef        | 0.00781  |\n","|    ent_coef_loss   | -0.377   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157915   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34250    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5574     |\n","|    total_timesteps | 3790560  |\n","| train/             |          |\n","|    actor_loss      | 11       |\n","|    critic_loss     | 34.5     |\n","|    ent_coef        | 0.00777  |\n","|    ent_coef_loss   | -0.467   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157935   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34260    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5574     |\n","|    total_timesteps | 3790848  |\n","| train/             |          |\n","|    actor_loss      | 10.5     |\n","|    critic_loss     | 41.6     |\n","|    ent_coef        | 0.00776  |\n","|    ent_coef_loss   | -0.641   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157947   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34270    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5575     |\n","|    total_timesteps | 3791976  |\n","| train/             |          |\n","|    actor_loss      | 12.6     |\n","|    critic_loss     | 27.5     |\n","|    ent_coef        | 0.00765  |\n","|    ent_coef_loss   | 2.34     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157994   |\n","---------------------------------\n","Eval num_timesteps=3792000, episode_reward=-92.05 +/- 1.34\n","Episode length: 65.40 +/- 1.96\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 65.4     |\n","|    mean_reward     | -92.1    |\n","| time/              |          |\n","|    total_timesteps | 3792000  |\n","| train/             |          |\n","|    actor_loss      | 11.5     |\n","|    critic_loss     | 19.6     |\n","|    ent_coef        | 0.00765  |\n","|    ent_coef_loss   | -0.0142  |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34280    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5576     |\n","|    total_timesteps | 3792336  |\n","| train/             |          |\n","|    actor_loss      | 10.4     |\n","|    critic_loss     | 19       |\n","|    ent_coef        | 0.00766  |\n","|    ent_coef_loss   | -1.51    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158009   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34290    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5577     |\n","|    total_timesteps | 3793080  |\n","| train/             |          |\n","|    actor_loss      | 11.5     |\n","|    critic_loss     | 4.36     |\n","|    ent_coef        | 0.00754  |\n","|    ent_coef_loss   | 0.333    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158040   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34300    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5577     |\n","|    total_timesteps | 3793728  |\n","| train/             |          |\n","|    actor_loss      | 12.2     |\n","|    critic_loss     | 10.9     |\n","|    ent_coef        | 0.00757  |\n","|    ent_coef_loss   | 1.62     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158067   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34310    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5578     |\n","|    total_timesteps | 3795168  |\n","| train/             |          |\n","|    actor_loss      | 11.8     |\n","|    critic_loss     | 5.38     |\n","|    ent_coef        | 0.00751  |\n","|    ent_coef_loss   | 1.62     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158127   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34320    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5579     |\n","|    total_timesteps | 3795600  |\n","| train/             |          |\n","|    actor_loss      | 11.5     |\n","|    critic_loss     | 33.7     |\n","|    ent_coef        | 0.0076   |\n","|    ent_coef_loss   | 2.59     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158145   |\n","---------------------------------\n","Eval num_timesteps=3796800, episode_reward=-96.31 +/- 3.79\n","Episode length: 167.00 +/- 80.83\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 167      |\n","|    mean_reward     | -96.3    |\n","| time/              |          |\n","|    total_timesteps | 3796800  |\n","| train/             |          |\n","|    actor_loss      | 10.3     |\n","|    critic_loss     | 23.4     |\n","|    ent_coef        | 0.00756  |\n","|    ent_coef_loss   | -0.708   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34330    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5582     |\n","|    total_timesteps | 3796896  |\n","| train/             |          |\n","|    actor_loss      | 11.7     |\n","|    critic_loss     | 16.1     |\n","|    ent_coef        | 0.00756  |\n","|    ent_coef_loss   | 2.67     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158199   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34340    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5584     |\n","|    total_timesteps | 3798120  |\n","| train/             |          |\n","|    actor_loss      | 11.5     |\n","|    critic_loss     | 28.7     |\n","|    ent_coef        | 0.00771  |\n","|    ent_coef_loss   | 0.446    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158250   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34350    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5584     |\n","|    total_timesteps | 3798672  |\n","| train/             |          |\n","|    actor_loss      | 10.9     |\n","|    critic_loss     | 5.54     |\n","|    ent_coef        | 0.00773  |\n","|    ent_coef_loss   | -1.07    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158273   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34360    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5585     |\n","|    total_timesteps | 3799440  |\n","| train/             |          |\n","|    actor_loss      | 11.3     |\n","|    critic_loss     | 11.2     |\n","|    ent_coef        | 0.00771  |\n","|    ent_coef_loss   | -0.492   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158305   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34370    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5586     |\n","|    total_timesteps | 3800088  |\n","| train/             |          |\n","|    actor_loss      | 11       |\n","|    critic_loss     | 13.5     |\n","|    ent_coef        | 0.00764  |\n","|    ent_coef_loss   | -3.16    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158332   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34380    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5588     |\n","|    total_timesteps | 3801096  |\n","| train/             |          |\n","|    actor_loss      | 11.3     |\n","|    critic_loss     | 5.68     |\n","|    ent_coef        | 0.00769  |\n","|    ent_coef_loss   | -1.49    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158374   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34390    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5588     |\n","|    total_timesteps | 3801360  |\n","| train/             |          |\n","|    actor_loss      | 11.1     |\n","|    critic_loss     | 12.9     |\n","|    ent_coef        | 0.00773  |\n","|    ent_coef_loss   | -0.0441  |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158385   |\n","---------------------------------\n","Eval num_timesteps=3801600, episode_reward=-90.74 +/- 0.62\n","Episode length: 71.40 +/- 2.94\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 71.4     |\n","|    mean_reward     | -90.7    |\n","| time/              |          |\n","|    total_timesteps | 3801600  |\n","| train/             |          |\n","|    actor_loss      | 10.3     |\n","|    critic_loss     | 12.3     |\n","|    ent_coef        | 0.00775  |\n","|    ent_coef_loss   | -0.632   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34400    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5592     |\n","|    total_timesteps | 3803112  |\n","| train/             |          |\n","|    actor_loss      | 11       |\n","|    critic_loss     | 16.5     |\n","|    ent_coef        | 0.00775  |\n","|    ent_coef_loss   | -1.55    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158458   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34410    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5592     |\n","|    total_timesteps | 3803184  |\n","| train/             |          |\n","|    actor_loss      | 11.4     |\n","|    critic_loss     | 3.55     |\n","|    ent_coef        | 0.00774  |\n","|    ent_coef_loss   | 1.06     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158461   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34420    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5594     |\n","|    total_timesteps | 3804744  |\n","| train/             |          |\n","|    actor_loss      | 11.4     |\n","|    critic_loss     | 13.9     |\n","|    ent_coef        | 0.00786  |\n","|    ent_coef_loss   | -0.121   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158526   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34430    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5594     |\n","|    total_timesteps | 3805080  |\n","| train/             |          |\n","|    actor_loss      | 10.1     |\n","|    critic_loss     | 16.8     |\n","|    ent_coef        | 0.00789  |\n","|    ent_coef_loss   | -2.46    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158540   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34440    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5595     |\n","|    total_timesteps | 3806280  |\n","| train/             |          |\n","|    actor_loss      | 10.7     |\n","|    critic_loss     | 10.1     |\n","|    ent_coef        | 0.00756  |\n","|    ent_coef_loss   | -1.77    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158590   |\n","---------------------------------\n","Eval num_timesteps=3806400, episode_reward=-91.64 +/- 0.49\n","Episode length: 93.20 +/- 20.58\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 93.2     |\n","|    mean_reward     | -91.6    |\n","| time/              |          |\n","|    total_timesteps | 3806400  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 7.79     |\n","|    ent_coef        | 0.00753  |\n","|    ent_coef_loss   | -1.23    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34450    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5597     |\n","|    total_timesteps | 3806928  |\n","| train/             |          |\n","|    actor_loss      | 11.3     |\n","|    critic_loss     | 27.6     |\n","|    ent_coef        | 0.0075   |\n","|    ent_coef_loss   | -1.02    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158617   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34460    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5597     |\n","|    total_timesteps | 3807312  |\n","| train/             |          |\n","|    actor_loss      | 10.9     |\n","|    critic_loss     | 6.97     |\n","|    ent_coef        | 0.00748  |\n","|    ent_coef_loss   | 0.0499   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158633   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34470    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5598     |\n","|    total_timesteps | 3808728  |\n","| train/             |          |\n","|    actor_loss      | 11       |\n","|    critic_loss     | 15.1     |\n","|    ent_coef        | 0.00752  |\n","|    ent_coef_loss   | -0.812   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158692   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34480    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5599     |\n","|    total_timesteps | 3809280  |\n","| train/             |          |\n","|    actor_loss      | 11.2     |\n","|    critic_loss     | 17.7     |\n","|    ent_coef        | 0.00751  |\n","|    ent_coef_loss   | 0.226    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158715   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34490    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5600     |\n","|    total_timesteps | 3810096  |\n","| train/             |          |\n","|    actor_loss      | 11.9     |\n","|    critic_loss     | 14.2     |\n","|    ent_coef        | 0.00759  |\n","|    ent_coef_loss   | 1.13     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158749   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34500    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5600     |\n","|    total_timesteps | 3810696  |\n","| train/             |          |\n","|    actor_loss      | 11.7     |\n","|    critic_loss     | 21.8     |\n","|    ent_coef        | 0.0076   |\n","|    ent_coef_loss   | -0.213   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158774   |\n","---------------------------------\n","Eval num_timesteps=3811200, episode_reward=-94.47 +/- 0.54\n","Episode length: 76.80 +/- 3.92\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 76.8     |\n","|    mean_reward     | -94.5    |\n","| time/              |          |\n","|    total_timesteps | 3811200  |\n","| train/             |          |\n","|    actor_loss      | 11.4     |\n","|    critic_loss     | 3.35     |\n","|    ent_coef        | 0.00764  |\n","|    ent_coef_loss   | 0.115    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34510    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5601     |\n","|    total_timesteps | 3811272  |\n","| train/             |          |\n","|    actor_loss      | 10.9     |\n","|    critic_loss     | 23.2     |\n","|    ent_coef        | 0.00764  |\n","|    ent_coef_loss   | -0.253   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158798   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34520    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5602     |\n","|    total_timesteps | 3812448  |\n","| train/             |          |\n","|    actor_loss      | 11       |\n","|    critic_loss     | 15.2     |\n","|    ent_coef        | 0.00757  |\n","|    ent_coef_loss   | 0.661    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158847   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34530    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5603     |\n","|    total_timesteps | 3812664  |\n","| train/             |          |\n","|    actor_loss      | 12.3     |\n","|    critic_loss     | 21.5     |\n","|    ent_coef        | 0.00758  |\n","|    ent_coef_loss   | 3.04     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158856   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34540    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5604     |\n","|    total_timesteps | 3813696  |\n","| train/             |          |\n","|    actor_loss      | 11.3     |\n","|    critic_loss     | 7.65     |\n","|    ent_coef        | 0.00787  |\n","|    ent_coef_loss   | -0.485   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158899   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34550    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5605     |\n","|    total_timesteps | 3814488  |\n","| train/             |          |\n","|    actor_loss      | 11.4     |\n","|    critic_loss     | 21.4     |\n","|    ent_coef        | 0.00783  |\n","|    ent_coef_loss   | 1.53     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158932   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34560    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5606     |\n","|    total_timesteps | 3815448  |\n","| train/             |          |\n","|    actor_loss      | 11       |\n","|    critic_loss     | 10.6     |\n","|    ent_coef        | 0.00775  |\n","|    ent_coef_loss   | -1.59    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158972   |\n","---------------------------------\n","Eval num_timesteps=3816000, episode_reward=-96.89 +/- 1.40\n","Episode length: 81.00 +/- 7.35\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 81       |\n","|    mean_reward     | -96.9    |\n","| time/              |          |\n","|    total_timesteps | 3816000  |\n","| train/             |          |\n","|    actor_loss      | 10.6     |\n","|    critic_loss     | 16.3     |\n","|    ent_coef        | 0.00773  |\n","|    ent_coef_loss   | -0.491   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34570    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5610     |\n","|    total_timesteps | 3817560  |\n","| train/             |          |\n","|    actor_loss      | 11.6     |\n","|    critic_loss     | 23.8     |\n","|    ent_coef        | 0.00791  |\n","|    ent_coef_loss   | 0.282    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159060   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34580    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5611     |\n","|    total_timesteps | 3817968  |\n","| train/             |          |\n","|    actor_loss      | 11.5     |\n","|    critic_loss     | 36.7     |\n","|    ent_coef        | 0.00791  |\n","|    ent_coef_loss   | -0.661   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159077   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34590    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5612     |\n","|    total_timesteps | 3818808  |\n","| train/             |          |\n","|    actor_loss      | 11.8     |\n","|    critic_loss     | 6.76     |\n","|    ent_coef        | 0.00794  |\n","|    ent_coef_loss   | -1.74    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159112   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34600    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5614     |\n","|    total_timesteps | 3819888  |\n","| train/             |          |\n","|    actor_loss      | 12.3     |\n","|    critic_loss     | 22.6     |\n","|    ent_coef        | 0.0081   |\n","|    ent_coef_loss   | 1.57     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159157   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34610    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5614     |\n","|    total_timesteps | 3820464  |\n","| train/             |          |\n","|    actor_loss      | 11.2     |\n","|    critic_loss     | 10.6     |\n","|    ent_coef        | 0.00812  |\n","|    ent_coef_loss   | -0.244   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159181   |\n","---------------------------------\n","Eval num_timesteps=3820800, episode_reward=-90.54 +/- 0.59\n","Episode length: 63.20 +/- 0.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 63.2     |\n","|    mean_reward     | -90.5    |\n","| time/              |          |\n","|    total_timesteps | 3820800  |\n","| train/             |          |\n","|    actor_loss      | 11.6     |\n","|    critic_loss     | 26.1     |\n","|    ent_coef        | 0.00808  |\n","|    ent_coef_loss   | 0.166    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34620    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5615     |\n","|    total_timesteps | 3821040  |\n","| train/             |          |\n","|    actor_loss      | 10.6     |\n","|    critic_loss     | 29.6     |\n","|    ent_coef        | 0.00808  |\n","|    ent_coef_loss   | -1.07    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159205   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34630    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5617     |\n","|    total_timesteps | 3822648  |\n","| train/             |          |\n","|    actor_loss      | 11.6     |\n","|    critic_loss     | 6.9      |\n","|    ent_coef        | 0.00818  |\n","|    ent_coef_loss   | 1.54     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159272   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34640    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5618     |\n","|    total_timesteps | 3823680  |\n","| train/             |          |\n","|    actor_loss      | 11.6     |\n","|    critic_loss     | 20.8     |\n","|    ent_coef        | 0.00822  |\n","|    ent_coef_loss   | -0.433   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159315   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34650    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5618     |\n","|    total_timesteps | 3824280  |\n","| train/             |          |\n","|    actor_loss      | 10.8     |\n","|    critic_loss     | 6.91     |\n","|    ent_coef        | 0.0082   |\n","|    ent_coef_loss   | -1.29    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159340   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34660    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5619     |\n","|    total_timesteps | 3825192  |\n","| train/             |          |\n","|    actor_loss      | 10.6     |\n","|    critic_loss     | 4.81     |\n","|    ent_coef        | 0.00832  |\n","|    ent_coef_loss   | -0.819   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159378   |\n","---------------------------------\n","Eval num_timesteps=3825600, episode_reward=-90.23 +/- 1.27\n","Episode length: 79.80 +/- 3.43\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 79.8     |\n","|    mean_reward     | -90.2    |\n","| time/              |          |\n","|    total_timesteps | 3825600  |\n","| train/             |          |\n","|    actor_loss      | 11.8     |\n","|    critic_loss     | 17.4     |\n","|    ent_coef        | 0.00831  |\n","|    ent_coef_loss   | -1.02    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34670    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5621     |\n","|    total_timesteps | 3826416  |\n","| train/             |          |\n","|    actor_loss      | 11.7     |\n","|    critic_loss     | 16.5     |\n","|    ent_coef        | 0.00819  |\n","|    ent_coef_loss   | 1.87     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159429   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34680    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5621     |\n","|    total_timesteps | 3826944  |\n","| train/             |          |\n","|    actor_loss      | 11.9     |\n","|    critic_loss     | 16.2     |\n","|    ent_coef        | 0.00828  |\n","|    ent_coef_loss   | 1.82     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159451   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34690    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5622     |\n","|    total_timesteps | 3827760  |\n","| train/             |          |\n","|    actor_loss      | 11.5     |\n","|    critic_loss     | 16       |\n","|    ent_coef        | 0.00841  |\n","|    ent_coef_loss   | -0.765   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159485   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34700    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5623     |\n","|    total_timesteps | 3828456  |\n","| train/             |          |\n","|    actor_loss      | 10.1     |\n","|    critic_loss     | 10.8     |\n","|    ent_coef        | 0.00847  |\n","|    ent_coef_loss   | -1.46    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159514   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34710    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5623     |\n","|    total_timesteps | 3828648  |\n","| train/             |          |\n","|    actor_loss      | 10.6     |\n","|    critic_loss     | 5.7      |\n","|    ent_coef        | 0.00849  |\n","|    ent_coef_loss   | -1.35    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159522   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34720    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5624     |\n","|    total_timesteps | 3829872  |\n","| train/             |          |\n","|    actor_loss      | 10.9     |\n","|    critic_loss     | 23.6     |\n","|    ent_coef        | 0.00858  |\n","|    ent_coef_loss   | -0.807   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159573   |\n","---------------------------------\n","Eval num_timesteps=3830400, episode_reward=-90.11 +/- 2.68\n","Episode length: 71.60 +/- 4.41\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 71.6     |\n","|    mean_reward     | -90.1    |\n","| time/              |          |\n","|    total_timesteps | 3830400  |\n","| train/             |          |\n","|    actor_loss      | 11       |\n","|    critic_loss     | 24.7     |\n","|    ent_coef        | 0.00847  |\n","|    ent_coef_loss   | -2.02    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34730    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5628     |\n","|    total_timesteps | 3831672  |\n","| train/             |          |\n","|    actor_loss      | 11.1     |\n","|    critic_loss     | 9.71     |\n","|    ent_coef        | 0.00822  |\n","|    ent_coef_loss   | -1.9     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159648   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34740    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5628     |\n","|    total_timesteps | 3831864  |\n","| train/             |          |\n","|    actor_loss      | 11.5     |\n","|    critic_loss     | 10.1     |\n","|    ent_coef        | 0.00819  |\n","|    ent_coef_loss   | 0.44     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159656   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34750    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5629     |\n","|    total_timesteps | 3832584  |\n","| train/             |          |\n","|    actor_loss      | 10.4     |\n","|    critic_loss     | 4.61     |\n","|    ent_coef        | 0.0081   |\n","|    ent_coef_loss   | -3.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159686   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34760    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5630     |\n","|    total_timesteps | 3833376  |\n","| train/             |          |\n","|    actor_loss      | 11.9     |\n","|    critic_loss     | 23.7     |\n","|    ent_coef        | 0.00792  |\n","|    ent_coef_loss   | -0.0777  |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159719   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34770    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5631     |\n","|    total_timesteps | 3833616  |\n","| train/             |          |\n","|    actor_loss      | 12.2     |\n","|    critic_loss     | 16       |\n","|    ent_coef        | 0.00791  |\n","|    ent_coef_loss   | 2.94     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159729   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34780    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5633     |\n","|    total_timesteps | 3835128  |\n","| train/             |          |\n","|    actor_loss      | 10.9     |\n","|    critic_loss     | 4.08     |\n","|    ent_coef        | 0.00805  |\n","|    ent_coef_loss   | -1.57    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159792   |\n","---------------------------------\n","Eval num_timesteps=3835200, episode_reward=-89.36 +/- 2.08\n","Episode length: 86.20 +/- 13.72\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 86.2     |\n","|    mean_reward     | -89.4    |\n","| time/              |          |\n","|    total_timesteps | 3835200  |\n","| train/             |          |\n","|    actor_loss      | 10.1     |\n","|    critic_loss     | 24.1     |\n","|    ent_coef        | 0.00804  |\n","|    ent_coef_loss   | -1.16    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34790    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5635     |\n","|    total_timesteps | 3835536  |\n","| train/             |          |\n","|    actor_loss      | 11.3     |\n","|    critic_loss     | 20.3     |\n","|    ent_coef        | 0.00797  |\n","|    ent_coef_loss   | -0.992   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159809   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34800    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5635     |\n","|    total_timesteps | 3836256  |\n","| train/             |          |\n","|    actor_loss      | 10.7     |\n","|    critic_loss     | 14.3     |\n","|    ent_coef        | 0.00795  |\n","|    ent_coef_loss   | -1.61    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159839   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34810    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5636     |\n","|    total_timesteps | 3837576  |\n","| train/             |          |\n","|    actor_loss      | 12       |\n","|    critic_loss     | 32.6     |\n","|    ent_coef        | 0.008    |\n","|    ent_coef_loss   | 1.62     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159894   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34820    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5637     |\n","|    total_timesteps | 3838392  |\n","| train/             |          |\n","|    actor_loss      | 10.3     |\n","|    critic_loss     | 5.75     |\n","|    ent_coef        | 0.0082   |\n","|    ent_coef_loss   | 0.324    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159928   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34830    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5638     |\n","|    total_timesteps | 3838920  |\n","| train/             |          |\n","|    actor_loss      | 10.9     |\n","|    critic_loss     | 14.7     |\n","|    ent_coef        | 0.00829  |\n","|    ent_coef_loss   | -0.637   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159950   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34840    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5638     |\n","|    total_timesteps | 3839592  |\n","| train/             |          |\n","|    actor_loss      | 10.7     |\n","|    critic_loss     | 13.3     |\n","|    ent_coef        | 0.00832  |\n","|    ent_coef_loss   | -1.18    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159978   |\n","---------------------------------\n","Eval num_timesteps=3840000, episode_reward=-89.03 +/- 1.79\n","Episode length: 69.60 +/- 1.96\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 69.6     |\n","|    mean_reward     | -89      |\n","| time/              |          |\n","|    total_timesteps | 3840000  |\n","| train/             |          |\n","|    actor_loss      | 11.6     |\n","|    critic_loss     | 4.38     |\n","|    ent_coef        | 0.00833  |\n","|    ent_coef_loss   | -0.458   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34850    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5639     |\n","|    total_timesteps | 3840192  |\n","| train/             |          |\n","|    actor_loss      | 12.6     |\n","|    critic_loss     | 24.3     |\n","|    ent_coef        | 0.00834  |\n","|    ent_coef_loss   | 1.86     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160003   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34860    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5640     |\n","|    total_timesteps | 3841416  |\n","| train/             |          |\n","|    actor_loss      | 12.2     |\n","|    critic_loss     | 8.73     |\n","|    ent_coef        | 0.00856  |\n","|    ent_coef_loss   | 0.233    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160054   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34870    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5641     |\n","|    total_timesteps | 3842376  |\n","| train/             |          |\n","|    actor_loss      | 10.6     |\n","|    critic_loss     | 3.18     |\n","|    ent_coef        | 0.00835  |\n","|    ent_coef_loss   | -0.0664  |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160094   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34880    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5642     |\n","|    total_timesteps | 3842952  |\n","| train/             |          |\n","|    actor_loss      | 12.2     |\n","|    critic_loss     | 25.2     |\n","|    ent_coef        | 0.00833  |\n","|    ent_coef_loss   | 1.47     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160118   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34890    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5643     |\n","|    total_timesteps | 3844080  |\n","| train/             |          |\n","|    actor_loss      | 11.4     |\n","|    critic_loss     | 22.4     |\n","|    ent_coef        | 0.00835  |\n","|    ent_coef_loss   | -1.69    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160165   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34900    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5643     |\n","|    total_timesteps | 3844776  |\n","| train/             |          |\n","|    actor_loss      | 10.7     |\n","|    critic_loss     | 3.03     |\n","|    ent_coef        | 0.00832  |\n","|    ent_coef_loss   | -0.00345 |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160194   |\n","---------------------------------\n","Eval num_timesteps=3844800, episode_reward=-89.00 +/- 0.89\n","Episode length: 69.60 +/- 10.29\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 69.6     |\n","|    mean_reward     | -89      |\n","| time/              |          |\n","|    total_timesteps | 3844800  |\n","| train/             |          |\n","|    actor_loss      | 11.8     |\n","|    critic_loss     | 14.1     |\n","|    ent_coef        | 0.00832  |\n","|    ent_coef_loss   | -0.491   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34910    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5646     |\n","|    total_timesteps | 3845712  |\n","| train/             |          |\n","|    actor_loss      | 12.3     |\n","|    critic_loss     | 6.16     |\n","|    ent_coef        | 0.00826  |\n","|    ent_coef_loss   | 2.16     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160233   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34920    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5647     |\n","|    total_timesteps | 3846216  |\n","| train/             |          |\n","|    actor_loss      | 11.9     |\n","|    critic_loss     | 12.7     |\n","|    ent_coef        | 0.00823  |\n","|    ent_coef_loss   | 2.45     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160254   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34930    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5648     |\n","|    total_timesteps | 3847512  |\n","| train/             |          |\n","|    actor_loss      | 10.4     |\n","|    critic_loss     | 9.81     |\n","|    ent_coef        | 0.00806  |\n","|    ent_coef_loss   | -2.01    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160308   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34940    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5649     |\n","|    total_timesteps | 3847680  |\n","| train/             |          |\n","|    actor_loss      | 12.2     |\n","|    critic_loss     | 15       |\n","|    ent_coef        | 0.00804  |\n","|    ent_coef_loss   | 0.201    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160315   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34950    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5650     |\n","|    total_timesteps | 3848760  |\n","| train/             |          |\n","|    actor_loss      | 11.8     |\n","|    critic_loss     | 20.5     |\n","|    ent_coef        | 0.00784  |\n","|    ent_coef_loss   | 1.8      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160360   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34960    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5651     |\n","|    total_timesteps | 3849504  |\n","| train/             |          |\n","|    actor_loss      | 12.6     |\n","|    critic_loss     | 6.24     |\n","|    ent_coef        | 0.00777  |\n","|    ent_coef_loss   | 1.33     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160391   |\n","---------------------------------\n","Eval num_timesteps=3849600, episode_reward=-91.37 +/- 1.42\n","Episode length: 74.80 +/- 6.37\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 74.8     |\n","|    mean_reward     | -91.4    |\n","| time/              |          |\n","|    total_timesteps | 3849600  |\n","| train/             |          |\n","|    actor_loss      | 10.7     |\n","|    critic_loss     | 15.3     |\n","|    ent_coef        | 0.00777  |\n","|    ent_coef_loss   | -0.0233  |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34970    |\n","|    fps             | 680      |\n","|    time_elapsed    | 5655     |\n","|    total_timesteps | 3851112  |\n","| train/             |          |\n","|    actor_loss      | 12.9     |\n","|    critic_loss     | 5.36     |\n","|    ent_coef        | 0.00789  |\n","|    ent_coef_loss   | 2.29     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160458   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34980    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5655     |\n","|    total_timesteps | 3851736  |\n","| train/             |          |\n","|    actor_loss      | 10.8     |\n","|    critic_loss     | 31.7     |\n","|    ent_coef        | 0.00796  |\n","|    ent_coef_loss   | 0.0855   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160484   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 34990    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5656     |\n","|    total_timesteps | 3852480  |\n","| train/             |          |\n","|    actor_loss      | 10.8     |\n","|    critic_loss     | 4.94     |\n","|    ent_coef        | 0.008    |\n","|    ent_coef_loss   | 1.47     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160515   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35000    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5657     |\n","|    total_timesteps | 3853776  |\n","| train/             |          |\n","|    actor_loss      | 11.4     |\n","|    critic_loss     | 5.02     |\n","|    ent_coef        | 0.00802  |\n","|    ent_coef_loss   | -1.12    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160569   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35010    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5657     |\n","|    total_timesteps | 3854256  |\n","| train/             |          |\n","|    actor_loss      | 10.6     |\n","|    critic_loss     | 14.2     |\n","|    ent_coef        | 0.00796  |\n","|    ent_coef_loss   | -1.03    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160589   |\n","---------------------------------\n","Eval num_timesteps=3854400, episode_reward=-94.25 +/- 0.48\n","Episode length: 99.00 +/- 7.35\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 99       |\n","|    mean_reward     | -94.2    |\n","| time/              |          |\n","|    total_timesteps | 3854400  |\n","| train/             |          |\n","|    actor_loss      | 11.9     |\n","|    critic_loss     | 9.99     |\n","|    ent_coef        | 0.00796  |\n","|    ent_coef_loss   | 0.88     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35020    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5659     |\n","|    total_timesteps | 3855528  |\n","| train/             |          |\n","|    actor_loss      | 12.2     |\n","|    critic_loss     | 19.9     |\n","|    ent_coef        | 0.00822  |\n","|    ent_coef_loss   | 2.27     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160642   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35030    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5660     |\n","|    total_timesteps | 3855984  |\n","| train/             |          |\n","|    actor_loss      | 11.4     |\n","|    critic_loss     | 8.63     |\n","|    ent_coef        | 0.00828  |\n","|    ent_coef_loss   | -0.433   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160661   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35040    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5661     |\n","|    total_timesteps | 3857496  |\n","| train/             |          |\n","|    actor_loss      | 11.4     |\n","|    critic_loss     | 59.1     |\n","|    ent_coef        | 0.00827  |\n","|    ent_coef_loss   | -0.481   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160724   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35050    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5662     |\n","|    total_timesteps | 3858168  |\n","| train/             |          |\n","|    actor_loss      | 9.39     |\n","|    critic_loss     | 2.55     |\n","|    ent_coef        | 0.00822  |\n","|    ent_coef_loss   | -1.91    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160752   |\n","---------------------------------\n","Eval num_timesteps=3859200, episode_reward=-88.59 +/- 1.32\n","Episode length: 69.80 +/- 10.78\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 69.8     |\n","|    mean_reward     | -88.6    |\n","| time/              |          |\n","|    total_timesteps | 3859200  |\n","| train/             |          |\n","|    actor_loss      | 11.1     |\n","|    critic_loss     | 7.11     |\n","|    ent_coef        | 0.00824  |\n","|    ent_coef_loss   | 1.99     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35060    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5665     |\n","|    total_timesteps | 3860424  |\n","| train/             |          |\n","|    actor_loss      | 11.4     |\n","|    critic_loss     | 12.2     |\n","|    ent_coef        | 0.00854  |\n","|    ent_coef_loss   | 0.036    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160846   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35070    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5665     |\n","|    total_timesteps | 3860856  |\n","| train/             |          |\n","|    actor_loss      | 12       |\n","|    critic_loss     | 10.2     |\n","|    ent_coef        | 0.00846  |\n","|    ent_coef_loss   | -1.67    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160864   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35080    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5666     |\n","|    total_timesteps | 3861768  |\n","| train/             |          |\n","|    actor_loss      | 12       |\n","|    critic_loss     | 4.56     |\n","|    ent_coef        | 0.00819  |\n","|    ent_coef_loss   | 0.00854  |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160902   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35090    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5667     |\n","|    total_timesteps | 3862632  |\n","| train/             |          |\n","|    actor_loss      | 12.6     |\n","|    critic_loss     | 20       |\n","|    ent_coef        | 0.00819  |\n","|    ent_coef_loss   | 0.257    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160938   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35100    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5669     |\n","|    total_timesteps | 3863400  |\n","| train/             |          |\n","|    actor_loss      | 12.2     |\n","|    critic_loss     | 32.1     |\n","|    ent_coef        | 0.00807  |\n","|    ent_coef_loss   | -0.748   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160970   |\n","---------------------------------\n","Eval num_timesteps=3864000, episode_reward=-90.68 +/- 0.79\n","Episode length: 87.00 +/- 7.35\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 87       |\n","|    mean_reward     | -90.7    |\n","| time/              |          |\n","|    total_timesteps | 3864000  |\n","| train/             |          |\n","|    actor_loss      | 12.8     |\n","|    critic_loss     | 39.3     |\n","|    ent_coef        | 0.0079   |\n","|    ent_coef_loss   | 0.984    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35110    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5673     |\n","|    total_timesteps | 3865080  |\n","| train/             |          |\n","|    actor_loss      | 11.2     |\n","|    critic_loss     | 9.48     |\n","|    ent_coef        | 0.00785  |\n","|    ent_coef_loss   | 0.432    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161040   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35120    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5673     |\n","|    total_timesteps | 3865440  |\n","| train/             |          |\n","|    actor_loss      | 11.8     |\n","|    critic_loss     | 16.3     |\n","|    ent_coef        | 0.00794  |\n","|    ent_coef_loss   | 1.95     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161055   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35130    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5674     |\n","|    total_timesteps | 3865680  |\n","| train/             |          |\n","|    actor_loss      | 11.1     |\n","|    critic_loss     | 15       |\n","|    ent_coef        | 0.00803  |\n","|    ent_coef_loss   | 2.5      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161065   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35140    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5676     |\n","|    total_timesteps | 3867744  |\n","| train/             |          |\n","|    actor_loss      | 11.1     |\n","|    critic_loss     | 5.7      |\n","|    ent_coef        | 0.00839  |\n","|    ent_coef_loss   | -0.00541 |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161151   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35150    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5676     |\n","|    total_timesteps | 3868344  |\n","| train/             |          |\n","|    actor_loss      | 12       |\n","|    critic_loss     | 22.5     |\n","|    ent_coef        | 0.00828  |\n","|    ent_coef_loss   | -1.91    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161176   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35160    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5676     |\n","|    total_timesteps | 3868632  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 19.9     |\n","|    ent_coef        | 0.00818  |\n","|    ent_coef_loss   | -1.14    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161188   |\n","---------------------------------\n","Eval num_timesteps=3868800, episode_reward=-92.83 +/- 2.56\n","Episode length: 80.60 +/- 0.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 80.6     |\n","|    mean_reward     | -92.8    |\n","| time/              |          |\n","|    total_timesteps | 3868800  |\n","| train/             |          |\n","|    actor_loss      | 12.7     |\n","|    critic_loss     | 19.7     |\n","|    ent_coef        | 0.00813  |\n","|    ent_coef_loss   | -0.573   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35170    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5679     |\n","|    total_timesteps | 3870528  |\n","| train/             |          |\n","|    actor_loss      | 11.8     |\n","|    critic_loss     | 29.3     |\n","|    ent_coef        | 0.00805  |\n","|    ent_coef_loss   | 1.61     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161267   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35180    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5680     |\n","|    total_timesteps | 3871320  |\n","| train/             |          |\n","|    actor_loss      | 11.4     |\n","|    critic_loss     | 26.3     |\n","|    ent_coef        | 0.00794  |\n","|    ent_coef_loss   | -1.65    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161300   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35190    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5681     |\n","|    total_timesteps | 3872664  |\n","| train/             |          |\n","|    actor_loss      | 11.4     |\n","|    critic_loss     | 8.71     |\n","|    ent_coef        | 0.00785  |\n","|    ent_coef_loss   | 0.24     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161356   |\n","---------------------------------\n","Eval num_timesteps=3873600, episode_reward=-87.66 +/- 0.69\n","Episode length: 64.00 +/- 9.80\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 64       |\n","|    mean_reward     | -87.7    |\n","| time/              |          |\n","|    total_timesteps | 3873600  |\n","| train/             |          |\n","|    actor_loss      | 12.2     |\n","|    critic_loss     | 14.5     |\n","|    ent_coef        | 0.00784  |\n","|    ent_coef_loss   | 4.62     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35200    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5682     |\n","|    total_timesteps | 3873672  |\n","| train/             |          |\n","|    actor_loss      | 11.8     |\n","|    critic_loss     | 4.06     |\n","|    ent_coef        | 0.00787  |\n","|    ent_coef_loss   | 3.54     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161398   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35210    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5683     |\n","|    total_timesteps | 3874968  |\n","| train/             |          |\n","|    actor_loss      | 11.4     |\n","|    critic_loss     | 33.5     |\n","|    ent_coef        | 0.00811  |\n","|    ent_coef_loss   | -0.472   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161452   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35220    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5684     |\n","|    total_timesteps | 3875400  |\n","| train/             |          |\n","|    actor_loss      | 10.5     |\n","|    critic_loss     | 6.63     |\n","|    ent_coef        | 0.00811  |\n","|    ent_coef_loss   | -2.38    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161470   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35230    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5685     |\n","|    total_timesteps | 3876768  |\n","| train/             |          |\n","|    actor_loss      | 11.4     |\n","|    critic_loss     | 2.9      |\n","|    ent_coef        | 0.00818  |\n","|    ent_coef_loss   | -2.38    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161527   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35240    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5686     |\n","|    total_timesteps | 3877560  |\n","| train/             |          |\n","|    actor_loss      | 12.3     |\n","|    critic_loss     | 6.97     |\n","|    ent_coef        | 0.00802  |\n","|    ent_coef_loss   | 0.126    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161560   |\n","---------------------------------\n","Eval num_timesteps=3878400, episode_reward=-89.97 +/- 1.08\n","Episode length: 74.20 +/- 10.78\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 74.2     |\n","|    mean_reward     | -90      |\n","| time/              |          |\n","|    total_timesteps | 3878400  |\n","| train/             |          |\n","|    actor_loss      | 12.8     |\n","|    critic_loss     | 19       |\n","|    ent_coef        | 0.00806  |\n","|    ent_coef_loss   | 2.22     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35250    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5689     |\n","|    total_timesteps | 3878592  |\n","| train/             |          |\n","|    actor_loss      | 11.5     |\n","|    critic_loss     | 19.3     |\n","|    ent_coef        | 0.0081   |\n","|    ent_coef_loss   | 1.45     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161603   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35260    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5691     |\n","|    total_timesteps | 3879576  |\n","| train/             |          |\n","|    actor_loss      | 12       |\n","|    critic_loss     | 23       |\n","|    ent_coef        | 0.0082   |\n","|    ent_coef_loss   | 0.387    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161644   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35270    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5691     |\n","|    total_timesteps | 3879792  |\n","| train/             |          |\n","|    actor_loss      | 11.8     |\n","|    critic_loss     | 9.26     |\n","|    ent_coef        | 0.00825  |\n","|    ent_coef_loss   | 2.22     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161653   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35280    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5693     |\n","|    total_timesteps | 3881088  |\n","| train/             |          |\n","|    actor_loss      | 12.7     |\n","|    critic_loss     | 8.11     |\n","|    ent_coef        | 0.00825  |\n","|    ent_coef_loss   | -0.239   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161707   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35290    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5695     |\n","|    total_timesteps | 3882360  |\n","| train/             |          |\n","|    actor_loss      | 10.9     |\n","|    critic_loss     | 62.9     |\n","|    ent_coef        | 0.0082   |\n","|    ent_coef_loss   | 0.153    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161760   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35300    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5695     |\n","|    total_timesteps | 3882744  |\n","| train/             |          |\n","|    actor_loss      | 12.3     |\n","|    critic_loss     | 5.39     |\n","|    ent_coef        | 0.00823  |\n","|    ent_coef_loss   | 1.08     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161776   |\n","---------------------------------\n","Eval num_timesteps=3883200, episode_reward=-94.30 +/- 2.22\n","Episode length: 118.60 +/- 21.56\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 119      |\n","|    mean_reward     | -94.3    |\n","| time/              |          |\n","|    total_timesteps | 3883200  |\n","| train/             |          |\n","|    actor_loss      | 12.2     |\n","|    critic_loss     | 6.99     |\n","|    ent_coef        | 0.00823  |\n","|    ent_coef_loss   | 0.939    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35310    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5698     |\n","|    total_timesteps | 3884352  |\n","| train/             |          |\n","|    actor_loss      | 11.5     |\n","|    critic_loss     | 18.1     |\n","|    ent_coef        | 0.00846  |\n","|    ent_coef_loss   | 0.464    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161843   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35320    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5698     |\n","|    total_timesteps | 3884640  |\n","| train/             |          |\n","|    actor_loss      | 11       |\n","|    critic_loss     | 20.2     |\n","|    ent_coef        | 0.00848  |\n","|    ent_coef_loss   | 0.464    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161855   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35330    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5700     |\n","|    total_timesteps | 3886728  |\n","| train/             |          |\n","|    actor_loss      | 12.7     |\n","|    critic_loss     | 16.1     |\n","|    ent_coef        | 0.00847  |\n","|    ent_coef_loss   | -0.905   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161942   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35340    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5701     |\n","|    total_timesteps | 3887976  |\n","| train/             |          |\n","|    actor_loss      | 10.6     |\n","|    critic_loss     | 15.9     |\n","|    ent_coef        | 0.00831  |\n","|    ent_coef_loss   | 0.0658   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161994   |\n","---------------------------------\n","Eval num_timesteps=3888000, episode_reward=-89.36 +/- 1.13\n","Episode length: 93.80 +/- 28.41\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 93.8     |\n","|    mean_reward     | -89.4    |\n","| time/              |          |\n","|    total_timesteps | 3888000  |\n","| train/             |          |\n","|    actor_loss      | 12.4     |\n","|    critic_loss     | 12       |\n","|    ent_coef        | 0.00831  |\n","|    ent_coef_loss   | 1.02     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35350    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5703     |\n","|    total_timesteps | 3889416  |\n","| train/             |          |\n","|    actor_loss      | 11.4     |\n","|    critic_loss     | 13.6     |\n","|    ent_coef        | 0.00831  |\n","|    ent_coef_loss   | 0.231    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162054   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35360    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5704     |\n","|    total_timesteps | 3890184  |\n","| train/             |          |\n","|    actor_loss      | 9.82     |\n","|    critic_loss     | 23.1     |\n","|    ent_coef        | 0.00825  |\n","|    ent_coef_loss   | -1.87    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162086   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35370    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5705     |\n","|    total_timesteps | 3891168  |\n","| train/             |          |\n","|    actor_loss      | 11.8     |\n","|    critic_loss     | 10.9     |\n","|    ent_coef        | 0.00837  |\n","|    ent_coef_loss   | 1.38     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162127   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35380    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5707     |\n","|    total_timesteps | 3892584  |\n","| train/             |          |\n","|    actor_loss      | 11.9     |\n","|    critic_loss     | 18.6     |\n","|    ent_coef        | 0.00833  |\n","|    ent_coef_loss   | -0.355   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162186   |\n","---------------------------------\n","Eval num_timesteps=3892800, episode_reward=-91.87 +/- 3.86\n","Episode length: 110.80 +/- 20.58\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 111      |\n","|    mean_reward     | -91.9    |\n","| time/              |          |\n","|    total_timesteps | 3892800  |\n","| train/             |          |\n","|    actor_loss      | 12.5     |\n","|    critic_loss     | 25.2     |\n","|    ent_coef        | 0.00833  |\n","|    ent_coef_loss   | 0.198    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35390    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5711     |\n","|    total_timesteps | 3893664  |\n","| train/             |          |\n","|    actor_loss      | 11.3     |\n","|    critic_loss     | 11.1     |\n","|    ent_coef        | 0.00834  |\n","|    ent_coef_loss   | -0.896   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162231   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35400    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5713     |\n","|    total_timesteps | 3895440  |\n","| train/             |          |\n","|    actor_loss      | 11.2     |\n","|    critic_loss     | 10.3     |\n","|    ent_coef        | 0.00805  |\n","|    ent_coef_loss   | -2.29    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162305   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35410    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5715     |\n","|    total_timesteps | 3896448  |\n","| train/             |          |\n","|    actor_loss      | 12       |\n","|    critic_loss     | 20.3     |\n","|    ent_coef        | 0.00784  |\n","|    ent_coef_loss   | -0.121   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162347   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35420    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5715     |\n","|    total_timesteps | 3897312  |\n","| train/             |          |\n","|    actor_loss      | 11       |\n","|    critic_loss     | 17.8     |\n","|    ent_coef        | 0.00773  |\n","|    ent_coef_loss   | 1.75     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162383   |\n","---------------------------------\n","Eval num_timesteps=3897600, episode_reward=-88.69 +/- 0.42\n","Episode length: 87.00 +/- 2.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 87       |\n","|    mean_reward     | -88.7    |\n","| time/              |          |\n","|    total_timesteps | 3897600  |\n","| train/             |          |\n","|    actor_loss      | 11.4     |\n","|    critic_loss     | 15.8     |\n","|    ent_coef        | 0.00776  |\n","|    ent_coef_loss   | -1.07    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35430    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5717     |\n","|    total_timesteps | 3898176  |\n","| train/             |          |\n","|    actor_loss      | 12.2     |\n","|    critic_loss     | 18.4     |\n","|    ent_coef        | 0.00775  |\n","|    ent_coef_loss   | 0.192    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162419   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35440    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5718     |\n","|    total_timesteps | 3899808  |\n","| train/             |          |\n","|    actor_loss      | 11.2     |\n","|    critic_loss     | 14.1     |\n","|    ent_coef        | 0.00801  |\n","|    ent_coef_loss   | 2.23     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162487   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35450    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5719     |\n","|    total_timesteps | 3900912  |\n","| train/             |          |\n","|    actor_loss      | 12.5     |\n","|    critic_loss     | 8.69     |\n","|    ent_coef        | 0.00815  |\n","|    ent_coef_loss   | 0.0475   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162533   |\n","---------------------------------\n","Eval num_timesteps=3902400, episode_reward=-89.61 +/- 0.73\n","Episode length: 87.60 +/- 4.41\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 87.6     |\n","|    mean_reward     | -89.6    |\n","| time/              |          |\n","|    total_timesteps | 3902400  |\n","| train/             |          |\n","|    actor_loss      | 11.2     |\n","|    critic_loss     | 19.7     |\n","|    ent_coef        | 0.00827  |\n","|    ent_coef_loss   | -1.14    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35460    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5722     |\n","|    total_timesteps | 3902784  |\n","| train/             |          |\n","|    actor_loss      | 10.4     |\n","|    critic_loss     | 10.5     |\n","|    ent_coef        | 0.00823  |\n","|    ent_coef_loss   | -2.23    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162611   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35470    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5723     |\n","|    total_timesteps | 3904440  |\n","| train/             |          |\n","|    actor_loss      | 12.2     |\n","|    critic_loss     | 35.6     |\n","|    ent_coef        | 0.00833  |\n","|    ent_coef_loss   | 2.52     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162680   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35480    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5724     |\n","|    total_timesteps | 3905304  |\n","| train/             |          |\n","|    actor_loss      | 11.9     |\n","|    critic_loss     | 27.8     |\n","|    ent_coef        | 0.0085   |\n","|    ent_coef_loss   | -0.156   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162716   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35490    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5725     |\n","|    total_timesteps | 3906624  |\n","| train/             |          |\n","|    actor_loss      | 12.7     |\n","|    critic_loss     | 43.9     |\n","|    ent_coef        | 0.00837  |\n","|    ent_coef_loss   | 1.31     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162771   |\n","---------------------------------\n","Eval num_timesteps=3907200, episode_reward=-96.43 +/- 2.42\n","Episode length: 119.80 +/- 25.96\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 120      |\n","|    mean_reward     | -96.4    |\n","| time/              |          |\n","|    total_timesteps | 3907200  |\n","| train/             |          |\n","|    actor_loss      | 11.1     |\n","|    critic_loss     | 20.1     |\n","|    ent_coef        | 0.0083   |\n","|    ent_coef_loss   | -0.0415  |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35500    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5730     |\n","|    total_timesteps | 3908448  |\n","| train/             |          |\n","|    actor_loss      | 12.4     |\n","|    critic_loss     | 20.1     |\n","|    ent_coef        | 0.00819  |\n","|    ent_coef_loss   | -1.31    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162847   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35510    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5732     |\n","|    total_timesteps | 3909456  |\n","| train/             |          |\n","|    actor_loss      | 11.3     |\n","|    critic_loss     | 31.7     |\n","|    ent_coef        | 0.00826  |\n","|    ent_coef_loss   | 3.52     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162889   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35520    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5733     |\n","|    total_timesteps | 3910392  |\n","| train/             |          |\n","|    actor_loss      | 10.8     |\n","|    critic_loss     | 3.85     |\n","|    ent_coef        | 0.00851  |\n","|    ent_coef_loss   | 1.33     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162928   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35530    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5735     |\n","|    total_timesteps | 3911520  |\n","| train/             |          |\n","|    actor_loss      | 11.3     |\n","|    critic_loss     | 5.65     |\n","|    ent_coef        | 0.00858  |\n","|    ent_coef_loss   | -1.37    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162975   |\n","---------------------------------\n","Eval num_timesteps=3912000, episode_reward=-90.82 +/- 1.01\n","Episode length: 104.20 +/- 16.17\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 104      |\n","|    mean_reward     | -90.8    |\n","| time/              |          |\n","|    total_timesteps | 3912000  |\n","| train/             |          |\n","|    actor_loss      | 12.4     |\n","|    critic_loss     | 9.12     |\n","|    ent_coef        | 0.00855  |\n","|    ent_coef_loss   | 0.162    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35540    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5737     |\n","|    total_timesteps | 3913152  |\n","| train/             |          |\n","|    actor_loss      | 11.6     |\n","|    critic_loss     | 17.1     |\n","|    ent_coef        | 0.00851  |\n","|    ent_coef_loss   | 0.242    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163043   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35550    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5738     |\n","|    total_timesteps | 3914616  |\n","| train/             |          |\n","|    actor_loss      | 11.8     |\n","|    critic_loss     | 8.85     |\n","|    ent_coef        | 0.00864  |\n","|    ent_coef_loss   | -1.02    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163104   |\n","---------------------------------\n","Eval num_timesteps=3916800, episode_reward=-89.12 +/- 0.20\n","Episode length: 97.80 +/- 6.37\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 97.8     |\n","|    mean_reward     | -89.1    |\n","| time/              |          |\n","|    total_timesteps | 3916800  |\n","| train/             |          |\n","|    actor_loss      | 12.2     |\n","|    critic_loss     | 30.4     |\n","|    ent_coef        | 0.00842  |\n","|    ent_coef_loss   | -0.377   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35560    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5742     |\n","|    total_timesteps | 3917400  |\n","| train/             |          |\n","|    actor_loss      | 11.1     |\n","|    critic_loss     | 10.4     |\n","|    ent_coef        | 0.00825  |\n","|    ent_coef_loss   | -2.49    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163220   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35570    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5743     |\n","|    total_timesteps | 3918696  |\n","| train/             |          |\n","|    actor_loss      | 10.5     |\n","|    critic_loss     | 6.27     |\n","|    ent_coef        | 0.008    |\n","|    ent_coef_loss   | -1.57    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163274   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35580    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5743     |\n","|    total_timesteps | 3919272  |\n","| train/             |          |\n","|    actor_loss      | 10.7     |\n","|    critic_loss     | 21       |\n","|    ent_coef        | 0.00788  |\n","|    ent_coef_loss   | -2.44    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163298   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35590    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5745     |\n","|    total_timesteps | 3920880  |\n","| train/             |          |\n","|    actor_loss      | 11.3     |\n","|    critic_loss     | 5.07     |\n","|    ent_coef        | 0.00769  |\n","|    ent_coef_loss   | 3.16     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163365   |\n","---------------------------------\n","Eval num_timesteps=3921600, episode_reward=-92.65 +/- 5.60\n","Episode length: 79.20 +/- 13.72\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 79.2     |\n","|    mean_reward     | -92.7    |\n","| time/              |          |\n","|    total_timesteps | 3921600  |\n","| train/             |          |\n","|    actor_loss      | 10.9     |\n","|    critic_loss     | 6.86     |\n","|    ent_coef        | 0.00784  |\n","|    ent_coef_loss   | -2.01    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35600    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5749     |\n","|    total_timesteps | 3922824  |\n","| train/             |          |\n","|    actor_loss      | 10.9     |\n","|    critic_loss     | 6.47     |\n","|    ent_coef        | 0.00775  |\n","|    ent_coef_loss   | -1.42    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163446   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35610    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5751     |\n","|    total_timesteps | 3924096  |\n","| train/             |          |\n","|    actor_loss      | 11.3     |\n","|    critic_loss     | 5.48     |\n","|    ent_coef        | 0.00784  |\n","|    ent_coef_loss   | 3.33     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163499   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35620    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5752     |\n","|    total_timesteps | 3925032  |\n","| train/             |          |\n","|    actor_loss      | 10.3     |\n","|    critic_loss     | 11.1     |\n","|    ent_coef        | 0.00799  |\n","|    ent_coef_loss   | 0.971    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163538   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35630    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5754     |\n","|    total_timesteps | 3926184  |\n","| train/             |          |\n","|    actor_loss      | 10.3     |\n","|    critic_loss     | 21.2     |\n","|    ent_coef        | 0.00821  |\n","|    ent_coef_loss   | -1.37    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163586   |\n","---------------------------------\n","Eval num_timesteps=3926400, episode_reward=-88.68 +/- 0.65\n","Episode length: 78.60 +/- 7.84\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 78.6     |\n","|    mean_reward     | -88.7    |\n","| time/              |          |\n","|    total_timesteps | 3926400  |\n","| train/             |          |\n","|    actor_loss      | 12.2     |\n","|    critic_loss     | 22.2     |\n","|    ent_coef        | 0.00822  |\n","|    ent_coef_loss   | 0.27     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35640    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5756     |\n","|    total_timesteps | 3926592  |\n","| train/             |          |\n","|    actor_loss      | 11.9     |\n","|    critic_loss     | 20.4     |\n","|    ent_coef        | 0.00824  |\n","|    ent_coef_loss   | -0.946   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163603   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35650    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5757     |\n","|    total_timesteps | 3928536  |\n","| train/             |          |\n","|    actor_loss      | 12       |\n","|    critic_loss     | 37.4     |\n","|    ent_coef        | 0.00792  |\n","|    ent_coef_loss   | -1.15    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163684   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35660    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5758     |\n","|    total_timesteps | 3929040  |\n","| train/             |          |\n","|    actor_loss      | 12.8     |\n","|    critic_loss     | 9.67     |\n","|    ent_coef        | 0.00788  |\n","|    ent_coef_loss   | 0.91     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163705   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35670    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5759     |\n","|    total_timesteps | 3930792  |\n","| train/             |          |\n","|    actor_loss      | 10.9     |\n","|    critic_loss     | 7.45     |\n","|    ent_coef        | 0.00774  |\n","|    ent_coef_loss   | -2.71    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163778   |\n","---------------------------------\n","Eval num_timesteps=3931200, episode_reward=-91.64 +/- 1.53\n","Episode length: 103.00 +/- 17.15\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 103      |\n","|    mean_reward     | -91.6    |\n","| time/              |          |\n","|    total_timesteps | 3931200  |\n","| train/             |          |\n","|    actor_loss      | 11.3     |\n","|    critic_loss     | 5.26     |\n","|    ent_coef        | 0.00768  |\n","|    ent_coef_loss   | -1.84    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35680    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5762     |\n","|    total_timesteps | 3932352  |\n","| train/             |          |\n","|    actor_loss      | 11.3     |\n","|    critic_loss     | 28.6     |\n","|    ent_coef        | 0.00754  |\n","|    ent_coef_loss   | -0.349   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163843   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35690    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5762     |\n","|    total_timesteps | 3932520  |\n","| train/             |          |\n","|    actor_loss      | 11.4     |\n","|    critic_loss     | 8.42     |\n","|    ent_coef        | 0.00754  |\n","|    ent_coef_loss   | -1.21    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163850   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35700    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5763     |\n","|    total_timesteps | 3933864  |\n","| train/             |          |\n","|    actor_loss      | 10.3     |\n","|    critic_loss     | 24.9     |\n","|    ent_coef        | 0.00737  |\n","|    ent_coef_loss   | -2.75    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163906   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35710    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5764     |\n","|    total_timesteps | 3934896  |\n","| train/             |          |\n","|    actor_loss      | 11.2     |\n","|    critic_loss     | 10.9     |\n","|    ent_coef        | 0.00732  |\n","|    ent_coef_loss   | 0.0351   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163949   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35720    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5764     |\n","|    total_timesteps | 3935496  |\n","| train/             |          |\n","|    actor_loss      | 11.9     |\n","|    critic_loss     | 21.7     |\n","|    ent_coef        | 0.00743  |\n","|    ent_coef_loss   | 0.935    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163974   |\n","---------------------------------\n","Eval num_timesteps=3936000, episode_reward=-91.29 +/- 2.85\n","Episode length: 139.20 +/- 18.13\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 139      |\n","|    mean_reward     | -91.3    |\n","| time/              |          |\n","|    total_timesteps | 3936000  |\n","| train/             |          |\n","|    actor_loss      | 9.84     |\n","|    critic_loss     | 3.47     |\n","|    ent_coef        | 0.00754  |\n","|    ent_coef_loss   | -0.913   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35730    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5770     |\n","|    total_timesteps | 3937776  |\n","| train/             |          |\n","|    actor_loss      | 11.4     |\n","|    critic_loss     | 5.88     |\n","|    ent_coef        | 0.00783  |\n","|    ent_coef_loss   | 0.407    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164069   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35740    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5771     |\n","|    total_timesteps | 3938880  |\n","| train/             |          |\n","|    actor_loss      | 10.9     |\n","|    critic_loss     | 9.98     |\n","|    ent_coef        | 0.00776  |\n","|    ent_coef_loss   | -0.379   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164115   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35750    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5774     |\n","|    total_timesteps | 3940584  |\n","| train/             |          |\n","|    actor_loss      | 13.4     |\n","|    critic_loss     | 21.3     |\n","|    ent_coef        | 0.00779  |\n","|    ent_coef_loss   | 1.34     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164186   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35760    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5774     |\n","|    total_timesteps | 3940728  |\n","| train/             |          |\n","|    actor_loss      | 10.7     |\n","|    critic_loss     | 19       |\n","|    ent_coef        | 0.00779  |\n","|    ent_coef_loss   | 0.0253   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164192   |\n","---------------------------------\n","Eval num_timesteps=3940800, episode_reward=-100.96 +/- 1.76\n","Episode length: 93.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 93       |\n","|    mean_reward     | -101     |\n","| time/              |          |\n","|    total_timesteps | 3940800  |\n","| train/             |          |\n","|    actor_loss      | 11.9     |\n","|    critic_loss     | 20.9     |\n","|    ent_coef        | 0.00779  |\n","|    ent_coef_loss   | 2.76     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35770    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5777     |\n","|    total_timesteps | 3943320  |\n","| train/             |          |\n","|    actor_loss      | 11.4     |\n","|    critic_loss     | 18.5     |\n","|    ent_coef        | 0.00784  |\n","|    ent_coef_loss   | 0.232    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164300   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35780    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5778     |\n","|    total_timesteps | 3944376  |\n","| train/             |          |\n","|    actor_loss      | 11.5     |\n","|    critic_loss     | 16.7     |\n","|    ent_coef        | 0.00787  |\n","|    ent_coef_loss   | -0.535   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164344   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35790    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5779     |\n","|    total_timesteps | 3945120  |\n","| train/             |          |\n","|    actor_loss      | 12       |\n","|    critic_loss     | 24       |\n","|    ent_coef        | 0.00782  |\n","|    ent_coef_loss   | 0.254    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164375   |\n","---------------------------------\n","Eval num_timesteps=3945600, episode_reward=-93.95 +/- 2.63\n","Episode length: 135.40 +/- 22.54\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 135      |\n","|    mean_reward     | -94      |\n","| time/              |          |\n","|    total_timesteps | 3945600  |\n","| train/             |          |\n","|    actor_loss      | 12.2     |\n","|    critic_loss     | 12.9     |\n","|    ent_coef        | 0.00788  |\n","|    ent_coef_loss   | 0.175    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35800    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5781     |\n","|    total_timesteps | 3946056  |\n","| train/             |          |\n","|    actor_loss      | 10       |\n","|    critic_loss     | 4.84     |\n","|    ent_coef        | 0.00789  |\n","|    ent_coef_loss   | -2.16    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164414   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35810    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5782     |\n","|    total_timesteps | 3946968  |\n","| train/             |          |\n","|    actor_loss      | 11.4     |\n","|    critic_loss     | 26.7     |\n","|    ent_coef        | 0.00795  |\n","|    ent_coef_loss   | -0.834   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164452   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35820    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5783     |\n","|    total_timesteps | 3948072  |\n","| train/             |          |\n","|    actor_loss      | 11       |\n","|    critic_loss     | 4.73     |\n","|    ent_coef        | 0.00787  |\n","|    ent_coef_loss   | 0.745    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164498   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35830    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5784     |\n","|    total_timesteps | 3949320  |\n","| train/             |          |\n","|    actor_loss      | 11.5     |\n","|    critic_loss     | 31       |\n","|    ent_coef        | 0.00776  |\n","|    ent_coef_loss   | 0.134    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164550   |\n","---------------------------------\n","Eval num_timesteps=3950400, episode_reward=-91.92 +/- 1.54\n","Episode length: 112.40 +/- 7.84\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 112      |\n","|    mean_reward     | -91.9    |\n","| time/              |          |\n","|    total_timesteps | 3950400  |\n","| train/             |          |\n","|    actor_loss      | 11.9     |\n","|    critic_loss     | 14.9     |\n","|    ent_coef        | 0.00779  |\n","|    ent_coef_loss   | -0.38    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35840    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5787     |\n","|    total_timesteps | 3950568  |\n","| train/             |          |\n","|    actor_loss      | 10.8     |\n","|    critic_loss     | 9.96     |\n","|    ent_coef        | 0.0078   |\n","|    ent_coef_loss   | -0.682   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164602   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35850    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5788     |\n","|    total_timesteps | 3951312  |\n","| train/             |          |\n","|    actor_loss      | 11.2     |\n","|    critic_loss     | 14.5     |\n","|    ent_coef        | 0.00781  |\n","|    ent_coef_loss   | 0.0455   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164633   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35860    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5791     |\n","|    total_timesteps | 3952656  |\n","| train/             |          |\n","|    actor_loss      | 12.1     |\n","|    critic_loss     | 3.64     |\n","|    ent_coef        | 0.00818  |\n","|    ent_coef_loss   | 1.99     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164689   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35870    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5792     |\n","|    total_timesteps | 3953424  |\n","| train/             |          |\n","|    actor_loss      | 12.3     |\n","|    critic_loss     | 35.6     |\n","|    ent_coef        | 0.00827  |\n","|    ent_coef_loss   | 0.214    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164721   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35880    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5793     |\n","|    total_timesteps | 3953904  |\n","| train/             |          |\n","|    actor_loss      | 11.7     |\n","|    critic_loss     | 38.3     |\n","|    ent_coef        | 0.00835  |\n","|    ent_coef_loss   | 0.933    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164741   |\n","---------------------------------\n","Eval num_timesteps=3955200, episode_reward=-91.37 +/- 0.84\n","Episode length: 119.00 +/- 26.94\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 119      |\n","|    mean_reward     | -91.4    |\n","| time/              |          |\n","|    total_timesteps | 3955200  |\n","| train/             |          |\n","|    actor_loss      | 10.1     |\n","|    critic_loss     | 19.1     |\n","|    ent_coef        | 0.0084   |\n","|    ent_coef_loss   | 0.266    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35890    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5799     |\n","|    total_timesteps | 3956160  |\n","| train/             |          |\n","|    actor_loss      | 12       |\n","|    critic_loss     | 11.3     |\n","|    ent_coef        | 0.00844  |\n","|    ent_coef_loss   | -0.359   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164835   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35900    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5803     |\n","|    total_timesteps | 3958848  |\n","| train/             |          |\n","|    actor_loss      | 10.9     |\n","|    critic_loss     | 2.59     |\n","|    ent_coef        | 0.00835  |\n","|    ent_coef_loss   | 0.389    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164947   |\n","---------------------------------\n","Eval num_timesteps=3960000, episode_reward=-89.61 +/- 1.10\n","Episode length: 98.80 +/- 18.62\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 98.8     |\n","|    mean_reward     | -89.6    |\n","| time/              |          |\n","|    total_timesteps | 3960000  |\n","| train/             |          |\n","|    actor_loss      | 11.8     |\n","|    critic_loss     | 26.5     |\n","|    ent_coef        | 0.00841  |\n","|    ent_coef_loss   | 0.205    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35910    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5806     |\n","|    total_timesteps | 3960528  |\n","| train/             |          |\n","|    actor_loss      | 12.3     |\n","|    critic_loss     | 8.79     |\n","|    ent_coef        | 0.00845  |\n","|    ent_coef_loss   | 1.61     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165017   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35920    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5807     |\n","|    total_timesteps | 3962472  |\n","| train/             |          |\n","|    actor_loss      | 11.9     |\n","|    critic_loss     | 19.2     |\n","|    ent_coef        | 0.00889  |\n","|    ent_coef_loss   | 0.808    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165098   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35930    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5808     |\n","|    total_timesteps | 3963408  |\n","| train/             |          |\n","|    actor_loss      | 10.8     |\n","|    critic_loss     | 15.4     |\n","|    ent_coef        | 0.00897  |\n","|    ent_coef_loss   | 0.826    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165137   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35940    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5809     |\n","|    total_timesteps | 3964272  |\n","| train/             |          |\n","|    actor_loss      | 10.8     |\n","|    critic_loss     | 18.4     |\n","|    ent_coef        | 0.00865  |\n","|    ent_coef_loss   | -1.27    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165173   |\n","---------------------------------\n","Eval num_timesteps=3964800, episode_reward=-91.01 +/- 0.75\n","Episode length: 141.20 +/- 33.31\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 141      |\n","|    mean_reward     | -91      |\n","| time/              |          |\n","|    total_timesteps | 3964800  |\n","| train/             |          |\n","|    actor_loss      | 11.7     |\n","|    critic_loss     | 9.9      |\n","|    ent_coef        | 0.00856  |\n","|    ent_coef_loss   | 0.946    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35950    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5814     |\n","|    total_timesteps | 3966576  |\n","| train/             |          |\n","|    actor_loss      | 10.7     |\n","|    critic_loss     | 10.9     |\n","|    ent_coef        | 0.00861  |\n","|    ent_coef_loss   | -1.59    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165269   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35960    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5815     |\n","|    total_timesteps | 3967416  |\n","| train/             |          |\n","|    actor_loss      | 11.9     |\n","|    critic_loss     | 23.5     |\n","|    ent_coef        | 0.0083   |\n","|    ent_coef_loss   | -2.72    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165304   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35970    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5817     |\n","|    total_timesteps | 3968712  |\n","| train/             |          |\n","|    actor_loss      | 12.1     |\n","|    critic_loss     | 7.92     |\n","|    ent_coef        | 0.00798  |\n","|    ent_coef_loss   | 0.246    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165358   |\n","---------------------------------\n","Eval num_timesteps=3969600, episode_reward=-89.95 +/- 0.60\n","Episode length: 191.00 +/- 68.59\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 191      |\n","|    mean_reward     | -89.9    |\n","| time/              |          |\n","|    total_timesteps | 3969600  |\n","| train/             |          |\n","|    actor_loss      | 11.7     |\n","|    critic_loss     | 42.7     |\n","|    ent_coef        | 0.00814  |\n","|    ent_coef_loss   | 1.49     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35980    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5822     |\n","|    total_timesteps | 3969816  |\n","| train/             |          |\n","|    actor_loss      | 10.9     |\n","|    critic_loss     | 16.4     |\n","|    ent_coef        | 0.00821  |\n","|    ent_coef_loss   | -0.646   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165404   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 35990    |\n","|    fps             | 681      |\n","|    time_elapsed    | 5823     |\n","|    total_timesteps | 3971352  |\n","| train/             |          |\n","|    actor_loss      | 10.9     |\n","|    critic_loss     | 5.97     |\n","|    ent_coef        | 0.00824  |\n","|    ent_coef_loss   | 0.833    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165468   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 36000    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5823     |\n","|    total_timesteps | 3972048  |\n","| train/             |          |\n","|    actor_loss      | 11       |\n","|    critic_loss     | 9.82     |\n","|    ent_coef        | 0.00828  |\n","|    ent_coef_loss   | 0.565    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165497   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 36010    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5824     |\n","|    total_timesteps | 3973224  |\n","| train/             |          |\n","|    actor_loss      | 11.1     |\n","|    critic_loss     | 12.1     |\n","|    ent_coef        | 0.00821  |\n","|    ent_coef_loss   | -4.02    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165546   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 36020    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5825     |\n","|    total_timesteps | 3974232  |\n","| train/             |          |\n","|    actor_loss      | 10.5     |\n","|    critic_loss     | 16.1     |\n","|    ent_coef        | 0.00795  |\n","|    ent_coef_loss   | -1.09    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165588   |\n","---------------------------------\n","Eval num_timesteps=3974400, episode_reward=-90.40 +/- 1.19\n","Episode length: 102.80 +/- 8.82\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 103      |\n","|    mean_reward     | -90.4    |\n","| time/              |          |\n","|    total_timesteps | 3974400  |\n","| train/             |          |\n","|    actor_loss      | 11.7     |\n","|    critic_loss     | 4.72     |\n","|    ent_coef        | 0.00794  |\n","|    ent_coef_loss   | 0.875    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 36030    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5828     |\n","|    total_timesteps | 3975936  |\n","| train/             |          |\n","|    actor_loss      | 12.1     |\n","|    critic_loss     | 4.64     |\n","|    ent_coef        | 0.0077   |\n","|    ent_coef_loss   | 0.602    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165659   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 36040    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5829     |\n","|    total_timesteps | 3976896  |\n","| train/             |          |\n","|    actor_loss      | 11.5     |\n","|    critic_loss     | 3.56     |\n","|    ent_coef        | 0.00772  |\n","|    ent_coef_loss   | -0.242   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165699   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 36050    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5830     |\n","|    total_timesteps | 3978360  |\n","| train/             |          |\n","|    actor_loss      | 11.2     |\n","|    critic_loss     | 20       |\n","|    ent_coef        | 0.0078   |\n","|    ent_coef_loss   | 0.105    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165760   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 36060    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5830     |\n","|    total_timesteps | 3978744  |\n","| train/             |          |\n","|    actor_loss      | 11.7     |\n","|    critic_loss     | 5.05     |\n","|    ent_coef        | 0.00777  |\n","|    ent_coef_loss   | 1.01     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165776   |\n","---------------------------------\n","Eval num_timesteps=3979200, episode_reward=-91.08 +/- 0.18\n","Episode length: 92.60 +/- 12.74\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 92.6     |\n","|    mean_reward     | -91.1    |\n","| time/              |          |\n","|    total_timesteps | 3979200  |\n","| train/             |          |\n","|    actor_loss      | 11.3     |\n","|    critic_loss     | 30.2     |\n","|    ent_coef        | 0.00776  |\n","|    ent_coef_loss   | 0.662    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 36070    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5833     |\n","|    total_timesteps | 3979488  |\n","| train/             |          |\n","|    actor_loss      | 11.2     |\n","|    critic_loss     | 17.5     |\n","|    ent_coef        | 0.00783  |\n","|    ent_coef_loss   | 1.22     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165807   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 36080    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5835     |\n","|    total_timesteps | 3980856  |\n","| train/             |          |\n","|    actor_loss      | 11.9     |\n","|    critic_loss     | 15.1     |\n","|    ent_coef        | 0.00815  |\n","|    ent_coef_loss   | 0.256    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165864   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 36090    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5836     |\n","|    total_timesteps | 3981480  |\n","| train/             |          |\n","|    actor_loss      | 11.6     |\n","|    critic_loss     | 10.8     |\n","|    ent_coef        | 0.00816  |\n","|    ent_coef_loss   | -0.0187  |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165890   |\n","---------------------------------\n","Eval num_timesteps=3984000, episode_reward=-94.49 +/- 0.64\n","Episode length: 95.80 +/- 11.27\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 95.8     |\n","|    mean_reward     | -94.5    |\n","| time/              |          |\n","|    total_timesteps | 3984000  |\n","| train/             |          |\n","|    actor_loss      | 11       |\n","|    critic_loss     | 8.89     |\n","|    ent_coef        | 0.00801  |\n","|    ent_coef_loss   | 0.285    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 36100    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5841     |\n","|    total_timesteps | 3984720  |\n","| train/             |          |\n","|    actor_loss      | 10.2     |\n","|    critic_loss     | 25.3     |\n","|    ent_coef        | 0.0082   |\n","|    ent_coef_loss   | -1.31    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166025   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 36110    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5843     |\n","|    total_timesteps | 3986184  |\n","| train/             |          |\n","|    actor_loss      | 10.6     |\n","|    critic_loss     | 21.9     |\n","|    ent_coef        | 0.00782  |\n","|    ent_coef_loss   | -3.35    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166086   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 36120    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5844     |\n","|    total_timesteps | 3988032  |\n","| train/             |          |\n","|    actor_loss      | 12.6     |\n","|    critic_loss     | 7.5      |\n","|    ent_coef        | 0.00772  |\n","|    ent_coef_loss   | 0.0561   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166163   |\n","---------------------------------\n","Eval num_timesteps=3988800, episode_reward=-95.06 +/- 4.33\n","Episode length: 90.20 +/- 30.86\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 90.2     |\n","|    mean_reward     | -95.1    |\n","| time/              |          |\n","|    total_timesteps | 3988800  |\n","| train/             |          |\n","|    actor_loss      | 12       |\n","|    critic_loss     | 10.4     |\n","|    ent_coef        | 0.00776  |\n","|    ent_coef_loss   | 1.46     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 36130    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5847     |\n","|    total_timesteps | 3989592  |\n","| train/             |          |\n","|    actor_loss      | 11.7     |\n","|    critic_loss     | 26.2     |\n","|    ent_coef        | 0.0078   |\n","|    ent_coef_loss   | 0.053    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166228   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 36140    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5848     |\n","|    total_timesteps | 3991320  |\n","| train/             |          |\n","|    actor_loss      | 12.2     |\n","|    critic_loss     | 12.2     |\n","|    ent_coef        | 0.00797  |\n","|    ent_coef_loss   | 0.861    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166300   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 36150    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5849     |\n","|    total_timesteps | 3992160  |\n","| train/             |          |\n","|    actor_loss      | 11.6     |\n","|    critic_loss     | 27.6     |\n","|    ent_coef        | 0.00817  |\n","|    ent_coef_loss   | 0.587    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166335   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 36160    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5850     |\n","|    total_timesteps | 3993240  |\n","| train/             |          |\n","|    actor_loss      | 11.5     |\n","|    critic_loss     | 29.4     |\n","|    ent_coef        | 0.00819  |\n","|    ent_coef_loss   | -1.38    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166380   |\n","---------------------------------\n","Eval num_timesteps=3993600, episode_reward=-86.02 +/- 5.23\n","Episode length: 101.60 +/- 9.31\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 102      |\n","|    mean_reward     | -86      |\n","| time/              |          |\n","|    total_timesteps | 3993600  |\n","| train/             |          |\n","|    actor_loss      | 11.6     |\n","|    critic_loss     | 24.2     |\n","|    ent_coef        | 0.00822  |\n","|    ent_coef_loss   | 0.299    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 36170    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5853     |\n","|    total_timesteps | 3994584  |\n","| train/             |          |\n","|    actor_loss      | 11.2     |\n","|    critic_loss     | 4.34     |\n","|    ent_coef        | 0.00834  |\n","|    ent_coef_loss   | 0.0566   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166436   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 36180    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5854     |\n","|    total_timesteps | 3995424  |\n","| train/             |          |\n","|    actor_loss      | 11.1     |\n","|    critic_loss     | 9.34     |\n","|    ent_coef        | 0.00822  |\n","|    ent_coef_loss   | -0.295   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166471   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 36190    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5856     |\n","|    total_timesteps | 3997008  |\n","| train/             |          |\n","|    actor_loss      | 11       |\n","|    critic_loss     | 16.4     |\n","|    ent_coef        | 0.00783  |\n","|    ent_coef_loss   | -2.76    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166537   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 36200    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5857     |\n","|    total_timesteps | 3997920  |\n","| train/             |          |\n","|    actor_loss      | 12       |\n","|    critic_loss     | 18.6     |\n","|    ent_coef        | 0.00767  |\n","|    ent_coef_loss   | -1.37    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166575   |\n","---------------------------------\n","Eval num_timesteps=3998400, episode_reward=-92.28 +/- 2.12\n","Episode length: 74.40 +/- 11.76\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 74.4     |\n","|    mean_reward     | -92.3    |\n","| time/              |          |\n","|    total_timesteps | 3998400  |\n","| train/             |          |\n","|    actor_loss      | 11.6     |\n","|    critic_loss     | 25       |\n","|    ent_coef        | 0.00765  |\n","|    ent_coef_loss   | -0.154   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 36210    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5860     |\n","|    total_timesteps | 3998592  |\n","| train/             |          |\n","|    actor_loss      | 12.6     |\n","|    critic_loss     | 4.97     |\n","|    ent_coef        | 0.00766  |\n","|    ent_coef_loss   | 2.98     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166603   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 36220    |\n","|    fps             | 682      |\n","|    time_elapsed    | 5861     |\n","|    total_timesteps | 3999792  |\n","| train/             |          |\n","|    actor_loss      | 12.7     |\n","|    critic_loss     | 23.7     |\n","|    ent_coef        | 0.00771  |\n","|    ent_coef_loss   | 1.36     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166653   |\n","---------------------------------\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"s_YTQjcU9ANN"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"mount_file_id":"1tNH0xWOkEmzP-ic_xuZPO9HHOk_62LLx","authorship_tag":"ABX9TyOtxJmplp9GZx7exFE7G06y"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}