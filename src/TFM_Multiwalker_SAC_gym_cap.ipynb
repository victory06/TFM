{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"d8JAmEUyj9De"},"outputs":[],"source":["# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n","mount='/content/gdrive'\n","drive_root = mount + \"/My Drive/TFM\"\n","\n","try:\n","  from google.colab import drive\n","  IN_COLAB=True\n","except:\n","  IN_COLAB=False"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19992,"status":"ok","timestamp":1698762870892,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-60},"id":"DrEo9QnxkAne","outputId":"e8aba4fa-d73c-47ea-a249-55d9ead1dd04"},"outputs":[{"output_type":"stream","name":"stdout","text":["We're running Colab\n","Colab: mounting Google drive on  /content/gdrive\n","Mounted at /content/gdrive\n","\n","Colab: making sure  /content/gdrive/My Drive/TFM  exists.\n","\n","Colab: Changing directory to  /content/gdrive/My Drive/TFM\n","/content/gdrive/My Drive/TFM\n","Archivos en el directorio: \n","['Entrenamientos antiguos sin logs', '.ipynb_checkpoints', 'Entrenamientos_log_no_eval', 'PPO_policies', 'TFM_PPO_pettingzoo_gym_cap.ipynb', 'TFM_PPO_new_pettingzoo_gym_cap.ipynb', 'DQN_new_pettingzoo_gym_cap.ipynb', 'multi_car_racing', 'policy_log_eval', 'DQN_policies', 'results_rllib', 'MCR_TFM.ipynb', 'multiwalker_ddpg_log_eval', 'multiwalker_sac_log_eval', 'multiwalker_ddpg.zip', 'multiwalker_ppo_log_eval', 'multiwalker_ppo.zip', 'multiwalker_td3_log_eval', 'multiwalker_sac2_log_eval', 'multiwalker_td3_2_log_eval', 'multiwalker_sac3_log_eval', 'multiwalker_sac3.zip', 'TFM_Multiwalker_SAC_gym_cap.ipynb', 'multiwalker_ppo_2_log_eval', 'multiwalker_ddpg2_log_eval', 'multiwalker_ppo_2.zip', 'TFM_Multiwalker_DDPG_PPO_gym_cap.ipynb', '=2.13', 'multiwalker_td3_3_log_eval', 'TFM_Multiwalker_DDPG_gym_cap.ipynb', 'TFM_Multiwalker_TD3_gym_cap.ipynb']\n"]}],"source":["# Switch to the directory on the Google Drive that you want to use\n","import os\n","if IN_COLAB:\n","  print(\"We're running Colab\")\n","\n","  if IN_COLAB:\n","    # Mount the Google Drive at mount\n","    print(\"Colab: mounting Google drive on \", mount)\n","\n","    drive.mount(mount)\n","\n","    # Create drive_root if it doesn't exist\n","    create_drive_root = True\n","    if create_drive_root:\n","      print(\"\\nColab: making sure \", drive_root, \" exists.\")\n","      os.makedirs(drive_root, exist_ok=True)\n","\n","    # Change to the directory\n","    print(\"\\nColab: Changing directory to \", drive_root)\n","    %cd $drive_root\n","# Verify we're in the correct working directory\n","%pwd\n","print(\"Archivos en el directorio: \")\n","print(os.listdir())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xAZDg478kEbs","outputId":"9ef5c751-7fa4-4c64-98f7-482b09bf3efb","executionInfo":{"status":"ok","timestamp":1698763105855,"user_tz":-60,"elapsed":234975,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gym==0.17.3\n","  Downloading gym-0.17.3.tar.gz (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym==0.17.3) (1.11.3)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.17.3) (1.23.5)\n","Collecting pyglet<=1.5.0,>=1.4.0 (from gym==0.17.3)\n","  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cloudpickle<1.7.0,>=1.2.0 (from gym==0.17.3)\n","  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3) (0.18.3)\n","Building wheels for collected packages: gym\n","  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym: filename=gym-0.17.3-py3-none-any.whl size=1654617 sha256=a02ece4c97af3584ec64830f12aeff2248d27dc9de3ad7e44a53f638b89b6d57\n","  Stored in directory: /root/.cache/pip/wheels/af/4b/74/fcfc8238472c34d7f96508a63c962ff3ac9485a9a4137afd4e\n","Successfully built gym\n","Installing collected packages: pyglet, cloudpickle, gym\n","  Attempting uninstall: cloudpickle\n","    Found existing installation: cloudpickle 2.2.1\n","    Uninstalling cloudpickle-2.2.1:\n","      Successfully uninstalled cloudpickle-2.2.1\n","  Attempting uninstall: gym\n","    Found existing installation: gym 0.25.2\n","    Uninstalling gym-0.25.2:\n","      Successfully uninstalled gym-0.25.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","bigframes 0.10.0 requires cloudpickle>=2.0.0, but you have cloudpickle 1.6.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed cloudpickle-1.6.0 gym-0.17.3 pyglet-1.5.0\n","Collecting git+https://github.com/Kojoley/atari-py.git\n","  Cloning https://github.com/Kojoley/atari-py.git to /tmp/pip-req-build-m21c5bxg\n","  Running command git clone --filter=blob:none --quiet https://github.com/Kojoley/atari-py.git /tmp/pip-req-build-m21c5bxg\n","  Resolved https://github.com/Kojoley/atari-py.git to commit 86a1e05c0a95e9e6233c3a413521fdb34ca8a089\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from atari-py==1.2.2) (1.23.5)\n","Building wheels for collected packages: atari-py\n","  Building wheel for atari-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for atari-py: filename=atari_py-1.2.2-cp310-cp310-linux_x86_64.whl size=4738551 sha256=c872c505af706df5e6b8b9861a2d56b33a6625c1135dd2f469ccb857c19f3c1b\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-cdoj9xcr/wheels/6c/9b/f1/8a80a63a233d6f4eb2e2ee8c7357f4875f21c3ffc7f2613f9f\n","Successfully built atari-py\n","Installing collected packages: atari-py\n","Successfully installed atari-py-1.2.2\n","Collecting keras-rl2==1.0.5\n","  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from keras-rl2==1.0.5) (2.14.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (23.5.26)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (16.0.6)\n","Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.2.0)\n","Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.23.5)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (23.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (4.5.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.34.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.59.0)\n","Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.14.1)\n","Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.14.0)\n","Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.14.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2==1.0.5) (0.41.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow->keras-rl2==1.0.5) (3.2.2)\n","Installing collected packages: keras-rl2\n","Successfully installed keras-rl2-1.0.5\n","Collecting tensorflow==2.8\n","  Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl (497.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.6.3)\n","Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (23.5.26)\n","Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.9.0)\n","Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.8)\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (16.0.6)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.23.5)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.3.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (4.5.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.14.1)\n","Collecting tensorboard<2.9,>=2.8 (from tensorflow==2.8)\n","  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tf-estimator-nightly==2.8.0.dev2021122109 (from tensorflow==2.8)\n","  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting keras<2.9,>=2.8.0rc0 (from tensorflow==2.8)\n","  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.34.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.59.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.8) (0.41.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.17.3)\n","Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.31.0)\n","Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n","  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m107.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n","  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.2.2)\n","Installing collected packages: tf-estimator-nightly, tensorboard-plugin-wit, keras, tensorboard-data-server, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.14.0\n","    Uninstalling keras-2.14.0:\n","      Successfully uninstalled keras-2.14.0\n","  Attempting uninstall: tensorboard-data-server\n","    Found existing installation: tensorboard-data-server 0.7.2\n","    Uninstalling tensorboard-data-server-0.7.2:\n","      Successfully uninstalled tensorboard-data-server-0.7.2\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 1.0.0\n","    Uninstalling google-auth-oauthlib-1.0.0:\n","      Successfully uninstalled google-auth-oauthlib-1.0.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.14.1\n","    Uninstalling tensorboard-2.14.1:\n","      Successfully uninstalled tensorboard-2.14.1\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.14.0\n","    Uninstalling tensorflow-2.14.0:\n","      Successfully uninstalled tensorflow-2.14.0\n","Successfully installed google-auth-oauthlib-0.4.6 keras-2.8.0 keras-preprocessing-1.1.2 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tf-estimator-nightly-2.8.0.dev2021122109\n","Collecting gym-cap\n","  Downloading gym_cap-0.1.4.12-py3-none-any.whl (32 kB)\n","Requirement already satisfied: gym>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from gym-cap) (0.17.3)\n","Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from gym-cap) (1.23.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym>=0.16.0->gym-cap) (1.11.3)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.16.0->gym-cap) (1.5.0)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.16.0->gym-cap) (1.6.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.16.0->gym-cap) (0.18.3)\n","Installing collected packages: gym-cap\n","Successfully installed gym-cap-0.1.4.12\n","Collecting pettingzoo[butterfly]\n","  Downloading pettingzoo-1.24.1-py3-none-any.whl (840 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.8/840.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[butterfly]) (1.23.5)\n","Collecting gymnasium>=0.28.0 (from pettingzoo[butterfly])\n","  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pygame==2.3.0 (from pettingzoo[butterfly])\n","  Downloading pygame-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pymunk==6.2.0 (from pettingzoo[butterfly])\n","  Downloading pymunk-6.2.0.zip (7.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: cffi>1.14.0 in /usr/local/lib/python3.10/dist-packages (from pymunk==6.2.0->pettingzoo[butterfly]) (1.16.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[butterfly]) (1.6.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[butterfly]) (4.5.0)\n","Collecting farama-notifications>=0.0.1 (from gymnasium>=0.28.0->pettingzoo[butterfly])\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>1.14.0->pymunk==6.2.0->pettingzoo[butterfly]) (2.21)\n","Building wheels for collected packages: pymunk\n","  Building wheel for pymunk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pymunk: filename=pymunk-6.2.0-cp310-cp310-linux_x86_64.whl size=801635 sha256=2eb009c1a95934909df255c21f2907157ad11039d8b0e644fb7d9f7003fffc23\n","  Stored in directory: /root/.cache/pip/wheels/2e/81/a2/f941a9ff417bb4020c37ae218fb7117d12d3fc019ea493d66f\n","Successfully built pymunk\n","Installing collected packages: farama-notifications, pygame, gymnasium, pymunk, pettingzoo\n","  Attempting uninstall: pygame\n","    Found existing installation: pygame 2.5.2\n","    Uninstalling pygame-2.5.2:\n","      Successfully uninstalled pygame-2.5.2\n","Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1 pettingzoo-1.24.1 pygame-2.3.0 pymunk-6.2.0\n"]}],"source":["if IN_COLAB:\n","  %pip install gym==0.17.3\n","  %pip install git+https://github.com/Kojoley/atari-py.git\n","  %pip install keras-rl2==1.0.5\n","  %pip install tensorflow==2.8\n","  %pip install gym-cap\n","  %pip install pettingzoo[butterfly]\n","else:\n","  %pip install gym==0.17.3\n","  %pip install git+https://github.com/Kojoley/atari-py.git\n","  %pip install pyglet==1.5.0\n","  %pip install h5py==3.1.0\n","  %pip install Pillow==9.5.0\n","  %pip install keras-rl2==1.0.5\n","  %pip install Keras==2.2.4\n","  %pip install tensorflow==2.5.3\n","  %pip install torch==2.0.1\n","  %pip install agents==1.4.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G8cw-IX3laE9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698763105856,"user_tz":-60,"elapsed":49,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"bd1ee699-7d35-4e10-8d35-110bd30946ed"},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'multi_car_racing' already exists and is not an empty directory.\n"]}],"source":["!git clone https://github.com/igilitschenski/multi_car_racing.git\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IqbMo3gK7vBG"},"outputs":[],"source":["!cd multi_car_racing\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jekec6f98b3A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698763117781,"user_tz":-60,"elapsed":11599,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"1614188e-84a6-4af4-d7d0-c33a35bf3fd2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting swig\n","  Downloading swig-4.1.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: swig\n","Successfully installed swig-4.1.1\n"]}],"source":["!pip install swig"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KKxRPBFx85k6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698763185504,"user_tz":-60,"elapsed":67772,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"af0d7039-5d7e-4519-b480-915d107f6ad8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting box2d\n","  Downloading Box2D-2.3.2.tar.gz (427 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.9/427.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: box2d\n","  Building wheel for box2d (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d: filename=Box2D-2.3.2-cp310-cp310-linux_x86_64.whl size=2391311 sha256=37eadd80dbcd98bf2dc3124b8d0a397642ba685f799613fdffd5a1d0ca4ce957\n","  Stored in directory: /root/.cache/pip/wheels/eb/cb/be/e663f3ce9aba6580611c0febaf7cd3cf7603f87047de2a52f9\n","Successfully built box2d\n","Installing collected packages: box2d\n","Successfully installed box2d-2.3.2\n"]}],"source":["!pip install box2d"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ijp5V0i09MRF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698763250825,"user_tz":-60,"elapsed":65371,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"5fc70969-8802-48e6-8813-88d0a36c4a49"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting box2d-py\n","  Downloading box2d-py-2.3.8.tar.gz (374 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/374.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/374.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.5/374.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: box2d-py\n","  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-py: filename=box2d_py-2.3.8-cp310-cp310-linux_x86_64.whl size=2373078 sha256=2eb6f64cd540d2c3174866a15616be3cdee4c4072a1fccda4d6ca4e5be62e6ab\n","  Stored in directory: /root/.cache/pip/wheels/47/01/d2/6a780da77ccb98b1d2facdd520a8d10838a03b590f6f8d50c0\n","Successfully built box2d-py\n","Installing collected packages: box2d-py\n","Successfully installed box2d-py-2.3.8\n"]}],"source":["!pip install box2d-py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BwjugqI99g0I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698763285121,"user_tz":-60,"elapsed":34346,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"cf715ea3-a7ec-4e50-d2d7-d8989fd038d8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Obtaining file:///content/gdrive/MyDrive/TFM/multi_car_racing\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: box2d-py~=2.3.5 in /usr/local/lib/python3.10/dist-packages (from gym-multi-car-racing==0.0.1) (2.3.8)\n","Collecting shapely~=1.7.0 (from gym-multi-car-racing==0.0.1)\n","  Downloading Shapely-1.7.1.tar.gz (383 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.2/383.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from gym-multi-car-racing==0.0.1) (1.23.5)\n","Requirement already satisfied: gym~=0.17.2 in /usr/local/lib/python3.10/dist-packages (from gym-multi-car-racing==0.0.1) (0.17.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym~=0.17.2->gym-multi-car-racing==0.0.1) (1.11.3)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from gym~=0.17.2->gym-multi-car-racing==0.0.1) (1.5.0)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym~=0.17.2->gym-multi-car-racing==0.0.1) (1.6.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym~=0.17.2->gym-multi-car-racing==0.0.1) (0.18.3)\n","Building wheels for collected packages: shapely\n","  Building wheel for shapely (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for shapely: filename=Shapely-1.7.1-cp310-cp310-linux_x86_64.whl size=997162 sha256=e53ca51d04adb341979774a22d553a8ff272d08809b0927b040c460daf800383\n","  Stored in directory: /root/.cache/pip/wheels/2e/fa/97/c85f587c35afcaf4a81c481741d36592518d1e50445572f0d4\n","Successfully built shapely\n","Installing collected packages: shapely, gym-multi-car-racing\n","  Attempting uninstall: shapely\n","    Found existing installation: shapely 2.0.2\n","    Uninstalling shapely-2.0.2:\n","      Successfully uninstalled shapely-2.0.2\n","  Running setup.py develop for gym-multi-car-racing\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires fastapi, which is not installed.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed gym-multi-car-racing-0.0.1 shapely-1.7.1\n"]}],"source":["!pip install -e multi_car_racing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IrAvXzCW-Z3e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698763294247,"user_tz":-60,"elapsed":9177,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"9438bda3-d49b-49d4-de96-f5e8de898e98"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  freeglut3 libegl-dev libgl-dev libgl1-mesa-dev libgles-dev libgles1\n","  libglu1-mesa libglu1-mesa-dev libglvnd-core-dev libglvnd-dev libglx-dev\n","  libice-dev libopengl-dev libsm-dev libxt-dev\n","Suggested packages:\n","  libice-doc libsm-doc libxt-doc\n","The following NEW packages will be installed:\n","  freeglut3 freeglut3-dev libegl-dev libgl-dev libgl1-mesa-dev libgles-dev\n","  libgles1 libglu1-mesa libglu1-mesa-dev libglvnd-core-dev libglvnd-dev\n","  libglx-dev libice-dev libopengl-dev libsm-dev libxt-dev\n","0 upgraded, 16 newly installed, 0 to remove and 19 not upgraded.\n","Need to get 1,261 kB of archives.\n","After this operation, 6,753 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3 amd64 2.8.1-6 [74.0 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglx-dev amd64 1.4.0-1 [14.1 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgl-dev amd64 1.4.0-1 [101 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-core-dev amd64 1.4.0-1 [12.7 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libegl-dev amd64 1.4.0-1 [18.0 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles1 amd64 1.4.0-1 [11.5 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles-dev amd64 1.4.0-1 [49.4 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libopengl-dev amd64 1.4.0-1 [3,400 B]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-dev amd64 1.4.0-1 [3,162 B]\n","Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgl1-mesa-dev amd64 23.0.4-0ubuntu1~22.04.1 [6,510 B]\n","Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n","Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa-dev amd64 9.0.2-1 [231 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 libice-dev amd64 2:1.0.10-1build2 [51.4 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsm-dev amd64 2:1.2.3-1build2 [18.1 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3-dev amd64 2.8.1-6 [126 kB]\n","Fetched 1,261 kB in 1s (1,911 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 16.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package freeglut3:amd64.\n","(Reading database ... 120874 files and directories currently installed.)\n","Preparing to unpack .../00-freeglut3_2.8.1-6_amd64.deb ...\n","Unpacking freeglut3:amd64 (2.8.1-6) ...\n","Selecting previously unselected package libglx-dev:amd64.\n","Preparing to unpack .../01-libglx-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglx-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgl-dev:amd64.\n","Preparing to unpack .../02-libgl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libgl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libglvnd-core-dev:amd64.\n","Preparing to unpack .../03-libglvnd-core-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglvnd-core-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libegl-dev:amd64.\n","Preparing to unpack .../04-libegl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libegl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgles1:amd64.\n","Preparing to unpack .../05-libgles1_1.4.0-1_amd64.deb ...\n","Unpacking libgles1:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgles-dev:amd64.\n","Preparing to unpack .../06-libgles-dev_1.4.0-1_amd64.deb ...\n","Unpacking libgles-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libopengl-dev:amd64.\n","Preparing to unpack .../07-libopengl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libopengl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libglvnd-dev:amd64.\n","Preparing to unpack .../08-libglvnd-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglvnd-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgl1-mesa-dev:amd64.\n","Preparing to unpack .../09-libgl1-mesa-dev_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n","Unpacking libgl1-mesa-dev:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Selecting previously unselected package libglu1-mesa:amd64.\n","Preparing to unpack .../10-libglu1-mesa_9.0.2-1_amd64.deb ...\n","Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n","Selecting previously unselected package libglu1-mesa-dev:amd64.\n","Preparing to unpack .../11-libglu1-mesa-dev_9.0.2-1_amd64.deb ...\n","Unpacking libglu1-mesa-dev:amd64 (9.0.2-1) ...\n","Selecting previously unselected package libice-dev:amd64.\n","Preparing to unpack .../12-libice-dev_2%3a1.0.10-1build2_amd64.deb ...\n","Unpacking libice-dev:amd64 (2:1.0.10-1build2) ...\n","Selecting previously unselected package libsm-dev:amd64.\n","Preparing to unpack .../13-libsm-dev_2%3a1.2.3-1build2_amd64.deb ...\n","Unpacking libsm-dev:amd64 (2:1.2.3-1build2) ...\n","Selecting previously unselected package libxt-dev:amd64.\n","Preparing to unpack .../14-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n","Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n","Selecting previously unselected package freeglut3-dev:amd64.\n","Preparing to unpack .../15-freeglut3-dev_2.8.1-6_amd64.deb ...\n","Unpacking freeglut3-dev:amd64 (2.8.1-6) ...\n","Setting up freeglut3:amd64 (2.8.1-6) ...\n","Setting up libglvnd-core-dev:amd64 (1.4.0-1) ...\n","Setting up libice-dev:amd64 (2:1.0.10-1build2) ...\n","Setting up libsm-dev:amd64 (2:1.2.3-1build2) ...\n","Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n","Setting up libgles1:amd64 (1.4.0-1) ...\n","Setting up libglx-dev:amd64 (1.4.0-1) ...\n","Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n","Setting up libopengl-dev:amd64 (1.4.0-1) ...\n","Setting up libgl-dev:amd64 (1.4.0-1) ...\n","Setting up libegl-dev:amd64 (1.4.0-1) ...\n","Setting up libglu1-mesa-dev:amd64 (9.0.2-1) ...\n","Setting up libgles-dev:amd64 (1.4.0-1) ...\n","Setting up libglvnd-dev:amd64 (1.4.0-1) ...\n","Setting up libgl1-mesa-dev:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Setting up freeglut3-dev:amd64 (2.8.1-6) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n"]}],"source":["!sudo apt-get install freeglut3-dev"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cgGdQ6n9EERW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698763320021,"user_tz":-60,"elapsed":25821,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"d2578b78-2865-4ce1-9658-5b44da9daf6e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n","  xserver-common\n","The following NEW packages will be installed:\n","  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n","  xserver-common xvfb\n","0 upgraded, 9 newly installed, 0 to remove and 19 not upgraded.\n","Need to get 7,814 kB of archives.\n","After this operation, 11.9 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.2 [28.1 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.2 [864 kB]\n","Fetched 7,814 kB in 2s (4,374 kB/s)\n","Selecting previously unselected package libfontenc1:amd64.\n","(Reading database ... 121332 files and directories currently installed.)\n","Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n","Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n","Selecting previously unselected package libxfont2:amd64.\n","Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n","Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n","Selecting previously unselected package libxkbfile1:amd64.\n","Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n","Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n","Selecting previously unselected package x11-xkb-utils.\n","Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n","Unpacking x11-xkb-utils (7.7+5build4) ...\n","Selecting previously unselected package xfonts-encodings.\n","Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n","Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n","Selecting previously unselected package xfonts-utils.\n","Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n","Unpacking xfonts-utils (1:7.7+6build2) ...\n","Selecting previously unselected package xfonts-base.\n","Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n","Unpacking xfonts-base (1:1.0.5) ...\n","Selecting previously unselected package xserver-common.\n","Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.2_all.deb ...\n","Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n","Selecting previously unselected package xvfb.\n","Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.2_amd64.deb ...\n","Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.2) ...\n","Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n","Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n","Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n","Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n","Setting up x11-xkb-utils (7.7+5build4) ...\n","Setting up xfonts-utils (1:7.7+6build2) ...\n","Setting up xfonts-base (1:1.0.5) ...\n","Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n","Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.2) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","Collecting pyvirtualdisplay\n","  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n","Installing collected packages: pyvirtualdisplay\n","Successfully installed pyvirtualdisplay-3.0\n","Collecting piglet\n","  Downloading piglet-1.0.0-py2.py3-none-any.whl (2.2 kB)\n","Collecting piglet-templates (from piglet)\n","  Downloading piglet_templates-1.3.0-py3-none-any.whl (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.5/67.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from piglet-templates->piglet) (3.1.1)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from piglet-templates->piglet) (23.1.0)\n","Requirement already satisfied: astunparse in /usr/local/lib/python3.10/dist-packages (from piglet-templates->piglet) (1.6.3)\n","Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from piglet-templates->piglet) (2.1.3)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse->piglet-templates->piglet) (0.41.2)\n","Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from astunparse->piglet-templates->piglet) (1.16.0)\n","Installing collected packages: piglet-templates, piglet\n","Successfully installed piglet-1.0.0 piglet-templates-1.3.0\n"]}],"source":["!apt install xvfb -y\n","!pip install pyvirtualdisplay\n","!pip install piglet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5OaWkBSmhm6R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698763339562,"user_tz":-60,"elapsed":19552,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"5d273d00-3544-4be7-8cb9-44d19701b1d1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting supersuit\n","  Downloading SuperSuit-3.9.0-py3-none-any.whl (49 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/49.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from supersuit) (1.23.5)\n","Requirement already satisfied: gymnasium>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from supersuit) (0.29.1)\n","Collecting tinyscaler>=1.2.6 (from supersuit)\n","  Downloading tinyscaler-1.2.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (517 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.1/517.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (1.6.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (0.0.4)\n","Installing collected packages: tinyscaler, supersuit\n","Successfully installed supersuit-3.9.0 tinyscaler-1.2.7\n","Collecting stable_baselines3\n","  Downloading stable_baselines3-2.1.0-py3-none-any.whl (178 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (0.29.1)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.23.5)\n","Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.1.0+cu118)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.6.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.5.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (3.7.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (0.0.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.12.4)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2.1.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.1.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (4.43.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (23.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable_baselines3) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable_baselines3) (1.3.0)\n","Installing collected packages: stable_baselines3\n","Successfully installed stable_baselines3-2.1.0\n"]}],"source":["!pip install supersuit\n","!pip install stable_baselines3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"thmOvcHdjKHw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698763348446,"user_tz":-60,"elapsed":8934,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"7b192950-76af-46bb-c794-98cb4cf3d37e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting shimmy>=0.2.1\n","  Downloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from shimmy>=0.2.1) (1.23.5)\n","Requirement already satisfied: gymnasium>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from shimmy>=0.2.1) (0.29.1)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy>=0.2.1) (1.6.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy>=0.2.1) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy>=0.2.1) (0.0.4)\n","Installing collected packages: shimmy\n","Successfully installed shimmy-1.3.0\n"]}],"source":["!pip install 'shimmy>=0.2.1'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k0iVvep_spQz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698763376056,"user_tz":-60,"elapsed":27656,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"7ab73396-1cee-41e4-c1ff-12e81a1f4cb7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tf-agents-nightly\n","  Downloading tf_agents_nightly-0.19.0.dev20231010-py3-none-any.whl (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.4.0)\n","Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.6.0)\n","Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (0.5.0)\n","Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (0.17.3)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.23.5)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (9.4.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.16.0)\n","Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (3.20.3)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (1.14.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents-nightly) (4.5.0)\n","Collecting pygame==2.1.3 (from tf-agents-nightly)\n","  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tfp-nightly (from tf-agents-nightly)\n","  Downloading tfp_nightly-0.23.0.dev20231031-py2.py3-none-any.whl (6.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents-nightly) (1.11.3)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents-nightly) (1.5.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tfp-nightly->tf-agents-nightly) (4.4.2)\n","Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tfp-nightly->tf-agents-nightly) (0.5.4)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tfp-nightly->tf-agents-nightly) (0.1.8)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<=0.23.0,>=0.17.0->tf-agents-nightly) (0.18.3)\n","Installing collected packages: tfp-nightly, pygame, tf-agents-nightly\n","  Attempting uninstall: pygame\n","    Found existing installation: pygame 2.3.0\n","    Uninstalling pygame-2.3.0:\n","      Successfully uninstalled pygame-2.3.0\n","Successfully installed pygame-2.1.3 tf-agents-nightly-0.19.0.dev20231010 tfp-nightly-0.23.0.dev20231031\n"]}],"source":["!pip install tf-agents-nightly"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UlXxViz9tdvH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698763382268,"user_tz":-60,"elapsed":6261,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"352ca007-65d2-40b9-fe5a-27b6ec88d5fa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: tensorflow 2.8.0\n","Uninstalling tensorflow-2.8.0:\n","  Would remove:\n","    /usr/local/bin/estimator_ckpt_converter\n","    /usr/local/bin/import_pb_to_tensorboard\n","    /usr/local/bin/saved_model_cli\n","    /usr/local/bin/tensorboard\n","    /usr/local/bin/tf_upgrade_v2\n","    /usr/local/bin/tflite_convert\n","    /usr/local/bin/toco\n","    /usr/local/bin/toco_from_protos\n","    /usr/local/lib/python3.10/dist-packages/tensorflow-2.8.0.dist-info/*\n","    /usr/local/lib/python3.10/dist-packages/tensorflow/*\n","Proceed (Y/n)? Y\n","  Successfully uninstalled tensorflow-2.8.0\n"]}],"source":["!pip uninstall tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dsWlVQ6MtKLj"},"outputs":[],"source":["!pip install tensorflow>=2.13"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wE5AiVtFtZDc"},"outputs":[],"source":["!pip show tensorflow"]},{"cell_type":"code","source":["!pip install pettingzoo[sisl]"],"metadata":{"id":"PZa1qybXZKSX"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JpA_YhKzCeC2"},"outputs":[],"source":["############################# Código para entrenar Multiwalker ######################"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tnz3fJDA33w8"},"outputs":[],"source":["# from stable_baselines3.dqn import MlpPolicy,CnnPolicy\n","from stable_baselines3 import SAC\n","from pettingzoo.sisl import multiwalker_v9\n","import supersuit as ss\n","from stable_baselines3.common.monitor import Monitor"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":218,"status":"ok","timestamp":1697644598142,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"6-kdhb3CI5VC","outputId":"4db87993-1f4b-464a-f0f0-20ed31aa0c34"},"outputs":[{"output_type":"stream","name":"stdout","text":["drive  gdrive  sample_data\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":395,"status":"ok","timestamp":1697644599460,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"04CbnRTvI9L2","outputId":"347477dc-2a63-4315-a08f-182fdf724421"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/TFM\n"]}],"source":["cd /content/drive/MyDrive/TFM"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":305,"status":"ok","timestamp":1697644601324,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"bUEH8254I_kb","outputId":"406e2b97-e693-46da-9e49-9d56c991f4e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["'=2.13'\t\t\t\t     multiwalker_sac_log_eval\n"," DQN_new_pettingzoo_gym_cap.ipynb    policy_log_eval\n"," DQN_policies\t\t\t     PPO_policies\n","'Entrenamientos antiguos sin logs'   results_rllib\n"," Entrenamientos_log_no_eval\t     TFM_Multiwalker_DDPG_gym_cap.ipynb\n"," MCR_TFM.ipynb\t\t\t     TFM_Multiwalker_SAC_gym_cap.ipynb\n"," multi_car_racing\t\t     TFM_PPO_new_pettingzoo_gym_cap.ipynb\n"," multiwalker_ddpg_log_eval\t     TFM_PPO_pettingzoo_gym_cap.ipynb\n"," multiwalker_ddpg.zip\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2HubK-2G3_vH"},"outputs":[],"source":["env = multiwalker_v9.parallel_env(render_mode=\"rgb_array\")\n","\n","# #---------------------------------\n","# log_dir = \"./policy_log\"\n","# os.makedirs(log_dir, exist_ok=True)\n","# env = Monitor(env, log_dir)\n","# #---------------------------------\n","# env = ss.color_reduction_v0(env, mode='B')\n","# env = ss.black_death_v3(env)\n","# env = ss.resize_v1(env, x_size=84, y_size=84)\n","env = ss.frame_stack_v1(env, 3)\n","env = ss.pettingzoo_env_to_vec_env_v1(env)\n","env = ss.concat_vec_envs_v1(env, 8, num_cpus=4, base_class='stable_baselines3')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"foI4bTFGbQo1"},"outputs":[],"source":["from stable_baselines3.common.callbacks import EvalCallback\n","eval_callback = EvalCallback(env, best_model_save_path=\"./multiwalker_sac_log_eval/\",\n","                             log_path=\"./multiwalker_sac_log_eval/\", eval_freq=100,\n","                             deterministic=False, render=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ib9VPIVgec47","outputId":"424c19bf-a122-45e9-aeb1-fdb859a98363"},"outputs":[{"output_type":"stream","name":"stdout","text":["Logging to multiwalker_sac_log_eval/\n","Using cuda device\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 10       |\n","|    fps             | 491      |\n","|    time_elapsed    | 3        |\n","|    total_timesteps | 1584     |\n","| train/             |          |\n","|    actor_loss      | -4.89    |\n","|    critic_loss     | 44.2     |\n","|    ent_coef        | 0.982    |\n","|    ent_coef_loss   | -0.121   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 61       |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 20       |\n","|    fps             | 496      |\n","|    time_elapsed    | 3        |\n","|    total_timesteps | 1728     |\n","| train/             |          |\n","|    actor_loss      | -4.86    |\n","|    critic_loss     | 87.3     |\n","|    ent_coef        | 0.98     |\n","|    ent_coef_loss   | -0.134   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 67       |\n","---------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n","|    total_timesteps | 3564000  |\n","| train/             |          |\n","|    actor_loss      | 2.32     |\n","|    critic_loss     | 0.319    |\n","|    ent_coef        | 0.00284  |\n","|    ent_coef_loss   | -3.11    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148495   |\n","---------------------------------\n","Eval num_timesteps=3566400, episode_reward=-49.31 +/- 46.13\n","Episode length: 329.60 +/- 139.13\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 330      |\n","|    mean_reward     | -49.3    |\n","| time/              |          |\n","|    total_timesteps | 3566400  |\n","| train/             |          |\n","|    actor_loss      | 2.96     |\n","|    critic_loss     | 0.293    |\n","|    ent_coef        | 0.00281  |\n","|    ent_coef_loss   | -2.25    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148595   |\n","---------------------------------\n","Eval num_timesteps=3568800, episode_reward=-48.24 +/- 46.57\n","Episode length: 436.40 +/- 51.93\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 436      |\n","|    mean_reward     | -48.2    |\n","| time/              |          |\n","|    total_timesteps | 3568800  |\n","| train/             |          |\n","|    actor_loss      | 2.6      |\n","|    critic_loss     | 0.444    |\n","|    ent_coef        | 0.00278  |\n","|    ent_coef_loss   | -2.96    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148695   |\n","---------------------------------\n","Eval num_timesteps=3571200, episode_reward=-103.94 +/- 0.52\n","Episode length: 94.00 +/- 7.35\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 94       |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 3571200  |\n","| train/             |          |\n","|    actor_loss      | 3.03     |\n","|    critic_loss     | 0.793    |\n","|    ent_coef        | 0.00271  |\n","|    ent_coef_loss   | -0.626   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5220     |\n","|    fps             | 292      |\n","|    time_elapsed    | 12217    |\n","|    total_timesteps | 3571608  |\n","| train/             |          |\n","|    actor_loss      | 2.37     |\n","|    critic_loss     | 20.7     |\n","|    ent_coef        | 0.0027   |\n","|    ent_coef_loss   | -3.13    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148812   |\n","---------------------------------\n","Eval num_timesteps=3573600, episode_reward=0.88 +/- 0.50\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.88     |\n","| time/              |          |\n","|    total_timesteps | 3573600  |\n","| train/             |          |\n","|    actor_loss      | 3.24     |\n","|    critic_loss     | 0.459    |\n","|    ent_coef        | 0.0027   |\n","|    ent_coef_loss   | 3.61     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148895   |\n","---------------------------------\n","Eval num_timesteps=3576000, episode_reward=5.92 +/- 2.55\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.92     |\n","| time/              |          |\n","|    total_timesteps | 3576000  |\n","| train/             |          |\n","|    actor_loss      | 2.9      |\n","|    critic_loss     | 0.301    |\n","|    ent_coef        | 0.00272  |\n","|    ent_coef_loss   | -2.11    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 148995   |\n","---------------------------------\n","Eval num_timesteps=3578400, episode_reward=1.99 +/- 1.98\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.99     |\n","| time/              |          |\n","|    total_timesteps | 3578400  |\n","| train/             |          |\n","|    actor_loss      | 2.72     |\n","|    critic_loss     | 0.15     |\n","|    ent_coef        | 0.0027   |\n","|    ent_coef_loss   | -1.44    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149095   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5230     |\n","|    fps             | 292      |\n","|    time_elapsed    | 12247    |\n","|    total_timesteps | 3580752  |\n","| train/             |          |\n","|    actor_loss      | 2.71     |\n","|    critic_loss     | 0.486    |\n","|    ent_coef        | 0.00269  |\n","|    ent_coef_loss   | 2.01     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149193   |\n","---------------------------------\n","Eval num_timesteps=3580800, episode_reward=-102.76 +/- 3.44\n","Episode length: 207.60 +/- 14.21\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 208      |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 3580800  |\n","| train/             |          |\n","|    actor_loss      | 2.16     |\n","|    critic_loss     | 0.195    |\n","|    ent_coef        | 0.00269  |\n","|    ent_coef_loss   | 1.3      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149195   |\n","---------------------------------\n","Eval num_timesteps=3583200, episode_reward=-55.99 +/- 49.59\n","Episode length: 278.60 +/- 180.77\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 279      |\n","|    mean_reward     | -56      |\n","| time/              |          |\n","|    total_timesteps | 3583200  |\n","| train/             |          |\n","|    actor_loss      | 2.33     |\n","|    critic_loss     | 9.8      |\n","|    ent_coef        | 0.00266  |\n","|    ent_coef_loss   | -2.81    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149295   |\n","---------------------------------\n","Eval num_timesteps=3585600, episode_reward=-35.37 +/- 49.27\n","Episode length: 350.00 +/- 183.71\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 350      |\n","|    mean_reward     | -35.4    |\n","| time/              |          |\n","|    total_timesteps | 3585600  |\n","| train/             |          |\n","|    actor_loss      | 2.72     |\n","|    critic_loss     | 0.664    |\n","|    ent_coef        | 0.00263  |\n","|    ent_coef_loss   | 2.43     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149395   |\n","---------------------------------\n","Eval num_timesteps=3588000, episode_reward=4.71 +/- 2.12\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.71     |\n","| time/              |          |\n","|    total_timesteps | 3588000  |\n","| train/             |          |\n","|    actor_loss      | 3.13     |\n","|    critic_loss     | 0.717    |\n","|    ent_coef        | 0.00274  |\n","|    ent_coef_loss   | 4.78     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149495   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5240     |\n","|    fps             | 292      |\n","|    time_elapsed    | 12277    |\n","|    total_timesteps | 3590136  |\n","| train/             |          |\n","|    actor_loss      | 2.42     |\n","|    critic_loss     | 0.433    |\n","|    ent_coef        | 0.00278  |\n","|    ent_coef_loss   | 3.51     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149584   |\n","---------------------------------\n","Eval num_timesteps=3590400, episode_reward=2.29 +/- 0.35\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.29     |\n","| time/              |          |\n","|    total_timesteps | 3590400  |\n","| train/             |          |\n","|    actor_loss      | 2.62     |\n","|    critic_loss     | 0.374    |\n","|    ent_coef        | 0.00278  |\n","|    ent_coef_loss   | 2.96     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149595   |\n","---------------------------------\n","Eval num_timesteps=3592800, episode_reward=-63.09 +/- 49.65\n","Episode length: 261.20 +/- 194.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 261      |\n","|    mean_reward     | -63.1    |\n","| time/              |          |\n","|    total_timesteps | 3592800  |\n","| train/             |          |\n","|    actor_loss      | 2.44     |\n","|    critic_loss     | 0.33     |\n","|    ent_coef        | 0.00278  |\n","|    ent_coef_loss   | -0.27    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149695   |\n","---------------------------------\n","Eval num_timesteps=3595200, episode_reward=-55.26 +/- 48.29\n","Episode length: 309.20 +/- 155.79\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 309      |\n","|    mean_reward     | -55.3    |\n","| time/              |          |\n","|    total_timesteps | 3595200  |\n","| train/             |          |\n","|    actor_loss      | 2.94     |\n","|    critic_loss     | 0.661    |\n","|    ent_coef        | 0.00279  |\n","|    ent_coef_loss   | -1.03    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149795   |\n","---------------------------------\n","Eval num_timesteps=3597600, episode_reward=1.46 +/- 0.17\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.46     |\n","| time/              |          |\n","|    total_timesteps | 3597600  |\n","| train/             |          |\n","|    actor_loss      | 3.06     |\n","|    critic_loss     | 0.532    |\n","|    ent_coef        | 0.00279  |\n","|    ent_coef_loss   | 0.413    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149895   |\n","---------------------------------\n","Eval num_timesteps=3600000, episode_reward=-3.26 +/- 0.16\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -3.26    |\n","| time/              |          |\n","|    total_timesteps | 3600000  |\n","| train/             |          |\n","|    actor_loss      | 2.65     |\n","|    critic_loss     | 38.9     |\n","|    ent_coef        | 0.0028   |\n","|    ent_coef_loss   | 4.6      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 149995   |\n","---------------------------------\n","Eval num_timesteps=3602400, episode_reward=-8.41 +/- 7.48\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -8.41    |\n","| time/              |          |\n","|    total_timesteps | 3602400  |\n","| train/             |          |\n","|    actor_loss      | 2.26     |\n","|    critic_loss     | 0.2      |\n","|    ent_coef        | 0.00286  |\n","|    ent_coef_loss   | 2.19     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150095   |\n","---------------------------------\n","Eval num_timesteps=3604800, episode_reward=1.32 +/- 2.52\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.32     |\n","| time/              |          |\n","|    total_timesteps | 3604800  |\n","| train/             |          |\n","|    actor_loss      | 2.68     |\n","|    critic_loss     | 0.148    |\n","|    ent_coef        | 0.00293  |\n","|    ent_coef_loss   | 0.205    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150195   |\n","---------------------------------\n","Eval num_timesteps=3607200, episode_reward=3.27 +/- 0.91\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.27     |\n","| time/              |          |\n","|    total_timesteps | 3607200  |\n","| train/             |          |\n","|    actor_loss      | 2.74     |\n","|    critic_loss     | 0.179    |\n","|    ent_coef        | 0.00285  |\n","|    ent_coef_loss   | -3.71    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150295   |\n","---------------------------------\n","Eval num_timesteps=3609600, episode_reward=-1.13 +/- 1.36\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.13    |\n","| time/              |          |\n","|    total_timesteps | 3609600  |\n","| train/             |          |\n","|    actor_loss      | 2.38     |\n","|    critic_loss     | 0.32     |\n","|    ent_coef        | 0.00276  |\n","|    ent_coef_loss   | 0.852    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5250     |\n","|    fps             | 292      |\n","|    time_elapsed    | 12356    |\n","|    total_timesteps | 3611808  |\n","| train/             |          |\n","|    actor_loss      | 2.96     |\n","|    critic_loss     | 0.19     |\n","|    ent_coef        | 0.00275  |\n","|    ent_coef_loss   | -2.54    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150487   |\n","---------------------------------\n","Eval num_timesteps=3612000, episode_reward=2.95 +/- 0.19\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.95     |\n","| time/              |          |\n","|    total_timesteps | 3612000  |\n","| train/             |          |\n","|    actor_loss      | 2.77     |\n","|    critic_loss     | 0.249    |\n","|    ent_coef        | 0.00274  |\n","|    ent_coef_loss   | -2.64    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150495   |\n","---------------------------------\n","Eval num_timesteps=3614400, episode_reward=-3.01 +/- 3.71\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -3.01    |\n","| time/              |          |\n","|    total_timesteps | 3614400  |\n","| train/             |          |\n","|    actor_loss      | 2.94     |\n","|    critic_loss     | 1.31     |\n","|    ent_coef        | 0.00279  |\n","|    ent_coef_loss   | 6.19     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150595   |\n","---------------------------------\n","Eval num_timesteps=3616800, episode_reward=2.57 +/- 1.02\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.57     |\n","| time/              |          |\n","|    total_timesteps | 3616800  |\n","| train/             |          |\n","|    actor_loss      | 2.66     |\n","|    critic_loss     | 0.63     |\n","|    ent_coef        | 0.00289  |\n","|    ent_coef_loss   | -1.41    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150695   |\n","---------------------------------\n","Eval num_timesteps=3619200, episode_reward=3.09 +/- 2.03\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.09     |\n","| time/              |          |\n","|    total_timesteps | 3619200  |\n","| train/             |          |\n","|    actor_loss      | 3.42     |\n","|    critic_loss     | 0.415    |\n","|    ent_coef        | 0.00289  |\n","|    ent_coef_loss   | -0.58    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150795   |\n","---------------------------------\n","Eval num_timesteps=3621600, episode_reward=-105.29 +/- 5.08\n","Episode length: 149.60 +/- 65.65\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 150      |\n","|    mean_reward     | -105     |\n","| time/              |          |\n","|    total_timesteps | 3621600  |\n","| train/             |          |\n","|    actor_loss      | 2.73     |\n","|    critic_loss     | 0.146    |\n","|    ent_coef        | 0.00284  |\n","|    ent_coef_loss   | -3.38    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150895   |\n","---------------------------------\n","Eval num_timesteps=3624000, episode_reward=-108.17 +/- 0.63\n","Episode length: 157.60 +/- 5.39\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 158      |\n","|    mean_reward     | -108     |\n","| time/              |          |\n","|    total_timesteps | 3624000  |\n","| train/             |          |\n","|    actor_loss      | 2.59     |\n","|    critic_loss     | 0.454    |\n","|    ent_coef        | 0.00281  |\n","|    ent_coef_loss   | 7.28     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 150995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5260     |\n","|    fps             | 292      |\n","|    time_elapsed    | 12399    |\n","|    total_timesteps | 3624168  |\n","| train/             |          |\n","|    actor_loss      | 2.5      |\n","|    critic_loss     | 0.453    |\n","|    ent_coef        | 0.00282  |\n","|    ent_coef_loss   | 6.45     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151002   |\n","---------------------------------\n","Eval num_timesteps=3626400, episode_reward=-2.66 +/- 0.04\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -2.66    |\n","| time/              |          |\n","|    total_timesteps | 3626400  |\n","| train/             |          |\n","|    actor_loss      | 3.8      |\n","|    critic_loss     | 0.554    |\n","|    ent_coef        | 0.00283  |\n","|    ent_coef_loss   | 0.161    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151095   |\n","---------------------------------\n","Eval num_timesteps=3628800, episode_reward=0.28 +/- 0.09\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.283    |\n","| time/              |          |\n","|    total_timesteps | 3628800  |\n","| train/             |          |\n","|    actor_loss      | 2.84     |\n","|    critic_loss     | 0.305    |\n","|    ent_coef        | 0.00277  |\n","|    ent_coef_loss   | -0.452   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151195   |\n","---------------------------------\n","Eval num_timesteps=3631200, episode_reward=-101.90 +/- 2.29\n","Episode length: 161.20 +/- 70.06\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 161      |\n","|    mean_reward     | -102     |\n","| time/              |          |\n","|    total_timesteps | 3631200  |\n","| train/             |          |\n","|    actor_loss      | 3.22     |\n","|    critic_loss     | 39.2     |\n","|    ent_coef        | 0.00274  |\n","|    ent_coef_loss   | 1.36     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151295   |\n","---------------------------------\n","Eval num_timesteps=3633600, episode_reward=-109.04 +/- 1.90\n","Episode length: 189.00 +/- 12.25\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 189      |\n","|    mean_reward     | -109     |\n","| time/              |          |\n","|    total_timesteps | 3633600  |\n","| train/             |          |\n","|    actor_loss      | 2.95     |\n","|    critic_loss     | 5.8      |\n","|    ent_coef        | 0.00274  |\n","|    ent_coef_loss   | 0.542    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5270     |\n","|    fps             | 292      |\n","|    time_elapsed    | 12426    |\n","|    total_timesteps | 3634344  |\n","| train/             |          |\n","|    actor_loss      | 2.76     |\n","|    critic_loss     | 0.21     |\n","|    ent_coef        | 0.00274  |\n","|    ent_coef_loss   | 0.141    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151426   |\n","---------------------------------\n","Eval num_timesteps=3636000, episode_reward=-1.87 +/- 0.41\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.87    |\n","| time/              |          |\n","|    total_timesteps | 3636000  |\n","| train/             |          |\n","|    actor_loss      | 2.67     |\n","|    critic_loss     | 0.183    |\n","|    ent_coef        | 0.00278  |\n","|    ent_coef_loss   | -0.0878  |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151495   |\n","---------------------------------\n","Eval num_timesteps=3638400, episode_reward=0.65 +/- 5.17\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.651    |\n","| time/              |          |\n","|    total_timesteps | 3638400  |\n","| train/             |          |\n","|    actor_loss      | 3.15     |\n","|    critic_loss     | 0.322    |\n","|    ent_coef        | 0.00278  |\n","|    ent_coef_loss   | -1.13    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151595   |\n","---------------------------------\n","Eval num_timesteps=3640800, episode_reward=-3.74 +/- 0.27\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -3.74    |\n","| time/              |          |\n","|    total_timesteps | 3640800  |\n","| train/             |          |\n","|    actor_loss      | 2.73     |\n","|    critic_loss     | 0.27     |\n","|    ent_coef        | 0.00275  |\n","|    ent_coef_loss   | -2.79    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151695   |\n","---------------------------------\n","Eval num_timesteps=3643200, episode_reward=-1.25 +/- 4.73\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.25    |\n","| time/              |          |\n","|    total_timesteps | 3643200  |\n","| train/             |          |\n","|    actor_loss      | 3.44     |\n","|    critic_loss     | 0.633    |\n","|    ent_coef        | 0.00269  |\n","|    ent_coef_loss   | -0.49    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151795   |\n","---------------------------------\n","Eval num_timesteps=3645600, episode_reward=0.73 +/- 0.35\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.729    |\n","| time/              |          |\n","|    total_timesteps | 3645600  |\n","| train/             |          |\n","|    actor_loss      | 2.53     |\n","|    critic_loss     | 0.395    |\n","|    ent_coef        | 0.00262  |\n","|    ent_coef_loss   | -1.66    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151895   |\n","---------------------------------\n","Eval num_timesteps=3648000, episode_reward=-62.70 +/- 55.48\n","Episode length: 468.20 +/- 25.96\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 468      |\n","|    mean_reward     | -62.7    |\n","| time/              |          |\n","|    total_timesteps | 3648000  |\n","| train/             |          |\n","|    actor_loss      | 2.61     |\n","|    critic_loss     | 0.461    |\n","|    ent_coef        | 0.00258  |\n","|    ent_coef_loss   | -0.75    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 151995   |\n","---------------------------------\n","Eval num_timesteps=3650400, episode_reward=-66.18 +/- 49.90\n","Episode length: 477.80 +/- 18.13\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 478      |\n","|    mean_reward     | -66.2    |\n","| time/              |          |\n","|    total_timesteps | 3650400  |\n","| train/             |          |\n","|    actor_loss      | 3.03     |\n","|    critic_loss     | 0.586    |\n","|    ent_coef        | 0.00259  |\n","|    ent_coef_loss   | 3.97     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152095   |\n","---------------------------------\n","Eval num_timesteps=3652800, episode_reward=0.81 +/- 0.78\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.808    |\n","| time/              |          |\n","|    total_timesteps | 3652800  |\n","| train/             |          |\n","|    actor_loss      | 2.47     |\n","|    critic_loss     | 0.364    |\n","|    ent_coef        | 0.00263  |\n","|    ent_coef_loss   | -0.477   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152195   |\n","---------------------------------\n","Eval num_timesteps=3655200, episode_reward=-71.42 +/- 56.89\n","Episode length: 483.80 +/- 13.23\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 484      |\n","|    mean_reward     | -71.4    |\n","| time/              |          |\n","|    total_timesteps | 3655200  |\n","| train/             |          |\n","|    actor_loss      | 3.04     |\n","|    critic_loss     | 0.448    |\n","|    ent_coef        | 0.00268  |\n","|    ent_coef_loss   | 2.44     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5280     |\n","|    fps             | 292      |\n","|    time_elapsed    | 12506    |\n","|    total_timesteps | 3657384  |\n","| train/             |          |\n","|    actor_loss      | 3.73     |\n","|    critic_loss     | 0.409    |\n","|    ent_coef        | 0.0027   |\n","|    ent_coef_loss   | 4.45     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152386   |\n","---------------------------------\n","Eval num_timesteps=3657600, episode_reward=-2.08 +/- 4.59\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -2.08    |\n","| time/              |          |\n","|    total_timesteps | 3657600  |\n","| train/             |          |\n","|    actor_loss      | 2.58     |\n","|    critic_loss     | 39       |\n","|    ent_coef        | 0.0027   |\n","|    ent_coef_loss   | 0.0556   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152395   |\n","---------------------------------\n","Eval num_timesteps=3660000, episode_reward=-56.86 +/- 48.12\n","Episode length: 434.00 +/- 53.89\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 434      |\n","|    mean_reward     | -56.9    |\n","| time/              |          |\n","|    total_timesteps | 3660000  |\n","| train/             |          |\n","|    actor_loss      | 3.81     |\n","|    critic_loss     | 31.7     |\n","|    ent_coef        | 0.0027   |\n","|    ent_coef_loss   | 1.89     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152495   |\n","---------------------------------\n","Eval num_timesteps=3662400, episode_reward=-45.35 +/- 58.50\n","Episode length: 459.20 +/- 49.97\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 459      |\n","|    mean_reward     | -45.3    |\n","| time/              |          |\n","|    total_timesteps | 3662400  |\n","| train/             |          |\n","|    actor_loss      | 3.07     |\n","|    critic_loss     | 43       |\n","|    ent_coef        | 0.00273  |\n","|    ent_coef_loss   | -0.169   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152595   |\n","---------------------------------\n","Eval num_timesteps=3664800, episode_reward=0.67 +/- 6.43\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.671    |\n","| time/              |          |\n","|    total_timesteps | 3664800  |\n","| train/             |          |\n","|    actor_loss      | 2.74     |\n","|    critic_loss     | 0.351    |\n","|    ent_coef        | 0.00276  |\n","|    ent_coef_loss   | -1.78    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152695   |\n","---------------------------------\n","Eval num_timesteps=3667200, episode_reward=-58.98 +/- 55.17\n","Episode length: 374.80 +/- 153.34\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 375      |\n","|    mean_reward     | -59      |\n","| time/              |          |\n","|    total_timesteps | 3667200  |\n","| train/             |          |\n","|    actor_loss      | 2.53     |\n","|    critic_loss     | 0.293    |\n","|    ent_coef        | 0.00284  |\n","|    ent_coef_loss   | 6.46     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152795   |\n","---------------------------------\n","Eval num_timesteps=3669600, episode_reward=-61.97 +/- 55.73\n","Episode length: 358.40 +/- 115.62\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 358      |\n","|    mean_reward     | -62      |\n","| time/              |          |\n","|    total_timesteps | 3669600  |\n","| train/             |          |\n","|    actor_loss      | 2.83     |\n","|    critic_loss     | 0.321    |\n","|    ent_coef        | 0.00296  |\n","|    ent_coef_loss   | 1.34     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152895   |\n","---------------------------------\n","Eval num_timesteps=3672000, episode_reward=-57.92 +/- 49.54\n","Episode length: 277.40 +/- 181.75\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 277      |\n","|    mean_reward     | -57.9    |\n","| time/              |          |\n","|    total_timesteps | 3672000  |\n","| train/             |          |\n","|    actor_loss      | 2.5      |\n","|    critic_loss     | 0.436    |\n","|    ent_coef        | 0.00298  |\n","|    ent_coef_loss   | 1.51     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 152995   |\n","---------------------------------\n","Eval num_timesteps=3674400, episode_reward=-37.27 +/- 49.57\n","Episode length: 364.40 +/- 166.08\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 364      |\n","|    mean_reward     | -37.3    |\n","| time/              |          |\n","|    total_timesteps | 3674400  |\n","| train/             |          |\n","|    actor_loss      | 3.08     |\n","|    critic_loss     | 0.393    |\n","|    ent_coef        | 0.00299  |\n","|    ent_coef_loss   | 2.99     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153095   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5290     |\n","|    fps             | 292      |\n","|    time_elapsed    | 12575    |\n","|    total_timesteps | 3676416  |\n","| train/             |          |\n","|    actor_loss      | 3.14     |\n","|    critic_loss     | 0.63     |\n","|    ent_coef        | 0.003    |\n","|    ent_coef_loss   | -3.41    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153179   |\n","---------------------------------\n","Eval num_timesteps=3676800, episode_reward=3.96 +/- 3.35\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.96     |\n","| time/              |          |\n","|    total_timesteps | 3676800  |\n","| train/             |          |\n","|    actor_loss      | 3.01     |\n","|    critic_loss     | 28.4     |\n","|    ent_coef        | 0.00298  |\n","|    ent_coef_loss   | -1.72    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153195   |\n","---------------------------------\n","Eval num_timesteps=3679200, episode_reward=-98.07 +/- 5.81\n","Episode length: 222.20 +/- 49.97\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 222      |\n","|    mean_reward     | -98.1    |\n","| time/              |          |\n","|    total_timesteps | 3679200  |\n","| train/             |          |\n","|    actor_loss      | 3.35     |\n","|    critic_loss     | 0.502    |\n","|    ent_coef        | 0.00293  |\n","|    ent_coef_loss   | -0.326   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5300     |\n","|    fps             | 292      |\n","|    time_elapsed    | 12590    |\n","|    total_timesteps | 3681576  |\n","| train/             |          |\n","|    actor_loss      | 2.64     |\n","|    critic_loss     | 0.385    |\n","|    ent_coef        | 0.00287  |\n","|    ent_coef_loss   | -1.48    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153394   |\n","---------------------------------\n","Eval num_timesteps=3681600, episode_reward=-99.56 +/- 4.62\n","Episode length: 305.20 +/- 106.31\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 305      |\n","|    mean_reward     | -99.6    |\n","| time/              |          |\n","|    total_timesteps | 3681600  |\n","| train/             |          |\n","|    actor_loss      | 2.78     |\n","|    critic_loss     | 0.105    |\n","|    ent_coef        | 0.00286  |\n","|    ent_coef_loss   | 0.145    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153395   |\n","---------------------------------\n","Eval num_timesteps=3684000, episode_reward=-60.01 +/- 49.62\n","Episode length: 311.60 +/- 153.83\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 312      |\n","|    mean_reward     | -60      |\n","| time/              |          |\n","|    total_timesteps | 3684000  |\n","| train/             |          |\n","|    actor_loss      | 2.71     |\n","|    critic_loss     | 0.42     |\n","|    ent_coef        | 0.00281  |\n","|    ent_coef_loss   | -1.1     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153495   |\n","---------------------------------\n","Eval num_timesteps=3686400, episode_reward=-96.11 +/- 4.85\n","Episode length: 169.40 +/- 5.39\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 169      |\n","|    mean_reward     | -96.1    |\n","| time/              |          |\n","|    total_timesteps | 3686400  |\n","| train/             |          |\n","|    actor_loss      | 2.97     |\n","|    critic_loss     | 38.6     |\n","|    ent_coef        | 0.00278  |\n","|    ent_coef_loss   | 0.393    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153595   |\n","---------------------------------\n","Eval num_timesteps=3688800, episode_reward=-52.66 +/- 46.55\n","Episode length: 354.20 +/- 119.05\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 354      |\n","|    mean_reward     | -52.7    |\n","| time/              |          |\n","|    total_timesteps | 3688800  |\n","| train/             |          |\n","|    actor_loss      | 3.09     |\n","|    critic_loss     | 0.753    |\n","|    ent_coef        | 0.00277  |\n","|    ent_coef_loss   | -1.19    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153695   |\n","---------------------------------\n","Eval num_timesteps=3691200, episode_reward=-94.41 +/- 5.98\n","Episode length: 258.40 +/- 82.79\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 258      |\n","|    mean_reward     | -94.4    |\n","| time/              |          |\n","|    total_timesteps | 3691200  |\n","| train/             |          |\n","|    actor_loss      | 3.47     |\n","|    critic_loss     | 0.492    |\n","|    ent_coef        | 0.00273  |\n","|    ent_coef_loss   | 3.47     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153795   |\n","---------------------------------\n","Eval num_timesteps=3693600, episode_reward=-62.12 +/- 48.30\n","Episode length: 387.20 +/- 92.10\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 387      |\n","|    mean_reward     | -62.1    |\n","| time/              |          |\n","|    total_timesteps | 3693600  |\n","| train/             |          |\n","|    actor_loss      | 3.11     |\n","|    critic_loss     | 0.757    |\n","|    ent_coef        | 0.00274  |\n","|    ent_coef_loss   | -0.339   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153895   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5310     |\n","|    fps             | 292      |\n","|    time_elapsed    | 12633    |\n","|    total_timesteps | 3695016  |\n","| train/             |          |\n","|    actor_loss      | 2.32     |\n","|    critic_loss     | 0.683    |\n","|    ent_coef        | 0.00272  |\n","|    ent_coef_loss   | -2.56    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153954   |\n","---------------------------------\n","Eval num_timesteps=3696000, episode_reward=3.62 +/- 3.20\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.62     |\n","| time/              |          |\n","|    total_timesteps | 3696000  |\n","| train/             |          |\n","|    actor_loss      | 2.77     |\n","|    critic_loss     | 0.333    |\n","|    ent_coef        | 0.00273  |\n","|    ent_coef_loss   | -1.28    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 153995   |\n","---------------------------------\n","Eval num_timesteps=3698400, episode_reward=0.09 +/- 1.92\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.0865   |\n","| time/              |          |\n","|    total_timesteps | 3698400  |\n","| train/             |          |\n","|    actor_loss      | 3.14     |\n","|    critic_loss     | 0.403    |\n","|    ent_coef        | 0.00272  |\n","|    ent_coef_loss   | 0.801    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154095   |\n","---------------------------------\n","Eval num_timesteps=3700800, episode_reward=-100.82 +/- 8.42\n","Episode length: 396.20 +/- 43.11\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 396      |\n","|    mean_reward     | -101     |\n","| time/              |          |\n","|    total_timesteps | 3700800  |\n","| train/             |          |\n","|    actor_loss      | 2.45     |\n","|    critic_loss     | 0.218    |\n","|    ent_coef        | 0.00276  |\n","|    ent_coef_loss   | 7.21     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5320     |\n","|    fps             | 292      |\n","|    time_elapsed    | 12659    |\n","|    total_timesteps | 3702024  |\n","| train/             |          |\n","|    actor_loss      | 2.61     |\n","|    critic_loss     | 0.428    |\n","|    ent_coef        | 0.00284  |\n","|    ent_coef_loss   | 4.51     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154246   |\n","---------------------------------\n","Eval num_timesteps=3703200, episode_reward=-7.77 +/- 2.11\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -7.77    |\n","| time/              |          |\n","|    total_timesteps | 3703200  |\n","| train/             |          |\n","|    actor_loss      | 3.75     |\n","|    critic_loss     | 40       |\n","|    ent_coef        | 0.00291  |\n","|    ent_coef_loss   | 2.51     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154295   |\n","---------------------------------\n","Eval num_timesteps=3705600, episode_reward=1.46 +/- 0.44\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.46     |\n","| time/              |          |\n","|    total_timesteps | 3705600  |\n","| train/             |          |\n","|    actor_loss      | 4.53     |\n","|    critic_loss     | 2.02     |\n","|    ent_coef        | 0.00293  |\n","|    ent_coef_loss   | 1.17     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154395   |\n","---------------------------------\n","Eval num_timesteps=3708000, episode_reward=-61.55 +/- 43.91\n","Episode length: 375.80 +/- 101.41\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 376      |\n","|    mean_reward     | -61.5    |\n","| time/              |          |\n","|    total_timesteps | 3708000  |\n","| train/             |          |\n","|    actor_loss      | 3.32     |\n","|    critic_loss     | 1.61     |\n","|    ent_coef        | 0.00304  |\n","|    ent_coef_loss   | 3.5      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154495   |\n","---------------------------------\n","Eval num_timesteps=3710400, episode_reward=-114.55 +/- 0.85\n","Episode length: 255.80 +/- 74.46\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 256      |\n","|    mean_reward     | -115     |\n","| time/              |          |\n","|    total_timesteps | 3710400  |\n","| train/             |          |\n","|    actor_loss      | 2.98     |\n","|    critic_loss     | 0.548    |\n","|    ent_coef        | 0.00312  |\n","|    ent_coef_loss   | 3.74     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154595   |\n","---------------------------------\n","Eval num_timesteps=3712800, episode_reward=-36.61 +/- 55.43\n","Episode length: 341.20 +/- 194.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 341      |\n","|    mean_reward     | -36.6    |\n","| time/              |          |\n","|    total_timesteps | 3712800  |\n","| train/             |          |\n","|    actor_loss      | 3.52     |\n","|    critic_loss     | 23.7     |\n","|    ent_coef        | 0.00323  |\n","|    ent_coef_loss   | 0.0776   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5330     |\n","|    fps             | 292      |\n","|    time_elapsed    | 12700    |\n","|    total_timesteps | 3714240  |\n","| train/             |          |\n","|    actor_loss      | 2.58     |\n","|    critic_loss     | 2.45     |\n","|    ent_coef        | 0.00323  |\n","|    ent_coef_loss   | -1.77    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154755   |\n","---------------------------------\n","Eval num_timesteps=3715200, episode_reward=-35.84 +/- 50.71\n","Episode length: 360.00 +/- 171.46\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 360      |\n","|    mean_reward     | -35.8    |\n","| time/              |          |\n","|    total_timesteps | 3715200  |\n","| train/             |          |\n","|    actor_loss      | 3.36     |\n","|    critic_loss     | 0.446    |\n","|    ent_coef        | 0.00323  |\n","|    ent_coef_loss   | 1.2      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154795   |\n","---------------------------------\n","Eval num_timesteps=3717600, episode_reward=2.68 +/- 0.45\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.68     |\n","| time/              |          |\n","|    total_timesteps | 3717600  |\n","| train/             |          |\n","|    actor_loss      | 3.16     |\n","|    critic_loss     | 0.359    |\n","|    ent_coef        | 0.00325  |\n","|    ent_coef_loss   | -0.141   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154895   |\n","---------------------------------\n","Eval num_timesteps=3720000, episode_reward=5.40 +/- 0.01\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.4      |\n","| time/              |          |\n","|    total_timesteps | 3720000  |\n","| train/             |          |\n","|    actor_loss      | 1.99     |\n","|    critic_loss     | 0.379    |\n","|    ent_coef        | 0.00336  |\n","|    ent_coef_loss   | -3.69    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 154995   |\n","---------------------------------\n","Eval num_timesteps=3722400, episode_reward=-2.66 +/- 1.53\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -2.66    |\n","| time/              |          |\n","|    total_timesteps | 3722400  |\n","| train/             |          |\n","|    actor_loss      | 3.44     |\n","|    critic_loss     | 0.721    |\n","|    ent_coef        | 0.0034   |\n","|    ent_coef_loss   | 0.502    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155095   |\n","---------------------------------\n","Eval num_timesteps=3724800, episode_reward=-64.81 +/- 48.93\n","Episode length: 419.60 +/- 65.65\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 420      |\n","|    mean_reward     | -64.8    |\n","| time/              |          |\n","|    total_timesteps | 3724800  |\n","| train/             |          |\n","|    actor_loss      | 2.96     |\n","|    critic_loss     | 1.13     |\n","|    ent_coef        | 0.00338  |\n","|    ent_coef_loss   | -0.361   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155195   |\n","---------------------------------\n","Eval num_timesteps=3727200, episode_reward=-0.00 +/- 0.29\n","Episode length: 500.00 +/- 0.00\n","----------------------------------\n","| eval/              |           |\n","|    mean_ep_length  | 500       |\n","|    mean_reward     | -0.000644 |\n","| time/              |           |\n","|    total_timesteps | 3727200   |\n","| train/             |           |\n","|    actor_loss      | 3.39      |\n","|    critic_loss     | 1.35      |\n","|    ent_coef        | 0.0033    |\n","|    ent_coef_loss   | -2.4      |\n","|    learning_rate   | 0.0003    |\n","|    n_updates       | 155295    |\n","----------------------------------\n","Eval num_timesteps=3729600, episode_reward=-61.35 +/- 51.11\n","Episode length: 398.00 +/- 83.28\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 398      |\n","|    mean_reward     | -61.3    |\n","| time/              |          |\n","|    total_timesteps | 3729600  |\n","| train/             |          |\n","|    actor_loss      | 2.99     |\n","|    critic_loss     | 0.423    |\n","|    ent_coef        | 0.00318  |\n","|    ent_coef_loss   | -3.06    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155395   |\n","---------------------------------\n","Eval num_timesteps=3732000, episode_reward=-42.07 +/- 51.88\n","Episode length: 348.00 +/- 186.16\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 348      |\n","|    mean_reward     | -42.1    |\n","| time/              |          |\n","|    total_timesteps | 3732000  |\n","| train/             |          |\n","|    actor_loss      | 2.91     |\n","|    critic_loss     | 0.48     |\n","|    ent_coef        | 0.00312  |\n","|    ent_coef_loss   | -4.12    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155495   |\n","---------------------------------\n","Eval num_timesteps=3734400, episode_reward=0.68 +/- 0.28\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.679    |\n","| time/              |          |\n","|    total_timesteps | 3734400  |\n","| train/             |          |\n","|    actor_loss      | 3.37     |\n","|    critic_loss     | 0.834    |\n","|    ent_coef        | 0.00306  |\n","|    ent_coef_loss   | -2.36    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155595   |\n","---------------------------------\n","Eval num_timesteps=3736800, episode_reward=-0.88 +/- 4.43\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.875   |\n","| time/              |          |\n","|    total_timesteps | 3736800  |\n","| train/             |          |\n","|    actor_loss      | 2.94     |\n","|    critic_loss     | 0.37     |\n","|    ent_coef        | 0.00299  |\n","|    ent_coef_loss   | -0.858   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155695   |\n","---------------------------------\n","Eval num_timesteps=3739200, episode_reward=0.78 +/- 1.10\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.781    |\n","| time/              |          |\n","|    total_timesteps | 3739200  |\n","| train/             |          |\n","|    actor_loss      | 2        |\n","|    critic_loss     | 0.149    |\n","|    ent_coef        | 0.00294  |\n","|    ent_coef_loss   | -1.79    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155795   |\n","---------------------------------\n","Eval num_timesteps=3741600, episode_reward=1.12 +/- 0.59\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.12     |\n","| time/              |          |\n","|    total_timesteps | 3741600  |\n","| train/             |          |\n","|    actor_loss      | 3.73     |\n","|    critic_loss     | 1.09     |\n","|    ent_coef        | 0.00296  |\n","|    ent_coef_loss   | 2.7      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155895   |\n","---------------------------------\n","Eval num_timesteps=3744000, episode_reward=3.97 +/- 2.27\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.97     |\n","| time/              |          |\n","|    total_timesteps | 3744000  |\n","| train/             |          |\n","|    actor_loss      | 2.88     |\n","|    critic_loss     | 0.481    |\n","|    ent_coef        | 0.00294  |\n","|    ent_coef_loss   | -2       |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 155995   |\n","---------------------------------\n","Eval num_timesteps=3746400, episode_reward=-57.50 +/- 52.30\n","Episode length: 341.60 +/- 129.33\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 342      |\n","|    mean_reward     | -57.5    |\n","| time/              |          |\n","|    total_timesteps | 3746400  |\n","| train/             |          |\n","|    actor_loss      | 2.83     |\n","|    critic_loss     | 0.654    |\n","|    ent_coef        | 0.00293  |\n","|    ent_coef_loss   | -4.94    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156095   |\n","---------------------------------\n","Eval num_timesteps=3748800, episode_reward=4.55 +/- 1.65\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.55     |\n","| time/              |          |\n","|    total_timesteps | 3748800  |\n","| train/             |          |\n","|    actor_loss      | 2.67     |\n","|    critic_loss     | 38.7     |\n","|    ent_coef        | 0.00296  |\n","|    ent_coef_loss   | -1.13    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156195   |\n","---------------------------------\n","Eval num_timesteps=3751200, episode_reward=-55.16 +/- 53.23\n","Episode length: 362.00 +/- 112.68\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 362      |\n","|    mean_reward     | -55.2    |\n","| time/              |          |\n","|    total_timesteps | 3751200  |\n","| train/             |          |\n","|    actor_loss      | 3.9      |\n","|    critic_loss     | 1.03     |\n","|    ent_coef        | 0.00299  |\n","|    ent_coef_loss   | 0.104    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156295   |\n","---------------------------------\n","Eval num_timesteps=3753600, episode_reward=5.21 +/- 0.21\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.21     |\n","| time/              |          |\n","|    total_timesteps | 3753600  |\n","| train/             |          |\n","|    actor_loss      | 3.01     |\n","|    critic_loss     | 0.269    |\n","|    ent_coef        | 0.00291  |\n","|    ent_coef_loss   | -2.27    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156395   |\n","---------------------------------\n","Eval num_timesteps=3756000, episode_reward=5.33 +/- 0.25\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.33     |\n","| time/              |          |\n","|    total_timesteps | 3756000  |\n","| train/             |          |\n","|    actor_loss      | 4.78     |\n","|    critic_loss     | 0.274    |\n","|    ent_coef        | 0.00288  |\n","|    ent_coef_loss   | -1.85    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156495   |\n","---------------------------------\n","Eval num_timesteps=3758400, episode_reward=-106.17 +/- 0.97\n","Episode length: 309.40 +/- 10.29\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 309      |\n","|    mean_reward     | -106     |\n","| time/              |          |\n","|    total_timesteps | 3758400  |\n","| train/             |          |\n","|    actor_loss      | 3.45     |\n","|    critic_loss     | 0.68     |\n","|    ent_coef        | 0.00282  |\n","|    ent_coef_loss   | -0.627   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156595   |\n","---------------------------------\n","Eval num_timesteps=3760800, episode_reward=-62.80 +/- 55.23\n","Episode length: 341.60 +/- 129.33\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 342      |\n","|    mean_reward     | -62.8    |\n","| time/              |          |\n","|    total_timesteps | 3760800  |\n","| train/             |          |\n","|    actor_loss      | 3.45     |\n","|    critic_loss     | 0.584    |\n","|    ent_coef        | 0.00277  |\n","|    ent_coef_loss   | 5.06     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5340     |\n","|    fps             | 292      |\n","|    time_elapsed    | 12874    |\n","|    total_timesteps | 3762432  |\n","| train/             |          |\n","|    actor_loss      | 3.34     |\n","|    critic_loss     | 0.599    |\n","|    ent_coef        | 0.0028   |\n","|    ent_coef_loss   | -0.282   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156763   |\n","---------------------------------\n","Eval num_timesteps=3763200, episode_reward=0.09 +/- 1.35\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.0897   |\n","| time/              |          |\n","|    total_timesteps | 3763200  |\n","| train/             |          |\n","|    actor_loss      | 3.31     |\n","|    critic_loss     | 0.678    |\n","|    ent_coef        | 0.0028   |\n","|    ent_coef_loss   | 0.72     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156795   |\n","---------------------------------\n","Eval num_timesteps=3765600, episode_reward=-5.39 +/- 1.28\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -5.39    |\n","| time/              |          |\n","|    total_timesteps | 3765600  |\n","| train/             |          |\n","|    actor_loss      | 3.1      |\n","|    critic_loss     | 0.632    |\n","|    ent_coef        | 0.00283  |\n","|    ent_coef_loss   | 2.81     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156895   |\n","---------------------------------\n","Eval num_timesteps=3768000, episode_reward=-102.85 +/- 1.01\n","Episode length: 410.40 +/- 22.54\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 410      |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 3768000  |\n","| train/             |          |\n","|    actor_loss      | 3.02     |\n","|    critic_loss     | 3.24     |\n","|    ent_coef        | 0.00284  |\n","|    ent_coef_loss   | -1.36    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 156995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5350     |\n","|    fps             | 292      |\n","|    time_elapsed    | 12900    |\n","|    total_timesteps | 3769488  |\n","| train/             |          |\n","|    actor_loss      | 4.38     |\n","|    critic_loss     | 0.388    |\n","|    ent_coef        | 0.00283  |\n","|    ent_coef_loss   | -0.783   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157057   |\n","---------------------------------\n","Eval num_timesteps=3770400, episode_reward=-60.26 +/- 52.45\n","Episode length: 413.60 +/- 70.55\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 414      |\n","|    mean_reward     | -60.3    |\n","| time/              |          |\n","|    total_timesteps | 3770400  |\n","| train/             |          |\n","|    actor_loss      | 2.91     |\n","|    critic_loss     | 1.43     |\n","|    ent_coef        | 0.00282  |\n","|    ent_coef_loss   | -1.72    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157095   |\n","---------------------------------\n","Eval num_timesteps=3772800, episode_reward=8.88 +/- 4.11\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 8.88     |\n","| time/              |          |\n","|    total_timesteps | 3772800  |\n","| train/             |          |\n","|    actor_loss      | 2.65     |\n","|    critic_loss     | 21.7     |\n","|    ent_coef        | 0.00283  |\n","|    ent_coef_loss   | 1.41     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157195   |\n","---------------------------------\n","Eval num_timesteps=3775200, episode_reward=-46.40 +/- 51.03\n","Episode length: 371.60 +/- 157.26\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 372      |\n","|    mean_reward     | -46.4    |\n","| time/              |          |\n","|    total_timesteps | 3775200  |\n","| train/             |          |\n","|    actor_loss      | 3.53     |\n","|    critic_loss     | 0.288    |\n","|    ent_coef        | 0.00285  |\n","|    ent_coef_loss   | 0.633    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157295   |\n","---------------------------------\n","Eval num_timesteps=3777600, episode_reward=0.69 +/- 0.15\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.687    |\n","| time/              |          |\n","|    total_timesteps | 3777600  |\n","| train/             |          |\n","|    actor_loss      | 2.99     |\n","|    critic_loss     | 0.635    |\n","|    ent_coef        | 0.00286  |\n","|    ent_coef_loss   | 0.508    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157395   |\n","---------------------------------\n","Eval num_timesteps=3780000, episode_reward=-3.74 +/- 0.29\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -3.74    |\n","| time/              |          |\n","|    total_timesteps | 3780000  |\n","| train/             |          |\n","|    actor_loss      | 3.06     |\n","|    critic_loss     | 0.272    |\n","|    ent_coef        | 0.00296  |\n","|    ent_coef_loss   | 4.66     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157495   |\n","---------------------------------\n","Eval num_timesteps=3782400, episode_reward=-39.62 +/- 56.74\n","Episode length: 481.20 +/- 23.03\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 481      |\n","|    mean_reward     | -39.6    |\n","| time/              |          |\n","|    total_timesteps | 3782400  |\n","| train/             |          |\n","|    actor_loss      | 3.01     |\n","|    critic_loss     | 1.76     |\n","|    ent_coef        | 0.00304  |\n","|    ent_coef_loss   | 0.81     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157595   |\n","---------------------------------\n","Eval num_timesteps=3784800, episode_reward=-0.92 +/- 1.28\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.922   |\n","| time/              |          |\n","|    total_timesteps | 3784800  |\n","| train/             |          |\n","|    actor_loss      | 3.08     |\n","|    critic_loss     | 0.486    |\n","|    ent_coef        | 0.00307  |\n","|    ent_coef_loss   | -1.28    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157695   |\n","---------------------------------\n","Eval num_timesteps=3787200, episode_reward=-103.12 +/- 0.45\n","Episode length: 408.80 +/- 57.81\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 409      |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 3787200  |\n","| train/             |          |\n","|    actor_loss      | 3.23     |\n","|    critic_loss     | 0.265    |\n","|    ent_coef        | 0.00308  |\n","|    ent_coef_loss   | -0.189   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5360     |\n","|    fps             | 292      |\n","|    time_elapsed    | 12967    |\n","|    total_timesteps | 3788256  |\n","| train/             |          |\n","|    actor_loss      | 2.73     |\n","|    critic_loss     | 1.4      |\n","|    ent_coef        | 0.00307  |\n","|    ent_coef_loss   | -0.254   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157839   |\n","---------------------------------\n","Eval num_timesteps=3789600, episode_reward=-56.72 +/- 49.26\n","Episode length: 325.40 +/- 142.56\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 325      |\n","|    mean_reward     | -56.7    |\n","| time/              |          |\n","|    total_timesteps | 3789600  |\n","| train/             |          |\n","|    actor_loss      | 2.83     |\n","|    critic_loss     | 0.122    |\n","|    ent_coef        | 0.00304  |\n","|    ent_coef_loss   | 0.209    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157895   |\n","---------------------------------\n","Eval num_timesteps=3792000, episode_reward=-3.76 +/- 3.41\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -3.76    |\n","| time/              |          |\n","|    total_timesteps | 3792000  |\n","| train/             |          |\n","|    actor_loss      | 2.6      |\n","|    critic_loss     | 0.453    |\n","|    ent_coef        | 0.003    |\n","|    ent_coef_loss   | 0.179    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 157995   |\n","---------------------------------\n","Eval num_timesteps=3794400, episode_reward=-0.73 +/- 1.50\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.73    |\n","| time/              |          |\n","|    total_timesteps | 3794400  |\n","| train/             |          |\n","|    actor_loss      | 3.53     |\n","|    critic_loss     | 9.17     |\n","|    ent_coef        | 0.00298  |\n","|    ent_coef_loss   | -2.09    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158095   |\n","---------------------------------\n","Eval num_timesteps=3796800, episode_reward=-56.24 +/- 54.32\n","Episode length: 450.20 +/- 40.66\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 450      |\n","|    mean_reward     | -56.2    |\n","| time/              |          |\n","|    total_timesteps | 3796800  |\n","| train/             |          |\n","|    actor_loss      | 3.27     |\n","|    critic_loss     | 0.282    |\n","|    ent_coef        | 0.00296  |\n","|    ent_coef_loss   | -2.07    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5370     |\n","|    fps             | 292      |\n","|    time_elapsed    | 13004    |\n","|    total_timesteps | 3798792  |\n","| train/             |          |\n","|    actor_loss      | 3.22     |\n","|    critic_loss     | 12.4     |\n","|    ent_coef        | 0.00289  |\n","|    ent_coef_loss   | -4.1     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158278   |\n","---------------------------------\n","Eval num_timesteps=3799200, episode_reward=3.58 +/- 4.56\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.58     |\n","| time/              |          |\n","|    total_timesteps | 3799200  |\n","| train/             |          |\n","|    actor_loss      | 3.37     |\n","|    critic_loss     | 3.03     |\n","|    ent_coef        | 0.00287  |\n","|    ent_coef_loss   | -0.883   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158295   |\n","---------------------------------\n","Eval num_timesteps=3801600, episode_reward=11.83 +/- 4.16\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 11.8     |\n","| time/              |          |\n","|    total_timesteps | 3801600  |\n","| train/             |          |\n","|    actor_loss      | 2.8      |\n","|    critic_loss     | 0.366    |\n","|    ent_coef        | 0.0028   |\n","|    ent_coef_loss   | 0.0477   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158395   |\n","---------------------------------\n","Eval num_timesteps=3804000, episode_reward=-41.83 +/- 44.39\n","Episode length: 403.20 +/- 118.56\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 403      |\n","|    mean_reward     | -41.8    |\n","| time/              |          |\n","|    total_timesteps | 3804000  |\n","| train/             |          |\n","|    actor_loss      | 3.03     |\n","|    critic_loss     | 0.2      |\n","|    ent_coef        | 0.0028   |\n","|    ent_coef_loss   | -0.984   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158495   |\n","---------------------------------\n","Eval num_timesteps=3806400, episode_reward=-63.70 +/- 55.60\n","Episode length: 336.20 +/- 133.74\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 336      |\n","|    mean_reward     | -63.7    |\n","| time/              |          |\n","|    total_timesteps | 3806400  |\n","| train/             |          |\n","|    actor_loss      | 2.39     |\n","|    critic_loss     | 46.5     |\n","|    ent_coef        | 0.00281  |\n","|    ent_coef_loss   | 0.506    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158595   |\n","---------------------------------\n","Eval num_timesteps=3808800, episode_reward=-41.58 +/- 51.68\n","Episode length: 405.20 +/- 116.11\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 405      |\n","|    mean_reward     | -41.6    |\n","| time/              |          |\n","|    total_timesteps | 3808800  |\n","| train/             |          |\n","|    actor_loss      | 4.46     |\n","|    critic_loss     | 62.7     |\n","|    ent_coef        | 0.00285  |\n","|    ent_coef_loss   | 1.17     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158695   |\n","---------------------------------\n","Eval num_timesteps=3811200, episode_reward=0.85 +/- 0.43\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.847    |\n","| time/              |          |\n","|    total_timesteps | 3811200  |\n","| train/             |          |\n","|    actor_loss      | 3.41     |\n","|    critic_loss     | 0.563    |\n","|    ent_coef        | 0.00282  |\n","|    ent_coef_loss   | -0.588   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158795   |\n","---------------------------------\n","Eval num_timesteps=3813600, episode_reward=-93.97 +/- 0.36\n","Episode length: 296.20 +/- 64.67\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 296      |\n","|    mean_reward     | -94      |\n","| time/              |          |\n","|    total_timesteps | 3813600  |\n","| train/             |          |\n","|    actor_loss      | 3.34     |\n","|    critic_loss     | 0.265    |\n","|    ent_coef        | 0.00286  |\n","|    ent_coef_loss   | 2.61     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158895   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5380     |\n","|    fps             | 292      |\n","|    time_elapsed    | 13061    |\n","|    total_timesteps | 3815304  |\n","| train/             |          |\n","|    actor_loss      | 3.27     |\n","|    critic_loss     | 0.236    |\n","|    ent_coef        | 0.00285  |\n","|    ent_coef_loss   | 0.874    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158966   |\n","---------------------------------\n","Eval num_timesteps=3816000, episode_reward=-39.88 +/- 48.22\n","Episode length: 372.80 +/- 155.79\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 373      |\n","|    mean_reward     | -39.9    |\n","| time/              |          |\n","|    total_timesteps | 3816000  |\n","| train/             |          |\n","|    actor_loss      | 3.1      |\n","|    critic_loss     | 17.2     |\n","|    ent_coef        | 0.00286  |\n","|    ent_coef_loss   | 3.01     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 158995   |\n","---------------------------------\n","Eval num_timesteps=3818400, episode_reward=-60.61 +/- 45.91\n","Episode length: 331.40 +/- 137.66\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 331      |\n","|    mean_reward     | -60.6    |\n","| time/              |          |\n","|    total_timesteps | 3818400  |\n","| train/             |          |\n","|    actor_loss      | 3.53     |\n","|    critic_loss     | 0.355    |\n","|    ent_coef        | 0.00297  |\n","|    ent_coef_loss   | 1.04     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159095   |\n","---------------------------------\n","Eval num_timesteps=3820800, episode_reward=-103.60 +/- 6.13\n","Episode length: 297.00 +/- 95.53\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 297      |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 3820800  |\n","| train/             |          |\n","|    actor_loss      | 3        |\n","|    critic_loss     | 0.224    |\n","|    ent_coef        | 0.00303  |\n","|    ent_coef_loss   | -2.2     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5390     |\n","|    fps             | 292      |\n","|    time_elapsed    | 13087    |\n","|    total_timesteps | 3822144  |\n","| train/             |          |\n","|    actor_loss      | 3.1      |\n","|    critic_loss     | 0.43     |\n","|    ent_coef        | 0.00304  |\n","|    ent_coef_loss   | -2.44    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159251   |\n","---------------------------------\n","Eval num_timesteps=3823200, episode_reward=-38.44 +/- 52.76\n","Episode length: 399.20 +/- 123.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 399      |\n","|    mean_reward     | -38.4    |\n","| time/              |          |\n","|    total_timesteps | 3823200  |\n","| train/             |          |\n","|    actor_loss      | 3.39     |\n","|    critic_loss     | 0.635    |\n","|    ent_coef        | 0.00301  |\n","|    ent_coef_loss   | -0.0913  |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159295   |\n","---------------------------------\n","Eval num_timesteps=3825600, episode_reward=-58.61 +/- 49.76\n","Episode length: 304.40 +/- 159.71\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 304      |\n","|    mean_reward     | -58.6    |\n","| time/              |          |\n","|    total_timesteps | 3825600  |\n","| train/             |          |\n","|    actor_loss      | 2.8      |\n","|    critic_loss     | 0.348    |\n","|    ent_coef        | 0.003    |\n","|    ent_coef_loss   | 2.95     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159395   |\n","---------------------------------\n","Eval num_timesteps=3828000, episode_reward=-36.61 +/- 50.60\n","Episode length: 374.80 +/- 153.34\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 375      |\n","|    mean_reward     | -36.6    |\n","| time/              |          |\n","|    total_timesteps | 3828000  |\n","| train/             |          |\n","|    actor_loss      | 3.66     |\n","|    critic_loss     | 0.375    |\n","|    ent_coef        | 0.00298  |\n","|    ent_coef_loss   | -1.87    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159495   |\n","---------------------------------\n","Eval num_timesteps=3830400, episode_reward=7.20 +/- 0.72\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.2      |\n","| time/              |          |\n","|    total_timesteps | 3830400  |\n","| train/             |          |\n","|    actor_loss      | 3.4      |\n","|    critic_loss     | 2.17     |\n","|    ent_coef        | 0.00288  |\n","|    ent_coef_loss   | -1.45    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159595   |\n","---------------------------------\n","Eval num_timesteps=3832800, episode_reward=-58.67 +/- 52.47\n","Episode length: 420.80 +/- 64.67\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 421      |\n","|    mean_reward     | -58.7    |\n","| time/              |          |\n","|    total_timesteps | 3832800  |\n","| train/             |          |\n","|    actor_loss      | 3.56     |\n","|    critic_loss     | 17.5     |\n","|    ent_coef        | 0.00283  |\n","|    ent_coef_loss   | -1.3     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5400     |\n","|    fps             | 291      |\n","|    time_elapsed    | 13131    |\n","|    total_timesteps | 3833880  |\n","| train/             |          |\n","|    actor_loss      | 3.49     |\n","|    critic_loss     | 1.5      |\n","|    ent_coef        | 0.0028   |\n","|    ent_coef_loss   | 1.59     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159740   |\n","---------------------------------\n","Eval num_timesteps=3835200, episode_reward=-2.97 +/- 0.04\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -2.97    |\n","| time/              |          |\n","|    total_timesteps | 3835200  |\n","| train/             |          |\n","|    actor_loss      | 2.59     |\n","|    critic_loss     | 0.399    |\n","|    ent_coef        | 0.00276  |\n","|    ent_coef_loss   | -2.43    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159795   |\n","---------------------------------\n","Eval num_timesteps=3837600, episode_reward=-61.49 +/- 51.16\n","Episode length: 387.80 +/- 91.61\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 388      |\n","|    mean_reward     | -61.5    |\n","| time/              |          |\n","|    total_timesteps | 3837600  |\n","| train/             |          |\n","|    actor_loss      | 3        |\n","|    critic_loss     | 0.291    |\n","|    ent_coef        | 0.0028   |\n","|    ent_coef_loss   | -0.179   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159895   |\n","---------------------------------\n","Eval num_timesteps=3840000, episode_reward=-94.31 +/- 1.19\n","Episode length: 271.00 +/- 68.59\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 271      |\n","|    mean_reward     | -94.3    |\n","| time/              |          |\n","|    total_timesteps | 3840000  |\n","| train/             |          |\n","|    actor_loss      | 3.09     |\n","|    critic_loss     | 0.925    |\n","|    ent_coef        | 0.0028   |\n","|    ent_coef_loss   | 0.357    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 159995   |\n","---------------------------------\n","Eval num_timesteps=3842400, episode_reward=0.51 +/- 0.27\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.509    |\n","| time/              |          |\n","|    total_timesteps | 3842400  |\n","| train/             |          |\n","|    actor_loss      | 4.17     |\n","|    critic_loss     | 0.39     |\n","|    ent_coef        | 0.00277  |\n","|    ent_coef_loss   | 2.52     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160095   |\n","---------------------------------\n","Eval num_timesteps=3844800, episode_reward=-41.11 +/- 49.10\n","Episode length: 364.00 +/- 166.57\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 364      |\n","|    mean_reward     | -41.1    |\n","| time/              |          |\n","|    total_timesteps | 3844800  |\n","| train/             |          |\n","|    actor_loss      | 4.29     |\n","|    critic_loss     | 0.361    |\n","|    ent_coef        | 0.00285  |\n","|    ent_coef_loss   | 6.88     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160195   |\n","---------------------------------\n","Eval num_timesteps=3847200, episode_reward=6.04 +/- 3.17\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.04     |\n","| time/              |          |\n","|    total_timesteps | 3847200  |\n","| train/             |          |\n","|    actor_loss      | 3.32     |\n","|    critic_loss     | 0.21     |\n","|    ent_coef        | 0.00293  |\n","|    ent_coef_loss   | -1.48    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160295   |\n","---------------------------------\n","Eval num_timesteps=3849600, episode_reward=-48.04 +/- 45.82\n","Episode length: 474.00 +/- 31.84\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 474      |\n","|    mean_reward     | -48      |\n","| time/              |          |\n","|    total_timesteps | 3849600  |\n","| train/             |          |\n","|    actor_loss      | 3.09     |\n","|    critic_loss     | 1.28     |\n","|    ent_coef        | 0.0029   |\n","|    ent_coef_loss   | -3.95    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160395   |\n","---------------------------------\n","Eval num_timesteps=3852000, episode_reward=-94.69 +/- 4.56\n","Episode length: 252.00 +/- 139.62\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 252      |\n","|    mean_reward     | -94.7    |\n","| time/              |          |\n","|    total_timesteps | 3852000  |\n","| train/             |          |\n","|    actor_loss      | 2.8      |\n","|    critic_loss     | 0.204    |\n","|    ent_coef        | 0.00291  |\n","|    ent_coef_loss   | 0.317    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160495   |\n","---------------------------------\n","Eval num_timesteps=3854400, episode_reward=-41.31 +/- 50.96\n","Episode length: 396.00 +/- 127.37\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 396      |\n","|    mean_reward     | -41.3    |\n","| time/              |          |\n","|    total_timesteps | 3854400  |\n","| train/             |          |\n","|    actor_loss      | 3.4      |\n","|    critic_loss     | 0.337    |\n","|    ent_coef        | 0.00288  |\n","|    ent_coef_loss   | -1.54    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5410     |\n","|    fps             | 291      |\n","|    time_elapsed    | 13203    |\n","|    total_timesteps | 3855024  |\n","| train/             |          |\n","|    actor_loss      | 3.35     |\n","|    critic_loss     | 0.441    |\n","|    ent_coef        | 0.00288  |\n","|    ent_coef_loss   | 1.89     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160621   |\n","---------------------------------\n","Eval num_timesteps=3856800, episode_reward=-33.74 +/- 45.45\n","Episode length: 498.00 +/- 2.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 498      |\n","|    mean_reward     | -33.7    |\n","| time/              |          |\n","|    total_timesteps | 3856800  |\n","| train/             |          |\n","|    actor_loss      | 3.24     |\n","|    critic_loss     | 0.179    |\n","|    ent_coef        | 0.00289  |\n","|    ent_coef_loss   | 1.99     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5420     |\n","|    fps             | 291      |\n","|    time_elapsed    | 13216    |\n","|    total_timesteps | 3859032  |\n","| train/             |          |\n","|    actor_loss      | 2.77     |\n","|    critic_loss     | 0.341    |\n","|    ent_coef        | 0.00293  |\n","|    ent_coef_loss   | 4.89     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160788   |\n","---------------------------------\n","Eval num_timesteps=3859200, episode_reward=-40.58 +/- 50.93\n","Episode length: 370.00 +/- 159.22\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 370      |\n","|    mean_reward     | -40.6    |\n","| time/              |          |\n","|    total_timesteps | 3859200  |\n","| train/             |          |\n","|    actor_loss      | 3.57     |\n","|    critic_loss     | 0.463    |\n","|    ent_coef        | 0.00294  |\n","|    ent_coef_loss   | 9.28     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160795   |\n","---------------------------------\n","Eval num_timesteps=3861600, episode_reward=-62.58 +/- 48.02\n","Episode length: 434.00 +/- 53.89\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 434      |\n","|    mean_reward     | -62.6    |\n","| time/              |          |\n","|    total_timesteps | 3861600  |\n","| train/             |          |\n","|    actor_loss      | 2.68     |\n","|    critic_loss     | 17.8     |\n","|    ent_coef        | 0.00301  |\n","|    ent_coef_loss   | -0.572   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160895   |\n","---------------------------------\n","Eval num_timesteps=3864000, episode_reward=-57.96 +/- 51.70\n","Episode length: 375.80 +/- 101.41\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 376      |\n","|    mean_reward     | -58      |\n","| time/              |          |\n","|    total_timesteps | 3864000  |\n","| train/             |          |\n","|    actor_loss      | 2.77     |\n","|    critic_loss     | 0.282    |\n","|    ent_coef        | 0.00303  |\n","|    ent_coef_loss   | -2.33    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 160995   |\n","---------------------------------\n","Eval num_timesteps=3866400, episode_reward=-101.38 +/- 9.41\n","Episode length: 220.00 +/- 75.93\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 220      |\n","|    mean_reward     | -101     |\n","| time/              |          |\n","|    total_timesteps | 3866400  |\n","| train/             |          |\n","|    actor_loss      | 3.12     |\n","|    critic_loss     | 0.265    |\n","|    ent_coef        | 0.00301  |\n","|    ent_coef_loss   | 3.16     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 161095   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5430     |\n","|    fps             | 291      |\n","|    time_elapsed    | 13244    |\n","|    total_timesteps | 3867048  |\n","| train/             |          |\n","|    actor_loss      | 3.47     |\n","|    critic_loss     | 0.449    |\n","|    ent_coef        | 0.00302  |\n","|    ent_coef_loss   | 3.9      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 161122   |\n","---------------------------------\n","Eval num_timesteps=3868800, episode_reward=-122.26 +/- 5.57\n","Episode length: 431.20 +/- 59.77\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 431      |\n","|    mean_reward     | -122     |\n","| time/              |          |\n","|    total_timesteps | 3868800  |\n","| train/             |          |\n","|    actor_loss      | 3.04     |\n","|    critic_loss     | 0.525    |\n","|    ent_coef        | 0.00311  |\n","|    ent_coef_loss   | -0.0854  |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 161195   |\n","---------------------------------\n","Eval num_timesteps=3871200, episode_reward=-74.56 +/- 51.90\n","Episode length: 402.20 +/- 79.85\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 402      |\n","|    mean_reward     | -74.6    |\n","| time/              |          |\n","|    total_timesteps | 3871200  |\n","| train/             |          |\n","|    actor_loss      | 3.22     |\n","|    critic_loss     | 34.2     |\n","|    ent_coef        | 0.00314  |\n","|    ent_coef_loss   | -0.454   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 161295   |\n","---------------------------------\n","Eval num_timesteps=3873600, episode_reward=-2.74 +/- 0.03\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -2.74    |\n","| time/              |          |\n","|    total_timesteps | 3873600  |\n","| train/             |          |\n","|    actor_loss      | 2.72     |\n","|    critic_loss     | 40.3     |\n","|    ent_coef        | 0.00315  |\n","|    ent_coef_loss   | -0.884   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 161395   |\n","---------------------------------\n","Eval num_timesteps=3876000, episode_reward=-116.68 +/- 10.53\n","Episode length: 308.00 +/- 63.69\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 308      |\n","|    mean_reward     | -117     |\n","| time/              |          |\n","|    total_timesteps | 3876000  |\n","| train/             |          |\n","|    actor_loss      | 3.33     |\n","|    critic_loss     | 34.6     |\n","|    ent_coef        | 0.00321  |\n","|    ent_coef_loss   | 4.02     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 161495   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5440     |\n","|    fps             | 291      |\n","|    time_elapsed    | 13279    |\n","|    total_timesteps | 3876504  |\n","| train/             |          |\n","|    actor_loss      | 2.94     |\n","|    critic_loss     | 0.291    |\n","|    ent_coef        | 0.00324  |\n","|    ent_coef_loss   | 0.628    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 161516   |\n","---------------------------------\n","Eval num_timesteps=3878400, episode_reward=-39.07 +/- 58.29\n","Episode length: 414.40 +/- 104.84\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 414      |\n","|    mean_reward     | -39.1    |\n","| time/              |          |\n","|    total_timesteps | 3878400  |\n","| train/             |          |\n","|    actor_loss      | 3.25     |\n","|    critic_loss     | 0.954    |\n","|    ent_coef        | 0.00326  |\n","|    ent_coef_loss   | 5.06     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 161595   |\n","---------------------------------\n","Eval num_timesteps=3880800, episode_reward=-10.48 +/- 0.33\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -10.5    |\n","| time/              |          |\n","|    total_timesteps | 3880800  |\n","| train/             |          |\n","|    actor_loss      | 3.31     |\n","|    critic_loss     | 0.287    |\n","|    ent_coef        | 0.00344  |\n","|    ent_coef_loss   | 3.71     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 161695   |\n","---------------------------------\n","Eval num_timesteps=3883200, episode_reward=-106.46 +/- 2.57\n","Episode length: 207.60 +/- 37.23\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 208      |\n","|    mean_reward     | -106     |\n","| time/              |          |\n","|    total_timesteps | 3883200  |\n","| train/             |          |\n","|    actor_loss      | 3.4      |\n","|    critic_loss     | 0.414    |\n","|    ent_coef        | 0.00357  |\n","|    ent_coef_loss   | 3.27     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 161795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5450     |\n","|    fps             | 292      |\n","|    time_elapsed    | 13303    |\n","|    total_timesteps | 3885312  |\n","| train/             |          |\n","|    actor_loss      | 3.12     |\n","|    critic_loss     | 0.427    |\n","|    ent_coef        | 0.0037   |\n","|    ent_coef_loss   | 8.17     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 161883   |\n","---------------------------------\n","Eval num_timesteps=3885600, episode_reward=-57.27 +/- 54.58\n","Episode length: 301.40 +/- 162.16\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 301      |\n","|    mean_reward     | -57.3    |\n","| time/              |          |\n","|    total_timesteps | 3885600  |\n","| train/             |          |\n","|    actor_loss      | 3.68     |\n","|    critic_loss     | 42       |\n","|    ent_coef        | 0.00374  |\n","|    ent_coef_loss   | 5.85     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 161895   |\n","---------------------------------\n","Eval num_timesteps=3888000, episode_reward=-97.34 +/- 2.01\n","Episode length: 171.40 +/- 17.64\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 171      |\n","|    mean_reward     | -97.3    |\n","| time/              |          |\n","|    total_timesteps | 3888000  |\n","| train/             |          |\n","|    actor_loss      | 3.46     |\n","|    critic_loss     | 23.8     |\n","|    ent_coef        | 0.00374  |\n","|    ent_coef_loss   | -1.38    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 161995   |\n","---------------------------------\n","Eval num_timesteps=3890400, episode_reward=-97.02 +/- 0.91\n","Episode length: 272.80 +/- 86.71\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 273      |\n","|    mean_reward     | -97      |\n","| time/              |          |\n","|    total_timesteps | 3890400  |\n","| train/             |          |\n","|    actor_loss      | 3.28     |\n","|    critic_loss     | 0.214    |\n","|    ent_coef        | 0.00368  |\n","|    ent_coef_loss   | -3.14    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162095   |\n","---------------------------------\n","Eval num_timesteps=3892800, episode_reward=-64.11 +/- 52.06\n","Episode length: 298.40 +/- 164.61\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 298      |\n","|    mean_reward     | -64.1    |\n","| time/              |          |\n","|    total_timesteps | 3892800  |\n","| train/             |          |\n","|    actor_loss      | 3.13     |\n","|    critic_loss     | 0.627    |\n","|    ent_coef        | 0.00363  |\n","|    ent_coef_loss   | -0.0423  |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162195   |\n","---------------------------------\n","Eval num_timesteps=3895200, episode_reward=-64.00 +/- 50.89\n","Episode length: 360.80 +/- 113.66\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 361      |\n","|    mean_reward     | -64      |\n","| time/              |          |\n","|    total_timesteps | 3895200  |\n","| train/             |          |\n","|    actor_loss      | 3.23     |\n","|    critic_loss     | 41.9     |\n","|    ent_coef        | 0.00366  |\n","|    ent_coef_loss   | 0.519    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162295   |\n","---------------------------------\n","Eval num_timesteps=3897600, episode_reward=-64.05 +/- 54.12\n","Episode length: 312.20 +/- 153.34\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 312      |\n","|    mean_reward     | -64      |\n","| time/              |          |\n","|    total_timesteps | 3897600  |\n","| train/             |          |\n","|    actor_loss      | 4.02     |\n","|    critic_loss     | 37.3     |\n","|    ent_coef        | 0.00371  |\n","|    ent_coef_loss   | 3.89     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162395   |\n","---------------------------------\n","Eval num_timesteps=3900000, episode_reward=-63.71 +/- 52.83\n","Episode length: 336.20 +/- 133.74\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 336      |\n","|    mean_reward     | -63.7    |\n","| time/              |          |\n","|    total_timesteps | 3900000  |\n","| train/             |          |\n","|    actor_loss      | 3.43     |\n","|    critic_loss     | 0.229    |\n","|    ent_coef        | 0.00374  |\n","|    ent_coef_loss   | -1.67    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162495   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5460     |\n","|    fps             | 292      |\n","|    time_elapsed    | 13356    |\n","|    total_timesteps | 3900432  |\n","| train/             |          |\n","|    actor_loss      | 4.08     |\n","|    critic_loss     | 1.29     |\n","|    ent_coef        | 0.00373  |\n","|    ent_coef_loss   | -1.98    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162513   |\n","---------------------------------\n","Eval num_timesteps=3902400, episode_reward=13.36 +/- 0.46\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 13.4     |\n","| time/              |          |\n","|    total_timesteps | 3902400  |\n","| train/             |          |\n","|    actor_loss      | 3.47     |\n","|    critic_loss     | 0.432    |\n","|    ent_coef        | 0.0037   |\n","|    ent_coef_loss   | 2.08     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162595   |\n","---------------------------------\n","Eval num_timesteps=3904800, episode_reward=-86.57 +/- 7.20\n","Episode length: 369.60 +/- 83.77\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 370      |\n","|    mean_reward     | -86.6    |\n","| time/              |          |\n","|    total_timesteps | 3904800  |\n","| train/             |          |\n","|    actor_loss      | 3.09     |\n","|    critic_loss     | 1.42     |\n","|    ent_coef        | 0.00372  |\n","|    ent_coef_loss   | -1.4     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5470     |\n","|    fps             | 292      |\n","|    time_elapsed    | 13372    |\n","|    total_timesteps | 3905592  |\n","| train/             |          |\n","|    actor_loss      | 2.94     |\n","|    critic_loss     | 0.236    |\n","|    ent_coef        | 0.00372  |\n","|    ent_coef_loss   | -0.764   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162728   |\n","---------------------------------\n","Eval num_timesteps=3907200, episode_reward=-100.65 +/- 4.94\n","Episode length: 178.40 +/- 15.19\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 178      |\n","|    mean_reward     | -101     |\n","| time/              |          |\n","|    total_timesteps | 3907200  |\n","| train/             |          |\n","|    actor_loss      | 3.6      |\n","|    critic_loss     | 0.292    |\n","|    ent_coef        | 0.00372  |\n","|    ent_coef_loss   | 1.43     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5480     |\n","|    fps             | 292      |\n","|    time_elapsed    | 13378    |\n","|    total_timesteps | 3907968  |\n","| train/             |          |\n","|    actor_loss      | 3        |\n","|    critic_loss     | 0.637    |\n","|    ent_coef        | 0.00372  |\n","|    ent_coef_loss   | -1.77    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162827   |\n","---------------------------------\n","Eval num_timesteps=3909600, episode_reward=-37.04 +/- 42.83\n","Episode length: 484.40 +/- 19.11\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 484      |\n","|    mean_reward     | -37      |\n","| time/              |          |\n","|    total_timesteps | 3909600  |\n","| train/             |          |\n","|    actor_loss      | 3.11     |\n","|    critic_loss     | 0.301    |\n","|    ent_coef        | 0.00369  |\n","|    ent_coef_loss   | -3.63    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162895   |\n","---------------------------------\n","Eval num_timesteps=3912000, episode_reward=-60.00 +/- 52.39\n","Episode length: 383.60 +/- 95.04\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 384      |\n","|    mean_reward     | -60      |\n","| time/              |          |\n","|    total_timesteps | 3912000  |\n","| train/             |          |\n","|    actor_loss      | 3.91     |\n","|    critic_loss     | 0.288    |\n","|    ent_coef        | 0.00363  |\n","|    ent_coef_loss   | -0.882   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 162995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5490     |\n","|    fps             | 292      |\n","|    time_elapsed    | 13394    |\n","|    total_timesteps | 3912864  |\n","| train/             |          |\n","|    actor_loss      | 3.16     |\n","|    critic_loss     | 42.5     |\n","|    ent_coef        | 0.0036   |\n","|    ent_coef_loss   | -3.27    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 163031   |\n","---------------------------------\n","Eval num_timesteps=3914400, episode_reward=-8.26 +/- 4.90\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -8.26    |\n","| time/              |          |\n","|    total_timesteps | 3914400  |\n","| train/             |          |\n","|    actor_loss      | 5.34     |\n","|    critic_loss     | 0.416    |\n","|    ent_coef        | 0.00354  |\n","|    ent_coef_loss   | -0.588   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 163095   |\n","---------------------------------\n","Eval num_timesteps=3916800, episode_reward=3.03 +/- 3.59\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.03     |\n","| time/              |          |\n","|    total_timesteps | 3916800  |\n","| train/             |          |\n","|    actor_loss      | 3.17     |\n","|    critic_loss     | 0.654    |\n","|    ent_coef        | 0.00347  |\n","|    ent_coef_loss   | -2.35    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 163195   |\n","---------------------------------\n","Eval num_timesteps=3919200, episode_reward=-4.42 +/- 0.29\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -4.42    |\n","| time/              |          |\n","|    total_timesteps | 3919200  |\n","| train/             |          |\n","|    actor_loss      | 3.27     |\n","|    critic_loss     | 0.701    |\n","|    ent_coef        | 0.00341  |\n","|    ent_coef_loss   | -1.33    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 163295   |\n","---------------------------------\n","Eval num_timesteps=3921600, episode_reward=3.38 +/- 1.64\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.38     |\n","| time/              |          |\n","|    total_timesteps | 3921600  |\n","| train/             |          |\n","|    actor_loss      | 3.78     |\n","|    critic_loss     | 0.409    |\n","|    ent_coef        | 0.00339  |\n","|    ent_coef_loss   | -1.28    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 163395   |\n","---------------------------------\n","Eval num_timesteps=3924000, episode_reward=1.41 +/- 0.17\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.41     |\n","| time/              |          |\n","|    total_timesteps | 3924000  |\n","| train/             |          |\n","|    actor_loss      | 5.61     |\n","|    critic_loss     | 0.355    |\n","|    ent_coef        | 0.0034   |\n","|    ent_coef_loss   | 2.3      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 163495   |\n","---------------------------------\n","Eval num_timesteps=3926400, episode_reward=-68.90 +/- 51.97\n","Episode length: 295.40 +/- 167.06\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 295      |\n","|    mean_reward     | -68.9    |\n","| time/              |          |\n","|    total_timesteps | 3926400  |\n","| train/             |          |\n","|    actor_loss      | 3.22     |\n","|    critic_loss     | 0.66     |\n","|    ent_coef        | 0.00337  |\n","|    ent_coef_loss   | 2.01     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 163595   |\n","---------------------------------\n","Eval num_timesteps=3928800, episode_reward=-101.18 +/- 6.79\n","Episode length: 223.00 +/- 41.64\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 223      |\n","|    mean_reward     | -101     |\n","| time/              |          |\n","|    total_timesteps | 3928800  |\n","| train/             |          |\n","|    actor_loss      | 3.16     |\n","|    critic_loss     | 0.641    |\n","|    ent_coef        | 0.00335  |\n","|    ent_coef_loss   | -0.717   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 163695   |\n","---------------------------------\n","Eval num_timesteps=3931200, episode_reward=-1.04 +/- 6.02\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.04    |\n","| time/              |          |\n","|    total_timesteps | 3931200  |\n","| train/             |          |\n","|    actor_loss      | 4.03     |\n","|    critic_loss     | 0.788    |\n","|    ent_coef        | 0.00331  |\n","|    ent_coef_loss   | -0.542   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 163795   |\n","---------------------------------\n","Eval num_timesteps=3933600, episode_reward=-63.74 +/- 53.19\n","Episode length: 276.20 +/- 182.73\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 276      |\n","|    mean_reward     | -63.7    |\n","| time/              |          |\n","|    total_timesteps | 3933600  |\n","| train/             |          |\n","|    actor_loss      | 3.27     |\n","|    critic_loss     | 0.207    |\n","|    ent_coef        | 0.00336  |\n","|    ent_coef_loss   | 4.94     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 163895   |\n","---------------------------------\n","Eval num_timesteps=3936000, episode_reward=-61.51 +/- 49.37\n","Episode length: 321.80 +/- 145.50\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 322      |\n","|    mean_reward     | -61.5    |\n","| time/              |          |\n","|    total_timesteps | 3936000  |\n","| train/             |          |\n","|    actor_loss      | 4.72     |\n","|    critic_loss     | 0.584    |\n","|    ent_coef        | 0.00354  |\n","|    ent_coef_loss   | 3.96     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 163995   |\n","---------------------------------\n","Eval num_timesteps=3938400, episode_reward=-62.68 +/- 48.51\n","Episode length: 293.00 +/- 169.01\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 293      |\n","|    mean_reward     | -62.7    |\n","| time/              |          |\n","|    total_timesteps | 3938400  |\n","| train/             |          |\n","|    actor_loss      | 3.49     |\n","|    critic_loss     | 0.523    |\n","|    ent_coef        | 0.00365  |\n","|    ent_coef_loss   | 0.128    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164095   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5500     |\n","|    fps             | 292      |\n","|    time_elapsed    | 13489    |\n","|    total_timesteps | 3940728  |\n","| train/             |          |\n","|    actor_loss      | 3.94     |\n","|    critic_loss     | 0.526    |\n","|    ent_coef        | 0.00363  |\n","|    ent_coef_loss   | 0.447    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164192   |\n","---------------------------------\n","Eval num_timesteps=3940800, episode_reward=-6.74 +/- 0.14\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -6.74    |\n","| time/              |          |\n","|    total_timesteps | 3940800  |\n","| train/             |          |\n","|    actor_loss      | 3.84     |\n","|    critic_loss     | 0.946    |\n","|    ent_coef        | 0.00363  |\n","|    ent_coef_loss   | 2.28     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164195   |\n","---------------------------------\n","Eval num_timesteps=3943200, episode_reward=-38.35 +/- 49.40\n","Episode length: 374.80 +/- 153.34\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 375      |\n","|    mean_reward     | -38.4    |\n","| time/              |          |\n","|    total_timesteps | 3943200  |\n","| train/             |          |\n","|    actor_loss      | 5.75     |\n","|    critic_loss     | 0.823    |\n","|    ent_coef        | 0.00366  |\n","|    ent_coef_loss   | 1.61     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164295   |\n","---------------------------------\n","Eval num_timesteps=3945600, episode_reward=-79.07 +/- 44.96\n","Episode length: 447.20 +/- 43.11\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 447      |\n","|    mean_reward     | -79.1    |\n","| time/              |          |\n","|    total_timesteps | 3945600  |\n","| train/             |          |\n","|    actor_loss      | 3.64     |\n","|    critic_loss     | 150      |\n","|    ent_coef        | 0.00368  |\n","|    ent_coef_loss   | -0.953   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164395   |\n","---------------------------------\n","Eval num_timesteps=3948000, episode_reward=-64.74 +/- 52.70\n","Episode length: 312.80 +/- 152.85\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 313      |\n","|    mean_reward     | -64.7    |\n","| time/              |          |\n","|    total_timesteps | 3948000  |\n","| train/             |          |\n","|    actor_loss      | 3.84     |\n","|    critic_loss     | 0.32     |\n","|    ent_coef        | 0.00374  |\n","|    ent_coef_loss   | 0.542    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164495   |\n","---------------------------------\n","Eval num_timesteps=3950400, episode_reward=-47.16 +/- 54.74\n","Episode length: 489.20 +/- 13.23\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 489      |\n","|    mean_reward     | -47.2    |\n","| time/              |          |\n","|    total_timesteps | 3950400  |\n","| train/             |          |\n","|    actor_loss      | 3.34     |\n","|    critic_loss     | 0.513    |\n","|    ent_coef        | 0.00378  |\n","|    ent_coef_loss   | -0.546   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164595   |\n","---------------------------------\n","Eval num_timesteps=3952800, episode_reward=-101.32 +/- 3.21\n","Episode length: 385.40 +/- 131.78\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 385      |\n","|    mean_reward     | -101     |\n","| time/              |          |\n","|    total_timesteps | 3952800  |\n","| train/             |          |\n","|    actor_loss      | 3.24     |\n","|    critic_loss     | 0.437    |\n","|    ent_coef        | 0.0038   |\n","|    ent_coef_loss   | 0.601    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5510     |\n","|    fps             | 291      |\n","|    time_elapsed    | 13537    |\n","|    total_timesteps | 3952968  |\n","| train/             |          |\n","|    actor_loss      | 3.85     |\n","|    critic_loss     | 0.483    |\n","|    ent_coef        | 0.00381  |\n","|    ent_coef_loss   | 3.92     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164702   |\n","---------------------------------\n","Eval num_timesteps=3955200, episode_reward=-61.32 +/- 50.08\n","Episode length: 344.60 +/- 126.88\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 345      |\n","|    mean_reward     | -61.3    |\n","| time/              |          |\n","|    total_timesteps | 3955200  |\n","| train/             |          |\n","|    actor_loss      | 3.42     |\n","|    critic_loss     | 0.188    |\n","|    ent_coef        | 0.00387  |\n","|    ent_coef_loss   | 1.04     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164795   |\n","---------------------------------\n","Eval num_timesteps=3957600, episode_reward=9.36 +/- 2.69\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 9.36     |\n","| time/              |          |\n","|    total_timesteps | 3957600  |\n","| train/             |          |\n","|    actor_loss      | 3.32     |\n","|    critic_loss     | 0.301    |\n","|    ent_coef        | 0.00386  |\n","|    ent_coef_loss   | -1.58    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164895   |\n","---------------------------------\n","Eval num_timesteps=3960000, episode_reward=-65.23 +/- 51.60\n","Episode length: 471.80 +/- 23.03\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 472      |\n","|    mean_reward     | -65.2    |\n","| time/              |          |\n","|    total_timesteps | 3960000  |\n","| train/             |          |\n","|    actor_loss      | 3.34     |\n","|    critic_loss     | 0.274    |\n","|    ent_coef        | 0.00384  |\n","|    ent_coef_loss   | 1.14     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 164995   |\n","---------------------------------\n","Eval num_timesteps=3962400, episode_reward=-0.55 +/- 0.28\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.551   |\n","| time/              |          |\n","|    total_timesteps | 3962400  |\n","| train/             |          |\n","|    actor_loss      | 3.88     |\n","|    critic_loss     | 0.465    |\n","|    ent_coef        | 0.00383  |\n","|    ent_coef_loss   | 1.22     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 165095   |\n","---------------------------------\n","Eval num_timesteps=3964800, episode_reward=-65.89 +/- 53.92\n","Episode length: 321.20 +/- 145.99\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 321      |\n","|    mean_reward     | -65.9    |\n","| time/              |          |\n","|    total_timesteps | 3964800  |\n","| train/             |          |\n","|    actor_loss      | 3.94     |\n","|    critic_loss     | 0.198    |\n","|    ent_coef        | 0.00386  |\n","|    ent_coef_loss   | 3.66     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 165195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5520     |\n","|    fps             | 292      |\n","|    time_elapsed    | 13583    |\n","|    total_timesteps | 3966864  |\n","| train/             |          |\n","|    actor_loss      | 3.88     |\n","|    critic_loss     | 0.691    |\n","|    ent_coef        | 0.00387  |\n","|    ent_coef_loss   | 0.82     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 165281   |\n","---------------------------------\n","Eval num_timesteps=3967200, episode_reward=-39.24 +/- 48.28\n","Episode length: 390.00 +/- 134.72\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 390      |\n","|    mean_reward     | -39.2    |\n","| time/              |          |\n","|    total_timesteps | 3967200  |\n","| train/             |          |\n","|    actor_loss      | 3.18     |\n","|    critic_loss     | 0.401    |\n","|    ent_coef        | 0.00387  |\n","|    ent_coef_loss   | -0.294   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 165295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5530     |\n","|    fps             | 291      |\n","|    time_elapsed    | 13593    |\n","|    total_timesteps | 3969336  |\n","| train/             |          |\n","|    actor_loss      | 3.65     |\n","|    critic_loss     | 0.291    |\n","|    ent_coef        | 0.00392  |\n","|    ent_coef_loss   | -3.59    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 165384   |\n","---------------------------------\n","Eval num_timesteps=3969600, episode_reward=-39.80 +/- 52.52\n","Episode length: 356.00 +/- 176.36\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 356      |\n","|    mean_reward     | -39.8    |\n","| time/              |          |\n","|    total_timesteps | 3969600  |\n","| train/             |          |\n","|    actor_loss      | 3.63     |\n","|    critic_loss     | 0.657    |\n","|    ent_coef        | 0.00392  |\n","|    ent_coef_loss   | -0.668   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 165395   |\n","---------------------------------\n","Eval num_timesteps=3972000, episode_reward=-58.95 +/- 48.14\n","Episode length: 373.40 +/- 103.37\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 373      |\n","|    mean_reward     | -58.9    |\n","| time/              |          |\n","|    total_timesteps | 3972000  |\n","| train/             |          |\n","|    actor_loss      | 3.84     |\n","|    critic_loss     | 1.07     |\n","|    ent_coef        | 0.00382  |\n","|    ent_coef_loss   | -2.11    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 165495   |\n","---------------------------------\n","Eval num_timesteps=3974400, episode_reward=1.15 +/- 0.32\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.15     |\n","| time/              |          |\n","|    total_timesteps | 3974400  |\n","| train/             |          |\n","|    actor_loss      | 3.8      |\n","|    critic_loss     | 0.269    |\n","|    ent_coef        | 0.00371  |\n","|    ent_coef_loss   | -0.852   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 165595   |\n","---------------------------------\n","Eval num_timesteps=3976800, episode_reward=5.24 +/- 1.40\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.24     |\n","| time/              |          |\n","|    total_timesteps | 3976800  |\n","| train/             |          |\n","|    actor_loss      | 4.19     |\n","|    critic_loss     | 0.305    |\n","|    ent_coef        | 0.00361  |\n","|    ent_coef_loss   | -0.519   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 165695   |\n","---------------------------------\n","Eval num_timesteps=3979200, episode_reward=-48.99 +/- 58.89\n","Episode length: 478.80 +/- 25.96\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 479      |\n","|    mean_reward     | -49      |\n","| time/              |          |\n","|    total_timesteps | 3979200  |\n","| train/             |          |\n","|    actor_loss      | 3.98     |\n","|    critic_loss     | 0.484    |\n","|    ent_coef        | 0.00355  |\n","|    ent_coef_loss   | -0.789   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 165795   |\n","---------------------------------\n","Eval num_timesteps=3981600, episode_reward=-106.25 +/- 7.98\n","Episode length: 217.40 +/- 64.18\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 217      |\n","|    mean_reward     | -106     |\n","| time/              |          |\n","|    total_timesteps | 3981600  |\n","| train/             |          |\n","|    actor_loss      | 3.55     |\n","|    critic_loss     | 37.2     |\n","|    ent_coef        | 0.00347  |\n","|    ent_coef_loss   | -2.59    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 165895   |\n","---------------------------------\n","Eval num_timesteps=3984000, episode_reward=6.72 +/- 2.18\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.72     |\n","| time/              |          |\n","|    total_timesteps | 3984000  |\n","| train/             |          |\n","|    actor_loss      | 3.58     |\n","|    critic_loss     | 0.826    |\n","|    ent_coef        | 0.00342  |\n","|    ent_coef_loss   | 0.297    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 165995   |\n","---------------------------------\n","Eval num_timesteps=3986400, episode_reward=2.83 +/- 0.98\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.83     |\n","| time/              |          |\n","|    total_timesteps | 3986400  |\n","| train/             |          |\n","|    actor_loss      | 3.36     |\n","|    critic_loss     | 0.528    |\n","|    ent_coef        | 0.00339  |\n","|    ent_coef_loss   | -3.23    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 166095   |\n","---------------------------------\n","Eval num_timesteps=3988800, episode_reward=-103.60 +/- 1.07\n","Episode length: 127.20 +/- 16.17\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 127      |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 3988800  |\n","| train/             |          |\n","|    actor_loss      | 4.3      |\n","|    critic_loss     | 3.51     |\n","|    ent_coef        | 0.00337  |\n","|    ent_coef_loss   | 2.1      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 166195   |\n","---------------------------------\n","Eval num_timesteps=3991200, episode_reward=-112.71 +/- 0.21\n","Episode length: 186.00 +/- 24.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 186      |\n","|    mean_reward     | -113     |\n","| time/              |          |\n","|    total_timesteps | 3991200  |\n","| train/             |          |\n","|    actor_loss      | 3.27     |\n","|    critic_loss     | 0.266    |\n","|    ent_coef        | 0.00341  |\n","|    ent_coef_loss   | -1.14    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 166295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5540     |\n","|    fps             | 292      |\n","|    time_elapsed    | 13665    |\n","|    total_timesteps | 3991680  |\n","| train/             |          |\n","|    actor_loss      | 3.24     |\n","|    critic_loss     | 0.842    |\n","|    ent_coef        | 0.0034   |\n","|    ent_coef_loss   | -3.18    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 166315   |\n","---------------------------------\n","Eval num_timesteps=3993600, episode_reward=4.12 +/- 3.88\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.12     |\n","| time/              |          |\n","|    total_timesteps | 3993600  |\n","| train/             |          |\n","|    actor_loss      | 4.61     |\n","|    critic_loss     | 0.771    |\n","|    ent_coef        | 0.00336  |\n","|    ent_coef_loss   | 1.68     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 166395   |\n","---------------------------------\n","Eval num_timesteps=3996000, episode_reward=8.75 +/- 2.79\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 8.75     |\n","| time/              |          |\n","|    total_timesteps | 3996000  |\n","| train/             |          |\n","|    actor_loss      | 3.94     |\n","|    critic_loss     | 26.6     |\n","|    ent_coef        | 0.00338  |\n","|    ent_coef_loss   | 0.017    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 166495   |\n","---------------------------------\n","Eval num_timesteps=3998400, episode_reward=-1.43 +/- 0.99\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.43    |\n","| time/              |          |\n","|    total_timesteps | 3998400  |\n","| train/             |          |\n","|    actor_loss      | 4.05     |\n","|    critic_loss     | 0.572    |\n","|    ent_coef        | 0.0034   |\n","|    ent_coef_loss   | 3.22     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 166595   |\n","---------------------------------\n","Eval num_timesteps=4000800, episode_reward=-39.47 +/- 52.67\n","Episode length: 472.00 +/- 34.29\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 472      |\n","|    mean_reward     | -39.5    |\n","| time/              |          |\n","|    total_timesteps | 4000800  |\n","| train/             |          |\n","|    actor_loss      | 4.17     |\n","|    critic_loss     | 0.658    |\n","|    ent_coef        | 0.00345  |\n","|    ent_coef_loss   | 1.99     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 166695   |\n","---------------------------------\n","Eval num_timesteps=4003200, episode_reward=6.72 +/- 0.69\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.72     |\n","| time/              |          |\n","|    total_timesteps | 4003200  |\n","| train/             |          |\n","|    actor_loss      | 3.54     |\n","|    critic_loss     | 9.23     |\n","|    ent_coef        | 0.00351  |\n","|    ent_coef_loss   | 1.27     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 166795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5550     |\n","|    fps             | 292      |\n","|    time_elapsed    | 13711    |\n","|    total_timesteps | 4005528  |\n","| train/             |          |\n","|    actor_loss      | 3.78     |\n","|    critic_loss     | 0.343    |\n","|    ent_coef        | 0.00361  |\n","|    ent_coef_loss   | -0.16    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 166892   |\n","---------------------------------\n","Eval num_timesteps=4005600, episode_reward=-56.57 +/- 47.87\n","Episode length: 491.00 +/- 7.35\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 491      |\n","|    mean_reward     | -56.6    |\n","| time/              |          |\n","|    total_timesteps | 4005600  |\n","| train/             |          |\n","|    actor_loss      | 3.61     |\n","|    critic_loss     | 0.563    |\n","|    ent_coef        | 0.00361  |\n","|    ent_coef_loss   | -0.388   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 166895   |\n","---------------------------------\n","Eval num_timesteps=4008000, episode_reward=-102.56 +/- 1.73\n","Episode length: 246.60 +/- 42.13\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 247      |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 4008000  |\n","| train/             |          |\n","|    actor_loss      | 4.07     |\n","|    critic_loss     | 23.7     |\n","|    ent_coef        | 0.00356  |\n","|    ent_coef_loss   | 0.605    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 166995   |\n","---------------------------------\n","Eval num_timesteps=4010400, episode_reward=-61.81 +/- 50.38\n","Episode length: 368.60 +/- 107.29\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 369      |\n","|    mean_reward     | -61.8    |\n","| time/              |          |\n","|    total_timesteps | 4010400  |\n","| train/             |          |\n","|    actor_loss      | 4.42     |\n","|    critic_loss     | 0.549    |\n","|    ent_coef        | 0.00357  |\n","|    ent_coef_loss   | 1.01     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 167095   |\n","---------------------------------\n","Eval num_timesteps=4012800, episode_reward=-97.25 +/- 1.33\n","Episode length: 358.20 +/- 23.03\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 358      |\n","|    mean_reward     | -97.2    |\n","| time/              |          |\n","|    total_timesteps | 4012800  |\n","| train/             |          |\n","|    actor_loss      | 4.4      |\n","|    critic_loss     | 0.554    |\n","|    ent_coef        | 0.00362  |\n","|    ent_coef_loss   | 0.378    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 167195   |\n","---------------------------------\n","Eval num_timesteps=4015200, episode_reward=-38.62 +/- 55.41\n","Episode length: 382.00 +/- 144.52\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 382      |\n","|    mean_reward     | -38.6    |\n","| time/              |          |\n","|    total_timesteps | 4015200  |\n","| train/             |          |\n","|    actor_loss      | 4.04     |\n","|    critic_loss     | 0.526    |\n","|    ent_coef        | 0.00371  |\n","|    ent_coef_loss   | 1.61     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 167295   |\n","---------------------------------\n","Eval num_timesteps=4017600, episode_reward=7.13 +/- 3.23\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.13     |\n","| time/              |          |\n","|    total_timesteps | 4017600  |\n","| train/             |          |\n","|    actor_loss      | 4.65     |\n","|    critic_loss     | 7.56     |\n","|    ent_coef        | 0.00377  |\n","|    ent_coef_loss   | 0.929    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 167395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5560     |\n","|    fps             | 292      |\n","|    time_elapsed    | 13760    |\n","|    total_timesteps | 4018920  |\n","| train/             |          |\n","|    actor_loss      | 3.98     |\n","|    critic_loss     | 0.4      |\n","|    ent_coef        | 0.00373  |\n","|    ent_coef_loss   | -3.09    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 167450   |\n","---------------------------------\n","Eval num_timesteps=4020000, episode_reward=-31.48 +/- 48.57\n","Episode length: 421.20 +/- 96.51\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 421      |\n","|    mean_reward     | -31.5    |\n","| time/              |          |\n","|    total_timesteps | 4020000  |\n","| train/             |          |\n","|    actor_loss      | 3.53     |\n","|    critic_loss     | 0.33     |\n","|    ent_coef        | 0.00373  |\n","|    ent_coef_loss   | 0.476    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 167495   |\n","---------------------------------\n","Eval num_timesteps=4022400, episode_reward=-100.57 +/- 2.97\n","Episode length: 185.20 +/- 5.88\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 185      |\n","|    mean_reward     | -101     |\n","| time/              |          |\n","|    total_timesteps | 4022400  |\n","| train/             |          |\n","|    actor_loss      | 3.62     |\n","|    critic_loss     | 0.495    |\n","|    ent_coef        | 0.00377  |\n","|    ent_coef_loss   | 1.82     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 167595   |\n","---------------------------------\n","Eval num_timesteps=4024800, episode_reward=-59.88 +/- 49.05\n","Episode length: 323.60 +/- 144.03\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 324      |\n","|    mean_reward     | -59.9    |\n","| time/              |          |\n","|    total_timesteps | 4024800  |\n","| train/             |          |\n","|    actor_loss      | 3.4      |\n","|    critic_loss     | 0.239    |\n","|    ent_coef        | 0.00387  |\n","|    ent_coef_loss   | 0.49     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 167695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5570     |\n","|    fps             | 292      |\n","|    time_elapsed    | 13782    |\n","|    total_timesteps | 4025232  |\n","| train/             |          |\n","|    actor_loss      | 3.22     |\n","|    critic_loss     | 0.296    |\n","|    ent_coef        | 0.00388  |\n","|    ent_coef_loss   | -1.05    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 167713   |\n","---------------------------------\n","Eval num_timesteps=4027200, episode_reward=-102.46 +/- 0.55\n","Episode length: 140.40 +/- 16.66\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 140      |\n","|    mean_reward     | -102     |\n","| time/              |          |\n","|    total_timesteps | 4027200  |\n","| train/             |          |\n","|    actor_loss      | 4.3      |\n","|    critic_loss     | 0.514    |\n","|    ent_coef        | 0.00392  |\n","|    ent_coef_loss   | 2.67     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 167795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5580     |\n","|    fps             | 292      |\n","|    time_elapsed    | 13787    |\n","|    total_timesteps | 4027248  |\n","| train/             |          |\n","|    actor_loss      | 4.77     |\n","|    critic_loss     | 0.395    |\n","|    ent_coef        | 0.00393  |\n","|    ent_coef_loss   | 3.96     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 167797   |\n","---------------------------------\n","Eval num_timesteps=4029600, episode_reward=7.98 +/- 4.63\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.98     |\n","| time/              |          |\n","|    total_timesteps | 4029600  |\n","| train/             |          |\n","|    actor_loss      | 3.73     |\n","|    critic_loss     | 37.4     |\n","|    ent_coef        | 0.00389  |\n","|    ent_coef_loss   | -0.212   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 167895   |\n","---------------------------------\n","Eval num_timesteps=4032000, episode_reward=2.25 +/- 2.38\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.25     |\n","| time/              |          |\n","|    total_timesteps | 4032000  |\n","| train/             |          |\n","|    actor_loss      | 3.63     |\n","|    critic_loss     | 0.347    |\n","|    ent_coef        | 0.00394  |\n","|    ent_coef_loss   | 0.262    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 167995   |\n","---------------------------------\n","Eval num_timesteps=4034400, episode_reward=1.31 +/- 2.26\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.31     |\n","| time/              |          |\n","|    total_timesteps | 4034400  |\n","| train/             |          |\n","|    actor_loss      | 3.79     |\n","|    critic_loss     | 0.33     |\n","|    ent_coef        | 0.00399  |\n","|    ent_coef_loss   | -0.592   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 168095   |\n","---------------------------------\n","Eval num_timesteps=4036800, episode_reward=-58.89 +/- 51.46\n","Episode length: 414.80 +/- 69.57\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 415      |\n","|    mean_reward     | -58.9    |\n","| time/              |          |\n","|    total_timesteps | 4036800  |\n","| train/             |          |\n","|    actor_loss      | 4.06     |\n","|    critic_loss     | 0.322    |\n","|    ent_coef        | 0.00407  |\n","|    ent_coef_loss   | 1.5      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 168195   |\n","---------------------------------\n","Eval num_timesteps=4039200, episode_reward=-61.66 +/- 52.07\n","Episode length: 276.80 +/- 182.24\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 277      |\n","|    mean_reward     | -61.7    |\n","| time/              |          |\n","|    total_timesteps | 4039200  |\n","| train/             |          |\n","|    actor_loss      | 4.82     |\n","|    critic_loss     | 0.272    |\n","|    ent_coef        | 0.00409  |\n","|    ent_coef_loss   | 3.04     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 168295   |\n","---------------------------------\n","Eval num_timesteps=4041600, episode_reward=-52.91 +/- 46.11\n","Episode length: 441.80 +/- 47.52\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 442      |\n","|    mean_reward     | -52.9    |\n","| time/              |          |\n","|    total_timesteps | 4041600  |\n","| train/             |          |\n","|    actor_loss      | 3.97     |\n","|    critic_loss     | 0.989    |\n","|    ent_coef        | 0.00411  |\n","|    ent_coef_loss   | -1.31    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 168395   |\n","---------------------------------\n","Eval num_timesteps=4044000, episode_reward=-95.38 +/- 3.24\n","Episode length: 229.40 +/- 10.29\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 229      |\n","|    mean_reward     | -95.4    |\n","| time/              |          |\n","|    total_timesteps | 4044000  |\n","| train/             |          |\n","|    actor_loss      | 4.1      |\n","|    critic_loss     | 10.7     |\n","|    ent_coef        | 0.00408  |\n","|    ent_coef_loss   | -0.419   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 168495   |\n","---------------------------------\n","Eval num_timesteps=4046400, episode_reward=1.91 +/- 4.27\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.91     |\n","| time/              |          |\n","|    total_timesteps | 4046400  |\n","| train/             |          |\n","|    actor_loss      | 3.79     |\n","|    critic_loss     | 0.322    |\n","|    ent_coef        | 0.00405  |\n","|    ent_coef_loss   | -2.53    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 168595   |\n","---------------------------------\n","Eval num_timesteps=4048800, episode_reward=-7.13 +/- 3.13\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -7.13    |\n","| time/              |          |\n","|    total_timesteps | 4048800  |\n","| train/             |          |\n","|    actor_loss      | 4.03     |\n","|    critic_loss     | 0.372    |\n","|    ent_coef        | 0.00395  |\n","|    ent_coef_loss   | 0.251    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 168695   |\n","---------------------------------\n","Eval num_timesteps=4051200, episode_reward=-7.06 +/- 1.25\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -7.06    |\n","| time/              |          |\n","|    total_timesteps | 4051200  |\n","| train/             |          |\n","|    actor_loss      | 4.27     |\n","|    critic_loss     | 0.354    |\n","|    ent_coef        | 0.00388  |\n","|    ent_coef_loss   | 3.72     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 168795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5590     |\n","|    fps             | 292      |\n","|    time_elapsed    | 13873    |\n","|    total_timesteps | 4052328  |\n","| train/             |          |\n","|    actor_loss      | 4.26     |\n","|    critic_loss     | 0.427    |\n","|    ent_coef        | 0.00393  |\n","|    ent_coef_loss   | -1.97    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 168842   |\n","---------------------------------\n","Eval num_timesteps=4053600, episode_reward=-41.97 +/- 53.09\n","Episode length: 382.40 +/- 144.03\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 382      |\n","|    mean_reward     | -42      |\n","| time/              |          |\n","|    total_timesteps | 4053600  |\n","| train/             |          |\n","|    actor_loss      | 4.49     |\n","|    critic_loss     | 0.37     |\n","|    ent_coef        | 0.00394  |\n","|    ent_coef_loss   | 0.0662   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 168895   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5600     |\n","|    fps             | 292      |\n","|    time_elapsed    | 13884    |\n","|    total_timesteps | 4055256  |\n","| train/             |          |\n","|    actor_loss      | 3.64     |\n","|    critic_loss     | 0.33     |\n","|    ent_coef        | 0.00392  |\n","|    ent_coef_loss   | -0.169   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 168964   |\n","---------------------------------\n","Eval num_timesteps=4056000, episode_reward=-0.67 +/- 2.66\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.667   |\n","| time/              |          |\n","|    total_timesteps | 4056000  |\n","| train/             |          |\n","|    actor_loss      | 4.15     |\n","|    critic_loss     | 0.771    |\n","|    ent_coef        | 0.0039   |\n","|    ent_coef_loss   | -0.753   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 168995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5610     |\n","|    fps             | 292      |\n","|    time_elapsed    | 13892    |\n","|    total_timesteps | 4058352  |\n","| train/             |          |\n","|    actor_loss      | 4.09     |\n","|    critic_loss     | 2.51     |\n","|    ent_coef        | 0.00375  |\n","|    ent_coef_loss   | -2.14    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 169093   |\n","---------------------------------\n","Eval num_timesteps=4058400, episode_reward=-58.71 +/- 50.90\n","Episode length: 311.60 +/- 153.83\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 312      |\n","|    mean_reward     | -58.7    |\n","| time/              |          |\n","|    total_timesteps | 4058400  |\n","| train/             |          |\n","|    actor_loss      | 4.56     |\n","|    critic_loss     | 7.57     |\n","|    ent_coef        | 0.00375  |\n","|    ent_coef_loss   | -2.85    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 169095   |\n","---------------------------------\n","Eval num_timesteps=4060800, episode_reward=-106.00 +/- 0.03\n","Episode length: 314.60 +/- 142.56\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 315      |\n","|    mean_reward     | -106     |\n","| time/              |          |\n","|    total_timesteps | 4060800  |\n","| train/             |          |\n","|    actor_loss      | 4.73     |\n","|    critic_loss     | 0.83     |\n","|    ent_coef        | 0.00361  |\n","|    ent_coef_loss   | -2.52    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 169195   |\n","---------------------------------\n","Eval num_timesteps=4063200, episode_reward=-38.73 +/- 46.28\n","Episode length: 354.00 +/- 178.81\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 354      |\n","|    mean_reward     | -38.7    |\n","| time/              |          |\n","|    total_timesteps | 4063200  |\n","| train/             |          |\n","|    actor_loss      | 4.32     |\n","|    critic_loss     | 0.604    |\n","|    ent_coef        | 0.00352  |\n","|    ent_coef_loss   | 1.28     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 169295   |\n","---------------------------------\n","Eval num_timesteps=4065600, episode_reward=-36.47 +/- 49.64\n","Episode length: 376.80 +/- 150.89\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 377      |\n","|    mean_reward     | -36.5    |\n","| time/              |          |\n","|    total_timesteps | 4065600  |\n","| train/             |          |\n","|    actor_loss      | 3.35     |\n","|    critic_loss     | 0.423    |\n","|    ent_coef        | 0.00352  |\n","|    ent_coef_loss   | -0.895   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 169395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5620     |\n","|    fps             | 292      |\n","|    time_elapsed    | 13925    |\n","|    total_timesteps | 4066968  |\n","| train/             |          |\n","|    actor_loss      | 4.12     |\n","|    critic_loss     | 32.1     |\n","|    ent_coef        | 0.0035   |\n","|    ent_coef_loss   | -0.733   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 169452   |\n","---------------------------------\n","Eval num_timesteps=4068000, episode_reward=-59.86 +/- 51.68\n","Episode length: 249.80 +/- 204.29\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 250      |\n","|    mean_reward     | -59.9    |\n","| time/              |          |\n","|    total_timesteps | 4068000  |\n","| train/             |          |\n","|    actor_loss      | 3.36     |\n","|    critic_loss     | 0.347    |\n","|    ent_coef        | 0.00351  |\n","|    ent_coef_loss   | 4.38     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 169495   |\n","---------------------------------\n","Eval num_timesteps=4070400, episode_reward=4.16 +/- 1.85\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.16     |\n","| time/              |          |\n","|    total_timesteps | 4070400  |\n","| train/             |          |\n","|    actor_loss      | 4.37     |\n","|    critic_loss     | 37.8     |\n","|    ent_coef        | 0.00354  |\n","|    ent_coef_loss   | -1.16    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 169595   |\n","---------------------------------\n","Eval num_timesteps=4072800, episode_reward=-39.62 +/- 52.92\n","Episode length: 456.80 +/- 52.91\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 457      |\n","|    mean_reward     | -39.6    |\n","| time/              |          |\n","|    total_timesteps | 4072800  |\n","| train/             |          |\n","|    actor_loss      | 3.94     |\n","|    critic_loss     | 1.3      |\n","|    ent_coef        | 0.00352  |\n","|    ent_coef_loss   | 1.55     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 169695   |\n","---------------------------------\n","Eval num_timesteps=4075200, episode_reward=-48.73 +/- 50.70\n","Episode length: 350.00 +/- 122.47\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 350      |\n","|    mean_reward     | -48.7    |\n","| time/              |          |\n","|    total_timesteps | 4075200  |\n","| train/             |          |\n","|    actor_loss      | 4.87     |\n","|    critic_loss     | 37.3     |\n","|    ent_coef        | 0.00359  |\n","|    ent_coef_loss   | -0.691   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 169795   |\n","---------------------------------\n","Eval num_timesteps=4077600, episode_reward=-56.94 +/- 56.95\n","Episode length: 443.60 +/- 46.05\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 444      |\n","|    mean_reward     | -56.9    |\n","| time/              |          |\n","|    total_timesteps | 4077600  |\n","| train/             |          |\n","|    actor_loss      | 3.67     |\n","|    critic_loss     | 11.8     |\n","|    ent_coef        | 0.00354  |\n","|    ent_coef_loss   | -1.87    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 169895   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5630     |\n","|    fps             | 292      |\n","|    time_elapsed    | 13969    |\n","|    total_timesteps | 4079856  |\n","| train/             |          |\n","|    actor_loss      | 3.69     |\n","|    critic_loss     | 0.352    |\n","|    ent_coef        | 0.00353  |\n","|    ent_coef_loss   | -2.86    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 169989   |\n","---------------------------------\n","Eval num_timesteps=4080000, episode_reward=-2.60 +/- 0.50\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -2.6     |\n","| time/              |          |\n","|    total_timesteps | 4080000  |\n","| train/             |          |\n","|    actor_loss      | 3.53     |\n","|    critic_loss     | 0.315    |\n","|    ent_coef        | 0.00352  |\n","|    ent_coef_loss   | -3.07    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 169995   |\n","---------------------------------\n","Eval num_timesteps=4082400, episode_reward=7.46 +/- 3.33\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.46     |\n","| time/              |          |\n","|    total_timesteps | 4082400  |\n","| train/             |          |\n","|    actor_loss      | 4.17     |\n","|    critic_loss     | 0.392    |\n","|    ent_coef        | 0.0035   |\n","|    ent_coef_loss   | 0.209    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 170095   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5640     |\n","|    fps             | 291      |\n","|    time_elapsed    | 13983    |\n","|    total_timesteps | 4082664  |\n","| train/             |          |\n","|    actor_loss      | 4.54     |\n","|    critic_loss     | 30.9     |\n","|    ent_coef        | 0.0035   |\n","|    ent_coef_loss   | 2.15     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 170106   |\n","---------------------------------\n","Eval num_timesteps=4084800, episode_reward=-58.22 +/- 51.70\n","Episode length: 326.00 +/- 142.07\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 326      |\n","|    mean_reward     | -58.2    |\n","| time/              |          |\n","|    total_timesteps | 4084800  |\n","| train/             |          |\n","|    actor_loss      | 3.44     |\n","|    critic_loss     | 0.567    |\n","|    ent_coef        | 0.0035   |\n","|    ent_coef_loss   | -1.46    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 170195   |\n","---------------------------------\n","Eval num_timesteps=4087200, episode_reward=-103.91 +/- 1.14\n","Episode length: 140.00 +/- 12.25\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 140      |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 4087200  |\n","| train/             |          |\n","|    actor_loss      | 5.02     |\n","|    critic_loss     | 1.13     |\n","|    ent_coef        | 0.00347  |\n","|    ent_coef_loss   | -0.592   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 170295   |\n","---------------------------------\n","Eval num_timesteps=4089600, episode_reward=-93.97 +/- 7.17\n","Episode length: 174.80 +/- 47.52\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 175      |\n","|    mean_reward     | -94      |\n","| time/              |          |\n","|    total_timesteps | 4089600  |\n","| train/             |          |\n","|    actor_loss      | 3.94     |\n","|    critic_loss     | 3.54     |\n","|    ent_coef        | 0.00353  |\n","|    ent_coef_loss   | 2.37     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 170395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5650     |\n","|    fps             | 292      |\n","|    time_elapsed    | 14003    |\n","|    total_timesteps | 4091592  |\n","| train/             |          |\n","|    actor_loss      | 4.08     |\n","|    critic_loss     | 0.603    |\n","|    ent_coef        | 0.00354  |\n","|    ent_coef_loss   | -1.5     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 170478   |\n","---------------------------------\n","Eval num_timesteps=4092000, episode_reward=-3.20 +/- 3.10\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -3.2     |\n","| time/              |          |\n","|    total_timesteps | 4092000  |\n","| train/             |          |\n","|    actor_loss      | 3.42     |\n","|    critic_loss     | 0.335    |\n","|    ent_coef        | 0.00354  |\n","|    ent_coef_loss   | -1.41    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 170495   |\n","---------------------------------\n","Eval num_timesteps=4094400, episode_reward=10.97 +/- 0.34\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 11       |\n","| time/              |          |\n","|    total_timesteps | 4094400  |\n","| train/             |          |\n","|    actor_loss      | 3.47     |\n","|    critic_loss     | 1.72     |\n","|    ent_coef        | 0.00353  |\n","|    ent_coef_loss   | -1.82    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 170595   |\n","---------------------------------\n","Eval num_timesteps=4096800, episode_reward=-30.30 +/- 50.12\n","Episode length: 366.00 +/- 164.12\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 366      |\n","|    mean_reward     | -30.3    |\n","| time/              |          |\n","|    total_timesteps | 4096800  |\n","| train/             |          |\n","|    actor_loss      | 4.25     |\n","|    critic_loss     | 0.446    |\n","|    ent_coef        | 0.00349  |\n","|    ent_coef_loss   | 0.712    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 170695   |\n","---------------------------------\n","Eval num_timesteps=4099200, episode_reward=5.87 +/- 3.76\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.87     |\n","| time/              |          |\n","|    total_timesteps | 4099200  |\n","| train/             |          |\n","|    actor_loss      | 3.75     |\n","|    critic_loss     | 0.464    |\n","|    ent_coef        | 0.00349  |\n","|    ent_coef_loss   | -1.11    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 170795   |\n","---------------------------------\n","Eval num_timesteps=4101600, episode_reward=5.86 +/- 1.14\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.86     |\n","| time/              |          |\n","|    total_timesteps | 4101600  |\n","| train/             |          |\n","|    actor_loss      | 5.23     |\n","|    critic_loss     | 0.605    |\n","|    ent_coef        | 0.00352  |\n","|    ent_coef_loss   | 0.866    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 170895   |\n","---------------------------------\n","Eval num_timesteps=4104000, episode_reward=-101.67 +/- 1.25\n","Episode length: 164.40 +/- 20.09\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 164      |\n","|    mean_reward     | -102     |\n","| time/              |          |\n","|    total_timesteps | 4104000  |\n","| train/             |          |\n","|    actor_loss      | 5.59     |\n","|    critic_loss     | 0.645    |\n","|    ent_coef        | 0.00356  |\n","|    ent_coef_loss   | 1.26     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 170995   |\n","---------------------------------\n","Eval num_timesteps=4106400, episode_reward=3.02 +/- 1.26\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.02     |\n","| time/              |          |\n","|    total_timesteps | 4106400  |\n","| train/             |          |\n","|    actor_loss      | 4.74     |\n","|    critic_loss     | 1.28     |\n","|    ent_coef        | 0.0036   |\n","|    ent_coef_loss   | 1.8      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 171095   |\n","---------------------------------\n","Eval num_timesteps=4108800, episode_reward=5.84 +/- 3.71\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.84     |\n","| time/              |          |\n","|    total_timesteps | 4108800  |\n","| train/             |          |\n","|    actor_loss      | 4.17     |\n","|    critic_loss     | 0.745    |\n","|    ent_coef        | 0.0037   |\n","|    ent_coef_loss   | 1.33     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 171195   |\n","---------------------------------\n","Eval num_timesteps=4111200, episode_reward=-61.01 +/- 51.97\n","Episode length: 288.80 +/- 172.44\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 289      |\n","|    mean_reward     | -61      |\n","| time/              |          |\n","|    total_timesteps | 4111200  |\n","| train/             |          |\n","|    actor_loss      | 6.04     |\n","|    critic_loss     | 0.639    |\n","|    ent_coef        | 0.00374  |\n","|    ent_coef_loss   | 1.89     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 171295   |\n","---------------------------------\n","Eval num_timesteps=4113600, episode_reward=2.91 +/- 4.95\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.91     |\n","| time/              |          |\n","|    total_timesteps | 4113600  |\n","| train/             |          |\n","|    actor_loss      | 3.68     |\n","|    critic_loss     | 0.425    |\n","|    ent_coef        | 0.00378  |\n","|    ent_coef_loss   | -1.96    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 171395   |\n","---------------------------------\n","Eval num_timesteps=4116000, episode_reward=-100.56 +/- 2.61\n","Episode length: 242.40 +/- 88.67\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 242      |\n","|    mean_reward     | -101     |\n","| time/              |          |\n","|    total_timesteps | 4116000  |\n","| train/             |          |\n","|    actor_loss      | 5.34     |\n","|    critic_loss     | 1.21     |\n","|    ent_coef        | 0.00378  |\n","|    ent_coef_loss   | 1.58     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 171495   |\n","---------------------------------\n","Eval num_timesteps=4118400, episode_reward=-94.80 +/- 5.18\n","Episode length: 387.20 +/- 43.11\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 387      |\n","|    mean_reward     | -94.8    |\n","| time/              |          |\n","|    total_timesteps | 4118400  |\n","| train/             |          |\n","|    actor_loss      | 5.01     |\n","|    critic_loss     | 18.7     |\n","|    ent_coef        | 0.00383  |\n","|    ent_coef_loss   | 1.1      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 171595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5660     |\n","|    fps             | 292      |\n","|    time_elapsed    | 14100    |\n","|    total_timesteps | 4119840  |\n","| train/             |          |\n","|    actor_loss      | 4.64     |\n","|    critic_loss     | 0.487    |\n","|    ent_coef        | 0.00382  |\n","|    ent_coef_loss   | -1.24    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 171655   |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    episodes        | 5670    |\n","|    fps             | 292     |\n","|    time_elapsed    | 14100   |\n","|    total_timesteps | 4119840 |\n","--------------------------------\n","Eval num_timesteps=4120800, episode_reward=-94.88 +/- 1.92\n","Episode length: 221.60 +/- 66.63\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 222      |\n","|    mean_reward     | -94.9    |\n","| time/              |          |\n","|    total_timesteps | 4120800  |\n","| train/             |          |\n","|    actor_loss      | 3.98     |\n","|    critic_loss     | 0.514    |\n","|    ent_coef        | 0.00382  |\n","|    ent_coef_loss   | 1.26     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 171695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5680     |\n","|    fps             | 292      |\n","|    time_elapsed    | 14105    |\n","|    total_timesteps | 4122600  |\n","| train/             |          |\n","|    actor_loss      | 4.35     |\n","|    critic_loss     | 0.363    |\n","|    ent_coef        | 0.00384  |\n","|    ent_coef_loss   | 0.0832   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 171770   |\n","---------------------------------\n","Eval num_timesteps=4123200, episode_reward=-36.40 +/- 48.15\n","Episode length: 396.80 +/- 126.39\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 397      |\n","|    mean_reward     | -36.4    |\n","| time/              |          |\n","|    total_timesteps | 4123200  |\n","| train/             |          |\n","|    actor_loss      | 3.7      |\n","|    critic_loss     | 0.516    |\n","|    ent_coef        | 0.00383  |\n","|    ent_coef_loss   | -1.06    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 171795   |\n","---------------------------------\n","Eval num_timesteps=4125600, episode_reward=-96.40 +/- 0.71\n","Episode length: 141.80 +/- 11.27\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 142      |\n","|    mean_reward     | -96.4    |\n","| time/              |          |\n","|    total_timesteps | 4125600  |\n","| train/             |          |\n","|    actor_loss      | 6.23     |\n","|    critic_loss     | 33.7     |\n","|    ent_coef        | 0.00378  |\n","|    ent_coef_loss   | 1.93     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 171895   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5690     |\n","|    fps             | 292      |\n","|    time_elapsed    | 14120    |\n","|    total_timesteps | 4127784  |\n","| train/             |          |\n","|    actor_loss      | 4.77     |\n","|    critic_loss     | 2.45     |\n","|    ent_coef        | 0.00376  |\n","|    ent_coef_loss   | 7.9      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 171986   |\n","---------------------------------\n","Eval num_timesteps=4128000, episode_reward=10.80 +/- 0.44\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 10.8     |\n","| time/              |          |\n","|    total_timesteps | 4128000  |\n","| train/             |          |\n","|    actor_loss      | 3.83     |\n","|    critic_loss     | 2.05     |\n","|    ent_coef        | 0.00379  |\n","|    ent_coef_loss   | 4.61     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 171995   |\n","---------------------------------\n","Eval num_timesteps=4130400, episode_reward=-56.15 +/- 54.09\n","Episode length: 281.60 +/- 178.32\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 282      |\n","|    mean_reward     | -56.2    |\n","| time/              |          |\n","|    total_timesteps | 4130400  |\n","| train/             |          |\n","|    actor_loss      | 4.32     |\n","|    critic_loss     | 0.471    |\n","|    ent_coef        | 0.00379  |\n","|    ent_coef_loss   | -0.5     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 172095   |\n","---------------------------------\n","Eval num_timesteps=4132800, episode_reward=4.56 +/- 0.21\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.56     |\n","| time/              |          |\n","|    total_timesteps | 4132800  |\n","| train/             |          |\n","|    actor_loss      | 4.41     |\n","|    critic_loss     | 22.1     |\n","|    ent_coef        | 0.00376  |\n","|    ent_coef_loss   | -0.0605  |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 172195   |\n","---------------------------------\n","Eval num_timesteps=4135200, episode_reward=5.50 +/- 0.86\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.5      |\n","| time/              |          |\n","|    total_timesteps | 4135200  |\n","| train/             |          |\n","|    actor_loss      | 3.88     |\n","|    critic_loss     | 0.286    |\n","|    ent_coef        | 0.00366  |\n","|    ent_coef_loss   | -0.918   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 172295   |\n","---------------------------------\n","Eval num_timesteps=4137600, episode_reward=0.69 +/- 0.62\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.686    |\n","| time/              |          |\n","|    total_timesteps | 4137600  |\n","| train/             |          |\n","|    actor_loss      | 5.01     |\n","|    critic_loss     | 0.396    |\n","|    ent_coef        | 0.00365  |\n","|    ent_coef_loss   | 4.95     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 172395   |\n","---------------------------------\n","Eval num_timesteps=4140000, episode_reward=7.06 +/- 1.68\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.06     |\n","| time/              |          |\n","|    total_timesteps | 4140000  |\n","| train/             |          |\n","|    actor_loss      | 4.56     |\n","|    critic_loss     | 0.737    |\n","|    ent_coef        | 0.00389  |\n","|    ent_coef_loss   | 1.4      |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 172495   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5700     |\n","|    fps             | 292      |\n","|    time_elapsed    | 14169    |\n","|    total_timesteps | 4142304  |\n","| train/             |          |\n","|    actor_loss      | 4.4      |\n","|    critic_loss     | 0.701    |\n","|    ent_coef        | 0.00388  |\n","|    ent_coef_loss   | -0.359   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 172591   |\n","---------------------------------\n","Eval num_timesteps=4142400, episode_reward=3.92 +/- 0.52\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.92     |\n","| time/              |          |\n","|    total_timesteps | 4142400  |\n","| train/             |          |\n","|    actor_loss      | 4.47     |\n","|    critic_loss     | 0.965    |\n","|    ent_coef        | 0.00388  |\n","|    ent_coef_loss   | 0.0917   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 172595   |\n","---------------------------------\n","Eval num_timesteps=4144800, episode_reward=-101.19 +/- 3.94\n","Episode length: 149.00 +/- 41.64\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 149      |\n","|    mean_reward     | -101     |\n","| time/              |          |\n","|    total_timesteps | 4144800  |\n","| train/             |          |\n","|    actor_loss      | 4.87     |\n","|    critic_loss     | 0.427    |\n","|    ent_coef        | 0.00389  |\n","|    ent_coef_loss   | 0.271    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 172695   |\n","---------------------------------\n","Eval num_timesteps=4147200, episode_reward=-35.63 +/- 51.17\n","Episode length: 374.00 +/- 154.32\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 374      |\n","|    mean_reward     | -35.6    |\n","| time/              |          |\n","|    total_timesteps | 4147200  |\n","| train/             |          |\n","|    actor_loss      | 4.35     |\n","|    critic_loss     | 1.71     |\n","|    ent_coef        | 0.00388  |\n","|    ent_coef_loss   | 0.441    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 172795   |\n","---------------------------------\n","Eval num_timesteps=4149600, episode_reward=-57.90 +/- 52.17\n","Episode length: 297.80 +/- 165.10\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 298      |\n","|    mean_reward     | -57.9    |\n","| time/              |          |\n","|    total_timesteps | 4149600  |\n","| train/             |          |\n","|    actor_loss      | 4.53     |\n","|    critic_loss     | 35.8     |\n","|    ent_coef        | 0.00389  |\n","|    ent_coef_loss   | 0.47     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 172895   |\n","---------------------------------\n","Eval num_timesteps=4152000, episode_reward=0.19 +/- 1.21\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.186    |\n","| time/              |          |\n","|    total_timesteps | 4152000  |\n","| train/             |          |\n","|    actor_loss      | 4.06     |\n","|    critic_loss     | 0.522    |\n","|    ent_coef        | 0.00387  |\n","|    ent_coef_loss   | -1.9     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 172995   |\n","---------------------------------\n","Eval num_timesteps=4154400, episode_reward=2.22 +/- 2.02\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.22     |\n","| time/              |          |\n","|    total_timesteps | 4154400  |\n","| train/             |          |\n","|    actor_loss      | 4.42     |\n","|    critic_loss     | 0.457    |\n","|    ent_coef        | 0.00385  |\n","|    ent_coef_loss   | -0.167   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 173095   |\n","---------------------------------\n","Eval num_timesteps=4156800, episode_reward=8.22 +/- 5.63\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 8.22     |\n","| time/              |          |\n","|    total_timesteps | 4156800  |\n","| train/             |          |\n","|    actor_loss      | 4.33     |\n","|    critic_loss     | 0.377    |\n","|    ent_coef        | 0.00388  |\n","|    ent_coef_loss   | -1.06    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 173195   |\n","---------------------------------\n","Eval num_timesteps=4159200, episode_reward=5.67 +/- 1.23\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 5.67     |\n","| time/              |          |\n","|    total_timesteps | 4159200  |\n","| train/             |          |\n","|    actor_loss      | 4.29     |\n","|    critic_loss     | 0.45     |\n","|    ent_coef        | 0.00388  |\n","|    ent_coef_loss   | 2.04     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 173295   |\n","---------------------------------\n","Eval num_timesteps=4161600, episode_reward=7.38 +/- 1.43\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.38     |\n","| time/              |          |\n","|    total_timesteps | 4161600  |\n","| train/             |          |\n","|    actor_loss      | 4.16     |\n","|    critic_loss     | 0.787    |\n","|    ent_coef        | 0.00388  |\n","|    ent_coef_loss   | -0.549   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 173395   |\n","---------------------------------\n","Eval num_timesteps=4164000, episode_reward=9.19 +/- 0.62\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 9.19     |\n","| time/              |          |\n","|    total_timesteps | 4164000  |\n","| train/             |          |\n","|    actor_loss      | 4.52     |\n","|    critic_loss     | 1.31     |\n","|    ent_coef        | 0.00389  |\n","|    ent_coef_loss   | -2.18    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 173495   |\n","---------------------------------\n","Eval num_timesteps=4166400, episode_reward=-0.86 +/- 3.15\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.859   |\n","| time/              |          |\n","|    total_timesteps | 4166400  |\n","| train/             |          |\n","|    actor_loss      | 5.21     |\n","|    critic_loss     | 11.3     |\n","|    ent_coef        | 0.00392  |\n","|    ent_coef_loss   | 3.07     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 173595   |\n","---------------------------------\n","Eval num_timesteps=4168800, episode_reward=-103.36 +/- 4.93\n","Episode length: 216.20 +/- 79.85\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 216      |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 4168800  |\n","| train/             |          |\n","|    actor_loss      | 4.2      |\n","|    critic_loss     | 2.01     |\n","|    ent_coef        | 0.00405  |\n","|    ent_coef_loss   | 1.82     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 173695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5710     |\n","|    fps             | 292      |\n","|    time_elapsed    | 14267    |\n","|    total_timesteps | 4170768  |\n","| train/             |          |\n","|    actor_loss      | 4.59     |\n","|    critic_loss     | 0.452    |\n","|    ent_coef        | 0.00413  |\n","|    ent_coef_loss   | 2.16     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 173777   |\n","---------------------------------\n","Eval num_timesteps=4171200, episode_reward=-61.67 +/- 50.82\n","Episode length: 287.60 +/- 173.42\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 288      |\n","|    mean_reward     | -61.7    |\n","| time/              |          |\n","|    total_timesteps | 4171200  |\n","| train/             |          |\n","|    actor_loss      | 4.21     |\n","|    critic_loss     | 0.726    |\n","|    ent_coef        | 0.00416  |\n","|    ent_coef_loss   | 2.59     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 173795   |\n","---------------------------------\n","Eval num_timesteps=4173600, episode_reward=7.59 +/- 0.11\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.59     |\n","| time/              |          |\n","|    total_timesteps | 4173600  |\n","| train/             |          |\n","|    actor_loss      | 5.04     |\n","|    critic_loss     | 0.489    |\n","|    ent_coef        | 0.00431  |\n","|    ent_coef_loss   | 2.65     |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 173895   |\n","---------------------------------\n","Eval num_timesteps=4176000, episode_reward=-1.16 +/- 2.97\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.16    |\n","| time/              |          |\n","|    total_timesteps | 4176000  |\n","| train/             |          |\n","|    actor_loss      | 4.27     |\n","|    critic_loss     | 0.241    |\n","|    ent_coef        | 0.0044   |\n","|    ent_coef_loss   | -1.04    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 173995   |\n","---------------------------------\n","Eval num_timesteps=4178400, episode_reward=-35.37 +/- 49.53\n","Episode length: 342.00 +/- 193.51\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 342      |\n","|    mean_reward     | -35.4    |\n","| time/              |          |\n","|    total_timesteps | 4178400  |\n","| train/             |          |\n","|    actor_loss      | 4.41     |\n","|    critic_loss     | 0.453    |\n","|    ent_coef        | 0.00446  |\n","|    ent_coef_loss   | 0.759    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 174095   |\n","---------------------------------\n","Eval num_timesteps=4180800, episode_reward=4.72 +/- 1.93\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.72     |\n","| time/              |          |\n","|    total_timesteps | 4180800  |\n","| train/             |          |\n","|    actor_loss      | 4.18     |\n","|    critic_loss     | 0.355    |\n","|    ent_coef        | 0.00453  |\n","|    ent_coef_loss   | -1.08    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 174195   |\n","---------------------------------\n","Eval num_timesteps=4183200, episode_reward=6.42 +/- 3.74\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.42     |\n","| time/              |          |\n","|    total_timesteps | 4183200  |\n","| train/             |          |\n","|    actor_loss      | 3.77     |\n","|    critic_loss     | 0.154    |\n","|    ent_coef        | 0.0045   |\n","|    ent_coef_loss   | -1.46    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 174295   |\n","---------------------------------\n","Eval num_timesteps=4185600, episode_reward=-37.74 +/- 54.48\n","Episode length: 378.00 +/- 149.42\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 378      |\n","|    mean_reward     | -37.7    |\n","| time/              |          |\n","|    total_timesteps | 4185600  |\n","| train/             |          |\n","|    actor_loss      | 4.44     |\n","|    critic_loss     | 0.279    |\n","|    ent_coef        | 0.00444  |\n","|    ent_coef_loss   | -0.661   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 174395   |\n","---------------------------------\n","Eval num_timesteps=4188000, episode_reward=-60.68 +/- 51.30\n","Episode length: 365.00 +/- 110.23\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 365      |\n","|    mean_reward     | -60.7    |\n","| time/              |          |\n","|    total_timesteps | 4188000  |\n","| train/             |          |\n","|    actor_loss      | 4.41     |\n","|    critic_loss     | 1.58     |\n","|    ent_coef        | 0.0044   |\n","|    ent_coef_loss   | 0.406    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 174495   |\n","---------------------------------\n","Eval num_timesteps=4190400, episode_reward=-31.11 +/- 55.97\n","Episode length: 353.20 +/- 179.79\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 353      |\n","|    mean_reward     | -31.1    |\n","| time/              |          |\n","|    total_timesteps | 4190400  |\n","| train/             |          |\n","|    actor_loss      | 3.91     |\n","|    critic_loss     | 0.465    |\n","|    ent_coef        | 0.00438  |\n","|    ent_coef_loss   | -2.58    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 174595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5720     |\n","|    fps             | 292      |\n","|    time_elapsed    | 14341    |\n","|    total_timesteps | 4190448  |\n","| train/             |          |\n","|    actor_loss      | 3.93     |\n","|    critic_loss     | 0.73     |\n","|    ent_coef        | 0.00438  |\n","|    ent_coef_loss   | -1.97    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 174597   |\n","---------------------------------\n","Eval num_timesteps=4192800, episode_reward=7.22 +/- 6.57\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.22     |\n","| time/              |          |\n","|    total_timesteps | 4192800  |\n","| train/             |          |\n","|    actor_loss      | 4.37     |\n","|    critic_loss     | 35.3     |\n","|    ent_coef        | 0.00429  |\n","|    ent_coef_loss   | -0.434   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 174695   |\n","---------------------------------\n","Eval num_timesteps=4195200, episode_reward=-55.49 +/- 48.89\n","Episode length: 276.80 +/- 182.24\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 277      |\n","|    mean_reward     | -55.5    |\n","| time/              |          |\n","|    total_timesteps | 4195200  |\n","| train/             |          |\n","|    actor_loss      | 3.99     |\n","|    critic_loss     | 0.251    |\n","|    ent_coef        | 0.00421  |\n","|    ent_coef_loss   | -1.12    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 174795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5730     |\n","|    fps             | 292      |\n","|    time_elapsed    | 14359    |\n","|    total_timesteps | 4197264  |\n","| train/             |          |\n","|    actor_loss      | 4.92     |\n","|    critic_loss     | 0.513    |\n","|    ent_coef        | 0.00413  |\n","|    ent_coef_loss   | -1.04    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 174881   |\n","---------------------------------\n","Eval num_timesteps=4197600, episode_reward=-57.36 +/- 53.02\n","Episode length: 261.80 +/- 194.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 262      |\n","|    mean_reward     | -57.4    |\n","| time/              |          |\n","|    total_timesteps | 4197600  |\n","| train/             |          |\n","|    actor_loss      | 4.43     |\n","|    critic_loss     | 0.964    |\n","|    ent_coef        | 0.00412  |\n","|    ent_coef_loss   | -2.73    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 174895   |\n","---------------------------------\n","Eval num_timesteps=4200000, episode_reward=-96.53 +/- 1.38\n","Episode length: 202.80 +/- 1.47\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 203      |\n","|    mean_reward     | -96.5    |\n","| time/              |          |\n","|    total_timesteps | 4200000  |\n","| train/             |          |\n","|    actor_loss      | 4.07     |\n","|    critic_loss     | 0.369    |\n","|    ent_coef        | 0.00405  |\n","|    ent_coef_loss   | -1.29    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 174995   |\n","---------------------------------\n","Eval num_timesteps=4202400, episode_reward=-38.28 +/- 51.20\n","Episode length: 331.20 +/- 206.74\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 331      |\n","|    mean_reward     | -38.3    |\n","| time/              |          |\n","|    total_timesteps | 4202400  |\n","| train/             |          |\n","|    actor_loss      | 4.3      |\n","|    critic_loss     | 0.538    |\n","|    ent_coef        | 0.00402  |\n","|    ent_coef_loss   | -0.778   |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 175095   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 5740     |\n","|    fps             | 292      |\n","|    time_elapsed    | 14381    |\n","|    total_timesteps | 4204656  |\n","| train/             |          |\n","|    actor_loss      | 5.07     |\n","|    critic_loss     | 0.726    |\n","|    ent_coef        | 0.00404  |\n","|    ent_coef_loss   | -1.05    |\n","|    learning_rate   | 0.0003   |\n","|    n_updates       | 175189   |\n","---------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n"]},{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-9-f37724f97294>\", line 10, in <cell line: 10>\n","    model.learn(total_timesteps=5000000, log_interval=10,callback=eval_callback)\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/sac/sac.py\", line 307, in learn\n","    return super().learn(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 312, in learn\n","    rollout = self.collect_rollouts(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 552, in collect_rollouts\n","    if callback.on_step() is False:\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 104, in on_step\n","    return self._on_step()\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 471, in _on_step\n","    np.savez(\n","  File \"<__array_function__ internals>\", line 180, in savez\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 615, in savez\n","    _savez(file, args, kwds, False)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 712, in _savez\n","    zipf = zipfile_factory(file, mode=\"w\", compression=compression)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 103, in zipfile_factory\n","    return zipfile.ZipFile(file, *args, **kwargs)\n","  File \"/usr/lib/python3.10/zipfile.py\", line 1251, in __init__\n","    self.fp = io.open(file, filemode)\n","OSError: [Errno 107] Transport endpoint is not connected: './multiwalker_sac_log_eval/evaluations.npz'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'OSError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-9-f37724f97294>\", line 10, in <cell line: 10>\n","    model.learn(total_timesteps=5000000, log_interval=10,callback=eval_callback)\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/sac/sac.py\", line 307, in learn\n","    return super().learn(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 312, in learn\n","    rollout = self.collect_rollouts(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 552, in collect_rollouts\n","    if callback.on_step() is False:\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 104, in on_step\n","    return self._on_step()\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 471, in _on_step\n","    np.savez(\n","  File \"<__array_function__ internals>\", line 180, in savez\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 615, in savez\n","    _savez(file, args, kwds, False)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 712, in _savez\n","    zipf = zipfile_factory(file, mode=\"w\", compression=compression)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 103, in zipfile_factory\n","    return zipfile.ZipFile(file, *args, **kwargs)\n","  File \"/usr/lib/python3.10/zipfile.py\", line 1251, in __init__\n","    self.fp = io.open(file, filemode)\n","OSError: [Errno 107] Transport endpoint is not connected: './multiwalker_sac_log_eval/evaluations.npz'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'OSError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n","    if (await self.run_code(code, result,  async_=asy)):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n","    self.showtraceback(running_compiled_code=True)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n","    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-9-f37724f97294>\", line 10, in <cell line: 10>\n","    model.learn(total_timesteps=5000000, log_interval=10,callback=eval_callback)\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/sac/sac.py\", line 307, in learn\n","    return super().learn(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 312, in learn\n","    rollout = self.collect_rollouts(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 552, in collect_rollouts\n","    if callback.on_step() is False:\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 104, in on_step\n","    return self._on_step()\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 471, in _on_step\n","    np.savez(\n","  File \"<__array_function__ internals>\", line 180, in savez\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 615, in savez\n","    _savez(file, args, kwds, False)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 712, in _savez\n","    zipf = zipfile_factory(file, mode=\"w\", compression=compression)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 103, in zipfile_factory\n","    return zipfile.ZipFile(file, *args, **kwargs)\n","  File \"/usr/lib/python3.10/zipfile.py\", line 1251, in __init__\n","    self.fp = io.open(file, filemode)\n","OSError: [Errno 107] Transport endpoint is not connected: './multiwalker_sac_log_eval/evaluations.npz'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'OSError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n","    if (await self.run_code(code, result,  async_=asy)):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n","    self.showtraceback(running_compiled_code=True)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n","    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n","    return runner(coro)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n","    coro.send(None)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n","    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3492, in run_ast_nodes\n","    self.showtraceback()\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1142, in structured_traceback\n","    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n"]}],"source":["from stable_baselines3.common.logger import configure\n","\n","tmp_path = \"multiwalker_sac_log_eval/\"\n","# set up logger\n","new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n","\n","model = SAC(\"MlpPolicy\", env, verbose=3) #,train_freq=1\n","\n","model.set_logger(new_logger)\n","model.learn(total_timesteps=5000000, log_interval=10,callback=eval_callback)\n","model.save(\"multiwalker_sac\")"]},{"cell_type":"code","source":["from stable_baselines3 import SAC\n","from pettingzoo.sisl import multiwalker_v9\n","import supersuit as ss\n","from stable_baselines3.common.monitor import Monitor"],"metadata":{"id":"-6BKh_FuTZzN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HTOVAZKHTdeV","executionInfo":{"status":"ok","timestamp":1697725712497,"user_tz":-120,"elapsed":334,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"67c4f770-06db-4aed-c296-ff714445355d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["'=2.13'\t\t\t\t     multiwalker_sac_log_eval\n"," DQN_new_pettingzoo_gym_cap.ipynb    multiwalker_td3_log_eval\n"," DQN_policies\t\t\t     policy_log_eval\n","'Entrenamientos antiguos sin logs'   PPO_policies\n"," Entrenamientos_log_no_eval\t     results_rllib\n"," MCR_TFM.ipynb\t\t\t     TFM_Multiwalker_DDPG_PPO_gym_cap.ipynb\n"," multi_car_racing\t\t     TFM_Multiwalker_SAC_gym_cap.ipynb\n"," multiwalker_ddpg_log_eval\t     TFM_Multiwalker_TD3_gym_cap.ipynb\n"," multiwalker_ddpg.zip\t\t     TFM_PPO_new_pettingzoo_gym_cap.ipynb\n"," multiwalker_ppo_log_eval\t     TFM_PPO_pettingzoo_gym_cap.ipynb\n"," multiwalker_ppo.zip\n"]}]},{"cell_type":"code","source":["env = multiwalker_v9.parallel_env(render_mode=\"rgb_array\")\n","\n","# #---------------------------------\n","# log_dir = \"./policy_log\"\n","# os.makedirs(log_dir, exist_ok=True)\n","# env = Monitor(env, log_dir)\n","# #---------------------------------\n","# env = ss.color_reduction_v0(env, mode='B')\n","# env = ss.black_death_v3(env)\n","# env = ss.resize_v1(env, x_size=84, y_size=84)\n","env = ss.frame_stack_v1(env, 3)\n","env = ss.pettingzoo_env_to_vec_env_v1(env)\n","env = ss.concat_vec_envs_v1(env, 8, num_cpus=4, base_class='stable_baselines3')"],"metadata":{"id":"U6TtjGBcTedF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3.common.callbacks import EvalCallback\n","eval_callback = EvalCallback(env, best_model_save_path=\"./multiwalker_sac2_log_eval/\",\n","                             log_path=\"./multiwalker_sac2_log_eval/\", eval_freq=100,\n","                             deterministic=False, render=False)\n"],"metadata":{"id":"1ah3u9HuTga_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3.common.logger import configure\n","\n","tmp_path = \"multiwalker_sac2_log_eval/\"\n","# set up logger\n","new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n","\n","model = SAC(\"MlpPolicy\", env, verbose=3,batch_size=512, learning_rate=0.0005) #,train_freq=1\n","\n","model.set_logger(new_logger)\n","model.learn(total_timesteps=5000000, log_interval=10,callback=eval_callback)\n","model.save(\"multiwalker_sac2\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"muMFxxDZTg_Z","outputId":"9bb008e4-6429-4d90-bc1c-c0f845f766f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Logging to multiwalker_sac2_log_eval/\n","Using cuda device\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 10       |\n","|    fps             | 328      |\n","|    time_elapsed    | 5        |\n","|    total_timesteps | 1656     |\n","| train/             |          |\n","|    actor_loss      | -5.06    |\n","|    critic_loss     | 22.1     |\n","|    ent_coef        | 0.969    |\n","|    ent_coef_loss   | -0.212   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 64       |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 20       |\n","|    fps             | 345      |\n","|    time_elapsed    | 5        |\n","|    total_timesteps | 1824     |\n","| train/             |          |\n","|    actor_loss      | -4.85    |\n","|    critic_loss     | 131      |\n","|    ent_coef        | 0.966    |\n","|    ent_coef_loss   | -0.236   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 71       |\n","---------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n","|    ent_coef        | 0.00381  |\n","|    ent_coef_loss   | 0.262    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 151895   |\n","---------------------------------\n","Eval num_timesteps=3648000, episode_reward=-100.25 +/- 0.10\n","Episode length: 289.40 +/- 167.06\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 289      |\n","|    mean_reward     | -100     |\n","| time/              |          |\n","|    total_timesteps | 3648000  |\n","| train/             |          |\n","|    actor_loss      | 6.29     |\n","|    critic_loss     | 1.23     |\n","|    ent_coef        | 0.00376  |\n","|    ent_coef_loss   | -1.2     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 151995   |\n","---------------------------------\n","Eval num_timesteps=3650400, episode_reward=-48.81 +/- 44.65\n","Episode length: 446.60 +/- 43.60\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 447      |\n","|    mean_reward     | -48.8    |\n","| time/              |          |\n","|    total_timesteps | 3650400  |\n","| train/             |          |\n","|    actor_loss      | 6.87     |\n","|    critic_loss     | 13.5     |\n","|    ent_coef        | 0.00376  |\n","|    ent_coef_loss   | 0.754    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 152095   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6070     |\n","|    fps             | 316      |\n","|    time_elapsed    | 11533    |\n","|    total_timesteps | 3652536  |\n","| train/             |          |\n","|    actor_loss      | 6.68     |\n","|    critic_loss     | 6.55     |\n","|    ent_coef        | 0.00378  |\n","|    ent_coef_loss   | 4.74     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 152184   |\n","---------------------------------\n","Eval num_timesteps=3652800, episode_reward=-53.25 +/- 61.71\n","Episode length: 429.80 +/- 57.32\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 430      |\n","|    mean_reward     | -53.2    |\n","| time/              |          |\n","|    total_timesteps | 3652800  |\n","| train/             |          |\n","|    actor_loss      | 6.17     |\n","|    critic_loss     | 11.9     |\n","|    ent_coef        | 0.00381  |\n","|    ent_coef_loss   | 3.61     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 152195   |\n","---------------------------------\n","Eval num_timesteps=3655200, episode_reward=-107.33 +/- 5.38\n","Episode length: 210.60 +/- 24.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 211      |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 3655200  |\n","| train/             |          |\n","|    actor_loss      | 6.87     |\n","|    critic_loss     | 0.683    |\n","|    ent_coef        | 0.00403  |\n","|    ent_coef_loss   | 1.74     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 152295   |\n","---------------------------------\n","Eval num_timesteps=3657600, episode_reward=-22.30 +/- 51.39\n","Episode length: 408.40 +/- 112.19\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 408      |\n","|    mean_reward     | -22.3    |\n","| time/              |          |\n","|    total_timesteps | 3657600  |\n","| train/             |          |\n","|    actor_loss      | 6.26     |\n","|    critic_loss     | 1.46     |\n","|    ent_coef        | 0.0042   |\n","|    ent_coef_loss   | -4.22    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 152395   |\n","---------------------------------\n","Eval num_timesteps=3660000, episode_reward=-49.67 +/- 62.27\n","Episode length: 384.20 +/- 94.55\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 384      |\n","|    mean_reward     | -49.7    |\n","| time/              |          |\n","|    total_timesteps | 3660000  |\n","| train/             |          |\n","|    actor_loss      | 6.47     |\n","|    critic_loss     | 0.898    |\n","|    ent_coef        | 0.00398  |\n","|    ent_coef_loss   | -0.772   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 152495   |\n","---------------------------------\n","Eval num_timesteps=3662400, episode_reward=-96.83 +/- 8.56\n","Episode length: 370.60 +/- 55.85\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 371      |\n","|    mean_reward     | -96.8    |\n","| time/              |          |\n","|    total_timesteps | 3662400  |\n","| train/             |          |\n","|    actor_loss      | 6.07     |\n","|    critic_loss     | 3.92     |\n","|    ent_coef        | 0.00394  |\n","|    ent_coef_loss   | 0.998    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 152595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6080     |\n","|    fps             | 316      |\n","|    time_elapsed    | 11569    |\n","|    total_timesteps | 3662544  |\n","| train/             |          |\n","|    actor_loss      | 6.78     |\n","|    critic_loss     | 1.4      |\n","|    ent_coef        | 0.00394  |\n","|    ent_coef_loss   | 2.85     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 152601   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6090     |\n","|    fps             | 316      |\n","|    time_elapsed    | 11570    |\n","|    total_timesteps | 3663864  |\n","| train/             |          |\n","|    actor_loss      | 7.45     |\n","|    critic_loss     | 14.2     |\n","|    ent_coef        | 0.00399  |\n","|    ent_coef_loss   | 0.385    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 152656   |\n","---------------------------------\n","Eval num_timesteps=3664800, episode_reward=47.73 +/- 4.72\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 47.7     |\n","| time/              |          |\n","|    total_timesteps | 3664800  |\n","| train/             |          |\n","|    actor_loss      | 5.95     |\n","|    critic_loss     | 10.2     |\n","|    ent_coef        | 0.00393  |\n","|    ent_coef_loss   | 1.07     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 152695   |\n","---------------------------------\n","New best mean reward!\n","Eval num_timesteps=3667200, episode_reward=-91.62 +/- 4.51\n","Episode length: 210.00 +/- 63.69\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 210      |\n","|    mean_reward     | -91.6    |\n","| time/              |          |\n","|    total_timesteps | 3667200  |\n","| train/             |          |\n","|    actor_loss      | 5.82     |\n","|    critic_loss     | 1.96     |\n","|    ent_coef        | 0.00396  |\n","|    ent_coef_loss   | -0.455   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 152795   |\n","---------------------------------\n","Eval num_timesteps=3669600, episode_reward=-61.61 +/- 53.12\n","Episode length: 323.00 +/- 144.52\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 323      |\n","|    mean_reward     | -61.6    |\n","| time/              |          |\n","|    total_timesteps | 3669600  |\n","| train/             |          |\n","|    actor_loss      | 6.33     |\n","|    critic_loss     | 1.56     |\n","|    ent_coef        | 0.00387  |\n","|    ent_coef_loss   | -0.812   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 152895   |\n","---------------------------------\n","Eval num_timesteps=3672000, episode_reward=-82.49 +/- 0.26\n","Episode length: 175.00 +/- 17.15\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 175      |\n","|    mean_reward     | -82.5    |\n","| time/              |          |\n","|    total_timesteps | 3672000  |\n","| train/             |          |\n","|    actor_loss      | 6.16     |\n","|    critic_loss     | 7.42     |\n","|    ent_coef        | 0.00379  |\n","|    ent_coef_loss   | 0.657    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 152995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6100     |\n","|    fps             | 316      |\n","|    time_elapsed    | 11595    |\n","|    total_timesteps | 3672096  |\n","| train/             |          |\n","|    actor_loss      | 6.89     |\n","|    critic_loss     | 6.6      |\n","|    ent_coef        | 0.0038   |\n","|    ent_coef_loss   | 3.22     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 152999   |\n","---------------------------------\n","Eval num_timesteps=3674400, episode_reward=-27.90 +/- 69.89\n","Episode length: 310.40 +/- 154.81\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 310      |\n","|    mean_reward     | -27.9    |\n","| time/              |          |\n","|    total_timesteps | 3674400  |\n","| train/             |          |\n","|    actor_loss      | 6.69     |\n","|    critic_loss     | 13       |\n","|    ent_coef        | 0.00387  |\n","|    ent_coef_loss   | -1.33    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 153095   |\n","---------------------------------\n","Eval num_timesteps=3676800, episode_reward=-91.53 +/- 5.81\n","Episode length: 170.40 +/- 64.18\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 170      |\n","|    mean_reward     | -91.5    |\n","| time/              |          |\n","|    total_timesteps | 3676800  |\n","| train/             |          |\n","|    actor_loss      | 7.46     |\n","|    critic_loss     | 2.41     |\n","|    ent_coef        | 0.00388  |\n","|    ent_coef_loss   | 0.892    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 153195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6110     |\n","|    fps             | 316      |\n","|    time_elapsed    | 11610    |\n","|    total_timesteps | 3677832  |\n","| train/             |          |\n","|    actor_loss      | 6.8      |\n","|    critic_loss     | 19.6     |\n","|    ent_coef        | 0.00386  |\n","|    ent_coef_loss   | 1.58     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 153238   |\n","---------------------------------\n","Eval num_timesteps=3679200, episode_reward=-44.66 +/- 48.38\n","Episode length: 399.20 +/- 82.30\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 399      |\n","|    mean_reward     | -44.7    |\n","| time/              |          |\n","|    total_timesteps | 3679200  |\n","| train/             |          |\n","|    actor_loss      | 6.86     |\n","|    critic_loss     | 1.71     |\n","|    ent_coef        | 0.00398  |\n","|    ent_coef_loss   | 1.89     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 153295   |\n","---------------------------------\n","Eval num_timesteps=3681600, episode_reward=-98.46 +/- 4.50\n","Episode length: 270.20 +/- 60.26\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 270      |\n","|    mean_reward     | -98.5    |\n","| time/              |          |\n","|    total_timesteps | 3681600  |\n","| train/             |          |\n","|    actor_loss      | 6.23     |\n","|    critic_loss     | 1.94     |\n","|    ent_coef        | 0.00381  |\n","|    ent_coef_loss   | -2.14    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 153395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6120     |\n","|    fps             | 316      |\n","|    time_elapsed    | 11625    |\n","|    total_timesteps | 3682392  |\n","| train/             |          |\n","|    actor_loss      | 6.65     |\n","|    critic_loss     | 10.1     |\n","|    ent_coef        | 0.00378  |\n","|    ent_coef_loss   | 3        |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 153428   |\n","---------------------------------\n","Eval num_timesteps=3684000, episode_reward=-34.07 +/- 39.96\n","Episode length: 478.40 +/- 26.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 478      |\n","|    mean_reward     | -34.1    |\n","| time/              |          |\n","|    total_timesteps | 3684000  |\n","| train/             |          |\n","|    actor_loss      | 7.02     |\n","|    critic_loss     | 1.54     |\n","|    ent_coef        | 0.0038   |\n","|    ent_coef_loss   | -1.78    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 153495   |\n","---------------------------------\n","Eval num_timesteps=3686400, episode_reward=-65.78 +/- 2.52\n","Episode length: 391.60 +/- 24.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 392      |\n","|    mean_reward     | -65.8    |\n","| time/              |          |\n","|    total_timesteps | 3686400  |\n","| train/             |          |\n","|    actor_loss      | 7.32     |\n","|    critic_loss     | 1.59     |\n","|    ent_coef        | 0.00375  |\n","|    ent_coef_loss   | -1.61    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 153595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6130     |\n","|    fps             | 316      |\n","|    time_elapsed    | 11640    |\n","|    total_timesteps | 3688512  |\n","| train/             |          |\n","|    actor_loss      | 6.99     |\n","|    critic_loss     | 3.15     |\n","|    ent_coef        | 0.00394  |\n","|    ent_coef_loss   | 6.35     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 153683   |\n","---------------------------------\n","Eval num_timesteps=3688800, episode_reward=-75.85 +/- 1.16\n","Episode length: 303.80 +/- 57.32\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 304      |\n","|    mean_reward     | -75.9    |\n","| time/              |          |\n","|    total_timesteps | 3688800  |\n","| train/             |          |\n","|    actor_loss      | 6.5      |\n","|    critic_loss     | 1.45     |\n","|    ent_coef        | 0.00398  |\n","|    ent_coef_loss   | -0.367   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 153695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6140     |\n","|    fps             | 316      |\n","|    time_elapsed    | 11647    |\n","|    total_timesteps | 3688968  |\n","| train/             |          |\n","|    actor_loss      | 6.93     |\n","|    critic_loss     | 6.84     |\n","|    ent_coef        | 0.00399  |\n","|    ent_coef_loss   | -3.26    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 153702   |\n","---------------------------------\n","Eval num_timesteps=3691200, episode_reward=-107.96 +/- 4.74\n","Episode length: 278.00 +/- 44.09\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 278      |\n","|    mean_reward     | -108     |\n","| time/              |          |\n","|    total_timesteps | 3691200  |\n","| train/             |          |\n","|    actor_loss      | 6.9      |\n","|    critic_loss     | 1.82     |\n","|    ent_coef        | 0.00387  |\n","|    ent_coef_loss   | -2.33    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 153795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6150     |\n","|    fps             | 316      |\n","|    time_elapsed    | 11652    |\n","|    total_timesteps | 3692208  |\n","| train/             |          |\n","|    actor_loss      | 6.66     |\n","|    critic_loss     | 1.05     |\n","|    ent_coef        | 0.00378  |\n","|    ent_coef_loss   | -2.49    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 153837   |\n","---------------------------------\n","Eval num_timesteps=3693600, episode_reward=13.56 +/- 11.19\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 13.6     |\n","| time/              |          |\n","|    total_timesteps | 3693600  |\n","| train/             |          |\n","|    actor_loss      | 6.66     |\n","|    critic_loss     | 1.81     |\n","|    ent_coef        | 0.00379  |\n","|    ent_coef_loss   | 5.84     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 153895   |\n","---------------------------------\n","Eval num_timesteps=3696000, episode_reward=-41.02 +/- 65.61\n","Episode length: 318.20 +/- 148.44\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 318      |\n","|    mean_reward     | -41      |\n","| time/              |          |\n","|    total_timesteps | 3696000  |\n","| train/             |          |\n","|    actor_loss      | 7.23     |\n","|    critic_loss     | 25.1     |\n","|    ent_coef        | 0.00377  |\n","|    ent_coef_loss   | 0.292    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 153995   |\n","---------------------------------\n","Eval num_timesteps=3698400, episode_reward=-29.53 +/- 43.05\n","Episode length: 416.80 +/- 101.90\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 417      |\n","|    mean_reward     | -29.5    |\n","| time/              |          |\n","|    total_timesteps | 3698400  |\n","| train/             |          |\n","|    actor_loss      | 7.64     |\n","|    critic_loss     | 2.18     |\n","|    ent_coef        | 0.0037   |\n","|    ent_coef_loss   | 3.27     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 154095   |\n","---------------------------------\n","Eval num_timesteps=3700800, episode_reward=-6.88 +/- 56.65\n","Episode length: 457.60 +/- 51.93\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 458      |\n","|    mean_reward     | -6.88    |\n","| time/              |          |\n","|    total_timesteps | 3700800  |\n","| train/             |          |\n","|    actor_loss      | 6.86     |\n","|    critic_loss     | 15.8     |\n","|    ent_coef        | 0.00377  |\n","|    ent_coef_loss   | -1.89    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 154195   |\n","---------------------------------\n","Eval num_timesteps=3703200, episode_reward=-114.70 +/- 2.64\n","Episode length: 211.60 +/- 34.78\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 212      |\n","|    mean_reward     | -115     |\n","| time/              |          |\n","|    total_timesteps | 3703200  |\n","| train/             |          |\n","|    actor_loss      | 6.41     |\n","|    critic_loss     | 1.19     |\n","|    ent_coef        | 0.00384  |\n","|    ent_coef_loss   | 0.131    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 154295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6160     |\n","|    fps             | 316      |\n","|    time_elapsed    | 11691    |\n","|    total_timesteps | 3704496  |\n","| train/             |          |\n","|    actor_loss      | 5.8      |\n","|    critic_loss     | 0.661    |\n","|    ent_coef        | 0.00378  |\n","|    ent_coef_loss   | -4.3     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 154349   |\n","---------------------------------\n","Eval num_timesteps=3705600, episode_reward=-102.14 +/- 1.44\n","Episode length: 371.80 +/- 126.39\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 372      |\n","|    mean_reward     | -102     |\n","| time/              |          |\n","|    total_timesteps | 3705600  |\n","| train/             |          |\n","|    actor_loss      | 6.67     |\n","|    critic_loss     | 1.48     |\n","|    ent_coef        | 0.00378  |\n","|    ent_coef_loss   | 1.13     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 154395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6170     |\n","|    fps             | 316      |\n","|    time_elapsed    | 11697    |\n","|    total_timesteps | 3706200  |\n","| train/             |          |\n","|    actor_loss      | 6.56     |\n","|    critic_loss     | 1.37     |\n","|    ent_coef        | 0.0038   |\n","|    ent_coef_loss   | 0.92     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 154420   |\n","---------------------------------\n","Eval num_timesteps=3708000, episode_reward=-98.68 +/- 0.17\n","Episode length: 343.20 +/- 96.51\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 343      |\n","|    mean_reward     | -98.7    |\n","| time/              |          |\n","|    total_timesteps | 3708000  |\n","| train/             |          |\n","|    actor_loss      | 6.43     |\n","|    critic_loss     | 15.3     |\n","|    ent_coef        | 0.00395  |\n","|    ent_coef_loss   | -0.573   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 154495   |\n","---------------------------------\n","Eval num_timesteps=3710400, episode_reward=-38.78 +/- 47.18\n","Episode length: 408.80 +/- 74.46\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 409      |\n","|    mean_reward     | -38.8    |\n","| time/              |          |\n","|    total_timesteps | 3710400  |\n","| train/             |          |\n","|    actor_loss      | 6.68     |\n","|    critic_loss     | 6.37     |\n","|    ent_coef        | 0.00406  |\n","|    ent_coef_loss   | 0.829    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 154595   |\n","---------------------------------\n","Eval num_timesteps=3712800, episode_reward=-38.97 +/- 50.55\n","Episode length: 411.60 +/- 108.27\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 412      |\n","|    mean_reward     | -39      |\n","| time/              |          |\n","|    total_timesteps | 3712800  |\n","| train/             |          |\n","|    actor_loss      | 6.28     |\n","|    critic_loss     | 6.96     |\n","|    ent_coef        | 0.00427  |\n","|    ent_coef_loss   | 1.81     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 154695   |\n","---------------------------------\n","Eval num_timesteps=3715200, episode_reward=-59.27 +/- 56.31\n","Episode length: 395.60 +/- 85.24\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 396      |\n","|    mean_reward     | -59.3    |\n","| time/              |          |\n","|    total_timesteps | 3715200  |\n","| train/             |          |\n","|    actor_loss      | 7.16     |\n","|    critic_loss     | 4.13     |\n","|    ent_coef        | 0.00431  |\n","|    ent_coef_loss   | 1.12     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 154795   |\n","---------------------------------\n","Eval num_timesteps=3717600, episode_reward=40.40 +/- 0.58\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 40.4     |\n","| time/              |          |\n","|    total_timesteps | 3717600  |\n","| train/             |          |\n","|    actor_loss      | 6.58     |\n","|    critic_loss     | 8.96     |\n","|    ent_coef        | 0.00447  |\n","|    ent_coef_loss   | 2.65     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 154895   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6180     |\n","|    fps             | 316      |\n","|    time_elapsed    | 11738    |\n","|    total_timesteps | 3719736  |\n","| train/             |          |\n","|    actor_loss      | 6.41     |\n","|    critic_loss     | 5.17     |\n","|    ent_coef        | 0.00458  |\n","|    ent_coef_loss   | -0.388   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 154984   |\n","---------------------------------\n","Eval num_timesteps=3720000, episode_reward=34.98 +/- 4.60\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 35       |\n","| time/              |          |\n","|    total_timesteps | 3720000  |\n","| train/             |          |\n","|    actor_loss      | 6.29     |\n","|    critic_loss     | 0.722    |\n","|    ent_coef        | 0.00457  |\n","|    ent_coef_loss   | -2.91    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 154995   |\n","---------------------------------\n","Eval num_timesteps=3722400, episode_reward=-104.82 +/- 2.93\n","Episode length: 229.80 +/- 54.87\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 230      |\n","|    mean_reward     | -105     |\n","| time/              |          |\n","|    total_timesteps | 3722400  |\n","| train/             |          |\n","|    actor_loss      | 6.14     |\n","|    critic_loss     | 1.8      |\n","|    ent_coef        | 0.00444  |\n","|    ent_coef_loss   | -1.13    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 155095   |\n","---------------------------------\n","Eval num_timesteps=3724800, episode_reward=-51.41 +/- 49.23\n","Episode length: 393.80 +/- 86.71\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 394      |\n","|    mean_reward     | -51.4    |\n","| time/              |          |\n","|    total_timesteps | 3724800  |\n","| train/             |          |\n","|    actor_loss      | 6.49     |\n","|    critic_loss     | 8.24     |\n","|    ent_coef        | 0.00436  |\n","|    ent_coef_loss   | -1.19    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 155195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6190     |\n","|    fps             | 316      |\n","|    time_elapsed    | 11764    |\n","|    total_timesteps | 3726648  |\n","| train/             |          |\n","|    actor_loss      | 7.35     |\n","|    critic_loss     | 3.68     |\n","|    ent_coef        | 0.00429  |\n","|    ent_coef_loss   | 1.41     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 155272   |\n","---------------------------------\n","Eval num_timesteps=3727200, episode_reward=32.14 +/- 5.02\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 32.1     |\n","| time/              |          |\n","|    total_timesteps | 3727200  |\n","| train/             |          |\n","|    actor_loss      | 7.41     |\n","|    critic_loss     | 6.12     |\n","|    ent_coef        | 0.00427  |\n","|    ent_coef_loss   | -1.23    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 155295   |\n","---------------------------------\n","Eval num_timesteps=3729600, episode_reward=-98.94 +/- 8.15\n","Episode length: 215.60 +/- 49.48\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 216      |\n","|    mean_reward     | -98.9    |\n","| time/              |          |\n","|    total_timesteps | 3729600  |\n","| train/             |          |\n","|    actor_loss      | 6.37     |\n","|    critic_loss     | 1.16     |\n","|    ent_coef        | 0.00415  |\n","|    ent_coef_loss   | -2.14    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 155395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6200     |\n","|    fps             | 316      |\n","|    time_elapsed    | 11780    |\n","|    total_timesteps | 3731856  |\n","| train/             |          |\n","|    actor_loss      | 8.3      |\n","|    critic_loss     | 16.6     |\n","|    ent_coef        | 0.00428  |\n","|    ent_coef_loss   | 2.8      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 155489   |\n","---------------------------------\n","Eval num_timesteps=3732000, episode_reward=-43.92 +/- 53.40\n","Episode length: 494.00 +/- 4.90\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 494      |\n","|    mean_reward     | -43.9    |\n","| time/              |          |\n","|    total_timesteps | 3732000  |\n","| train/             |          |\n","|    actor_loss      | 6.57     |\n","|    critic_loss     | 22       |\n","|    ent_coef        | 0.00429  |\n","|    ent_coef_loss   | 0.495    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 155495   |\n","---------------------------------\n","Eval num_timesteps=3734400, episode_reward=0.56 +/- 61.36\n","Episode length: 439.20 +/- 74.46\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 439      |\n","|    mean_reward     | 0.562    |\n","| time/              |          |\n","|    total_timesteps | 3734400  |\n","| train/             |          |\n","|    actor_loss      | 7.09     |\n","|    critic_loss     | 1.77     |\n","|    ent_coef        | 0.00408  |\n","|    ent_coef_loss   | -0.522   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 155595   |\n","---------------------------------\n","Eval num_timesteps=3736800, episode_reward=40.69 +/- 4.78\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 40.7     |\n","| time/              |          |\n","|    total_timesteps | 3736800  |\n","| train/             |          |\n","|    actor_loss      | 6.55     |\n","|    critic_loss     | 16.2     |\n","|    ent_coef        | 0.00402  |\n","|    ent_coef_loss   | -1.09    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 155695   |\n","---------------------------------\n","Eval num_timesteps=3739200, episode_reward=-37.66 +/- 54.17\n","Episode length: 410.00 +/- 73.48\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 410      |\n","|    mean_reward     | -37.7    |\n","| time/              |          |\n","|    total_timesteps | 3739200  |\n","| train/             |          |\n","|    actor_loss      | 6.64     |\n","|    critic_loss     | 6.11     |\n","|    ent_coef        | 0.00397  |\n","|    ent_coef_loss   | -2.89    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 155795   |\n","---------------------------------\n","Eval num_timesteps=3741600, episode_reward=-34.29 +/- 58.54\n","Episode length: 354.80 +/- 118.56\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 355      |\n","|    mean_reward     | -34.3    |\n","| time/              |          |\n","|    total_timesteps | 3741600  |\n","| train/             |          |\n","|    actor_loss      | 6.6      |\n","|    critic_loss     | 2.02     |\n","|    ent_coef        | 0.00398  |\n","|    ent_coef_loss   | 1.9      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 155895   |\n","---------------------------------\n","Eval num_timesteps=3744000, episode_reward=-4.47 +/- 52.60\n","Episode length: 491.60 +/- 10.29\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 492      |\n","|    mean_reward     | -4.47    |\n","| time/              |          |\n","|    total_timesteps | 3744000  |\n","| train/             |          |\n","|    actor_loss      | 7.57     |\n","|    critic_loss     | 1.56     |\n","|    ent_coef        | 0.00395  |\n","|    ent_coef_loss   | 0.0316   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 155995   |\n","---------------------------------\n","Eval num_timesteps=3746400, episode_reward=23.20 +/- 2.04\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 23.2     |\n","| time/              |          |\n","|    total_timesteps | 3746400  |\n","| train/             |          |\n","|    actor_loss      | 6.66     |\n","|    critic_loss     | 1.85     |\n","|    ent_coef        | 0.00388  |\n","|    ent_coef_loss   | 0.516    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 156095   |\n","---------------------------------\n","Eval num_timesteps=3748800, episode_reward=-101.13 +/- 0.95\n","Episode length: 263.20 +/- 0.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 263      |\n","|    mean_reward     | -101     |\n","| time/              |          |\n","|    total_timesteps | 3748800  |\n","| train/             |          |\n","|    actor_loss      | 6.25     |\n","|    critic_loss     | 9.32     |\n","|    ent_coef        | 0.00395  |\n","|    ent_coef_loss   | 2.4      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 156195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6210     |\n","|    fps             | 316      |\n","|    time_elapsed    | 11842    |\n","|    total_timesteps | 3750168  |\n","| train/             |          |\n","|    actor_loss      | 6.68     |\n","|    critic_loss     | 1.32     |\n","|    ent_coef        | 0.00405  |\n","|    ent_coef_loss   | 2.84     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 156252   |\n","---------------------------------\n","Eval num_timesteps=3751200, episode_reward=14.35 +/- 6.65\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 14.3     |\n","| time/              |          |\n","|    total_timesteps | 3751200  |\n","| train/             |          |\n","|    actor_loss      | 6.41     |\n","|    critic_loss     | 0.913    |\n","|    ent_coef        | 0.00415  |\n","|    ent_coef_loss   | 4.1      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 156295   |\n","---------------------------------\n","Eval num_timesteps=3753600, episode_reward=-27.18 +/- 45.43\n","Episode length: 434.40 +/- 80.34\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 434      |\n","|    mean_reward     | -27.2    |\n","| time/              |          |\n","|    total_timesteps | 3753600  |\n","| train/             |          |\n","|    actor_loss      | 7.2      |\n","|    critic_loss     | 1.76     |\n","|    ent_coef        | 0.00425  |\n","|    ent_coef_loss   | -1.45    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 156395   |\n","---------------------------------\n","Eval num_timesteps=3756000, episode_reward=13.05 +/- 1.31\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 13       |\n","| time/              |          |\n","|    total_timesteps | 3756000  |\n","| train/             |          |\n","|    actor_loss      | 6.86     |\n","|    critic_loss     | 3.28     |\n","|    ent_coef        | 0.00441  |\n","|    ent_coef_loss   | 3.82     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 156495   |\n","---------------------------------\n","Eval num_timesteps=3758400, episode_reward=-1.57 +/- 3.81\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.57    |\n","| time/              |          |\n","|    total_timesteps | 3758400  |\n","| train/             |          |\n","|    actor_loss      | 7.14     |\n","|    critic_loss     | 1.66     |\n","|    ent_coef        | 0.0046   |\n","|    ent_coef_loss   | 0.767    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 156595   |\n","---------------------------------\n","Eval num_timesteps=3760800, episode_reward=-82.39 +/- 10.52\n","Episode length: 340.40 +/- 175.87\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 340      |\n","|    mean_reward     | -82.4    |\n","| time/              |          |\n","|    total_timesteps | 3760800  |\n","| train/             |          |\n","|    actor_loss      | 7.3      |\n","|    critic_loss     | 1.51     |\n","|    ent_coef        | 0.00473  |\n","|    ent_coef_loss   | 2.11     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 156695   |\n","---------------------------------\n","Eval num_timesteps=3763200, episode_reward=17.95 +/- 1.98\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 17.9     |\n","| time/              |          |\n","|    total_timesteps | 3763200  |\n","| train/             |          |\n","|    actor_loss      | 7.43     |\n","|    critic_loss     | 1.59     |\n","|    ent_coef        | 0.00478  |\n","|    ent_coef_loss   | -0.273   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 156795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6220     |\n","|    fps             | 316      |\n","|    time_elapsed    | 11893    |\n","|    total_timesteps | 3765216  |\n","| train/             |          |\n","|    actor_loss      | 6.48     |\n","|    critic_loss     | 2.48     |\n","|    ent_coef        | 0.00471  |\n","|    ent_coef_loss   | -5.52    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 156879   |\n","---------------------------------\n","Eval num_timesteps=3765600, episode_reward=-10.68 +/- 55.02\n","Episode length: 485.60 +/- 17.64\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 486      |\n","|    mean_reward     | -10.7    |\n","| time/              |          |\n","|    total_timesteps | 3765600  |\n","| train/             |          |\n","|    actor_loss      | 7.06     |\n","|    critic_loss     | 1.62     |\n","|    ent_coef        | 0.00465  |\n","|    ent_coef_loss   | -4.58    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 156895   |\n","---------------------------------\n","Eval num_timesteps=3768000, episode_reward=39.23 +/- 3.55\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 39.2     |\n","| time/              |          |\n","|    total_timesteps | 3768000  |\n","| train/             |          |\n","|    actor_loss      | 6.09     |\n","|    critic_loss     | 1.37     |\n","|    ent_coef        | 0.00438  |\n","|    ent_coef_loss   | -3.24    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 156995   |\n","---------------------------------\n","Eval num_timesteps=3770400, episode_reward=-40.94 +/- 53.90\n","Episode length: 377.60 +/- 149.91\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 378      |\n","|    mean_reward     | -40.9    |\n","| time/              |          |\n","|    total_timesteps | 3770400  |\n","| train/             |          |\n","|    actor_loss      | 6.92     |\n","|    critic_loss     | 1.42     |\n","|    ent_coef        | 0.0043   |\n","|    ent_coef_loss   | -0.814   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 157095   |\n","---------------------------------\n","Eval num_timesteps=3772800, episode_reward=8.03 +/- 5.08\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 8.03     |\n","| time/              |          |\n","|    total_timesteps | 3772800  |\n","| train/             |          |\n","|    actor_loss      | 6.34     |\n","|    critic_loss     | 4.38     |\n","|    ent_coef        | 0.0044   |\n","|    ent_coef_loss   | -0.113   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 157195   |\n","---------------------------------\n","Eval num_timesteps=3775200, episode_reward=-12.44 +/- 54.83\n","Episode length: 440.60 +/- 48.50\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 441      |\n","|    mean_reward     | -12.4    |\n","| time/              |          |\n","|    total_timesteps | 3775200  |\n","| train/             |          |\n","|    actor_loss      | 6.97     |\n","|    critic_loss     | 1.69     |\n","|    ent_coef        | 0.00435  |\n","|    ent_coef_loss   | -2.72    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 157295   |\n","---------------------------------\n","Eval num_timesteps=3777600, episode_reward=1.58 +/- 51.91\n","Episode length: 493.20 +/- 8.33\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 493      |\n","|    mean_reward     | 1.58     |\n","| time/              |          |\n","|    total_timesteps | 3777600  |\n","| train/             |          |\n","|    actor_loss      | 6.27     |\n","|    critic_loss     | 6.34     |\n","|    ent_coef        | 0.0045   |\n","|    ent_coef_loss   | 0.488    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 157395   |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    episodes        | 6230    |\n","|    fps             | 316     |\n","|    time_elapsed    | 11941   |\n","|    total_timesteps | 3777600 |\n","--------------------------------\n","Eval num_timesteps=3780000, episode_reward=-55.13 +/- 65.38\n","Episode length: 277.40 +/- 181.75\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 277      |\n","|    mean_reward     | -55.1    |\n","| time/              |          |\n","|    total_timesteps | 3780000  |\n","| train/             |          |\n","|    actor_loss      | 7.29     |\n","|    critic_loss     | 1.39     |\n","|    ent_coef        | 0.00449  |\n","|    ent_coef_loss   | 0.0748   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 157495   |\n","---------------------------------\n","Eval num_timesteps=3782400, episode_reward=-51.69 +/- 72.62\n","Episode length: 321.80 +/- 145.50\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 322      |\n","|    mean_reward     | -51.7    |\n","| time/              |          |\n","|    total_timesteps | 3782400  |\n","| train/             |          |\n","|    actor_loss      | 7.76     |\n","|    critic_loss     | 15.1     |\n","|    ent_coef        | 0.0044   |\n","|    ent_coef_loss   | 2.03     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 157595   |\n","---------------------------------\n","Eval num_timesteps=3784800, episode_reward=-121.22 +/- 5.13\n","Episode length: 291.80 +/- 106.31\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 292      |\n","|    mean_reward     | -121     |\n","| time/              |          |\n","|    total_timesteps | 3784800  |\n","| train/             |          |\n","|    actor_loss      | 6.04     |\n","|    critic_loss     | 3.77     |\n","|    ent_coef        | 0.00429  |\n","|    ent_coef_loss   | -0.931   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 157695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6240     |\n","|    fps             | 316      |\n","|    time_elapsed    | 11964    |\n","|    total_timesteps | 3785232  |\n","| train/             |          |\n","|    actor_loss      | 6.28     |\n","|    critic_loss     | 4.26     |\n","|    ent_coef        | 0.0043   |\n","|    ent_coef_loss   | 0.179    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 157713   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6250     |\n","|    fps             | 316      |\n","|    time_elapsed    | 11966    |\n","|    total_timesteps | 3786672  |\n","| train/             |          |\n","|    actor_loss      | 6.45     |\n","|    critic_loss     | 1.62     |\n","|    ent_coef        | 0.00434  |\n","|    ent_coef_loss   | -2.27    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 157773   |\n","---------------------------------\n","Eval num_timesteps=3787200, episode_reward=-14.82 +/- 13.02\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -14.8    |\n","| time/              |          |\n","|    total_timesteps | 3787200  |\n","| train/             |          |\n","|    actor_loss      | 7.81     |\n","|    critic_loss     | 6.19     |\n","|    ent_coef        | 0.00434  |\n","|    ent_coef_loss   | 0.0902   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 157795   |\n","---------------------------------\n","Eval num_timesteps=3789600, episode_reward=35.15 +/- 1.04\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 35.1     |\n","| time/              |          |\n","|    total_timesteps | 3789600  |\n","| train/             |          |\n","|    actor_loss      | 6.34     |\n","|    critic_loss     | 2.62     |\n","|    ent_coef        | 0.00426  |\n","|    ent_coef_loss   | 3.65     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 157895   |\n","---------------------------------\n","Eval num_timesteps=3792000, episode_reward=25.41 +/- 0.92\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 25.4     |\n","| time/              |          |\n","|    total_timesteps | 3792000  |\n","| train/             |          |\n","|    actor_loss      | 6.88     |\n","|    critic_loss     | 1.09     |\n","|    ent_coef        | 0.00441  |\n","|    ent_coef_loss   | 0.813    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 157995   |\n","---------------------------------\n","Eval num_timesteps=3794400, episode_reward=-48.89 +/- 63.70\n","Episode length: 334.40 +/- 135.21\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 334      |\n","|    mean_reward     | -48.9    |\n","| time/              |          |\n","|    total_timesteps | 3794400  |\n","| train/             |          |\n","|    actor_loss      | 7.07     |\n","|    critic_loss     | 14.4     |\n","|    ent_coef        | 0.00459  |\n","|    ent_coef_loss   | 0.608    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 158095   |\n","---------------------------------\n","Eval num_timesteps=3796800, episode_reward=-40.53 +/- 50.98\n","Episode length: 468.00 +/- 39.19\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 468      |\n","|    mean_reward     | -40.5    |\n","| time/              |          |\n","|    total_timesteps | 3796800  |\n","| train/             |          |\n","|    actor_loss      | 6.98     |\n","|    critic_loss     | 1.8      |\n","|    ent_coef        | 0.00466  |\n","|    ent_coef_loss   | 0.398    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 158195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6260     |\n","|    fps             | 316      |\n","|    time_elapsed    | 12006    |\n","|    total_timesteps | 3797568  |\n","| train/             |          |\n","|    actor_loss      | 6.58     |\n","|    critic_loss     | 0.913    |\n","|    ent_coef        | 0.00465  |\n","|    ent_coef_loss   | -1.53    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 158227   |\n","---------------------------------\n","Eval num_timesteps=3799200, episode_reward=14.61 +/- 0.11\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 14.6     |\n","| time/              |          |\n","|    total_timesteps | 3799200  |\n","| train/             |          |\n","|    actor_loss      | 6.53     |\n","|    critic_loss     | 12.9     |\n","|    ent_coef        | 0.00469  |\n","|    ent_coef_loss   | -0.975   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 158295   |\n","---------------------------------\n","Eval num_timesteps=3801600, episode_reward=-49.47 +/- 46.31\n","Episode length: 441.60 +/- 71.53\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 442      |\n","|    mean_reward     | -49.5    |\n","| time/              |          |\n","|    total_timesteps | 3801600  |\n","| train/             |          |\n","|    actor_loss      | 7.06     |\n","|    critic_loss     | 3.9      |\n","|    ent_coef        | 0.00489  |\n","|    ent_coef_loss   | 1.8      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 158395   |\n","---------------------------------\n","Eval num_timesteps=3804000, episode_reward=23.63 +/- 5.46\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 23.6     |\n","| time/              |          |\n","|    total_timesteps | 3804000  |\n","| train/             |          |\n","|    actor_loss      | 7.04     |\n","|    critic_loss     | 1.12     |\n","|    ent_coef        | 0.00494  |\n","|    ent_coef_loss   | 0.62     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 158495   |\n","---------------------------------\n","Eval num_timesteps=3806400, episode_reward=-69.51 +/- 3.08\n","Episode length: 353.20 +/- 25.96\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 353      |\n","|    mean_reward     | -69.5    |\n","| time/              |          |\n","|    total_timesteps | 3806400  |\n","| train/             |          |\n","|    actor_loss      | 6.4      |\n","|    critic_loss     | 0.958    |\n","|    ent_coef        | 0.00498  |\n","|    ent_coef_loss   | 3.14     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 158595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6270     |\n","|    fps             | 316      |\n","|    time_elapsed    | 12038    |\n","|    total_timesteps | 3806712  |\n","| train/             |          |\n","|    actor_loss      | 6.82     |\n","|    critic_loss     | 13.6     |\n","|    ent_coef        | 0.00501  |\n","|    ent_coef_loss   | 2.65     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 158608   |\n","---------------------------------\n","Eval num_timesteps=3808800, episode_reward=-48.26 +/- 64.74\n","Episode length: 324.20 +/- 143.54\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 324      |\n","|    mean_reward     | -48.3    |\n","| time/              |          |\n","|    total_timesteps | 3808800  |\n","| train/             |          |\n","|    actor_loss      | 6.68     |\n","|    critic_loss     | 1.39     |\n","|    ent_coef        | 0.00517  |\n","|    ent_coef_loss   | -0.794   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 158695   |\n","---------------------------------\n","Eval num_timesteps=3811200, episode_reward=-47.32 +/- 50.63\n","Episode length: 375.80 +/- 101.41\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 376      |\n","|    mean_reward     | -47.3    |\n","| time/              |          |\n","|    total_timesteps | 3811200  |\n","| train/             |          |\n","|    actor_loss      | 7.21     |\n","|    critic_loss     | 2.24     |\n","|    ent_coef        | 0.00512  |\n","|    ent_coef_loss   | -0.271   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 158795   |\n","---------------------------------\n","Eval num_timesteps=3813600, episode_reward=-76.00 +/- 8.11\n","Episode length: 381.60 +/- 12.74\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 382      |\n","|    mean_reward     | -76      |\n","| time/              |          |\n","|    total_timesteps | 3813600  |\n","| train/             |          |\n","|    actor_loss      | 6.59     |\n","|    critic_loss     | 19.9     |\n","|    ent_coef        | 0.0049   |\n","|    ent_coef_loss   | -4.18    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 158895   |\n","---------------------------------\n","Eval num_timesteps=3816000, episode_reward=33.97 +/- 2.20\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 34       |\n","| time/              |          |\n","|    total_timesteps | 3816000  |\n","| train/             |          |\n","|    actor_loss      | 7.23     |\n","|    critic_loss     | 1.13     |\n","|    ent_coef        | 0.00476  |\n","|    ent_coef_loss   | 0.146    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 158995   |\n","---------------------------------\n","Eval num_timesteps=3818400, episode_reward=-62.29 +/- 53.08\n","Episode length: 326.00 +/- 142.07\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 326      |\n","|    mean_reward     | -62.3    |\n","| time/              |          |\n","|    total_timesteps | 3818400  |\n","| train/             |          |\n","|    actor_loss      | 5.85     |\n","|    critic_loss     | 1.52     |\n","|    ent_coef        | 0.00484  |\n","|    ent_coef_loss   | -3.79    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 159095   |\n","---------------------------------\n","Eval num_timesteps=3820800, episode_reward=12.13 +/- 7.76\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 12.1     |\n","| time/              |          |\n","|    total_timesteps | 3820800  |\n","| train/             |          |\n","|    actor_loss      | 6.39     |\n","|    critic_loss     | 11.7     |\n","|    ent_coef        | 0.0047   |\n","|    ent_coef_loss   | -1.54    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 159195   |\n","---------------------------------\n","Eval num_timesteps=3823200, episode_reward=-13.82 +/- 52.26\n","Episode length: 454.80 +/- 55.36\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 455      |\n","|    mean_reward     | -13.8    |\n","| time/              |          |\n","|    total_timesteps | 3823200  |\n","| train/             |          |\n","|    actor_loss      | 7.18     |\n","|    critic_loss     | 4.05     |\n","|    ent_coef        | 0.00452  |\n","|    ent_coef_loss   | -1.59    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 159295   |\n","---------------------------------\n","Eval num_timesteps=3825600, episode_reward=-8.40 +/- 4.89\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -8.4     |\n","| time/              |          |\n","|    total_timesteps | 3825600  |\n","| train/             |          |\n","|    actor_loss      | 6.63     |\n","|    critic_loss     | 8.48     |\n","|    ent_coef        | 0.0046   |\n","|    ent_coef_loss   | -0.343   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 159395   |\n","---------------------------------\n","Eval num_timesteps=3828000, episode_reward=45.11 +/- 2.42\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 45.1     |\n","| time/              |          |\n","|    total_timesteps | 3828000  |\n","| train/             |          |\n","|    actor_loss      | 7.15     |\n","|    critic_loss     | 12.4     |\n","|    ent_coef        | 0.00467  |\n","|    ent_coef_loss   | 0.679    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 159495   |\n","---------------------------------\n","Eval num_timesteps=3830400, episode_reward=-0.40 +/- 66.59\n","Episode length: 407.20 +/- 113.66\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 407      |\n","|    mean_reward     | -0.402   |\n","| time/              |          |\n","|    total_timesteps | 3830400  |\n","| train/             |          |\n","|    actor_loss      | 6.18     |\n","|    critic_loss     | 1.3      |\n","|    ent_coef        | 0.00491  |\n","|    ent_coef_loss   | -2.39    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 159595   |\n","---------------------------------\n","Eval num_timesteps=3832800, episode_reward=17.22 +/- 0.03\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 17.2     |\n","| time/              |          |\n","|    total_timesteps | 3832800  |\n","| train/             |          |\n","|    actor_loss      | 7.06     |\n","|    critic_loss     | 0.835    |\n","|    ent_coef        | 0.00474  |\n","|    ent_coef_loss   | -1.06    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 159695   |\n","---------------------------------\n","Eval num_timesteps=3835200, episode_reward=4.61 +/- 1.66\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.61     |\n","| time/              |          |\n","|    total_timesteps | 3835200  |\n","| train/             |          |\n","|    actor_loss      | 6.51     |\n","|    critic_loss     | 10.1     |\n","|    ent_coef        | 0.0046   |\n","|    ent_coef_loss   | -2.04    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 159795   |\n","---------------------------------\n","Eval num_timesteps=3837600, episode_reward=25.55 +/- 3.86\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 25.5     |\n","| time/              |          |\n","|    total_timesteps | 3837600  |\n","| train/             |          |\n","|    actor_loss      | 6.9      |\n","|    critic_loss     | 2.82     |\n","|    ent_coef        | 0.00449  |\n","|    ent_coef_loss   | -4.09    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 159895   |\n","---------------------------------\n","Eval num_timesteps=3840000, episode_reward=-34.27 +/- 57.34\n","Episode length: 374.60 +/- 102.39\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 375      |\n","|    mean_reward     | -34.3    |\n","| time/              |          |\n","|    total_timesteps | 3840000  |\n","| train/             |          |\n","|    actor_loss      | 7.35     |\n","|    critic_loss     | 3.61     |\n","|    ent_coef        | 0.00441  |\n","|    ent_coef_loss   | 5.8      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 159995   |\n","---------------------------------\n","Eval num_timesteps=3842400, episode_reward=4.43 +/- 5.30\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.43     |\n","| time/              |          |\n","|    total_timesteps | 3842400  |\n","| train/             |          |\n","|    actor_loss      | 6.41     |\n","|    critic_loss     | 1.73     |\n","|    ent_coef        | 0.00442  |\n","|    ent_coef_loss   | -2.49    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 160095   |\n","---------------------------------\n","Eval num_timesteps=3844800, episode_reward=-110.39 +/- 4.32\n","Episode length: 441.80 +/- 8.33\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 442      |\n","|    mean_reward     | -110     |\n","| time/              |          |\n","|    total_timesteps | 3844800  |\n","| train/             |          |\n","|    actor_loss      | 6.12     |\n","|    critic_loss     | 3.72     |\n","|    ent_coef        | 0.00433  |\n","|    ent_coef_loss   | -0.441   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 160195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6280     |\n","|    fps             | 316      |\n","|    time_elapsed    | 12169    |\n","|    total_timesteps | 3845952  |\n","| train/             |          |\n","|    actor_loss      | 7.42     |\n","|    critic_loss     | 1.43     |\n","|    ent_coef        | 0.00434  |\n","|    ent_coef_loss   | 1.19     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 160243   |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    episodes        | 6290    |\n","|    fps             | 316     |\n","|    time_elapsed    | 12169   |\n","|    total_timesteps | 3845952 |\n","--------------------------------\n","Eval num_timesteps=3847200, episode_reward=10.97 +/- 3.22\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 11       |\n","| time/              |          |\n","|    total_timesteps | 3847200  |\n","| train/             |          |\n","|    actor_loss      | 6.28     |\n","|    critic_loss     | 1.25     |\n","|    ent_coef        | 0.00441  |\n","|    ent_coef_loss   | 3.37     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 160295   |\n","---------------------------------\n","Eval num_timesteps=3849600, episode_reward=6.31 +/- 4.87\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.31     |\n","| time/              |          |\n","|    total_timesteps | 3849600  |\n","| train/             |          |\n","|    actor_loss      | 7.06     |\n","|    critic_loss     | 3.7      |\n","|    ent_coef        | 0.00447  |\n","|    ent_coef_loss   | -0.228   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 160395   |\n","---------------------------------\n","Eval num_timesteps=3852000, episode_reward=12.80 +/- 3.97\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 12.8     |\n","| time/              |          |\n","|    total_timesteps | 3852000  |\n","| train/             |          |\n","|    actor_loss      | 7.28     |\n","|    critic_loss     | 1.91     |\n","|    ent_coef        | 0.00443  |\n","|    ent_coef_loss   | 0.361    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 160495   |\n","---------------------------------\n","Eval num_timesteps=3854400, episode_reward=4.62 +/- 2.48\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.62     |\n","| time/              |          |\n","|    total_timesteps | 3854400  |\n","| train/             |          |\n","|    actor_loss      | 7.5      |\n","|    critic_loss     | 2.51     |\n","|    ent_coef        | 0.00445  |\n","|    ent_coef_loss   | -0.3     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 160595   |\n","---------------------------------\n","Eval num_timesteps=3856800, episode_reward=27.05 +/- 15.19\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 27.1     |\n","| time/              |          |\n","|    total_timesteps | 3856800  |\n","| train/             |          |\n","|    actor_loss      | 6.25     |\n","|    critic_loss     | 17.6     |\n","|    ent_coef        | 0.00439  |\n","|    ent_coef_loss   | -2.16    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 160695   |\n","---------------------------------\n","Eval num_timesteps=3859200, episode_reward=14.26 +/- 4.53\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 14.3     |\n","| time/              |          |\n","|    total_timesteps | 3859200  |\n","| train/             |          |\n","|    actor_loss      | 7.35     |\n","|    critic_loss     | 1.21     |\n","|    ent_coef        | 0.00429  |\n","|    ent_coef_loss   | 1.09     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 160795   |\n","---------------------------------\n","Eval num_timesteps=3861600, episode_reward=-46.30 +/- 48.20\n","Episode length: 351.80 +/- 121.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 352      |\n","|    mean_reward     | -46.3    |\n","| time/              |          |\n","|    total_timesteps | 3861600  |\n","| train/             |          |\n","|    actor_loss      | 7.41     |\n","|    critic_loss     | 1.85     |\n","|    ent_coef        | 0.00431  |\n","|    ent_coef_loss   | 3.37     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 160895   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6300     |\n","|    fps             | 315      |\n","|    time_elapsed    | 12228    |\n","|    total_timesteps | 3863016  |\n","| train/             |          |\n","|    actor_loss      | 6.44     |\n","|    critic_loss     | 1.16     |\n","|    ent_coef        | 0.00434  |\n","|    ent_coef_loss   | -1.5     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 160954   |\n","---------------------------------\n","Eval num_timesteps=3864000, episode_reward=4.03 +/- 6.74\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 4.03     |\n","| time/              |          |\n","|    total_timesteps | 3864000  |\n","| train/             |          |\n","|    actor_loss      | 7.13     |\n","|    critic_loss     | 1.08     |\n","|    ent_coef        | 0.00429  |\n","|    ent_coef_loss   | -1.15    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 160995   |\n","---------------------------------\n","Eval num_timesteps=3866400, episode_reward=3.58 +/- 4.65\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.58     |\n","| time/              |          |\n","|    total_timesteps | 3866400  |\n","| train/             |          |\n","|    actor_loss      | 6.81     |\n","|    critic_loss     | 3.57     |\n","|    ent_coef        | 0.00408  |\n","|    ent_coef_loss   | -1.33    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 161095   |\n","---------------------------------\n","Eval num_timesteps=3868800, episode_reward=36.13 +/- 0.35\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 36.1     |\n","| time/              |          |\n","|    total_timesteps | 3868800  |\n","| train/             |          |\n","|    actor_loss      | 7.35     |\n","|    critic_loss     | 1.32     |\n","|    ent_coef        | 0.00402  |\n","|    ent_coef_loss   | -0.605   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 161195   |\n","---------------------------------\n","Eval num_timesteps=3871200, episode_reward=25.15 +/- 2.05\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 25.1     |\n","| time/              |          |\n","|    total_timesteps | 3871200  |\n","| train/             |          |\n","|    actor_loss      | 6.49     |\n","|    critic_loss     | 1.68     |\n","|    ent_coef        | 0.00392  |\n","|    ent_coef_loss   | -2.64    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 161295   |\n","---------------------------------\n","Eval num_timesteps=3873600, episode_reward=31.91 +/- 2.06\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 31.9     |\n","| time/              |          |\n","|    total_timesteps | 3873600  |\n","| train/             |          |\n","|    actor_loss      | 7.65     |\n","|    critic_loss     | 18.4     |\n","|    ent_coef        | 0.00396  |\n","|    ent_coef_loss   | -1.56    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 161395   |\n","---------------------------------\n","Eval num_timesteps=3876000, episode_reward=9.19 +/- 2.11\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 9.19     |\n","| time/              |          |\n","|    total_timesteps | 3876000  |\n","| train/             |          |\n","|    actor_loss      | 6.29     |\n","|    critic_loss     | 11.5     |\n","|    ent_coef        | 0.00403  |\n","|    ent_coef_loss   | 1.85     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 161495   |\n","---------------------------------\n","Eval num_timesteps=3878400, episode_reward=-46.42 +/- 55.44\n","Episode length: 458.00 +/- 34.29\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 458      |\n","|    mean_reward     | -46.4    |\n","| time/              |          |\n","|    total_timesteps | 3878400  |\n","| train/             |          |\n","|    actor_loss      | 6.5      |\n","|    critic_loss     | 1.61     |\n","|    ent_coef        | 0.00416  |\n","|    ent_coef_loss   | -2.82    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 161595   |\n","---------------------------------\n","Eval num_timesteps=3880800, episode_reward=13.47 +/- 2.32\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 13.5     |\n","| time/              |          |\n","|    total_timesteps | 3880800  |\n","| train/             |          |\n","|    actor_loss      | 7.11     |\n","|    critic_loss     | 4.4      |\n","|    ent_coef        | 0.00404  |\n","|    ent_coef_loss   | -2.12    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 161695   |\n","---------------------------------\n","Eval num_timesteps=3883200, episode_reward=2.76 +/- 7.16\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 2.76     |\n","| time/              |          |\n","|    total_timesteps | 3883200  |\n","| train/             |          |\n","|    actor_loss      | 6.73     |\n","|    critic_loss     | 1.16     |\n","|    ent_coef        | 0.00388  |\n","|    ent_coef_loss   | 0.492    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 161795   |\n","---------------------------------\n","Eval num_timesteps=3885600, episode_reward=-51.61 +/- 52.81\n","Episode length: 431.00 +/- 56.34\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 431      |\n","|    mean_reward     | -51.6    |\n","| time/              |          |\n","|    total_timesteps | 3885600  |\n","| train/             |          |\n","|    actor_loss      | 6.73     |\n","|    critic_loss     | 2.7      |\n","|    ent_coef        | 0.00382  |\n","|    ent_coef_loss   | 1.91     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 161895   |\n","---------------------------------\n","Eval num_timesteps=3888000, episode_reward=1.21 +/- 3.65\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.21     |\n","| time/              |          |\n","|    total_timesteps | 3888000  |\n","| train/             |          |\n","|    actor_loss      | 7.62     |\n","|    critic_loss     | 1.51     |\n","|    ent_coef        | 0.00389  |\n","|    ent_coef_loss   | 0.0487   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 161995   |\n","---------------------------------\n","Eval num_timesteps=3890400, episode_reward=15.02 +/- 2.59\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 15       |\n","| time/              |          |\n","|    total_timesteps | 3890400  |\n","| train/             |          |\n","|    actor_loss      | 7.17     |\n","|    critic_loss     | 3.42     |\n","|    ent_coef        | 0.00379  |\n","|    ent_coef_loss   | -1.07    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 162095   |\n","---------------------------------\n","Eval num_timesteps=3892800, episode_reward=-95.71 +/- 3.32\n","Episode length: 339.20 +/- 25.96\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 339      |\n","|    mean_reward     | -95.7    |\n","| time/              |          |\n","|    total_timesteps | 3892800  |\n","| train/             |          |\n","|    actor_loss      | 5.82     |\n","|    critic_loss     | 1.19     |\n","|    ent_coef        | 0.0037   |\n","|    ent_coef_loss   | -1.27    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 162195   |\n","---------------------------------\n","Eval num_timesteps=3895200, episode_reward=9.77 +/- 0.38\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 9.77     |\n","| time/              |          |\n","|    total_timesteps | 3895200  |\n","| train/             |          |\n","|    actor_loss      | 7.59     |\n","|    critic_loss     | 1.75     |\n","|    ent_coef        | 0.00379  |\n","|    ent_coef_loss   | 1.62     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 162295   |\n","---------------------------------\n","Eval num_timesteps=3897600, episode_reward=34.88 +/- 7.37\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 34.9     |\n","| time/              |          |\n","|    total_timesteps | 3897600  |\n","| train/             |          |\n","|    actor_loss      | 6.16     |\n","|    critic_loss     | 5.02     |\n","|    ent_coef        | 0.00383  |\n","|    ent_coef_loss   | 0.216    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 162395   |\n","---------------------------------\n","Eval num_timesteps=3900000, episode_reward=34.06 +/- 2.74\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 34.1     |\n","| time/              |          |\n","|    total_timesteps | 3900000  |\n","| train/             |          |\n","|    actor_loss      | 6.04     |\n","|    critic_loss     | 16.9     |\n","|    ent_coef        | 0.0038   |\n","|    ent_coef_loss   | 0.238    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 162495   |\n","---------------------------------\n","Eval num_timesteps=3902400, episode_reward=16.56 +/- 1.09\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 16.6     |\n","| time/              |          |\n","|    total_timesteps | 3902400  |\n","| train/             |          |\n","|    actor_loss      | 6.17     |\n","|    critic_loss     | 9.79     |\n","|    ent_coef        | 0.00377  |\n","|    ent_coef_loss   | -0.272   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 162595   |\n","---------------------------------\n","Eval num_timesteps=3904800, episode_reward=19.19 +/- 0.48\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 19.2     |\n","| time/              |          |\n","|    total_timesteps | 3904800  |\n","| train/             |          |\n","|    actor_loss      | 6.33     |\n","|    critic_loss     | 5.32     |\n","|    ent_coef        | 0.00381  |\n","|    ent_coef_loss   | -0.637   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 162695   |\n","---------------------------------\n","Eval num_timesteps=3907200, episode_reward=10.29 +/- 0.14\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 10.3     |\n","| time/              |          |\n","|    total_timesteps | 3907200  |\n","| train/             |          |\n","|    actor_loss      | 6.7      |\n","|    critic_loss     | 1.47     |\n","|    ent_coef        | 0.0039   |\n","|    ent_coef_loss   | -2.32    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 162795   |\n","---------------------------------\n","Eval num_timesteps=3909600, episode_reward=25.24 +/- 0.73\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 25.2     |\n","| time/              |          |\n","|    total_timesteps | 3909600  |\n","| train/             |          |\n","|    actor_loss      | 6.85     |\n","|    critic_loss     | 2.07     |\n","|    ent_coef        | 0.00383  |\n","|    ent_coef_loss   | -0.916   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 162895   |\n","---------------------------------\n","Eval num_timesteps=3912000, episode_reward=-1.60 +/- 3.91\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.6     |\n","| time/              |          |\n","|    total_timesteps | 3912000  |\n","| train/             |          |\n","|    actor_loss      | 6.16     |\n","|    critic_loss     | 1.87     |\n","|    ent_coef        | 0.00376  |\n","|    ent_coef_loss   | -0.431   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 162995   |\n","---------------------------------\n","Eval num_timesteps=3914400, episode_reward=26.54 +/- 4.59\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 26.5     |\n","| time/              |          |\n","|    total_timesteps | 3914400  |\n","| train/             |          |\n","|    actor_loss      | 6.93     |\n","|    critic_loss     | 1.51     |\n","|    ent_coef        | 0.00388  |\n","|    ent_coef_loss   | 0.275    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 163095   |\n","---------------------------------\n","Eval num_timesteps=3916800, episode_reward=35.36 +/- 1.33\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 35.4     |\n","| time/              |          |\n","|    total_timesteps | 3916800  |\n","| train/             |          |\n","|    actor_loss      | 6.43     |\n","|    critic_loss     | 0.652    |\n","|    ent_coef        | 0.00374  |\n","|    ent_coef_loss   | -0.23    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 163195   |\n","---------------------------------\n","Eval num_timesteps=3919200, episode_reward=-58.68 +/- 50.52\n","Episode length: 408.80 +/- 74.46\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 409      |\n","|    mean_reward     | -58.7    |\n","| time/              |          |\n","|    total_timesteps | 3919200  |\n","| train/             |          |\n","|    actor_loss      | 6.85     |\n","|    critic_loss     | 1.06     |\n","|    ent_coef        | 0.00378  |\n","|    ent_coef_loss   | 5.09     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 163295   |\n","---------------------------------\n","Eval num_timesteps=3921600, episode_reward=-112.90 +/- 7.63\n","Episode length: 347.20 +/- 40.66\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 347      |\n","|    mean_reward     | -113     |\n","| time/              |          |\n","|    total_timesteps | 3921600  |\n","| train/             |          |\n","|    actor_loss      | 6.41     |\n","|    critic_loss     | 1.49     |\n","|    ent_coef        | 0.00414  |\n","|    ent_coef_loss   | 4.41     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 163395   |\n","---------------------------------\n","Eval num_timesteps=3924000, episode_reward=-6.36 +/- 3.64\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -6.36    |\n","| time/              |          |\n","|    total_timesteps | 3924000  |\n","| train/             |          |\n","|    actor_loss      | 6.92     |\n","|    critic_loss     | 1.35     |\n","|    ent_coef        | 0.00418  |\n","|    ent_coef_loss   | 2.08     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 163495   |\n","---------------------------------\n","Eval num_timesteps=3926400, episode_reward=-4.77 +/- 14.71\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -4.77    |\n","| time/              |          |\n","|    total_timesteps | 3926400  |\n","| train/             |          |\n","|    actor_loss      | 5.92     |\n","|    critic_loss     | 1.89     |\n","|    ent_coef        | 0.00416  |\n","|    ent_coef_loss   | -2.36    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 163595   |\n","---------------------------------\n","Eval num_timesteps=3928800, episode_reward=17.85 +/- 17.22\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 17.9     |\n","| time/              |          |\n","|    total_timesteps | 3928800  |\n","| train/             |          |\n","|    actor_loss      | 6.33     |\n","|    critic_loss     | 1.24     |\n","|    ent_coef        | 0.00414  |\n","|    ent_coef_loss   | 2.6      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 163695   |\n","---------------------------------\n","Eval num_timesteps=3931200, episode_reward=15.73 +/- 0.86\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 15.7     |\n","| time/              |          |\n","|    total_timesteps | 3931200  |\n","| train/             |          |\n","|    actor_loss      | 6.86     |\n","|    critic_loss     | 8.14     |\n","|    ent_coef        | 0.00407  |\n","|    ent_coef_loss   | -1.5     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 163795   |\n","---------------------------------\n","Eval num_timesteps=3933600, episode_reward=-45.88 +/- 50.30\n","Episode length: 387.20 +/- 138.15\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 387      |\n","|    mean_reward     | -45.9    |\n","| time/              |          |\n","|    total_timesteps | 3933600  |\n","| train/             |          |\n","|    actor_loss      | 6.11     |\n","|    critic_loss     | 0.659    |\n","|    ent_coef        | 0.00394  |\n","|    ent_coef_loss   | -1.19    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 163895   |\n","---------------------------------\n","Eval num_timesteps=3936000, episode_reward=-28.90 +/- 47.50\n","Episode length: 488.00 +/- 14.70\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 488      |\n","|    mean_reward     | -28.9    |\n","| time/              |          |\n","|    total_timesteps | 3936000  |\n","| train/             |          |\n","|    actor_loss      | 6.61     |\n","|    critic_loss     | 2.17     |\n","|    ent_coef        | 0.00394  |\n","|    ent_coef_loss   | 0.227    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 163995   |\n","---------------------------------\n","Eval num_timesteps=3938400, episode_reward=-57.46 +/- 47.32\n","Episode length: 452.80 +/- 57.81\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 453      |\n","|    mean_reward     | -57.5    |\n","| time/              |          |\n","|    total_timesteps | 3938400  |\n","| train/             |          |\n","|    actor_loss      | 6.02     |\n","|    critic_loss     | 0.868    |\n","|    ent_coef        | 0.00428  |\n","|    ent_coef_loss   | 3.88     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 164095   |\n","---------------------------------\n","Eval num_timesteps=3940800, episode_reward=-67.71 +/- 54.43\n","Episode length: 353.60 +/- 119.54\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 354      |\n","|    mean_reward     | -67.7    |\n","| time/              |          |\n","|    total_timesteps | 3940800  |\n","| train/             |          |\n","|    actor_loss      | 6.87     |\n","|    critic_loss     | 0.955    |\n","|    ent_coef        | 0.00439  |\n","|    ent_coef_loss   | 2.92     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 164195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6310     |\n","|    fps             | 315      |\n","|    time_elapsed    | 12498    |\n","|    total_timesteps | 3942120  |\n","| train/             |          |\n","|    actor_loss      | 6.68     |\n","|    critic_loss     | 1.9      |\n","|    ent_coef        | 0.00453  |\n","|    ent_coef_loss   | 2.03     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 164250   |\n","---------------------------------\n","Eval num_timesteps=3943200, episode_reward=-28.10 +/- 62.26\n","Episode length: 418.00 +/- 100.43\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 418      |\n","|    mean_reward     | -28.1    |\n","| time/              |          |\n","|    total_timesteps | 3943200  |\n","| train/             |          |\n","|    actor_loss      | 6.69     |\n","|    critic_loss     | 7.69     |\n","|    ent_coef        | 0.00454  |\n","|    ent_coef_loss   | -1.21    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 164295   |\n","---------------------------------\n","Eval num_timesteps=3945600, episode_reward=23.61 +/- 4.28\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 23.6     |\n","| time/              |          |\n","|    total_timesteps | 3945600  |\n","| train/             |          |\n","|    actor_loss      | 6.93     |\n","|    critic_loss     | 1.2      |\n","|    ent_coef        | 0.00442  |\n","|    ent_coef_loss   | -0.567   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 164395   |\n","---------------------------------\n","Eval num_timesteps=3948000, episode_reward=17.09 +/- 0.25\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 17.1     |\n","| time/              |          |\n","|    total_timesteps | 3948000  |\n","| train/             |          |\n","|    actor_loss      | 6.7      |\n","|    critic_loss     | 13.8     |\n","|    ent_coef        | 0.0044   |\n","|    ent_coef_loss   | 2.3      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 164495   |\n","---------------------------------\n","Eval num_timesteps=3950400, episode_reward=10.85 +/- 10.24\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 10.9     |\n","| time/              |          |\n","|    total_timesteps | 3950400  |\n","| train/             |          |\n","|    actor_loss      | 6.76     |\n","|    critic_loss     | 1.22     |\n","|    ent_coef        | 0.00435  |\n","|    ent_coef_loss   | 0.587    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 164595   |\n","---------------------------------\n","Eval num_timesteps=3952800, episode_reward=25.95 +/- 15.23\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 26       |\n","| time/              |          |\n","|    total_timesteps | 3952800  |\n","| train/             |          |\n","|    actor_loss      | 6.34     |\n","|    critic_loss     | 7.9      |\n","|    ent_coef        | 0.00424  |\n","|    ent_coef_loss   | -1.89    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 164695   |\n","---------------------------------\n","Eval num_timesteps=3955200, episode_reward=-108.81 +/- 1.49\n","Episode length: 263.00 +/- 66.14\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 263      |\n","|    mean_reward     | -109     |\n","| time/              |          |\n","|    total_timesteps | 3955200  |\n","| train/             |          |\n","|    actor_loss      | 6.75     |\n","|    critic_loss     | 1.04     |\n","|    ent_coef        | 0.00416  |\n","|    ent_coef_loss   | 0.656    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 164795   |\n","---------------------------------\n","Eval num_timesteps=3957600, episode_reward=28.47 +/- 5.83\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 28.5     |\n","| time/              |          |\n","|    total_timesteps | 3957600  |\n","| train/             |          |\n","|    actor_loss      | 6.35     |\n","|    critic_loss     | 0.907    |\n","|    ent_coef        | 0.00409  |\n","|    ent_coef_loss   | -1.93    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 164895   |\n","---------------------------------\n","Eval num_timesteps=3960000, episode_reward=-17.19 +/- 38.43\n","Episode length: 497.20 +/- 3.43\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 497      |\n","|    mean_reward     | -17.2    |\n","| time/              |          |\n","|    total_timesteps | 3960000  |\n","| train/             |          |\n","|    actor_loss      | 6.3      |\n","|    critic_loss     | 1.1      |\n","|    ent_coef        | 0.00417  |\n","|    ent_coef_loss   | -0.113   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 164995   |\n","---------------------------------\n","Eval num_timesteps=3962400, episode_reward=33.70 +/- 1.06\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 33.7     |\n","| time/              |          |\n","|    total_timesteps | 3962400  |\n","| train/             |          |\n","|    actor_loss      | 6.29     |\n","|    critic_loss     | 2.5      |\n","|    ent_coef        | 0.00408  |\n","|    ent_coef_loss   | -1.16    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 165095   |\n","---------------------------------\n","Eval num_timesteps=3964800, episode_reward=42.77 +/- 1.73\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 42.8     |\n","| time/              |          |\n","|    total_timesteps | 3964800  |\n","| train/             |          |\n","|    actor_loss      | 7.33     |\n","|    critic_loss     | 8.62     |\n","|    ent_coef        | 0.00401  |\n","|    ent_coef_loss   | -1.47    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 165195   |\n","---------------------------------\n","Eval num_timesteps=3967200, episode_reward=27.75 +/- 6.44\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 27.8     |\n","| time/              |          |\n","|    total_timesteps | 3967200  |\n","| train/             |          |\n","|    actor_loss      | 6.46     |\n","|    critic_loss     | 0.963    |\n","|    ent_coef        | 0.00389  |\n","|    ent_coef_loss   | -3.48    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 165295   |\n","---------------------------------\n","Eval num_timesteps=3969600, episode_reward=34.23 +/- 4.02\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 34.2     |\n","| time/              |          |\n","|    total_timesteps | 3969600  |\n","| train/             |          |\n","|    actor_loss      | 6.34     |\n","|    critic_loss     | 1.55     |\n","|    ent_coef        | 0.00376  |\n","|    ent_coef_loss   | 0.84     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 165395   |\n","---------------------------------\n","Eval num_timesteps=3972000, episode_reward=-23.03 +/- 71.39\n","Episode length: 326.00 +/- 213.11\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 326      |\n","|    mean_reward     | -23      |\n","| time/              |          |\n","|    total_timesteps | 3972000  |\n","| train/             |          |\n","|    actor_loss      | 6.51     |\n","|    critic_loss     | 2.27     |\n","|    ent_coef        | 0.00358  |\n","|    ent_coef_loss   | -1.81    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 165495   |\n","---------------------------------\n","Eval num_timesteps=3974400, episode_reward=14.53 +/- 9.82\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 14.5     |\n","| time/              |          |\n","|    total_timesteps | 3974400  |\n","| train/             |          |\n","|    actor_loss      | 6.43     |\n","|    critic_loss     | 18.5     |\n","|    ent_coef        | 0.00345  |\n","|    ent_coef_loss   | -2.88    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 165595   |\n","---------------------------------\n","Eval num_timesteps=3976800, episode_reward=17.59 +/- 5.05\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 17.6     |\n","| time/              |          |\n","|    total_timesteps | 3976800  |\n","| train/             |          |\n","|    actor_loss      | 5.95     |\n","|    critic_loss     | 1.27     |\n","|    ent_coef        | 0.00342  |\n","|    ent_coef_loss   | 3.78     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 165695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6320     |\n","|    fps             | 315      |\n","|    time_elapsed    | 12620    |\n","|    total_timesteps | 3978552  |\n","| train/             |          |\n","|    actor_loss      | 6.99     |\n","|    critic_loss     | 17.1     |\n","|    ent_coef        | 0.00352  |\n","|    ent_coef_loss   | 0.793    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 165768   |\n","---------------------------------\n","Eval num_timesteps=3979200, episode_reward=10.72 +/- 9.36\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 10.7     |\n","| time/              |          |\n","|    total_timesteps | 3979200  |\n","| train/             |          |\n","|    actor_loss      | 6.54     |\n","|    critic_loss     | 22.3     |\n","|    ent_coef        | 0.00352  |\n","|    ent_coef_loss   | 1.4      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 165795   |\n","---------------------------------\n","Eval num_timesteps=3981600, episode_reward=-10.62 +/- 64.97\n","Episode length: 457.20 +/- 52.42\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 457      |\n","|    mean_reward     | -10.6    |\n","| time/              |          |\n","|    total_timesteps | 3981600  |\n","| train/             |          |\n","|    actor_loss      | 6.3      |\n","|    critic_loss     | 2.53     |\n","|    ent_coef        | 0.00363  |\n","|    ent_coef_loss   | 2.3      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 165895   |\n","---------------------------------\n","Eval num_timesteps=3984000, episode_reward=52.28 +/- 0.10\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 52.3     |\n","| time/              |          |\n","|    total_timesteps | 3984000  |\n","| train/             |          |\n","|    actor_loss      | 6.56     |\n","|    critic_loss     | 1.38     |\n","|    ent_coef        | 0.00389  |\n","|    ent_coef_loss   | 9.03     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 165995   |\n","---------------------------------\n","New best mean reward!\n","Eval num_timesteps=3986400, episode_reward=33.97 +/- 3.38\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 34       |\n","| time/              |          |\n","|    total_timesteps | 3986400  |\n","| train/             |          |\n","|    actor_loss      | 6.62     |\n","|    critic_loss     | 11       |\n","|    ent_coef        | 0.00403  |\n","|    ent_coef_loss   | 4.12     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 166095   |\n","---------------------------------\n","Eval num_timesteps=3988800, episode_reward=20.80 +/- 13.51\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 20.8     |\n","| time/              |          |\n","|    total_timesteps | 3988800  |\n","| train/             |          |\n","|    actor_loss      | 6.27     |\n","|    critic_loss     | 0.948    |\n","|    ent_coef        | 0.00408  |\n","|    ent_coef_loss   | -4.33    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 166195   |\n","---------------------------------\n","Eval num_timesteps=3991200, episode_reward=-27.47 +/- 57.51\n","Episode length: 371.60 +/- 104.84\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 372      |\n","|    mean_reward     | -27.5    |\n","| time/              |          |\n","|    total_timesteps | 3991200  |\n","| train/             |          |\n","|    actor_loss      | 6.68     |\n","|    critic_loss     | 4.53     |\n","|    ent_coef        | 0.00394  |\n","|    ent_coef_loss   | 0.924    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 166295   |\n","---------------------------------\n","Eval num_timesteps=3993600, episode_reward=44.78 +/- 1.20\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 44.8     |\n","| time/              |          |\n","|    total_timesteps | 3993600  |\n","| train/             |          |\n","|    actor_loss      | 6.17     |\n","|    critic_loss     | 0.851    |\n","|    ent_coef        | 0.00389  |\n","|    ent_coef_loss   | -4.57    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 166395   |\n","---------------------------------\n","Eval num_timesteps=3996000, episode_reward=-82.32 +/- 6.79\n","Episode length: 256.00 +/- 36.74\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 256      |\n","|    mean_reward     | -82.3    |\n","| time/              |          |\n","|    total_timesteps | 3996000  |\n","| train/             |          |\n","|    actor_loss      | 6.11     |\n","|    critic_loss     | 4.72     |\n","|    ent_coef        | 0.00378  |\n","|    ent_coef_loss   | 6        |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 166495   |\n","---------------------------------\n","Eval num_timesteps=3998400, episode_reward=-33.24 +/- 67.22\n","Episode length: 306.20 +/- 158.24\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 306      |\n","|    mean_reward     | -33.2    |\n","| time/              |          |\n","|    total_timesteps | 3998400  |\n","| train/             |          |\n","|    actor_loss      | 6.15     |\n","|    critic_loss     | 1.17     |\n","|    ent_coef        | 0.00391  |\n","|    ent_coef_loss   | 1.95     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 166595   |\n","---------------------------------\n","Eval num_timesteps=4000800, episode_reward=38.40 +/- 21.04\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 38.4     |\n","| time/              |          |\n","|    total_timesteps | 4000800  |\n","| train/             |          |\n","|    actor_loss      | 6.26     |\n","|    critic_loss     | 4.69     |\n","|    ent_coef        | 0.00395  |\n","|    ent_coef_loss   | -2.93    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 166695   |\n","---------------------------------\n","Eval num_timesteps=4003200, episode_reward=37.53 +/- 0.76\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 37.5     |\n","| time/              |          |\n","|    total_timesteps | 4003200  |\n","| train/             |          |\n","|    actor_loss      | 6.69     |\n","|    critic_loss     | 0.815    |\n","|    ent_coef        | 0.00393  |\n","|    ent_coef_loss   | 0.521    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 166795   |\n","---------------------------------\n","Eval num_timesteps=4005600, episode_reward=42.78 +/- 2.00\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 42.8     |\n","| time/              |          |\n","|    total_timesteps | 4005600  |\n","| train/             |          |\n","|    actor_loss      | 6.09     |\n","|    critic_loss     | 1.25     |\n","|    ent_coef        | 0.00386  |\n","|    ent_coef_loss   | -1.04    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 166895   |\n","---------------------------------\n","Eval num_timesteps=4008000, episode_reward=33.62 +/- 0.61\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 33.6     |\n","| time/              |          |\n","|    total_timesteps | 4008000  |\n","| train/             |          |\n","|    actor_loss      | 6.12     |\n","|    critic_loss     | 8.29     |\n","|    ent_coef        | 0.004    |\n","|    ent_coef_loss   | -1.89    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 166995   |\n","---------------------------------\n","Eval num_timesteps=4010400, episode_reward=-11.48 +/- 61.83\n","Episode length: 386.40 +/- 139.13\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 386      |\n","|    mean_reward     | -11.5    |\n","| time/              |          |\n","|    total_timesteps | 4010400  |\n","| train/             |          |\n","|    actor_loss      | 5.75     |\n","|    critic_loss     | 1.44     |\n","|    ent_coef        | 0.004    |\n","|    ent_coef_loss   | -0.843   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 167095   |\n","---------------------------------\n","Eval num_timesteps=4012800, episode_reward=8.88 +/- 2.91\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 8.88     |\n","| time/              |          |\n","|    total_timesteps | 4012800  |\n","| train/             |          |\n","|    actor_loss      | 5.78     |\n","|    critic_loss     | 0.692    |\n","|    ent_coef        | 0.00387  |\n","|    ent_coef_loss   | -0.421   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 167195   |\n","---------------------------------\n","Eval num_timesteps=4015200, episode_reward=-72.31 +/- 58.18\n","Episode length: 321.20 +/- 145.99\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 321      |\n","|    mean_reward     | -72.3    |\n","| time/              |          |\n","|    total_timesteps | 4015200  |\n","| train/             |          |\n","|    actor_loss      | 6.15     |\n","|    critic_loss     | 8.52     |\n","|    ent_coef        | 0.00387  |\n","|    ent_coef_loss   | 1.25     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 167295   |\n","---------------------------------\n","Eval num_timesteps=4017600, episode_reward=-107.83 +/- 0.96\n","Episode length: 417.40 +/- 59.28\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 417      |\n","|    mean_reward     | -108     |\n","| time/              |          |\n","|    total_timesteps | 4017600  |\n","| train/             |          |\n","|    actor_loss      | 6.84     |\n","|    critic_loss     | 1.6      |\n","|    ent_coef        | 0.00387  |\n","|    ent_coef_loss   | 1.87     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 167395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6330     |\n","|    fps             | 315      |\n","|    time_elapsed    | 12754    |\n","|    total_timesteps | 4017840  |\n","| train/             |          |\n","|    actor_loss      | 6.75     |\n","|    critic_loss     | 4.54     |\n","|    ent_coef        | 0.00388  |\n","|    ent_coef_loss   | 3.4      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 167405   |\n","---------------------------------\n","Eval num_timesteps=4020000, episode_reward=-45.11 +/- 54.71\n","Episode length: 487.60 +/- 15.19\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 488      |\n","|    mean_reward     | -45.1    |\n","| time/              |          |\n","|    total_timesteps | 4020000  |\n","| train/             |          |\n","|    actor_loss      | 6.04     |\n","|    critic_loss     | 1.3      |\n","|    ent_coef        | 0.00383  |\n","|    ent_coef_loss   | -2.08    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 167495   |\n","---------------------------------\n","Eval num_timesteps=4022400, episode_reward=20.63 +/- 1.07\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 20.6     |\n","| time/              |          |\n","|    total_timesteps | 4022400  |\n","| train/             |          |\n","|    actor_loss      | 6.05     |\n","|    critic_loss     | 28.8     |\n","|    ent_coef        | 0.00372  |\n","|    ent_coef_loss   | -0.132   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 167595   |\n","---------------------------------\n","Eval num_timesteps=4024800, episode_reward=-10.53 +/- 53.69\n","Episode length: 483.20 +/- 20.58\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 483      |\n","|    mean_reward     | -10.5    |\n","| time/              |          |\n","|    total_timesteps | 4024800  |\n","| train/             |          |\n","|    actor_loss      | 6.72     |\n","|    critic_loss     | 1.5      |\n","|    ent_coef        | 0.00386  |\n","|    ent_coef_loss   | 1.75     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 167695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6340     |\n","|    fps             | 314      |\n","|    time_elapsed    | 12785    |\n","|    total_timesteps | 4027152  |\n","| train/             |          |\n","|    actor_loss      | 5.62     |\n","|    critic_loss     | 0.778    |\n","|    ent_coef        | 0.00399  |\n","|    ent_coef_loss   | -4.53    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 167793   |\n","---------------------------------\n","Eval num_timesteps=4027200, episode_reward=15.64 +/- 1.69\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 15.6     |\n","| time/              |          |\n","|    total_timesteps | 4027200  |\n","| train/             |          |\n","|    actor_loss      | 6.08     |\n","|    critic_loss     | 7.2      |\n","|    ent_coef        | 0.00399  |\n","|    ent_coef_loss   | -1.57    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 167795   |\n","---------------------------------\n","Eval num_timesteps=4029600, episode_reward=-60.84 +/- 53.43\n","Episode length: 326.60 +/- 141.58\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 327      |\n","|    mean_reward     | -60.8    |\n","| time/              |          |\n","|    total_timesteps | 4029600  |\n","| train/             |          |\n","|    actor_loss      | 6.06     |\n","|    critic_loss     | 2.59     |\n","|    ent_coef        | 0.00392  |\n","|    ent_coef_loss   | 5.69     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 167895   |\n","---------------------------------\n","Eval num_timesteps=4032000, episode_reward=36.30 +/- 0.76\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 36.3     |\n","| time/              |          |\n","|    total_timesteps | 4032000  |\n","| train/             |          |\n","|    actor_loss      | 6.03     |\n","|    critic_loss     | 0.964    |\n","|    ent_coef        | 0.00406  |\n","|    ent_coef_loss   | -2.46    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 167995   |\n","---------------------------------\n","Eval num_timesteps=4034400, episode_reward=20.06 +/- 1.63\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 20.1     |\n","| time/              |          |\n","|    total_timesteps | 4034400  |\n","| train/             |          |\n","|    actor_loss      | 5.88     |\n","|    critic_loss     | 4.99     |\n","|    ent_coef        | 0.00394  |\n","|    ent_coef_loss   | 0.22     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 168095   |\n","---------------------------------\n","Eval num_timesteps=4036800, episode_reward=9.26 +/- 1.78\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 9.26     |\n","| time/              |          |\n","|    total_timesteps | 4036800  |\n","| train/             |          |\n","|    actor_loss      | 6.43     |\n","|    critic_loss     | 1.61     |\n","|    ent_coef        | 0.00392  |\n","|    ent_coef_loss   | 0.0847   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 168195   |\n","---------------------------------\n","Eval num_timesteps=4039200, episode_reward=11.50 +/- 8.00\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 11.5     |\n","| time/              |          |\n","|    total_timesteps | 4039200  |\n","| train/             |          |\n","|    actor_loss      | 6.02     |\n","|    critic_loss     | 5.23     |\n","|    ent_coef        | 0.0038   |\n","|    ent_coef_loss   | -0.861   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 168295   |\n","---------------------------------\n","Eval num_timesteps=4041600, episode_reward=26.15 +/- 2.80\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 26.1     |\n","| time/              |          |\n","|    total_timesteps | 4041600  |\n","| train/             |          |\n","|    actor_loss      | 5.95     |\n","|    critic_loss     | 1.1      |\n","|    ent_coef        | 0.00373  |\n","|    ent_coef_loss   | -0.335   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 168395   |\n","---------------------------------\n","Eval num_timesteps=4044000, episode_reward=-95.24 +/- 3.99\n","Episode length: 165.80 +/- 65.16\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 166      |\n","|    mean_reward     | -95.2    |\n","| time/              |          |\n","|    total_timesteps | 4044000  |\n","| train/             |          |\n","|    actor_loss      | 5.6      |\n","|    critic_loss     | 1.52     |\n","|    ent_coef        | 0.00363  |\n","|    ent_coef_loss   | -2.38    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 168495   |\n","---------------------------------\n","Eval num_timesteps=4046400, episode_reward=-83.32 +/- 9.49\n","Episode length: 335.40 +/- 130.31\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 335      |\n","|    mean_reward     | -83.3    |\n","| time/              |          |\n","|    total_timesteps | 4046400  |\n","| train/             |          |\n","|    actor_loss      | 5.9      |\n","|    critic_loss     | 1.26     |\n","|    ent_coef        | 0.00366  |\n","|    ent_coef_loss   | -0.912   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 168595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6350     |\n","|    fps             | 314      |\n","|    time_elapsed    | 12854    |\n","|    total_timesteps | 4048656  |\n","| train/             |          |\n","|    actor_loss      | 5.68     |\n","|    critic_loss     | 1.57     |\n","|    ent_coef        | 0.00364  |\n","|    ent_coef_loss   | -2.91    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 168689   |\n","---------------------------------\n","Eval num_timesteps=4048800, episode_reward=-53.90 +/- 47.96\n","Episode length: 287.00 +/- 173.91\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 287      |\n","|    mean_reward     | -53.9    |\n","| time/              |          |\n","|    total_timesteps | 4048800  |\n","| train/             |          |\n","|    actor_loss      | 5.78     |\n","|    critic_loss     | 2.06     |\n","|    ent_coef        | 0.00364  |\n","|    ent_coef_loss   | -2.18    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 168695   |\n","---------------------------------\n","Eval num_timesteps=4051200, episode_reward=24.97 +/- 0.98\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 25       |\n","| time/              |          |\n","|    total_timesteps | 4051200  |\n","| train/             |          |\n","|    actor_loss      | 6.48     |\n","|    critic_loss     | 4.76     |\n","|    ent_coef        | 0.00383  |\n","|    ent_coef_loss   | -2.63    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 168795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6360     |\n","|    fps             | 314      |\n","|    time_elapsed    | 12868    |\n","|    total_timesteps | 4051824  |\n","| train/             |          |\n","|    actor_loss      | 6.39     |\n","|    critic_loss     | 8.28     |\n","|    ent_coef        | 0.00379  |\n","|    ent_coef_loss   | -2.53    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 168821   |\n","---------------------------------\n","Eval num_timesteps=4053600, episode_reward=14.99 +/- 1.72\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 15       |\n","| time/              |          |\n","|    total_timesteps | 4053600  |\n","| train/             |          |\n","|    actor_loss      | 6.95     |\n","|    critic_loss     | 17.2     |\n","|    ent_coef        | 0.0038   |\n","|    ent_coef_loss   | 4.66     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 168895   |\n","---------------------------------\n","Eval num_timesteps=4056000, episode_reward=30.72 +/- 8.30\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 30.7     |\n","| time/              |          |\n","|    total_timesteps | 4056000  |\n","| train/             |          |\n","|    actor_loss      | 6.43     |\n","|    critic_loss     | 1.95     |\n","|    ent_coef        | 0.00381  |\n","|    ent_coef_loss   | -0.672   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 168995   |\n","---------------------------------\n","Eval num_timesteps=4058400, episode_reward=-10.14 +/- 48.03\n","Episode length: 482.00 +/- 22.05\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 482      |\n","|    mean_reward     | -10.1    |\n","| time/              |          |\n","|    total_timesteps | 4058400  |\n","| train/             |          |\n","|    actor_loss      | 5.94     |\n","|    critic_loss     | 1.05     |\n","|    ent_coef        | 0.00375  |\n","|    ent_coef_loss   | -2.44    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 169095   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6370     |\n","|    fps             | 314      |\n","|    time_elapsed    | 12893    |\n","|    total_timesteps | 4059744  |\n","| train/             |          |\n","|    actor_loss      | 5.69     |\n","|    critic_loss     | 1.29     |\n","|    ent_coef        | 0.00369  |\n","|    ent_coef_loss   | 0.797    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 169151   |\n","---------------------------------\n","Eval num_timesteps=4060800, episode_reward=-35.38 +/- 58.04\n","Episode length: 399.20 +/- 82.30\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 399      |\n","|    mean_reward     | -35.4    |\n","| time/              |          |\n","|    total_timesteps | 4060800  |\n","| train/             |          |\n","|    actor_loss      | 5.72     |\n","|    critic_loss     | 0.842    |\n","|    ent_coef        | 0.00371  |\n","|    ent_coef_loss   | -1.47    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 169195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6380     |\n","|    fps             | 314      |\n","|    time_elapsed    | 12904    |\n","|    total_timesteps | 4062816  |\n","| train/             |          |\n","|    actor_loss      | 6        |\n","|    critic_loss     | 1.15     |\n","|    ent_coef        | 0.00367  |\n","|    ent_coef_loss   | 0.317    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 169279   |\n","---------------------------------\n","Eval num_timesteps=4063200, episode_reward=-93.16 +/- 1.18\n","Episode length: 136.20 +/- 23.52\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 136      |\n","|    mean_reward     | -93.2    |\n","| time/              |          |\n","|    total_timesteps | 4063200  |\n","| train/             |          |\n","|    actor_loss      | 5.72     |\n","|    critic_loss     | 0.847    |\n","|    ent_coef        | 0.00367  |\n","|    ent_coef_loss   | -1.78    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 169295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6390     |\n","|    fps             | 314      |\n","|    time_elapsed    | 12906    |\n","|    total_timesteps | 4063968  |\n","| train/             |          |\n","|    actor_loss      | 6.54     |\n","|    critic_loss     | 1.72     |\n","|    ent_coef        | 0.00366  |\n","|    ent_coef_loss   | 2.99     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 169327   |\n","---------------------------------\n","Eval num_timesteps=4065600, episode_reward=12.00 +/- 6.77\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 12       |\n","| time/              |          |\n","|    total_timesteps | 4065600  |\n","| train/             |          |\n","|    actor_loss      | 6.2      |\n","|    critic_loss     | 19.4     |\n","|    ent_coef        | 0.00372  |\n","|    ent_coef_loss   | -0.0192  |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 169395   |\n","---------------------------------\n","Eval num_timesteps=4068000, episode_reward=-72.73 +/- 5.35\n","Episode length: 404.20 +/- 74.46\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 404      |\n","|    mean_reward     | -72.7    |\n","| time/              |          |\n","|    total_timesteps | 4068000  |\n","| train/             |          |\n","|    actor_loss      | 5.75     |\n","|    critic_loss     | 3.05     |\n","|    ent_coef        | 0.00371  |\n","|    ent_coef_loss   | 0.5      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 169495   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6400     |\n","|    fps             | 314      |\n","|    time_elapsed    | 12923    |\n","|    total_timesteps | 4068840  |\n","| train/             |          |\n","|    actor_loss      | 6.18     |\n","|    critic_loss     | 0.645    |\n","|    ent_coef        | 0.00372  |\n","|    ent_coef_loss   | 0.00336  |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 169530   |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    episodes        | 6410    |\n","|    fps             | 314     |\n","|    time_elapsed    | 12923   |\n","|    total_timesteps | 4068840 |\n","--------------------------------\n","Eval num_timesteps=4070400, episode_reward=35.39 +/- 0.48\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 35.4     |\n","| time/              |          |\n","|    total_timesteps | 4070400  |\n","| train/             |          |\n","|    actor_loss      | 6.53     |\n","|    critic_loss     | 1.16     |\n","|    ent_coef        | 0.00372  |\n","|    ent_coef_loss   | 0.627    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 169595   |\n","---------------------------------\n","Eval num_timesteps=4072800, episode_reward=-65.34 +/- 45.32\n","Episode length: 309.80 +/- 155.30\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 310      |\n","|    mean_reward     | -65.3    |\n","| time/              |          |\n","|    total_timesteps | 4072800  |\n","| train/             |          |\n","|    actor_loss      | 6.42     |\n","|    critic_loss     | 1.7      |\n","|    ent_coef        | 0.0037   |\n","|    ent_coef_loss   | 2.25     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 169695   |\n","---------------------------------\n","Eval num_timesteps=4075200, episode_reward=-31.45 +/- 45.98\n","Episode length: 412.40 +/- 71.53\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 412      |\n","|    mean_reward     | -31.4    |\n","| time/              |          |\n","|    total_timesteps | 4075200  |\n","| train/             |          |\n","|    actor_loss      | 6.46     |\n","|    critic_loss     | 0.989    |\n","|    ent_coef        | 0.0038   |\n","|    ent_coef_loss   | -0.455   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 169795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6420     |\n","|    fps             | 314      |\n","|    time_elapsed    | 12946    |\n","|    total_timesteps | 4076208  |\n","| train/             |          |\n","|    actor_loss      | 5.86     |\n","|    critic_loss     | 2.11     |\n","|    ent_coef        | 0.00382  |\n","|    ent_coef_loss   | 0.189    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 169837   |\n","---------------------------------\n","Eval num_timesteps=4077600, episode_reward=55.91 +/- 2.42\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 55.9     |\n","| time/              |          |\n","|    total_timesteps | 4077600  |\n","| train/             |          |\n","|    actor_loss      | 6.92     |\n","|    critic_loss     | 37.5     |\n","|    ent_coef        | 0.00379  |\n","|    ent_coef_loss   | 2.48     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 169895   |\n","---------------------------------\n","New best mean reward!\n","Eval num_timesteps=4080000, episode_reward=49.49 +/- 1.25\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 49.5     |\n","| time/              |          |\n","|    total_timesteps | 4080000  |\n","| train/             |          |\n","|    actor_loss      | 6.38     |\n","|    critic_loss     | 3.01     |\n","|    ent_coef        | 0.00381  |\n","|    ent_coef_loss   | 0.746    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 169995   |\n","---------------------------------\n","Eval num_timesteps=4082400, episode_reward=-50.97 +/- 45.96\n","Episode length: 305.00 +/- 159.22\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 305      |\n","|    mean_reward     | -51      |\n","| time/              |          |\n","|    total_timesteps | 4082400  |\n","| train/             |          |\n","|    actor_loss      | 5.49     |\n","|    critic_loss     | 16.2     |\n","|    ent_coef        | 0.0039   |\n","|    ent_coef_loss   | 0.662    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 170095   |\n","---------------------------------\n","Eval num_timesteps=4084800, episode_reward=-10.73 +/- 51.37\n","Episode length: 474.20 +/- 21.07\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 474      |\n","|    mean_reward     | -10.7    |\n","| time/              |          |\n","|    total_timesteps | 4084800  |\n","| train/             |          |\n","|    actor_loss      | 6.61     |\n","|    critic_loss     | 2.4      |\n","|    ent_coef        | 0.00395  |\n","|    ent_coef_loss   | 1.17     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 170195   |\n","---------------------------------\n","Eval num_timesteps=4087200, episode_reward=39.04 +/- 5.09\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 39       |\n","| time/              |          |\n","|    total_timesteps | 4087200  |\n","| train/             |          |\n","|    actor_loss      | 6.03     |\n","|    critic_loss     | 1.53     |\n","|    ent_coef        | 0.00399  |\n","|    ent_coef_loss   | 3.23     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 170295   |\n","---------------------------------\n","Eval num_timesteps=4089600, episode_reward=-51.01 +/- 71.50\n","Episode length: 291.20 +/- 170.48\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 291      |\n","|    mean_reward     | -51      |\n","| time/              |          |\n","|    total_timesteps | 4089600  |\n","| train/             |          |\n","|    actor_loss      | 6.57     |\n","|    critic_loss     | 1.55     |\n","|    ent_coef        | 0.00432  |\n","|    ent_coef_loss   | 2.61     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 170395   |\n","---------------------------------\n","Eval num_timesteps=4092000, episode_reward=-94.26 +/- 2.66\n","Episode length: 195.40 +/- 76.42\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 195      |\n","|    mean_reward     | -94.3    |\n","| time/              |          |\n","|    total_timesteps | 4092000  |\n","| train/             |          |\n","|    actor_loss      | 6.33     |\n","|    critic_loss     | 19.9     |\n","|    ent_coef        | 0.00452  |\n","|    ent_coef_loss   | 4.41     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 170495   |\n","---------------------------------\n","Eval num_timesteps=4094400, episode_reward=-96.19 +/- 8.96\n","Episode length: 106.80 +/- 52.42\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 107      |\n","|    mean_reward     | -96.2    |\n","| time/              |          |\n","|    total_timesteps | 4094400  |\n","| train/             |          |\n","|    actor_loss      | 6.39     |\n","|    critic_loss     | 10.6     |\n","|    ent_coef        | 0.00475  |\n","|    ent_coef_loss   | 3.22     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 170595   |\n","---------------------------------\n","Eval num_timesteps=4096800, episode_reward=45.90 +/- 0.33\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 45.9     |\n","| time/              |          |\n","|    total_timesteps | 4096800  |\n","| train/             |          |\n","|    actor_loss      | 5.46     |\n","|    critic_loss     | 0.922    |\n","|    ent_coef        | 0.005    |\n","|    ent_coef_loss   | -3.75    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 170695   |\n","---------------------------------\n","Eval num_timesteps=4099200, episode_reward=53.46 +/- 0.29\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 53.5     |\n","| time/              |          |\n","|    total_timesteps | 4099200  |\n","| train/             |          |\n","|    actor_loss      | 5.71     |\n","|    critic_loss     | 0.808    |\n","|    ent_coef        | 0.00484  |\n","|    ent_coef_loss   | -1.89    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 170795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6430     |\n","|    fps             | 314      |\n","|    time_elapsed    | 13024    |\n","|    total_timesteps | 4101216  |\n","| train/             |          |\n","|    actor_loss      | 6.17     |\n","|    critic_loss     | 1.86     |\n","|    ent_coef        | 0.00475  |\n","|    ent_coef_loss   | -4.72    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 170879   |\n","---------------------------------\n","Eval num_timesteps=4101600, episode_reward=-33.52 +/- 65.84\n","Episode length: 368.60 +/- 107.29\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 369      |\n","|    mean_reward     | -33.5    |\n","| time/              |          |\n","|    total_timesteps | 4101600  |\n","| train/             |          |\n","|    actor_loss      | 6.51     |\n","|    critic_loss     | 1.29     |\n","|    ent_coef        | 0.00469  |\n","|    ent_coef_loss   | -2.56    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 170895   |\n","---------------------------------\n","Eval num_timesteps=4104000, episode_reward=38.83 +/- 5.37\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 38.8     |\n","| time/              |          |\n","|    total_timesteps | 4104000  |\n","| train/             |          |\n","|    actor_loss      | 5.69     |\n","|    critic_loss     | 1.89     |\n","|    ent_coef        | 0.00448  |\n","|    ent_coef_loss   | -4.71    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 170995   |\n","---------------------------------\n","Eval num_timesteps=4106400, episode_reward=46.47 +/- 0.55\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 46.5     |\n","| time/              |          |\n","|    total_timesteps | 4106400  |\n","| train/             |          |\n","|    actor_loss      | 5.62     |\n","|    critic_loss     | 0.803    |\n","|    ent_coef        | 0.00429  |\n","|    ent_coef_loss   | -4.75    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 171095   |\n","---------------------------------\n","Eval num_timesteps=4108800, episode_reward=41.08 +/- 0.22\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 41.1     |\n","| time/              |          |\n","|    total_timesteps | 4108800  |\n","| train/             |          |\n","|    actor_loss      | 6.73     |\n","|    critic_loss     | 3.55     |\n","|    ent_coef        | 0.00421  |\n","|    ent_coef_loss   | 1.13     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 171195   |\n","---------------------------------\n","Eval num_timesteps=4111200, episode_reward=41.15 +/- 4.07\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 41.2     |\n","| time/              |          |\n","|    total_timesteps | 4111200  |\n","| train/             |          |\n","|    actor_loss      | 5.96     |\n","|    critic_loss     | 0.784    |\n","|    ent_coef        | 0.00415  |\n","|    ent_coef_loss   | -0.445   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 171295   |\n","---------------------------------\n","Eval num_timesteps=4113600, episode_reward=48.95 +/- 9.10\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 49       |\n","| time/              |          |\n","|    total_timesteps | 4113600  |\n","| train/             |          |\n","|    actor_loss      | 6.06     |\n","|    critic_loss     | 1.69     |\n","|    ent_coef        | 0.00423  |\n","|    ent_coef_loss   | -0.63    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 171395   |\n","---------------------------------\n","Eval num_timesteps=4116000, episode_reward=6.68 +/- 52.78\n","Episode length: 492.40 +/- 9.31\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 492      |\n","|    mean_reward     | 6.68     |\n","| time/              |          |\n","|    total_timesteps | 4116000  |\n","| train/             |          |\n","|    actor_loss      | 5.7      |\n","|    critic_loss     | 2.28     |\n","|    ent_coef        | 0.00423  |\n","|    ent_coef_loss   | -3.72    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 171495   |\n","---------------------------------\n","Eval num_timesteps=4118400, episode_reward=-47.76 +/- 62.37\n","Episode length: 293.60 +/- 168.52\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 294      |\n","|    mean_reward     | -47.8    |\n","| time/              |          |\n","|    total_timesteps | 4118400  |\n","| train/             |          |\n","|    actor_loss      | 6.09     |\n","|    critic_loss     | 6.1      |\n","|    ent_coef        | 0.00418  |\n","|    ent_coef_loss   | -1.16    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 171595   |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    episodes        | 6440    |\n","|    fps             | 314     |\n","|    time_elapsed    | 13086   |\n","|    total_timesteps | 4118400 |\n","--------------------------------\n","Eval num_timesteps=4120800, episode_reward=37.85 +/- 4.78\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 37.8     |\n","| time/              |          |\n","|    total_timesteps | 4120800  |\n","| train/             |          |\n","|    actor_loss      | 6.68     |\n","|    critic_loss     | 2.12     |\n","|    ent_coef        | 0.00425  |\n","|    ent_coef_loss   | 3.64     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 171695   |\n","---------------------------------\n","Eval num_timesteps=4123200, episode_reward=-9.04 +/- 57.27\n","Episode length: 418.80 +/- 99.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 419      |\n","|    mean_reward     | -9.04    |\n","| time/              |          |\n","|    total_timesteps | 4123200  |\n","| train/             |          |\n","|    actor_loss      | 5.38     |\n","|    critic_loss     | 0.652    |\n","|    ent_coef        | 0.00422  |\n","|    ent_coef_loss   | -0.979   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 171795   |\n","---------------------------------\n","Eval num_timesteps=4125600, episode_reward=20.60 +/- 6.01\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 20.6     |\n","| time/              |          |\n","|    total_timesteps | 4125600  |\n","| train/             |          |\n","|    actor_loss      | 5.85     |\n","|    critic_loss     | 1.46     |\n","|    ent_coef        | 0.0044   |\n","|    ent_coef_loss   | 1.08     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 171895   |\n","---------------------------------\n","Eval num_timesteps=4128000, episode_reward=39.13 +/- 8.14\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 39.1     |\n","| time/              |          |\n","|    total_timesteps | 4128000  |\n","| train/             |          |\n","|    actor_loss      | 6.51     |\n","|    critic_loss     | 1.69     |\n","|    ent_coef        | 0.0045   |\n","|    ent_coef_loss   | 0.266    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 171995   |\n","---------------------------------\n","Eval num_timesteps=4130400, episode_reward=50.80 +/- 3.76\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 50.8     |\n","| time/              |          |\n","|    total_timesteps | 4130400  |\n","| train/             |          |\n","|    actor_loss      | 5.99     |\n","|    critic_loss     | 1.63     |\n","|    ent_coef        | 0.00439  |\n","|    ent_coef_loss   | -0.372   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 172095   |\n","---------------------------------\n","Eval num_timesteps=4132800, episode_reward=15.71 +/- 2.20\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 15.7     |\n","| time/              |          |\n","|    total_timesteps | 4132800  |\n","| train/             |          |\n","|    actor_loss      | 5.42     |\n","|    critic_loss     | 0.979    |\n","|    ent_coef        | 0.00436  |\n","|    ent_coef_loss   | 0.932    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 172195   |\n","---------------------------------\n","Eval num_timesteps=4135200, episode_reward=1.77 +/- 61.97\n","Episode length: 414.80 +/- 104.35\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 415      |\n","|    mean_reward     | 1.77     |\n","| time/              |          |\n","|    total_timesteps | 4135200  |\n","| train/             |          |\n","|    actor_loss      | 6.2      |\n","|    critic_loss     | 1.21     |\n","|    ent_coef        | 0.00436  |\n","|    ent_coef_loss   | 2.27     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 172295   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6450     |\n","|    fps             | 314      |\n","|    time_elapsed    | 13145    |\n","|    total_timesteps | 4136736  |\n","| train/             |          |\n","|    actor_loss      | 5.83     |\n","|    critic_loss     | 0.934    |\n","|    ent_coef        | 0.00458  |\n","|    ent_coef_loss   | 0.025    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 172359   |\n","---------------------------------\n","Eval num_timesteps=4137600, episode_reward=-87.18 +/- 0.74\n","Episode length: 208.80 +/- 8.82\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 209      |\n","|    mean_reward     | -87.2    |\n","| time/              |          |\n","|    total_timesteps | 4137600  |\n","| train/             |          |\n","|    actor_loss      | 5.46     |\n","|    critic_loss     | 11.1     |\n","|    ent_coef        | 0.00456  |\n","|    ent_coef_loss   | -3.28    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 172395   |\n","---------------------------------\n","Eval num_timesteps=4140000, episode_reward=-37.93 +/- 66.43\n","Episode length: 332.00 +/- 137.17\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 332      |\n","|    mean_reward     | -37.9    |\n","| time/              |          |\n","|    total_timesteps | 4140000  |\n","| train/             |          |\n","|    actor_loss      | 6.33     |\n","|    critic_loss     | 1.71     |\n","|    ent_coef        | 0.00468  |\n","|    ent_coef_loss   | -0.188   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 172495   |\n","---------------------------------\n","Eval num_timesteps=4142400, episode_reward=6.11 +/- 1.46\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 6.11     |\n","| time/              |          |\n","|    total_timesteps | 4142400  |\n","| train/             |          |\n","|    actor_loss      | 6.03     |\n","|    critic_loss     | 7.62     |\n","|    ent_coef        | 0.00473  |\n","|    ent_coef_loss   | 2.42     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 172595   |\n","---------------------------------\n","Eval num_timesteps=4144800, episode_reward=48.11 +/- 3.29\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 48.1     |\n","| time/              |          |\n","|    total_timesteps | 4144800  |\n","| train/             |          |\n","|    actor_loss      | 5.28     |\n","|    critic_loss     | 0.682    |\n","|    ent_coef        | 0.00497  |\n","|    ent_coef_loss   | -0.173   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 172695   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6460     |\n","|    fps             | 314      |\n","|    time_elapsed    | 13172    |\n","|    total_timesteps | 4145496  |\n","| train/             |          |\n","|    actor_loss      | 5.82     |\n","|    critic_loss     | 5.61     |\n","|    ent_coef        | 0.005    |\n","|    ent_coef_loss   | 1.23     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 172724   |\n","---------------------------------\n","Eval num_timesteps=4147200, episode_reward=40.82 +/- 2.97\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 40.8     |\n","| time/              |          |\n","|    total_timesteps | 4147200  |\n","| train/             |          |\n","|    actor_loss      | 5.76     |\n","|    critic_loss     | 14.6     |\n","|    ent_coef        | 0.00499  |\n","|    ent_coef_loss   | -1.1     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 172795   |\n","---------------------------------\n","Eval num_timesteps=4149600, episode_reward=3.94 +/- 0.24\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 3.94     |\n","| time/              |          |\n","|    total_timesteps | 4149600  |\n","| train/             |          |\n","|    actor_loss      | 6.56     |\n","|    critic_loss     | 6.26     |\n","|    ent_coef        | 0.00484  |\n","|    ent_coef_loss   | -3.1     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 172895   |\n","---------------------------------\n","Eval num_timesteps=4152000, episode_reward=8.35 +/- 10.90\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 8.35     |\n","| time/              |          |\n","|    total_timesteps | 4152000  |\n","| train/             |          |\n","|    actor_loss      | 5.67     |\n","|    critic_loss     | 0.775    |\n","|    ent_coef        | 0.00449  |\n","|    ent_coef_loss   | -2.37    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 172995   |\n","---------------------------------\n","Eval num_timesteps=4154400, episode_reward=-33.59 +/- 63.86\n","Episode length: 461.60 +/- 31.35\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 462      |\n","|    mean_reward     | -33.6    |\n","| time/              |          |\n","|    total_timesteps | 4154400  |\n","| train/             |          |\n","|    actor_loss      | 5.32     |\n","|    critic_loss     | 8.32     |\n","|    ent_coef        | 0.00436  |\n","|    ent_coef_loss   | -1.32    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 173095   |\n","---------------------------------\n","Eval num_timesteps=4156800, episode_reward=40.59 +/- 2.71\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 40.6     |\n","| time/              |          |\n","|    total_timesteps | 4156800  |\n","| train/             |          |\n","|    actor_loss      | 5.95     |\n","|    critic_loss     | 1.42     |\n","|    ent_coef        | 0.0043   |\n","|    ent_coef_loss   | -1.25    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 173195   |\n","---------------------------------\n","Eval num_timesteps=4159200, episode_reward=-24.38 +/- 67.42\n","Episode length: 348.20 +/- 123.94\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 348      |\n","|    mean_reward     | -24.4    |\n","| time/              |          |\n","|    total_timesteps | 4159200  |\n","| train/             |          |\n","|    actor_loss      | 6.03     |\n","|    critic_loss     | 1.63     |\n","|    ent_coef        | 0.00409  |\n","|    ent_coef_loss   | 2.53     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 173295   |\n","---------------------------------\n","Eval num_timesteps=4161600, episode_reward=32.98 +/- 8.03\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 33       |\n","| time/              |          |\n","|    total_timesteps | 4161600  |\n","| train/             |          |\n","|    actor_loss      | 6.41     |\n","|    critic_loss     | 37.4     |\n","|    ent_coef        | 0.00403  |\n","|    ent_coef_loss   | -1.8     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 173395   |\n","---------------------------------\n","Eval num_timesteps=4164000, episode_reward=28.24 +/- 0.08\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 28.2     |\n","| time/              |          |\n","|    total_timesteps | 4164000  |\n","| train/             |          |\n","|    actor_loss      | 6.24     |\n","|    critic_loss     | 13.3     |\n","|    ent_coef        | 0.00386  |\n","|    ent_coef_loss   | -2.27    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 173495   |\n","---------------------------------\n","Eval num_timesteps=4166400, episode_reward=-9.67 +/- 43.70\n","Episode length: 459.20 +/- 49.97\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 459      |\n","|    mean_reward     | -9.67    |\n","| time/              |          |\n","|    total_timesteps | 4166400  |\n","| train/             |          |\n","|    actor_loss      | 5.86     |\n","|    critic_loss     | 2.13     |\n","|    ent_coef        | 0.0038   |\n","|    ent_coef_loss   | -0.0771  |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 173595   |\n","---------------------------------\n","Eval num_timesteps=4168800, episode_reward=-16.41 +/- 54.62\n","Episode length: 458.60 +/- 33.80\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 459      |\n","|    mean_reward     | -16.4    |\n","| time/              |          |\n","|    total_timesteps | 4168800  |\n","| train/             |          |\n","|    actor_loss      | 5.5      |\n","|    critic_loss     | 1.1      |\n","|    ent_coef        | 0.00385  |\n","|    ent_coef_loss   | -2.78    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 173695   |\n","---------------------------------\n","Eval num_timesteps=4171200, episode_reward=-15.40 +/- 60.56\n","Episode length: 461.60 +/- 47.03\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 462      |\n","|    mean_reward     | -15.4    |\n","| time/              |          |\n","|    total_timesteps | 4171200  |\n","| train/             |          |\n","|    actor_loss      | 5.21     |\n","|    critic_loss     | 0.822    |\n","|    ent_coef        | 0.00394  |\n","|    ent_coef_loss   | 3.32     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 173795   |\n","---------------------------------\n","Eval num_timesteps=4173600, episode_reward=31.53 +/- 4.55\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 31.5     |\n","| time/              |          |\n","|    total_timesteps | 4173600  |\n","| train/             |          |\n","|    actor_loss      | 5.65     |\n","|    critic_loss     | 1.13     |\n","|    ent_coef        | 0.00389  |\n","|    ent_coef_loss   | -3.75    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 173895   |\n","---------------------------------\n","Eval num_timesteps=4176000, episode_reward=25.99 +/- 6.73\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 26       |\n","| time/              |          |\n","|    total_timesteps | 4176000  |\n","| train/             |          |\n","|    actor_loss      | 5.8      |\n","|    critic_loss     | 1.14     |\n","|    ent_coef        | 0.00374  |\n","|    ent_coef_loss   | 0.948    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 173995   |\n","---------------------------------\n","Eval num_timesteps=4178400, episode_reward=42.57 +/- 1.39\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 42.6     |\n","| time/              |          |\n","|    total_timesteps | 4178400  |\n","| train/             |          |\n","|    actor_loss      | 5.8      |\n","|    critic_loss     | 1.32     |\n","|    ent_coef        | 0.00383  |\n","|    ent_coef_loss   | -0.892   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 174095   |\n","---------------------------------\n","Eval num_timesteps=4180800, episode_reward=56.67 +/- 4.57\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 56.7     |\n","| time/              |          |\n","|    total_timesteps | 4180800  |\n","| train/             |          |\n","|    actor_loss      | 4.96     |\n","|    critic_loss     | 0.894    |\n","|    ent_coef        | 0.00398  |\n","|    ent_coef_loss   | 5.4      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 174195   |\n","---------------------------------\n","New best mean reward!\n","Eval num_timesteps=4183200, episode_reward=-23.68 +/- 48.89\n","Episode length: 442.00 +/- 71.04\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 442      |\n","|    mean_reward     | -23.7    |\n","| time/              |          |\n","|    total_timesteps | 4183200  |\n","| train/             |          |\n","|    actor_loss      | 5.37     |\n","|    critic_loss     | 0.545    |\n","|    ent_coef        | 0.00419  |\n","|    ent_coef_loss   | -0.377   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 174295   |\n","---------------------------------\n","Eval num_timesteps=4185600, episode_reward=-55.65 +/- 61.87\n","Episode length: 347.60 +/- 124.43\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 348      |\n","|    mean_reward     | -55.6    |\n","| time/              |          |\n","|    total_timesteps | 4185600  |\n","| train/             |          |\n","|    actor_loss      | 6.14     |\n","|    critic_loss     | 10.2     |\n","|    ent_coef        | 0.00427  |\n","|    ent_coef_loss   | 3.91     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 174395   |\n","---------------------------------\n","Eval num_timesteps=4188000, episode_reward=13.04 +/- 16.00\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 13       |\n","| time/              |          |\n","|    total_timesteps | 4188000  |\n","| train/             |          |\n","|    actor_loss      | 5.74     |\n","|    critic_loss     | 1.28     |\n","|    ent_coef        | 0.00445  |\n","|    ent_coef_loss   | 1.56     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 174495   |\n","---------------------------------\n","Eval num_timesteps=4190400, episode_reward=18.24 +/- 11.53\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 18.2     |\n","| time/              |          |\n","|    total_timesteps | 4190400  |\n","| train/             |          |\n","|    actor_loss      | 5.45     |\n","|    critic_loss     | 0.794    |\n","|    ent_coef        | 0.00452  |\n","|    ent_coef_loss   | 0.106    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 174595   |\n","---------------------------------\n","Eval num_timesteps=4192800, episode_reward=-25.38 +/- 50.60\n","Episode length: 443.00 +/- 46.54\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 443      |\n","|    mean_reward     | -25.4    |\n","| time/              |          |\n","|    total_timesteps | 4192800  |\n","| train/             |          |\n","|    actor_loss      | 5.16     |\n","|    critic_loss     | 15.5     |\n","|    ent_coef        | 0.00473  |\n","|    ent_coef_loss   | -2.35    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 174695   |\n","---------------------------------\n","Eval num_timesteps=4195200, episode_reward=-37.82 +/- 49.08\n","Episode length: 490.40 +/- 7.84\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 490      |\n","|    mean_reward     | -37.8    |\n","| time/              |          |\n","|    total_timesteps | 4195200  |\n","| train/             |          |\n","|    actor_loss      | 4.88     |\n","|    critic_loss     | 2.67     |\n","|    ent_coef        | 0.00472  |\n","|    ent_coef_loss   | -1.49    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 174795   |\n","---------------------------------\n","Eval num_timesteps=4197600, episode_reward=29.28 +/- 19.68\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 29.3     |\n","| time/              |          |\n","|    total_timesteps | 4197600  |\n","| train/             |          |\n","|    actor_loss      | 5.32     |\n","|    critic_loss     | 34.2     |\n","|    ent_coef        | 0.00476  |\n","|    ent_coef_loss   | -3.03    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 174895   |\n","---------------------------------\n","Eval num_timesteps=4200000, episode_reward=-35.65 +/- 54.85\n","Episode length: 426.20 +/- 60.26\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 426      |\n","|    mean_reward     | -35.7    |\n","| time/              |          |\n","|    total_timesteps | 4200000  |\n","| train/             |          |\n","|    actor_loss      | 4.74     |\n","|    critic_loss     | 0.651    |\n","|    ent_coef        | 0.00462  |\n","|    ent_coef_loss   | -2.99    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 174995   |\n","---------------------------------\n","Eval num_timesteps=4202400, episode_reward=41.95 +/- 6.34\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 41.9     |\n","| time/              |          |\n","|    total_timesteps | 4202400  |\n","| train/             |          |\n","|    actor_loss      | 5.07     |\n","|    critic_loss     | 0.754    |\n","|    ent_coef        | 0.00449  |\n","|    ent_coef_loss   | -2.66    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 175095   |\n","---------------------------------\n","Eval num_timesteps=4204800, episode_reward=43.82 +/- 4.02\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 43.8     |\n","| time/              |          |\n","|    total_timesteps | 4204800  |\n","| train/             |          |\n","|    actor_loss      | 5.51     |\n","|    critic_loss     | 14.5     |\n","|    ent_coef        | 0.00439  |\n","|    ent_coef_loss   | -2.16    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 175195   |\n","---------------------------------\n","Eval num_timesteps=4207200, episode_reward=39.29 +/- 4.50\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 39.3     |\n","| time/              |          |\n","|    total_timesteps | 4207200  |\n","| train/             |          |\n","|    actor_loss      | 5.35     |\n","|    critic_loss     | 1.02     |\n","|    ent_coef        | 0.00415  |\n","|    ent_coef_loss   | -0.602   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 175295   |\n","---------------------------------\n","Eval num_timesteps=4209600, episode_reward=29.10 +/- 1.47\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 29.1     |\n","| time/              |          |\n","|    total_timesteps | 4209600  |\n","| train/             |          |\n","|    actor_loss      | 5.96     |\n","|    critic_loss     | 1.16     |\n","|    ent_coef        | 0.00397  |\n","|    ent_coef_loss   | 0.216    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 175395   |\n","---------------------------------\n","Eval num_timesteps=4212000, episode_reward=41.75 +/- 0.55\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 41.8     |\n","| time/              |          |\n","|    total_timesteps | 4212000  |\n","| train/             |          |\n","|    actor_loss      | 5.26     |\n","|    critic_loss     | 0.88     |\n","|    ent_coef        | 0.00386  |\n","|    ent_coef_loss   | -2.63    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 175495   |\n","---------------------------------\n","Eval num_timesteps=4214400, episode_reward=-54.13 +/- 61.78\n","Episode length: 406.40 +/- 76.42\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 406      |\n","|    mean_reward     | -54.1    |\n","| time/              |          |\n","|    total_timesteps | 4214400  |\n","| train/             |          |\n","|    actor_loss      | 5.38     |\n","|    critic_loss     | 1.01     |\n","|    ent_coef        | 0.00378  |\n","|    ent_coef_loss   | 2.27     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 175595   |\n","---------------------------------\n","Eval num_timesteps=4216800, episode_reward=-54.76 +/- 53.43\n","Episode length: 329.00 +/- 139.62\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 329      |\n","|    mean_reward     | -54.8    |\n","| time/              |          |\n","|    total_timesteps | 4216800  |\n","| train/             |          |\n","|    actor_loss      | 5.25     |\n","|    critic_loss     | 0.648    |\n","|    ent_coef        | 0.00405  |\n","|    ent_coef_loss   | -0.0277  |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 175695   |\n","---------------------------------\n","Eval num_timesteps=4219200, episode_reward=-93.55 +/- 3.25\n","Episode length: 153.80 +/- 0.98\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 154      |\n","|    mean_reward     | -93.6    |\n","| time/              |          |\n","|    total_timesteps | 4219200  |\n","| train/             |          |\n","|    actor_loss      | 4.84     |\n","|    critic_loss     | 2.68     |\n","|    ent_coef        | 0.00412  |\n","|    ent_coef_loss   | 1.45     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 175795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6470     |\n","|    fps             | 314      |\n","|    time_elapsed    | 13436    |\n","|    total_timesteps | 4220112  |\n","| train/             |          |\n","|    actor_loss      | 5.3      |\n","|    critic_loss     | 0.92     |\n","|    ent_coef        | 0.00422  |\n","|    ent_coef_loss   | 6.27     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 175833   |\n","---------------------------------\n","Eval num_timesteps=4221600, episode_reward=7.97 +/- 66.40\n","Episode length: 454.40 +/- 55.85\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 454      |\n","|    mean_reward     | 7.97     |\n","| time/              |          |\n","|    total_timesteps | 4221600  |\n","| train/             |          |\n","|    actor_loss      | 5.25     |\n","|    critic_loss     | 1.5      |\n","|    ent_coef        | 0.00425  |\n","|    ent_coef_loss   | 0.148    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 175895   |\n","---------------------------------\n","Eval num_timesteps=4224000, episode_reward=25.37 +/- 0.59\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 25.4     |\n","| time/              |          |\n","|    total_timesteps | 4224000  |\n","| train/             |          |\n","|    actor_loss      | 5.17     |\n","|    critic_loss     | 5.01     |\n","|    ent_coef        | 0.00407  |\n","|    ent_coef_loss   | -3.3     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 175995   |\n","---------------------------------\n","Eval num_timesteps=4226400, episode_reward=18.87 +/- 3.06\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 18.9     |\n","| time/              |          |\n","|    total_timesteps | 4226400  |\n","| train/             |          |\n","|    actor_loss      | 4.87     |\n","|    critic_loss     | 0.517    |\n","|    ent_coef        | 0.00395  |\n","|    ent_coef_loss   | -1.68    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 176095   |\n","---------------------------------\n","Eval num_timesteps=4228800, episode_reward=28.32 +/- 8.94\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 28.3     |\n","| time/              |          |\n","|    total_timesteps | 4228800  |\n","| train/             |          |\n","|    actor_loss      | 5.92     |\n","|    critic_loss     | 1.3      |\n","|    ent_coef        | 0.00399  |\n","|    ent_coef_loss   | -2.74    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 176195   |\n","---------------------------------\n","Eval num_timesteps=4231200, episode_reward=37.33 +/- 3.04\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 37.3     |\n","| time/              |          |\n","|    total_timesteps | 4231200  |\n","| train/             |          |\n","|    actor_loss      | 5.07     |\n","|    critic_loss     | 1.13     |\n","|    ent_coef        | 0.00386  |\n","|    ent_coef_loss   | -0.077   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 176295   |\n","---------------------------------\n","Eval num_timesteps=4233600, episode_reward=-75.57 +/- 7.32\n","Episode length: 347.80 +/- 47.52\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 348      |\n","|    mean_reward     | -75.6    |\n","| time/              |          |\n","|    total_timesteps | 4233600  |\n","| train/             |          |\n","|    actor_loss      | 5.08     |\n","|    critic_loss     | 1.79     |\n","|    ent_coef        | 0.00385  |\n","|    ent_coef_loss   | 3.19     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 176395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6480     |\n","|    fps             | 314      |\n","|    time_elapsed    | 13484    |\n","|    total_timesteps | 4235304  |\n","| train/             |          |\n","|    actor_loss      | 5.09     |\n","|    critic_loss     | 1.52     |\n","|    ent_coef        | 0.00391  |\n","|    ent_coef_loss   | -1.9     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 176466   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6490     |\n","|    fps             | 314      |\n","|    time_elapsed    | 13484    |\n","|    total_timesteps | 4235856  |\n","| train/             |          |\n","|    actor_loss      | 5.42     |\n","|    critic_loss     | 1.14     |\n","|    ent_coef        | 0.00389  |\n","|    ent_coef_loss   | -0.605   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 176489   |\n","---------------------------------\n","Eval num_timesteps=4236000, episode_reward=-73.07 +/- 4.07\n","Episode length: 468.40 +/- 11.76\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 468      |\n","|    mean_reward     | -73.1    |\n","| time/              |          |\n","|    total_timesteps | 4236000  |\n","| train/             |          |\n","|    actor_loss      | 5.43     |\n","|    critic_loss     | 0.774    |\n","|    ent_coef        | 0.00388  |\n","|    ent_coef_loss   | -0.544   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 176495   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6500     |\n","|    fps             | 313      |\n","|    time_elapsed    | 13492    |\n","|    total_timesteps | 4236528  |\n","| train/             |          |\n","|    actor_loss      | 4.58     |\n","|    critic_loss     | 0.902    |\n","|    ent_coef        | 0.00387  |\n","|    ent_coef_loss   | -1.62    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 176517   |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    episodes        | 6510    |\n","|    fps             | 313     |\n","|    time_elapsed    | 13492   |\n","|    total_timesteps | 4236528 |\n","--------------------------------\n","Eval num_timesteps=4238400, episode_reward=-22.09 +/- 61.46\n","Episode length: 400.80 +/- 121.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 401      |\n","|    mean_reward     | -22.1    |\n","| time/              |          |\n","|    total_timesteps | 4238400  |\n","| train/             |          |\n","|    actor_loss      | 5.64     |\n","|    critic_loss     | 0.794    |\n","|    ent_coef        | 0.00387  |\n","|    ent_coef_loss   | -0.543   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 176595   |\n","---------------------------------\n","Eval num_timesteps=4240800, episode_reward=1.41 +/- 2.07\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.41     |\n","| time/              |          |\n","|    total_timesteps | 4240800  |\n","| train/             |          |\n","|    actor_loss      | 5.77     |\n","|    critic_loss     | 2.59     |\n","|    ent_coef        | 0.0039   |\n","|    ent_coef_loss   | 0.825    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 176695   |\n","---------------------------------\n","Eval num_timesteps=4243200, episode_reward=-102.60 +/- 12.47\n","Episode length: 235.00 +/- 68.59\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 235      |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 4243200  |\n","| train/             |          |\n","|    actor_loss      | 5.57     |\n","|    critic_loss     | 1.15     |\n","|    ent_coef        | 0.00392  |\n","|    ent_coef_loss   | 6.79     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 176795   |\n","---------------------------------\n","Eval num_timesteps=4245600, episode_reward=52.36 +/- 9.90\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 52.4     |\n","| time/              |          |\n","|    total_timesteps | 4245600  |\n","| train/             |          |\n","|    actor_loss      | 5.36     |\n","|    critic_loss     | 1.66     |\n","|    ent_coef        | 0.00393  |\n","|    ent_coef_loss   | -1.47    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 176895   |\n","---------------------------------\n","Eval num_timesteps=4248000, episode_reward=45.39 +/- 1.14\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 45.4     |\n","| time/              |          |\n","|    total_timesteps | 4248000  |\n","| train/             |          |\n","|    actor_loss      | 5.42     |\n","|    critic_loss     | 0.938    |\n","|    ent_coef        | 0.00381  |\n","|    ent_coef_loss   | -2.6     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 176995   |\n","---------------------------------\n","Eval num_timesteps=4250400, episode_reward=11.13 +/- 3.31\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 11.1     |\n","| time/              |          |\n","|    total_timesteps | 4250400  |\n","| train/             |          |\n","|    actor_loss      | 5.25     |\n","|    critic_loss     | 3.94     |\n","|    ent_coef        | 0.00368  |\n","|    ent_coef_loss   | 0.501    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 177095   |\n","---------------------------------\n","Eval num_timesteps=4252800, episode_reward=-11.29 +/- 53.12\n","Episode length: 460.80 +/- 48.01\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 461      |\n","|    mean_reward     | -11.3    |\n","| time/              |          |\n","|    total_timesteps | 4252800  |\n","| train/             |          |\n","|    actor_loss      | 5.22     |\n","|    critic_loss     | 2.6      |\n","|    ent_coef        | 0.00363  |\n","|    ent_coef_loss   | -1.98    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 177195   |\n","---------------------------------\n","Eval num_timesteps=4255200, episode_reward=-21.56 +/- 68.56\n","Episode length: 417.20 +/- 101.41\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 417      |\n","|    mean_reward     | -21.6    |\n","| time/              |          |\n","|    total_timesteps | 4255200  |\n","| train/             |          |\n","|    actor_loss      | 5.75     |\n","|    critic_loss     | 0.969    |\n","|    ent_coef        | 0.00379  |\n","|    ent_coef_loss   | -1.05    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 177295   |\n","---------------------------------\n","Eval num_timesteps=4257600, episode_reward=-36.72 +/- 67.63\n","Episode length: 440.60 +/- 48.50\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 441      |\n","|    mean_reward     | -36.7    |\n","| time/              |          |\n","|    total_timesteps | 4257600  |\n","| train/             |          |\n","|    actor_loss      | 5.53     |\n","|    critic_loss     | 1.13     |\n","|    ent_coef        | 0.00368  |\n","|    ent_coef_loss   | -1.21    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 177395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6520     |\n","|    fps             | 313      |\n","|    time_elapsed    | 13564    |\n","|    total_timesteps | 4257816  |\n","| train/             |          |\n","|    actor_loss      | 5.23     |\n","|    critic_loss     | 1.1      |\n","|    ent_coef        | 0.00367  |\n","|    ent_coef_loss   | -3.87    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 177404   |\n","---------------------------------\n","Eval num_timesteps=4260000, episode_reward=32.18 +/- 1.23\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 32.2     |\n","| time/              |          |\n","|    total_timesteps | 4260000  |\n","| train/             |          |\n","|    actor_loss      | 4.7      |\n","|    critic_loss     | 1.25     |\n","|    ent_coef        | 0.00367  |\n","|    ent_coef_loss   | 0.154    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 177495   |\n","---------------------------------\n","Eval num_timesteps=4262400, episode_reward=-32.66 +/- 54.60\n","Episode length: 468.40 +/- 38.70\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 468      |\n","|    mean_reward     | -32.7    |\n","| time/              |          |\n","|    total_timesteps | 4262400  |\n","| train/             |          |\n","|    actor_loss      | 4.81     |\n","|    critic_loss     | 1.24     |\n","|    ent_coef        | 0.00363  |\n","|    ent_coef_loss   | -4.28    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 177595   |\n","---------------------------------\n","Eval num_timesteps=4264800, episode_reward=15.91 +/- 3.32\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 15.9     |\n","| time/              |          |\n","|    total_timesteps | 4264800  |\n","| train/             |          |\n","|    actor_loss      | 5.09     |\n","|    critic_loss     | 1.23     |\n","|    ent_coef        | 0.0036   |\n","|    ent_coef_loss   | 1.57     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 177695   |\n","---------------------------------\n","Eval num_timesteps=4267200, episode_reward=25.46 +/- 0.70\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 25.5     |\n","| time/              |          |\n","|    total_timesteps | 4267200  |\n","| train/             |          |\n","|    actor_loss      | 4.64     |\n","|    critic_loss     | 0.627    |\n","|    ent_coef        | 0.00389  |\n","|    ent_coef_loss   | -1.84    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 177795   |\n","---------------------------------\n","Eval num_timesteps=4269600, episode_reward=55.61 +/- 3.04\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 55.6     |\n","| time/              |          |\n","|    total_timesteps | 4269600  |\n","| train/             |          |\n","|    actor_loss      | 5.71     |\n","|    critic_loss     | 1.47     |\n","|    ent_coef        | 0.00391  |\n","|    ent_coef_loss   | -1.41    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 177895   |\n","---------------------------------\n","Eval num_timesteps=4272000, episode_reward=-38.88 +/- 54.65\n","Episode length: 347.00 +/- 124.92\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 347      |\n","|    mean_reward     | -38.9    |\n","| time/              |          |\n","|    total_timesteps | 4272000  |\n","| train/             |          |\n","|    actor_loss      | 5.13     |\n","|    critic_loss     | 0.663    |\n","|    ent_coef        | 0.00379  |\n","|    ent_coef_loss   | -2.47    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 177995   |\n","---------------------------------\n","Eval num_timesteps=4274400, episode_reward=-12.46 +/- 63.74\n","Episode length: 430.40 +/- 85.24\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 430      |\n","|    mean_reward     | -12.5    |\n","| time/              |          |\n","|    total_timesteps | 4274400  |\n","| train/             |          |\n","|    actor_loss      | 4.98     |\n","|    critic_loss     | 0.894    |\n","|    ent_coef        | 0.00378  |\n","|    ent_coef_loss   | -2.2     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 178095   |\n","---------------------------------\n","Eval num_timesteps=4276800, episode_reward=-86.05 +/- 5.19\n","Episode length: 365.20 +/- 108.76\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 365      |\n","|    mean_reward     | -86      |\n","| time/              |          |\n","|    total_timesteps | 4276800  |\n","| train/             |          |\n","|    actor_loss      | 4.78     |\n","|    critic_loss     | 4.61     |\n","|    ent_coef        | 0.00365  |\n","|    ent_coef_loss   | -2.59    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 178195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 6530     |\n","|    fps             | 313      |\n","|    time_elapsed    | 13630    |\n","|    total_timesteps | 4277904  |\n","| train/             |          |\n","|    actor_loss      | 5.18     |\n","|    critic_loss     | 1.2      |\n","|    ent_coef        | 0.00366  |\n","|    ent_coef_loss   | -0.665   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 178241   |\n","---------------------------------\n","Eval num_timesteps=4279200, episode_reward=-13.33 +/- 68.20\n","Episode length: 348.80 +/- 185.18\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 349      |\n","|    mean_reward     | -13.3    |\n","| time/              |          |\n","|    total_timesteps | 4279200  |\n","| train/             |          |\n","|    actor_loss      | 5.27     |\n","|    critic_loss     | 13.1     |\n","|    ent_coef        | 0.00364  |\n","|    ent_coef_loss   | -0.932   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 178295   |\n","---------------------------------\n","Eval num_timesteps=4281600, episode_reward=-45.03 +/- 60.00\n","Episode length: 315.80 +/- 150.40\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 316      |\n","|    mean_reward     | -45      |\n","| time/              |          |\n","|    total_timesteps | 4281600  |\n","| train/             |          |\n","|    actor_loss      | 6.09     |\n","|    critic_loss     | 2.16     |\n","|    ent_coef        | 0.00358  |\n","|    ent_coef_loss   | -0.586   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 178395   |\n","---------------------------------\n","Eval num_timesteps=4284000, episode_reward=-51.97 +/- 56.11\n","Episode length: 442.40 +/- 70.55\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 442      |\n","|    mean_reward     | -52      |\n","| time/              |          |\n","|    total_timesteps | 4284000  |\n","| train/             |          |\n","|    actor_loss      | 4.8      |\n","|    critic_loss     | 2.83     |\n","|    ent_coef        | 0.00355  |\n","|    ent_coef_loss   | 5.74     |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 178495   |\n","---------------------------------\n","Eval num_timesteps=4286400, episode_reward=-38.54 +/- 51.33\n","Episode length: 401.20 +/- 121.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 401      |\n","|    mean_reward     | -38.5    |\n","| time/              |          |\n","|    total_timesteps | 4286400  |\n","| train/             |          |\n","|    actor_loss      | 5.1      |\n","|    critic_loss     | 13.6     |\n","|    ent_coef        | 0.00374  |\n","|    ent_coef_loss   | -0.62    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 178595   |\n","---------------------------------\n","Eval num_timesteps=4288800, episode_reward=11.15 +/- 1.47\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 11.2     |\n","| time/              |          |\n","|    total_timesteps | 4288800  |\n","| train/             |          |\n","|    actor_loss      | 4.73     |\n","|    critic_loss     | 0.46     |\n","|    ent_coef        | 0.00366  |\n","|    ent_coef_loss   | 0.409    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 178695   |\n","---------------------------------\n","Eval num_timesteps=4291200, episode_reward=7.06 +/- 0.62\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 7.06     |\n","| time/              |          |\n","|    total_timesteps | 4291200  |\n","| train/             |          |\n","|    actor_loss      | 5.32     |\n","|    critic_loss     | 16.8     |\n","|    ent_coef        | 0.00355  |\n","|    ent_coef_loss   | -0.63    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 178795   |\n","---------------------------------\n","Eval num_timesteps=4293600, episode_reward=9.39 +/- 2.95\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 9.39     |\n","| time/              |          |\n","|    total_timesteps | 4293600  |\n","| train/             |          |\n","|    actor_loss      | 5.15     |\n","|    critic_loss     | 0.907    |\n","|    ent_coef        | 0.00364  |\n","|    ent_coef_loss   | -0.251   |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 178895   |\n","---------------------------------\n","Eval num_timesteps=4296000, episode_reward=14.76 +/- 5.14\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 14.8     |\n","| time/              |          |\n","|    total_timesteps | 4296000  |\n","| train/             |          |\n","|    actor_loss      | 5.03     |\n","|    critic_loss     | 33.4     |\n","|    ent_coef        | 0.00376  |\n","|    ent_coef_loss   | 3.6      |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 178995   |\n","---------------------------------\n","Eval num_timesteps=4298400, episode_reward=18.04 +/- 2.34\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 18       |\n","| time/              |          |\n","|    total_timesteps | 4298400  |\n","| train/             |          |\n","|    actor_loss      | 4.78     |\n","|    critic_loss     | 1.46     |\n","|    ent_coef        | 0.00378  |\n","|    ent_coef_loss   | 0.996    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 179095   |\n","---------------------------------\n","Eval num_timesteps=4300800, episode_reward=17.26 +/- 7.67\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 17.3     |\n","| time/              |          |\n","|    total_timesteps | 4300800  |\n","| train/             |          |\n","|    actor_loss      | 4.8      |\n","|    critic_loss     | 20.8     |\n","|    ent_coef        | 0.00371  |\n","|    ent_coef_loss   | 0.117    |\n","|    learning_rate   | 0.0005   |\n","|    n_updates       | 179195   |\n","---------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n"]},{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-23-665a1d3a1020>\", line 10, in <cell line: 10>\n","    model.learn(total_timesteps=5000000, log_interval=10,callback=eval_callback)\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/sac/sac.py\", line 307, in learn\n","    return super().learn(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 312, in learn\n","    rollout = self.collect_rollouts(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 552, in collect_rollouts\n","    if callback.on_step() is False:\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 104, in on_step\n","    return self._on_step()\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 471, in _on_step\n","    np.savez(\n","  File \"<__array_function__ internals>\", line 180, in savez\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 615, in savez\n","    _savez(file, args, kwds, False)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 712, in _savez\n","    zipf = zipfile_factory(file, mode=\"w\", compression=compression)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 103, in zipfile_factory\n","    return zipfile.ZipFile(file, *args, **kwargs)\n","  File \"/usr/lib/python3.10/zipfile.py\", line 1251, in __init__\n","    self.fp = io.open(file, filemode)\n","OSError: [Errno 107] Transport endpoint is not connected: './multiwalker_sac2_log_eval/evaluations.npz'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'OSError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-23-665a1d3a1020>\", line 10, in <cell line: 10>\n","    model.learn(total_timesteps=5000000, log_interval=10,callback=eval_callback)\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/sac/sac.py\", line 307, in learn\n","    return super().learn(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 312, in learn\n","    rollout = self.collect_rollouts(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 552, in collect_rollouts\n","    if callback.on_step() is False:\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 104, in on_step\n","    return self._on_step()\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 471, in _on_step\n","    np.savez(\n","  File \"<__array_function__ internals>\", line 180, in savez\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 615, in savez\n","    _savez(file, args, kwds, False)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 712, in _savez\n","    zipf = zipfile_factory(file, mode=\"w\", compression=compression)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 103, in zipfile_factory\n","    return zipfile.ZipFile(file, *args, **kwargs)\n","  File \"/usr/lib/python3.10/zipfile.py\", line 1251, in __init__\n","    self.fp = io.open(file, filemode)\n","OSError: [Errno 107] Transport endpoint is not connected: './multiwalker_sac2_log_eval/evaluations.npz'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'OSError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n","    if (await self.run_code(code, result,  async_=asy)):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n","    self.showtraceback(running_compiled_code=True)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n","    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-23-665a1d3a1020>\", line 10, in <cell line: 10>\n","    model.learn(total_timesteps=5000000, log_interval=10,callback=eval_callback)\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/sac/sac.py\", line 307, in learn\n","    return super().learn(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 312, in learn\n","    rollout = self.collect_rollouts(\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/off_policy_algorithm.py\", line 552, in collect_rollouts\n","    if callback.on_step() is False:\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 104, in on_step\n","    return self._on_step()\n","  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\", line 471, in _on_step\n","    np.savez(\n","  File \"<__array_function__ internals>\", line 180, in savez\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 615, in savez\n","    _savez(file, args, kwds, False)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 712, in _savez\n","    zipf = zipfile_factory(file, mode=\"w\", compression=compression)\n","  File \"/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\", line 103, in zipfile_factory\n","    return zipfile.ZipFile(file, *args, **kwargs)\n","  File \"/usr/lib/python3.10/zipfile.py\", line 1251, in __init__\n","    self.fp = io.open(file, filemode)\n","OSError: [Errno 107] Transport endpoint is not connected: './multiwalker_sac2_log_eval/evaluations.npz'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'OSError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n","    if (await self.run_code(code, result,  async_=asy)):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n","    self.showtraceback(running_compiled_code=True)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n","    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n","    return runner(coro)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n","    coro.send(None)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n","    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3492, in run_ast_nodes\n","    self.showtraceback()\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1142, in structured_traceback\n","    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"OooX9WwN9AGO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3 import SAC\n","from pettingzoo.sisl import multiwalker_v9\n","import supersuit as ss\n","from stable_baselines3.common.monitor import Monitor"],"metadata":{"id":"5ypcPewx9AxR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697887381542,"user_tz":-120,"elapsed":59,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}},"outputId":"bcbc999c-9da8-47cf-d68d-f554b75a1956","id":"mqyCzx7d9AxS"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["'=2.13'\t\t\t\t     multiwalker_sac2_log_eval\n"," DQN_new_pettingzoo_gym_cap.ipynb    multiwalker_sac_log_eval\n"," DQN_policies\t\t\t     multiwalker_td3_log_eval\n","'Entrenamientos antiguos sin logs'   policy_log_eval\n"," Entrenamientos_log_no_eval\t     PPO_policies\n"," MCR_TFM.ipynb\t\t\t     results_rllib\n"," multi_car_racing\t\t     TFM_Multiwalker_DDPG_PPO_gym_cap.ipynb\n"," multiwalker_ddpg_log_eval\t     TFM_Multiwalker_SAC_gym_cap.ipynb\n"," multiwalker_ddpg.zip\t\t     TFM_Multiwalker_TD3_gym_cap.ipynb\n"," multiwalker_ppo_log_eval\t     TFM_PPO_new_pettingzoo_gym_cap.ipynb\n"," multiwalker_ppo.zip\t\t     TFM_PPO_pettingzoo_gym_cap.ipynb\n"]}]},{"cell_type":"code","source":["env = multiwalker_v9.parallel_env(render_mode=\"rgb_array\")\n","\n","# #---------------------------------\n","# log_dir = \"./policy_log\"\n","# os.makedirs(log_dir, exist_ok=True)\n","# env = Monitor(env, log_dir)\n","# #---------------------------------\n","# env = ss.color_reduction_v0(env, mode='B')\n","# env = ss.black_death_v3(env)\n","# env = ss.resize_v1(env, x_size=84, y_size=84)\n","env = ss.frame_stack_v1(env, 3)\n","env = ss.pettingzoo_env_to_vec_env_v1(env)\n","env = ss.concat_vec_envs_v1(env, 8, num_cpus=4, base_class='stable_baselines3')"],"metadata":{"id":"oBQ_pARy9AxT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3.common.callbacks import EvalCallback\n","eval_callback = EvalCallback(env, best_model_save_path=\"./multiwalker_sac3_log_eval/\",\n","                             log_path=\"./multiwalker_sac3_log_eval/\", eval_freq=200,\n","                             deterministic=False, render=False)\n"],"metadata":{"id":"iN3G9S1T9AxU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3.common.logger import configure\n","\n","tmp_path = \"multiwalker_sac3_log_eval/\"\n","# set up logger\n","new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n","\n","model = SAC(\"MlpPolicy\", env, verbose=3,batch_size=512, learning_rate=0.001,gamma=0.9,tau=0.01) #,train_freq=1\n","\n","model.set_logger(new_logger)\n","model.learn(total_timesteps=4000000, log_interval=10,callback=eval_callback)\n","model.save(\"multiwalker_sac3\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b893b993-ef91-439d-9184-82f54be9b7db","id":"eyrgIrJl9AxV","executionInfo":{"status":"ok","timestamp":1697895311514,"user_tz":-120,"elapsed":7925621,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Logging to multiwalker_sac3_log_eval/\n","Using cuda device\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 10       |\n","|    fps             | 238      |\n","|    time_elapsed    | 6        |\n","|    total_timesteps | 1656     |\n","| train/             |          |\n","|    actor_loss      | -4.94    |\n","|    critic_loss     | 84.9     |\n","|    ent_coef        | 0.939    |\n","|    ent_coef_loss   | -0.425   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 64       |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 20       |\n","|    fps             | 303      |\n","|    time_elapsed    | 7        |\n","|    total_timesteps | 2280     |\n","| train/             |          |\n","|    actor_loss      | -5.05    |\n","|    critic_loss     | 64.9     |\n","|    ent_coef        | 0.915    |\n","|    ent_coef_loss   | -0.597   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 90       |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 30       |\n","|    fps             | 382      |\n","|    time_elapsed    | 8        |\n","|    total_timesteps | 3168     |\n","| train/             |          |\n","|    actor_loss      | -5.36    |\n","|    critic_loss     | 84.3     |\n","|    ent_coef        | 0.882    |\n","|    ent_coef_loss   | -0.839   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 127      |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 40       |\n","|    fps             | 447      |\n","|    time_elapsed    | 9        |\n","|    total_timesteps | 4080     |\n","| train/             |          |\n","|    actor_loss      | -5.3     |\n","|    critic_loss     | 57.8     |\n","|    ent_coef        | 0.849    |\n","|    ent_coef_loss   | -1.08    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165      |\n","---------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n","|    ent_coef_loss   | -3.72    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 141597   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 14760    |\n","|    fps             | 505      |\n","|    time_elapsed    | 6736     |\n","|    total_timesteps | 3402672  |\n","| train/             |          |\n","|    actor_loss      | 2.91     |\n","|    critic_loss     | 1.32     |\n","|    ent_coef        | 0.00411  |\n","|    ent_coef_loss   | 0.511    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 141773   |\n","---------------------------------\n","Eval num_timesteps=3403200, episode_reward=-0.59 +/- 0.65\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.589   |\n","| time/              |          |\n","|    total_timesteps | 3403200  |\n","| train/             |          |\n","|    actor_loss      | 2.86     |\n","|    critic_loss     | 2.15     |\n","|    ent_coef        | 0.00408  |\n","|    ent_coef_loss   | 1.65     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 141795   |\n","---------------------------------\n","Eval num_timesteps=3408000, episode_reward=1.52 +/- 0.53\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.52     |\n","| time/              |          |\n","|    total_timesteps | 3408000  |\n","| train/             |          |\n","|    actor_loss      | 2.22     |\n","|    critic_loss     | 0.696    |\n","|    ent_coef        | 0.0044   |\n","|    ent_coef_loss   | -0.134   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 141995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 14770    |\n","|    fps             | 504      |\n","|    time_elapsed    | 6757     |\n","|    total_timesteps | 3411072  |\n","| train/             |          |\n","|    actor_loss      | 2.45     |\n","|    critic_loss     | 1.83     |\n","|    ent_coef        | 0.00415  |\n","|    ent_coef_loss   | -0.881   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 142123   |\n","---------------------------------\n","Eval num_timesteps=3412800, episode_reward=-42.09 +/- 51.75\n","Episode length: 362.40 +/- 168.52\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 362      |\n","|    mean_reward     | -42.1    |\n","| time/              |          |\n","|    total_timesteps | 3412800  |\n","| train/             |          |\n","|    actor_loss      | 2.45     |\n","|    critic_loss     | 2.31     |\n","|    ent_coef        | 0.00402  |\n","|    ent_coef_loss   | -0.406   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 142195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 14780    |\n","|    fps             | 504      |\n","|    time_elapsed    | 6770     |\n","|    total_timesteps | 3416280  |\n","| train/             |          |\n","|    actor_loss      | 1.87     |\n","|    critic_loss     | 0.389    |\n","|    ent_coef        | 0.00397  |\n","|    ent_coef_loss   | -1.97    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 142340   |\n","---------------------------------\n","Eval num_timesteps=3417600, episode_reward=-0.85 +/- 0.71\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.854   |\n","| time/              |          |\n","|    total_timesteps | 3417600  |\n","| train/             |          |\n","|    actor_loss      | 2.44     |\n","|    critic_loss     | 1.35     |\n","|    ent_coef        | 0.00395  |\n","|    ent_coef_loss   | 0.91     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 142395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 14790    |\n","|    fps             | 504      |\n","|    time_elapsed    | 6778     |\n","|    total_timesteps | 3420072  |\n","| train/             |          |\n","|    actor_loss      | 1.91     |\n","|    critic_loss     | 0.759    |\n","|    ent_coef        | 0.00402  |\n","|    ent_coef_loss   | 1.83     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 142498   |\n","---------------------------------\n","Eval num_timesteps=3422400, episode_reward=-0.68 +/- 1.74\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.683   |\n","| time/              |          |\n","|    total_timesteps | 3422400  |\n","| train/             |          |\n","|    actor_loss      | 2        |\n","|    critic_loss     | 0.649    |\n","|    ent_coef        | 0.00406  |\n","|    ent_coef_loss   | -0.633   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 142595   |\n","---------------------------------\n","Eval num_timesteps=3427200, episode_reward=-46.04 +/- 60.08\n","Episode length: 376.40 +/- 151.38\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 376      |\n","|    mean_reward     | -46      |\n","| time/              |          |\n","|    total_timesteps | 3427200  |\n","| train/             |          |\n","|    actor_loss      | 2.17     |\n","|    critic_loss     | 0.508    |\n","|    ent_coef        | 0.00422  |\n","|    ent_coef_loss   | 0.533    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 142795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 14800    |\n","|    fps             | 504      |\n","|    time_elapsed    | 6801     |\n","|    total_timesteps | 3430944  |\n","| train/             |          |\n","|    actor_loss      | 2.2      |\n","|    critic_loss     | 1.16     |\n","|    ent_coef        | 0.00446  |\n","|    ent_coef_loss   | 0.602    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 142951   |\n","---------------------------------\n","Eval num_timesteps=3432000, episode_reward=-64.74 +/- 51.69\n","Episode length: 402.80 +/- 79.36\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 403      |\n","|    mean_reward     | -64.7    |\n","| time/              |          |\n","|    total_timesteps | 3432000  |\n","| train/             |          |\n","|    actor_loss      | 2.96     |\n","|    critic_loss     | 0.649    |\n","|    ent_coef        | 0.00438  |\n","|    ent_coef_loss   | 0.283    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 142995   |\n","---------------------------------\n","Eval num_timesteps=3436800, episode_reward=-2.89 +/- 0.28\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -2.89    |\n","| time/              |          |\n","|    total_timesteps | 3436800  |\n","| train/             |          |\n","|    actor_loss      | 2.64     |\n","|    critic_loss     | 1.19     |\n","|    ent_coef        | 0.00379  |\n","|    ent_coef_loss   | -0.284   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 143195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 14810    |\n","|    fps             | 504      |\n","|    time_elapsed    | 6824     |\n","|    total_timesteps | 3440592  |\n","| train/             |          |\n","|    actor_loss      | 2.14     |\n","|    critic_loss     | 0.849    |\n","|    ent_coef        | 0.00375  |\n","|    ent_coef_loss   | -0.526   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 143353   |\n","---------------------------------\n","Eval num_timesteps=3441600, episode_reward=-105.82 +/- 0.24\n","Episode length: 206.80 +/- 15.68\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 207      |\n","|    mean_reward     | -106     |\n","| time/              |          |\n","|    total_timesteps | 3441600  |\n","| train/             |          |\n","|    actor_loss      | 2.48     |\n","|    critic_loss     | 20.5     |\n","|    ent_coef        | 0.00376  |\n","|    ent_coef_loss   | -1.53    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 143395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 14820    |\n","|    fps             | 504      |\n","|    time_elapsed    | 6831     |\n","|    total_timesteps | 3443496  |\n","| train/             |          |\n","|    actor_loss      | 2.03     |\n","|    critic_loss     | 3.05     |\n","|    ent_coef        | 0.00387  |\n","|    ent_coef_loss   | -1.67    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 143474   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 14830    |\n","|    fps             | 504      |\n","|    time_elapsed    | 6832     |\n","|    total_timesteps | 3444168  |\n","| train/             |          |\n","|    actor_loss      | 2.68     |\n","|    critic_loss     | 4.55     |\n","|    ent_coef        | 0.00383  |\n","|    ent_coef_loss   | 0.494    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 143502   |\n","---------------------------------\n","Eval num_timesteps=3446400, episode_reward=-64.47 +/- 51.33\n","Episode length: 487.40 +/- 10.29\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 487      |\n","|    mean_reward     | -64.5    |\n","| time/              |          |\n","|    total_timesteps | 3446400  |\n","| train/             |          |\n","|    actor_loss      | 2.45     |\n","|    critic_loss     | 1.73     |\n","|    ent_coef        | 0.0039   |\n","|    ent_coef_loss   | 0.661    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 143595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 14840    |\n","|    fps             | 504      |\n","|    time_elapsed    | 6842     |\n","|    total_timesteps | 3449736  |\n","| train/             |          |\n","|    actor_loss      | 2.46     |\n","|    critic_loss     | 3.81     |\n","|    ent_coef        | 0.00395  |\n","|    ent_coef_loss   | -0.615   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 143734   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 14850    |\n","|    fps             | 504      |\n","|    time_elapsed    | 6843     |\n","|    total_timesteps | 3451128  |\n","| train/             |          |\n","|    actor_loss      | 2.1      |\n","|    critic_loss     | 1.07     |\n","|    ent_coef        | 0.00389  |\n","|    ent_coef_loss   | 0.774    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 143792   |\n","---------------------------------\n","Eval num_timesteps=3451200, episode_reward=-113.91 +/- 2.41\n","Episode length: 143.20 +/- 5.88\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 143      |\n","|    mean_reward     | -114     |\n","| time/              |          |\n","|    total_timesteps | 3451200  |\n","| train/             |          |\n","|    actor_loss      | 2.34     |\n","|    critic_loss     | 20.1     |\n","|    ent_coef        | 0.0039   |\n","|    ent_coef_loss   | 0.328    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 143795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 14860    |\n","|    fps             | 504      |\n","|    time_elapsed    | 6850     |\n","|    total_timesteps | 3454080  |\n","| train/             |          |\n","|    actor_loss      | 2.26     |\n","|    critic_loss     | 21.8     |\n","|    ent_coef        | 0.00417  |\n","|    ent_coef_loss   | -0.715   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 143915   |\n","---------------------------------\n","Eval num_timesteps=3456000, episode_reward=-122.40 +/- 0.28\n","Episode length: 120.20 +/- 3.92\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 120      |\n","|    mean_reward     | -122     |\n","| time/              |          |\n","|    total_timesteps | 3456000  |\n","| train/             |          |\n","|    actor_loss      | 2.47     |\n","|    critic_loss     | 1.26     |\n","|    ent_coef        | 0.00421  |\n","|    ent_coef_loss   | 1.88     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 143995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 14870    |\n","|    fps             | 504      |\n","|    time_elapsed    | 6854     |\n","|    total_timesteps | 3456024  |\n","| train/             |          |\n","|    actor_loss      | 2.39     |\n","|    critic_loss     | 4.12     |\n","|    ent_coef        | 0.00421  |\n","|    ent_coef_loss   | 2.44     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 143996   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 14880    |\n","|    fps             | 504      |\n","|    time_elapsed    | 6856     |\n","|    total_timesteps | 3459480  |\n","| train/             |          |\n","|    actor_loss      | 2.43     |\n","|    critic_loss     | 1.08     |\n","|    ent_coef        | 0.00392  |\n","|    ent_coef_loss   | 1.02     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 144140   |\n","---------------------------------\n","Eval num_timesteps=3460800, episode_reward=-47.23 +/- 55.01\n","Episode length: 360.80 +/- 170.48\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 361      |\n","|    mean_reward     | -47.2    |\n","| time/              |          |\n","|    total_timesteps | 3460800  |\n","| train/             |          |\n","|    actor_loss      | 2.55     |\n","|    critic_loss     | 1.27     |\n","|    ent_coef        | 0.00389  |\n","|    ent_coef_loss   | 0.119    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 144195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 14890    |\n","|    fps             | 504      |\n","|    time_elapsed    | 6864     |\n","|    total_timesteps | 3462144  |\n","| train/             |          |\n","|    actor_loss      | 1.62     |\n","|    critic_loss     | 0.652    |\n","|    ent_coef        | 0.00394  |\n","|    ent_coef_loss   | -1.58    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 144251   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 14900    |\n","|    fps             | 504      |\n","|    time_elapsed    | 6865     |\n","|    total_timesteps | 3462840  |\n","| train/             |          |\n","|    actor_loss      | 2.35     |\n","|    critic_loss     | 0.866    |\n","|    ent_coef        | 0.0039   |\n","|    ent_coef_loss   | -0.672   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 144280   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 14910    |\n","|    fps             | 504      |\n","|    time_elapsed    | 6867     |\n","|    total_timesteps | 3464016  |\n","| train/             |          |\n","|    actor_loss      | 2.38     |\n","|    critic_loss     | 0.545    |\n","|    ent_coef        | 0.00368  |\n","|    ent_coef_loss   | 1.51     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 144329   |\n","---------------------------------\n","Eval num_timesteps=3465600, episode_reward=-72.28 +/- 59.56\n","Episode length: 324.20 +/- 143.54\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 324      |\n","|    mean_reward     | -72.3    |\n","| time/              |          |\n","|    total_timesteps | 3465600  |\n","| train/             |          |\n","|    actor_loss      | 2.56     |\n","|    critic_loss     | 3.36     |\n","|    ent_coef        | 0.00372  |\n","|    ent_coef_loss   | -0.991   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 144395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 14920    |\n","|    fps             | 504      |\n","|    time_elapsed    | 6877     |\n","|    total_timesteps | 3468000  |\n","| train/             |          |\n","|    actor_loss      | 2.85     |\n","|    critic_loss     | 14       |\n","|    ent_coef        | 0.00374  |\n","|    ent_coef_loss   | 1.69     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 144495   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 14930    |\n","|    fps             | 504      |\n","|    time_elapsed    | 6879     |\n","|    total_timesteps | 3469800  |\n","| train/             |          |\n","|    actor_loss      | 2.09     |\n","|    critic_loss     | 0.569    |\n","|    ent_coef        | 0.00373  |\n","|    ent_coef_loss   | -2.11    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 144570   |\n","---------------------------------\n","Eval num_timesteps=3470400, episode_reward=-116.83 +/- 12.97\n","Episode length: 170.00 +/- 24.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 170      |\n","|    mean_reward     | -117     |\n","| time/              |          |\n","|    total_timesteps | 3470400  |\n","| train/             |          |\n","|    actor_loss      | 2.28     |\n","|    critic_loss     | 0.984    |\n","|    ent_coef        | 0.00374  |\n","|    ent_coef_loss   | -1.37    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 144595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 14940    |\n","|    fps             | 504      |\n","|    time_elapsed    | 6882     |\n","|    total_timesteps | 3471912  |\n","| train/             |          |\n","|    actor_loss      | 2.47     |\n","|    critic_loss     | 1.58     |\n","|    ent_coef        | 0.00387  |\n","|    ent_coef_loss   | 0.0037   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 144658   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 14950    |\n","|    fps             | 504      |\n","|    time_elapsed    | 6885     |\n","|    total_timesteps | 3474096  |\n","| train/             |          |\n","|    actor_loss      | 2.43     |\n","|    critic_loss     | 10.8     |\n","|    ent_coef        | 0.00393  |\n","|    ent_coef_loss   | 0.588    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 144749   |\n","---------------------------------\n","Eval num_timesteps=3475200, episode_reward=-107.70 +/- 0.10\n","Episode length: 102.60 +/- 2.94\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 103      |\n","|    mean_reward     | -108     |\n","| time/              |          |\n","|    total_timesteps | 3475200  |\n","| train/             |          |\n","|    actor_loss      | 2.34     |\n","|    critic_loss     | 3.26     |\n","|    ent_coef        | 0.00395  |\n","|    ent_coef_loss   | -0.128   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 144795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 14960    |\n","|    fps             | 504      |\n","|    time_elapsed    | 6889     |\n","|    total_timesteps | 3475440  |\n","| train/             |          |\n","|    actor_loss      | 2.52     |\n","|    critic_loss     | 0.728    |\n","|    ent_coef        | 0.00396  |\n","|    ent_coef_loss   | 0.314    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 144805   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 14970    |\n","|    fps             | 504      |\n","|    time_elapsed    | 6892     |\n","|    total_timesteps | 3477648  |\n","| train/             |          |\n","|    actor_loss      | 2.73     |\n","|    critic_loss     | 1.55     |\n","|    ent_coef        | 0.00381  |\n","|    ent_coef_loss   | 0.106    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 144897   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 14980    |\n","|    fps             | 504      |\n","|    time_elapsed    | 6893     |\n","|    total_timesteps | 3478656  |\n","| train/             |          |\n","|    actor_loss      | 2.73     |\n","|    critic_loss     | 15.9     |\n","|    ent_coef        | 0.00388  |\n","|    ent_coef_loss   | 1.2      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 144939   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 14990    |\n","|    fps             | 504      |\n","|    time_elapsed    | 6894     |\n","|    total_timesteps | 3479976  |\n","| train/             |          |\n","|    actor_loss      | 2.58     |\n","|    critic_loss     | 4.47     |\n","|    ent_coef        | 0.00378  |\n","|    ent_coef_loss   | 1.71     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 144994   |\n","---------------------------------\n","Eval num_timesteps=3480000, episode_reward=-65.80 +/- 50.71\n","Episode length: 290.00 +/- 171.46\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 290      |\n","|    mean_reward     | -65.8    |\n","| time/              |          |\n","|    total_timesteps | 3480000  |\n","| train/             |          |\n","|    actor_loss      | 2.63     |\n","|    critic_loss     | 1.2      |\n","|    ent_coef        | 0.00378  |\n","|    ent_coef_loss   | 0.76     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 144995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15000    |\n","|    fps             | 504      |\n","|    time_elapsed    | 6901     |\n","|    total_timesteps | 3481512  |\n","| train/             |          |\n","|    actor_loss      | 3.05     |\n","|    critic_loss     | 0.812    |\n","|    ent_coef        | 0.00366  |\n","|    ent_coef_loss   | 0.938    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 145058   |\n","---------------------------------\n","Eval num_timesteps=3484800, episode_reward=0.52 +/- 1.44\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.521    |\n","| time/              |          |\n","|    total_timesteps | 3484800  |\n","| train/             |          |\n","|    actor_loss      | 1.86     |\n","|    critic_loss     | 0.575    |\n","|    ent_coef        | 0.00382  |\n","|    ent_coef_loss   | 0.0494   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 145195   |\n","---------------------------------\n","Eval num_timesteps=3489600, episode_reward=-45.08 +/- 57.27\n","Episode length: 407.20 +/- 113.66\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 407      |\n","|    mean_reward     | -45.1    |\n","| time/              |          |\n","|    total_timesteps | 3489600  |\n","| train/             |          |\n","|    actor_loss      | 2.82     |\n","|    critic_loss     | 5.56     |\n","|    ent_coef        | 0.00409  |\n","|    ent_coef_loss   | 0.82     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 145395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15010    |\n","|    fps             | 504      |\n","|    time_elapsed    | 6924     |\n","|    total_timesteps | 3492600  |\n","| train/             |          |\n","|    actor_loss      | 2.07     |\n","|    critic_loss     | 4.22     |\n","|    ent_coef        | 0.00421  |\n","|    ent_coef_loss   | 1.2      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 145520   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15020    |\n","|    fps             | 504      |\n","|    time_elapsed    | 6925     |\n","|    total_timesteps | 3493680  |\n","| train/             |          |\n","|    actor_loss      | 2.9      |\n","|    critic_loss     | 4.47     |\n","|    ent_coef        | 0.00414  |\n","|    ent_coef_loss   | -1.45    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 145565   |\n","---------------------------------\n","Eval num_timesteps=3494400, episode_reward=-123.21 +/- 1.47\n","Episode length: 381.80 +/- 52.91\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 382      |\n","|    mean_reward     | -123     |\n","| time/              |          |\n","|    total_timesteps | 3494400  |\n","| train/             |          |\n","|    actor_loss      | 2.14     |\n","|    critic_loss     | 1.51     |\n","|    ent_coef        | 0.00405  |\n","|    ent_coef_loss   | -1.07    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 145595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15030    |\n","|    fps             | 504      |\n","|    time_elapsed    | 6935     |\n","|    total_timesteps | 3496200  |\n","| train/             |          |\n","|    actor_loss      | 2.88     |\n","|    critic_loss     | 1.62     |\n","|    ent_coef        | 0.00396  |\n","|    ent_coef_loss   | 1.28     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 145670   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15040    |\n","|    fps             | 504      |\n","|    time_elapsed    | 6937     |\n","|    total_timesteps | 3498336  |\n","| train/             |          |\n","|    actor_loss      | 2.84     |\n","|    critic_loss     | 0.848    |\n","|    ent_coef        | 0.00395  |\n","|    ent_coef_loss   | -1.72    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 145759   |\n","---------------------------------\n","Eval num_timesteps=3499200, episode_reward=-118.80 +/- 4.46\n","Episode length: 153.60 +/- 42.13\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 154      |\n","|    mean_reward     | -119     |\n","| time/              |          |\n","|    total_timesteps | 3499200  |\n","| train/             |          |\n","|    actor_loss      | 2.29     |\n","|    critic_loss     | 21.8     |\n","|    ent_coef        | 0.00403  |\n","|    ent_coef_loss   | -0.328   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 145795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15050    |\n","|    fps             | 504      |\n","|    time_elapsed    | 6941     |\n","|    total_timesteps | 3500760  |\n","| train/             |          |\n","|    actor_loss      | 2.86     |\n","|    critic_loss     | 21.5     |\n","|    ent_coef        | 0.0041   |\n","|    ent_coef_loss   | 0.896    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 145860   |\n","---------------------------------\n","Eval num_timesteps=3504000, episode_reward=-73.15 +/- 57.83\n","Episode length: 381.20 +/- 97.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 381      |\n","|    mean_reward     | -73.2    |\n","| time/              |          |\n","|    total_timesteps | 3504000  |\n","| train/             |          |\n","|    actor_loss      | 2.43     |\n","|    critic_loss     | 1.92     |\n","|    ent_coef        | 0.00399  |\n","|    ent_coef_loss   | -0.0937  |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 145995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15060    |\n","|    fps             | 504      |\n","|    time_elapsed    | 6956     |\n","|    total_timesteps | 3508752  |\n","| train/             |          |\n","|    actor_loss      | 2.39     |\n","|    critic_loss     | 1.24     |\n","|    ent_coef        | 0.00401  |\n","|    ent_coef_loss   | 0.757    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 146193   |\n","---------------------------------\n","Eval num_timesteps=3508800, episode_reward=-3.62 +/- 2.40\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -3.62    |\n","| time/              |          |\n","|    total_timesteps | 3508800  |\n","| train/             |          |\n","|    actor_loss      | 2.45     |\n","|    critic_loss     | 0.709    |\n","|    ent_coef        | 0.00401  |\n","|    ent_coef_loss   | 0.3      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 146195   |\n","---------------------------------\n","Eval num_timesteps=3513600, episode_reward=-113.07 +/- 4.90\n","Episode length: 251.80 +/- 5.88\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 252      |\n","|    mean_reward     | -113     |\n","| time/              |          |\n","|    total_timesteps | 3513600  |\n","| train/             |          |\n","|    actor_loss      | 2.81     |\n","|    critic_loss     | 1.11     |\n","|    ent_coef        | 0.00374  |\n","|    ent_coef_loss   | 1.08     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 146395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15070    |\n","|    fps             | 504      |\n","|    time_elapsed    | 6974     |\n","|    total_timesteps | 3517128  |\n","| train/             |          |\n","|    actor_loss      | 2.56     |\n","|    critic_loss     | 0.834    |\n","|    ent_coef        | 0.00379  |\n","|    ent_coef_loss   | 0.0301   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 146542   |\n","---------------------------------\n","Eval num_timesteps=3518400, episode_reward=-0.05 +/- 0.52\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.0473  |\n","| time/              |          |\n","|    total_timesteps | 3518400  |\n","| train/             |          |\n","|    actor_loss      | 2.38     |\n","|    critic_loss     | 1.03     |\n","|    ent_coef        | 0.00376  |\n","|    ent_coef_loss   | 0.959    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 146595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15080    |\n","|    fps             | 504      |\n","|    time_elapsed    | 6983     |\n","|    total_timesteps | 3521760  |\n","| train/             |          |\n","|    actor_loss      | 2.81     |\n","|    critic_loss     | 13.2     |\n","|    ent_coef        | 0.00394  |\n","|    ent_coef_loss   | 0.865    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 146735   |\n","---------------------------------\n","Eval num_timesteps=3523200, episode_reward=-72.47 +/- 57.98\n","Episode length: 387.20 +/- 92.10\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 387      |\n","|    mean_reward     | -72.5    |\n","| time/              |          |\n","|    total_timesteps | 3523200  |\n","| train/             |          |\n","|    actor_loss      | 2.89     |\n","|    critic_loss     | 17       |\n","|    ent_coef        | 0.00424  |\n","|    ent_coef_loss   | 2.66     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 146795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15090    |\n","|    fps             | 503      |\n","|    time_elapsed    | 6994     |\n","|    total_timesteps | 3524760  |\n","| train/             |          |\n","|    actor_loss      | 2.5      |\n","|    critic_loss     | 18       |\n","|    ent_coef        | 0.0042   |\n","|    ent_coef_loss   | -0.748   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 146860   |\n","---------------------------------\n","Eval num_timesteps=3528000, episode_reward=-113.40 +/- 4.07\n","Episode length: 432.00 +/- 24.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 432      |\n","|    mean_reward     | -113     |\n","| time/              |          |\n","|    total_timesteps | 3528000  |\n","| train/             |          |\n","|    actor_loss      | 2.31     |\n","|    critic_loss     | 20.4     |\n","|    ent_coef        | 0.00408  |\n","|    ent_coef_loss   | -1.34    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 146995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15100    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7003     |\n","|    total_timesteps | 3529152  |\n","| train/             |          |\n","|    actor_loss      | 2.1      |\n","|    critic_loss     | 0.631    |\n","|    ent_coef        | 0.00405  |\n","|    ent_coef_loss   | -2.29    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 147043   |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    episodes        | 15110   |\n","|    fps             | 503     |\n","|    time_elapsed    | 7003    |\n","|    total_timesteps | 3529152 |\n","--------------------------------\n","Eval num_timesteps=3532800, episode_reward=-45.19 +/- 53.15\n","Episode length: 378.40 +/- 148.93\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 378      |\n","|    mean_reward     | -45.2    |\n","| time/              |          |\n","|    total_timesteps | 3532800  |\n","| train/             |          |\n","|    actor_loss      | 2.72     |\n","|    critic_loss     | 0.786    |\n","|    ent_coef        | 0.00392  |\n","|    ent_coef_loss   | 0.557    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 147195   |\n","---------------------------------\n","Eval num_timesteps=3537600, episode_reward=-71.39 +/- 58.27\n","Episode length: 297.20 +/- 165.59\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 297      |\n","|    mean_reward     | -71.4    |\n","| time/              |          |\n","|    total_timesteps | 3537600  |\n","| train/             |          |\n","|    actor_loss      | 2.89     |\n","|    critic_loss     | 2.7      |\n","|    ent_coef        | 0.00406  |\n","|    ent_coef_loss   | 0.835    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 147395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15120    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7024     |\n","|    total_timesteps | 3537816  |\n","| train/             |          |\n","|    actor_loss      | 2.49     |\n","|    critic_loss     | 1.75     |\n","|    ent_coef        | 0.0041   |\n","|    ent_coef_loss   | -1.48    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 147404   |\n","---------------------------------\n","Eval num_timesteps=3542400, episode_reward=-121.56 +/- 2.33\n","Episode length: 308.40 +/- 50.95\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 308      |\n","|    mean_reward     | -122     |\n","| time/              |          |\n","|    total_timesteps | 3542400  |\n","| train/             |          |\n","|    actor_loss      | 2.96     |\n","|    critic_loss     | 2.2      |\n","|    ent_coef        | 0.00374  |\n","|    ent_coef_loss   | -0.51    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 147595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15130    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7037     |\n","|    total_timesteps | 3544752  |\n","| train/             |          |\n","|    actor_loss      | 2.5      |\n","|    critic_loss     | 0.894    |\n","|    ent_coef        | 0.00352  |\n","|    ent_coef_loss   | -1.69    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 147693   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15140    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7038     |\n","|    total_timesteps | 3546000  |\n","| train/             |          |\n","|    actor_loss      | 2.81     |\n","|    critic_loss     | 8.13     |\n","|    ent_coef        | 0.00355  |\n","|    ent_coef_loss   | 0.563    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 147745   |\n","---------------------------------\n","Eval num_timesteps=3547200, episode_reward=-65.38 +/- 52.81\n","Episode length: 292.40 +/- 169.50\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 292      |\n","|    mean_reward     | -65.4    |\n","| time/              |          |\n","|    total_timesteps | 3547200  |\n","| train/             |          |\n","|    actor_loss      | 2.24     |\n","|    critic_loss     | 1.26     |\n","|    ent_coef        | 0.00351  |\n","|    ent_coef_loss   | -0.846   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 147795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15150    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7046     |\n","|    total_timesteps | 3549552  |\n","| train/             |          |\n","|    actor_loss      | 2.63     |\n","|    critic_loss     | 1.4      |\n","|    ent_coef        | 0.00353  |\n","|    ent_coef_loss   | 1.23     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 147893   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15160    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7048     |\n","|    total_timesteps | 3551544  |\n","| train/             |          |\n","|    actor_loss      | 2.26     |\n","|    critic_loss     | 3.41     |\n","|    ent_coef        | 0.00358  |\n","|    ent_coef_loss   | -0.848   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 147976   |\n","---------------------------------\n","Eval num_timesteps=3552000, episode_reward=-117.13 +/- 5.83\n","Episode length: 220.20 +/- 18.13\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 220      |\n","|    mean_reward     | -117     |\n","| time/              |          |\n","|    total_timesteps | 3552000  |\n","| train/             |          |\n","|    actor_loss      | 2.71     |\n","|    critic_loss     | 1.14     |\n","|    ent_coef        | 0.00358  |\n","|    ent_coef_loss   | 1.99     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 147995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15170    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7057     |\n","|    total_timesteps | 3554928  |\n","| train/             |          |\n","|    actor_loss      | 2.69     |\n","|    critic_loss     | 0.862    |\n","|    ent_coef        | 0.0039   |\n","|    ent_coef_loss   | 2.3      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 148117   |\n","---------------------------------\n","Eval num_timesteps=3556800, episode_reward=-48.94 +/- 48.64\n","Episode length: 447.20 +/- 64.67\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 447      |\n","|    mean_reward     | -48.9    |\n","| time/              |          |\n","|    total_timesteps | 3556800  |\n","| train/             |          |\n","|    actor_loss      | 2.96     |\n","|    critic_loss     | 19.5     |\n","|    ent_coef        | 0.00401  |\n","|    ent_coef_loss   | -1.41    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 148195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15180    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7066     |\n","|    total_timesteps | 3559464  |\n","| train/             |          |\n","|    actor_loss      | 2.44     |\n","|    critic_loss     | 21.3     |\n","|    ent_coef        | 0.0039   |\n","|    ent_coef_loss   | -1.73    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 148306   |\n","---------------------------------\n","Eval num_timesteps=3561600, episode_reward=-122.13 +/- 2.30\n","Episode length: 290.40 +/- 75.44\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 290      |\n","|    mean_reward     | -122     |\n","| time/              |          |\n","|    total_timesteps | 3561600  |\n","| train/             |          |\n","|    actor_loss      | 2.78     |\n","|    critic_loss     | 6.24     |\n","|    ent_coef        | 0.00387  |\n","|    ent_coef_loss   | -0.87    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 148395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15190    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7074     |\n","|    total_timesteps | 3562128  |\n","| train/             |          |\n","|    actor_loss      | 2.62     |\n","|    critic_loss     | 22.6     |\n","|    ent_coef        | 0.00381  |\n","|    ent_coef_loss   | -0.616   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 148417   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15200    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7077     |\n","|    total_timesteps | 3563880  |\n","| train/             |          |\n","|    actor_loss      | 2.72     |\n","|    critic_loss     | 0.922    |\n","|    ent_coef        | 0.00388  |\n","|    ent_coef_loss   | -1.39    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 148490   |\n","---------------------------------\n","Eval num_timesteps=3566400, episode_reward=-2.79 +/- 3.95\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -2.79    |\n","| time/              |          |\n","|    total_timesteps | 3566400  |\n","| train/             |          |\n","|    actor_loss      | 2.84     |\n","|    critic_loss     | 13.3     |\n","|    ent_coef        | 0.00381  |\n","|    ent_coef_loss   | 1.69     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 148595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15210    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7086     |\n","|    total_timesteps | 3568488  |\n","| train/             |          |\n","|    actor_loss      | 2.25     |\n","|    critic_loss     | 21.5     |\n","|    ent_coef        | 0.00376  |\n","|    ent_coef_loss   | -1.67    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 148682   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15220    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7086     |\n","|    total_timesteps | 3568968  |\n","| train/             |          |\n","|    actor_loss      | 2.39     |\n","|    critic_loss     | 20.9     |\n","|    ent_coef        | 0.00372  |\n","|    ent_coef_loss   | -1.91    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 148702   |\n","---------------------------------\n","Eval num_timesteps=3571200, episode_reward=-124.14 +/- 1.17\n","Episode length: 227.60 +/- 0.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 228      |\n","|    mean_reward     | -124     |\n","| time/              |          |\n","|    total_timesteps | 3571200  |\n","| train/             |          |\n","|    actor_loss      | 2.85     |\n","|    critic_loss     | 3.65     |\n","|    ent_coef        | 0.0036   |\n","|    ent_coef_loss   | -2.56    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 148795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15230    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7094     |\n","|    total_timesteps | 3572832  |\n","| train/             |          |\n","|    actor_loss      | 2.69     |\n","|    critic_loss     | 1.07     |\n","|    ent_coef        | 0.00348  |\n","|    ent_coef_loss   | 2.88     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 148863   |\n","---------------------------------\n","Eval num_timesteps=3576000, episode_reward=-4.14 +/- 0.03\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -4.14    |\n","| time/              |          |\n","|    total_timesteps | 3576000  |\n","| train/             |          |\n","|    actor_loss      | 2.88     |\n","|    critic_loss     | 16.1     |\n","|    ent_coef        | 0.00341  |\n","|    ent_coef_loss   | 0.139    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 148995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15240    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7104     |\n","|    total_timesteps | 3576600  |\n","| train/             |          |\n","|    actor_loss      | 2.66     |\n","|    critic_loss     | 14.2     |\n","|    ent_coef        | 0.00348  |\n","|    ent_coef_loss   | -1.49    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 149020   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15250    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7107     |\n","|    total_timesteps | 3580080  |\n","| train/             |          |\n","|    actor_loss      | 2.03     |\n","|    critic_loss     | 20.6     |\n","|    ent_coef        | 0.00347  |\n","|    ent_coef_loss   | 0.532    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 149165   |\n","---------------------------------\n","Eval num_timesteps=3580800, episode_reward=-115.55 +/- 7.99\n","Episode length: 176.00 +/- 24.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 176      |\n","|    mean_reward     | -116     |\n","| time/              |          |\n","|    total_timesteps | 3580800  |\n","| train/             |          |\n","|    actor_loss      | 2.15     |\n","|    critic_loss     | 1        |\n","|    ent_coef        | 0.00357  |\n","|    ent_coef_loss   | 0.699    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 149195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15260    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7111     |\n","|    total_timesteps | 3581904  |\n","| train/             |          |\n","|    actor_loss      | 2.44     |\n","|    critic_loss     | 1.07     |\n","|    ent_coef        | 0.00374  |\n","|    ent_coef_loss   | 0.932    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 149241   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15270    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7114     |\n","|    total_timesteps | 3583584  |\n","| train/             |          |\n","|    actor_loss      | 2.87     |\n","|    critic_loss     | 0.771    |\n","|    ent_coef        | 0.00401  |\n","|    ent_coef_loss   | -1.27    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 149311   |\n","---------------------------------\n","Eval num_timesteps=3585600, episode_reward=-0.39 +/- 0.70\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.395   |\n","| time/              |          |\n","|    total_timesteps | 3585600  |\n","| train/             |          |\n","|    actor_loss      | 3.3      |\n","|    critic_loss     | 1.25     |\n","|    ent_coef        | 0.00399  |\n","|    ent_coef_loss   | 0.754    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 149395   |\n","---------------------------------\n","Eval num_timesteps=3590400, episode_reward=-112.25 +/- 3.76\n","Episode length: 126.80 +/- 38.21\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 127      |\n","|    mean_reward     | -112     |\n","| time/              |          |\n","|    total_timesteps | 3590400  |\n","| train/             |          |\n","|    actor_loss      | 2.14     |\n","|    critic_loss     | 0.476    |\n","|    ent_coef        | 0.00435  |\n","|    ent_coef_loss   | -0.46    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 149595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15280    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7128     |\n","|    total_timesteps | 3590760  |\n","| train/             |          |\n","|    actor_loss      | 2.57     |\n","|    critic_loss     | 0.911    |\n","|    ent_coef        | 0.00431  |\n","|    ent_coef_loss   | -2.26    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 149610   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15290    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7131     |\n","|    total_timesteps | 3593160  |\n","| train/             |          |\n","|    actor_loss      | 2.56     |\n","|    critic_loss     | 1.18     |\n","|    ent_coef        | 0.00409  |\n","|    ent_coef_loss   | 0.114    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 149710   |\n","---------------------------------\n","Eval num_timesteps=3595200, episode_reward=-3.63 +/- 1.17\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -3.63    |\n","| time/              |          |\n","|    total_timesteps | 3595200  |\n","| train/             |          |\n","|    actor_loss      | 2.49     |\n","|    critic_loss     | 0.718    |\n","|    ent_coef        | 0.00406  |\n","|    ent_coef_loss   | -0.0432  |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 149795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15300    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7143     |\n","|    total_timesteps | 3598128  |\n","| train/             |          |\n","|    actor_loss      | 2.2      |\n","|    critic_loss     | 1.52     |\n","|    ent_coef        | 0.00397  |\n","|    ent_coef_loss   | 4.11     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 149917   |\n","---------------------------------\n","Eval num_timesteps=3600000, episode_reward=-104.70 +/- 2.02\n","Episode length: 99.20 +/- 8.33\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 99.2     |\n","|    mean_reward     | -105     |\n","| time/              |          |\n","|    total_timesteps | 3600000  |\n","| train/             |          |\n","|    actor_loss      | 2.09     |\n","|    critic_loss     | 1.24     |\n","|    ent_coef        | 0.00406  |\n","|    ent_coef_loss   | 0.793    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 149995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15310    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7146     |\n","|    total_timesteps | 3600648  |\n","| train/             |          |\n","|    actor_loss      | 2.8      |\n","|    critic_loss     | 3.63     |\n","|    ent_coef        | 0.00409  |\n","|    ent_coef_loss   | 1.31     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 150022   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15320    |\n","|    fps             | 504      |\n","|    time_elapsed    | 7149     |\n","|    total_timesteps | 3603384  |\n","| train/             |          |\n","|    actor_loss      | 2.74     |\n","|    critic_loss     | 22.5     |\n","|    ent_coef        | 0.00412  |\n","|    ent_coef_loss   | -0.385   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 150136   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15330    |\n","|    fps             | 504      |\n","|    time_elapsed    | 7150     |\n","|    total_timesteps | 3604464  |\n","| train/             |          |\n","|    actor_loss      | 3.31     |\n","|    critic_loss     | 1.81     |\n","|    ent_coef        | 0.00399  |\n","|    ent_coef_loss   | -1.94    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 150181   |\n","---------------------------------\n","Eval num_timesteps=3604800, episode_reward=-66.00 +/- 53.11\n","Episode length: 269.60 +/- 188.12\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 270      |\n","|    mean_reward     | -66      |\n","| time/              |          |\n","|    total_timesteps | 3604800  |\n","| train/             |          |\n","|    actor_loss      | 2.9      |\n","|    critic_loss     | 5.09     |\n","|    ent_coef        | 0.00394  |\n","|    ent_coef_loss   | -0.205   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 150195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15340    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7162     |\n","|    total_timesteps | 3608304  |\n","| train/             |          |\n","|    actor_loss      | 2.36     |\n","|    critic_loss     | 4.52     |\n","|    ent_coef        | 0.00398  |\n","|    ent_coef_loss   | 0.925    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 150341   |\n","---------------------------------\n","Eval num_timesteps=3609600, episode_reward=-119.54 +/- 4.04\n","Episode length: 127.80 +/- 1.47\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 128      |\n","|    mean_reward     | -120     |\n","| time/              |          |\n","|    total_timesteps | 3609600  |\n","| train/             |          |\n","|    actor_loss      | 3.85     |\n","|    critic_loss     | 8.09     |\n","|    ent_coef        | 0.00423  |\n","|    ent_coef_loss   | 2.51     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 150395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15350    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7165     |\n","|    total_timesteps | 3609768  |\n","| train/             |          |\n","|    actor_loss      | 2.03     |\n","|    critic_loss     | 3.93     |\n","|    ent_coef        | 0.00426  |\n","|    ent_coef_loss   | -1.15    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 150402   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15360    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7166     |\n","|    total_timesteps | 3610872  |\n","| train/             |          |\n","|    actor_loss      | 3.24     |\n","|    critic_loss     | 1.07     |\n","|    ent_coef        | 0.00424  |\n","|    ent_coef_loss   | 0.59     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 150448   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15370    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7167     |\n","|    total_timesteps | 3612240  |\n","| train/             |          |\n","|    actor_loss      | 3.19     |\n","|    critic_loss     | 40.9     |\n","|    ent_coef        | 0.00404  |\n","|    ent_coef_loss   | -2.49    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 150505   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15380    |\n","|    fps             | 504      |\n","|    time_elapsed    | 7168     |\n","|    total_timesteps | 3613272  |\n","| train/             |          |\n","|    actor_loss      | 2.31     |\n","|    critic_loss     | 1.04     |\n","|    ent_coef        | 0.0039   |\n","|    ent_coef_loss   | -0.356   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 150548   |\n","---------------------------------\n","Eval num_timesteps=3614400, episode_reward=-4.21 +/- 1.19\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -4.21    |\n","| time/              |          |\n","|    total_timesteps | 3614400  |\n","| train/             |          |\n","|    actor_loss      | 3.15     |\n","|    critic_loss     | 16.5     |\n","|    ent_coef        | 0.00382  |\n","|    ent_coef_loss   | 0.654    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 150595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15390    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7183     |\n","|    total_timesteps | 3619152  |\n","| train/             |          |\n","|    actor_loss      | 2.68     |\n","|    critic_loss     | 1.1      |\n","|    ent_coef        | 0.00385  |\n","|    ent_coef_loss   | -2.23    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 150793   |\n","---------------------------------\n","Eval num_timesteps=3619200, episode_reward=-0.14 +/- 0.23\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.138   |\n","| time/              |          |\n","|    total_timesteps | 3619200  |\n","| train/             |          |\n","|    actor_loss      | 2.23     |\n","|    critic_loss     | 0.824    |\n","|    ent_coef        | 0.00384  |\n","|    ent_coef_loss   | -1.34    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 150795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15400    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7190     |\n","|    total_timesteps | 3622344  |\n","| train/             |          |\n","|    actor_loss      | 2.53     |\n","|    critic_loss     | 0.807    |\n","|    ent_coef        | 0.00374  |\n","|    ent_coef_loss   | -2.43    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 150926   |\n","---------------------------------\n","Eval num_timesteps=3624000, episode_reward=-105.38 +/- 9.53\n","Episode length: 168.00 +/- 29.39\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 168      |\n","|    mean_reward     | -105     |\n","| time/              |          |\n","|    total_timesteps | 3624000  |\n","| train/             |          |\n","|    actor_loss      | 2.79     |\n","|    critic_loss     | 1.08     |\n","|    ent_coef        | 0.00377  |\n","|    ent_coef_loss   | 1.36     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 150995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15410    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7195     |\n","|    total_timesteps | 3624144  |\n","| train/             |          |\n","|    actor_loss      | 2.46     |\n","|    critic_loss     | 2.22     |\n","|    ent_coef        | 0.00381  |\n","|    ent_coef_loss   | -0.202   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 151001   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15420    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7197     |\n","|    total_timesteps | 3625488  |\n","| train/             |          |\n","|    actor_loss      | 2.73     |\n","|    critic_loss     | 1.06     |\n","|    ent_coef        | 0.00386  |\n","|    ent_coef_loss   | -0.586   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 151057   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15430    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7200     |\n","|    total_timesteps | 3627840  |\n","| train/             |          |\n","|    actor_loss      | 2.35     |\n","|    critic_loss     | 0.674    |\n","|    ent_coef        | 0.00367  |\n","|    ent_coef_loss   | 0.111    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 151155   |\n","---------------------------------\n","Eval num_timesteps=3628800, episode_reward=-104.46 +/- 2.47\n","Episode length: 159.40 +/- 118.07\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 159      |\n","|    mean_reward     | -104     |\n","| time/              |          |\n","|    total_timesteps | 3628800  |\n","| train/             |          |\n","|    actor_loss      | 3.54     |\n","|    critic_loss     | 5.85     |\n","|    ent_coef        | 0.0036   |\n","|    ent_coef_loss   | -0.74    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 151195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15440    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7206     |\n","|    total_timesteps | 3629928  |\n","| train/             |          |\n","|    actor_loss      | 2.86     |\n","|    critic_loss     | 8.37     |\n","|    ent_coef        | 0.00342  |\n","|    ent_coef_loss   | 0.762    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 151242   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15450    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7208     |\n","|    total_timesteps | 3632208  |\n","| train/             |          |\n","|    actor_loss      | 3.06     |\n","|    critic_loss     | 1.42     |\n","|    ent_coef        | 0.00345  |\n","|    ent_coef_loss   | 0.443    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 151337   |\n","---------------------------------\n","Eval num_timesteps=3633600, episode_reward=-0.04 +/- 0.31\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.041   |\n","| time/              |          |\n","|    total_timesteps | 3633600  |\n","| train/             |          |\n","|    actor_loss      | 3.12     |\n","|    critic_loss     | 1.2      |\n","|    ent_coef        | 0.00343  |\n","|    ent_coef_loss   | 1.07     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 151395   |\n","---------------------------------\n","Eval num_timesteps=3638400, episode_reward=-61.56 +/- 48.43\n","Episode length: 282.20 +/- 177.83\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 282      |\n","|    mean_reward     | -61.6    |\n","| time/              |          |\n","|    total_timesteps | 3638400  |\n","| train/             |          |\n","|    actor_loss      | 2.73     |\n","|    critic_loss     | 1.8      |\n","|    ent_coef        | 0.00321  |\n","|    ent_coef_loss   | 0.625    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 151595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15460    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7230     |\n","|    total_timesteps | 3641688  |\n","| train/             |          |\n","|    actor_loss      | 2.8      |\n","|    critic_loss     | 5.6      |\n","|    ent_coef        | 0.00345  |\n","|    ent_coef_loss   | 1.26     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 151732   |\n","---------------------------------\n","Eval num_timesteps=3643200, episode_reward=-113.31 +/- 2.71\n","Episode length: 338.00 +/- 73.48\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 338      |\n","|    mean_reward     | -113     |\n","| time/              |          |\n","|    total_timesteps | 3643200  |\n","| train/             |          |\n","|    actor_loss      | 2.86     |\n","|    critic_loss     | 0.601    |\n","|    ent_coef        | 0.00343  |\n","|    ent_coef_loss   | 0.502    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 151795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15470    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7240     |\n","|    total_timesteps | 3645648  |\n","| train/             |          |\n","|    actor_loss      | 2.52     |\n","|    critic_loss     | 0.83     |\n","|    ent_coef        | 0.00348  |\n","|    ent_coef_loss   | -1.15    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 151897   |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    episodes        | 15480   |\n","|    fps             | 503     |\n","|    time_elapsed    | 7240    |\n","|    total_timesteps | 3645648 |\n","--------------------------------\n","Eval num_timesteps=3648000, episode_reward=-62.85 +/- 49.12\n","Episode length: 257.00 +/- 198.41\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 257      |\n","|    mean_reward     | -62.8    |\n","| time/              |          |\n","|    total_timesteps | 3648000  |\n","| train/             |          |\n","|    actor_loss      | 2.66     |\n","|    critic_loss     | 8.33     |\n","|    ent_coef        | 0.00354  |\n","|    ent_coef_loss   | 0.479    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 151995   |\n","---------------------------------\n","Eval num_timesteps=3652800, episode_reward=-0.89 +/- 0.27\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.892   |\n","| time/              |          |\n","|    total_timesteps | 3652800  |\n","| train/             |          |\n","|    actor_loss      | 3.01     |\n","|    critic_loss     | 28.2     |\n","|    ent_coef        | 0.00352  |\n","|    ent_coef_loss   | 1.49     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 152195   |\n","---------------------------------\n","Eval num_timesteps=3657600, episode_reward=-3.04 +/- 0.36\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -3.04    |\n","| time/              |          |\n","|    total_timesteps | 3657600  |\n","| train/             |          |\n","|    actor_loss      | 2.5      |\n","|    critic_loss     | 6.83     |\n","|    ent_coef        | 0.00362  |\n","|    ent_coef_loss   | -0.994   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 152395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15490    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7272     |\n","|    total_timesteps | 3659496  |\n","| train/             |          |\n","|    actor_loss      | 2.52     |\n","|    critic_loss     | 1.1      |\n","|    ent_coef        | 0.00345  |\n","|    ent_coef_loss   | -1.45    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 152474   |\n","---------------------------------\n","Eval num_timesteps=3662400, episode_reward=-47.88 +/- 56.95\n","Episode length: 360.40 +/- 170.97\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 360      |\n","|    mean_reward     | -47.9    |\n","| time/              |          |\n","|    total_timesteps | 3662400  |\n","| train/             |          |\n","|    actor_loss      | 2.63     |\n","|    critic_loss     | 1.14     |\n","|    ent_coef        | 0.00332  |\n","|    ent_coef_loss   | 1.57     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 152595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15500    |\n","|    fps             | 502      |\n","|    time_elapsed    | 7285     |\n","|    total_timesteps | 3664032  |\n","| train/             |          |\n","|    actor_loss      | 3.25     |\n","|    critic_loss     | 0.701    |\n","|    ent_coef        | 0.0034   |\n","|    ent_coef_loss   | -0.189   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 152663   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15510    |\n","|    fps             | 502      |\n","|    time_elapsed    | 7286     |\n","|    total_timesteps | 3664704  |\n","| train/             |          |\n","|    actor_loss      | 3.23     |\n","|    critic_loss     | 5.69     |\n","|    ent_coef        | 0.00338  |\n","|    ent_coef_loss   | 3.36     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 152691   |\n","---------------------------------\n","Eval num_timesteps=3667200, episode_reward=-114.39 +/- 3.09\n","Episode length: 125.60 +/- 20.09\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 126      |\n","|    mean_reward     | -114     |\n","| time/              |          |\n","|    total_timesteps | 3667200  |\n","| train/             |          |\n","|    actor_loss      | 2.54     |\n","|    critic_loss     | 0.819    |\n","|    ent_coef        | 0.00335  |\n","|    ent_coef_loss   | 0.309    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 152795   |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    episodes        | 15520   |\n","|    fps             | 503     |\n","|    time_elapsed    | 7290    |\n","|    total_timesteps | 3667200 |\n","--------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15530    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7290     |\n","|    total_timesteps | 3668232  |\n","| train/             |          |\n","|    actor_loss      | 2.64     |\n","|    critic_loss     | 52.2     |\n","|    ent_coef        | 0.00343  |\n","|    ent_coef_loss   | 1.45     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 152838   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15540    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7292     |\n","|    total_timesteps | 3669648  |\n","| train/             |          |\n","|    actor_loss      | 2.22     |\n","|    critic_loss     | 0.727    |\n","|    ent_coef        | 0.0036   |\n","|    ent_coef_loss   | 1.48     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 152897   |\n","---------------------------------\n","Eval num_timesteps=3672000, episode_reward=-5.48 +/- 2.70\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -5.48    |\n","| time/              |          |\n","|    total_timesteps | 3672000  |\n","| train/             |          |\n","|    actor_loss      | 2.33     |\n","|    critic_loss     | 21.3     |\n","|    ent_coef        | 0.00385  |\n","|    ent_coef_loss   | -1.2     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 152995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15550    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7306     |\n","|    total_timesteps | 3675624  |\n","| train/             |          |\n","|    actor_loss      | 2.62     |\n","|    critic_loss     | 0.728    |\n","|    ent_coef        | 0.0038   |\n","|    ent_coef_loss   | -2.02    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 153146   |\n","---------------------------------\n","Eval num_timesteps=3676800, episode_reward=-2.23 +/- 0.95\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -2.23    |\n","| time/              |          |\n","|    total_timesteps | 3676800  |\n","| train/             |          |\n","|    actor_loss      | 2.65     |\n","|    critic_loss     | 1.4      |\n","|    ent_coef        | 0.00373  |\n","|    ent_coef_loss   | -2.58    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 153195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15560    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7315     |\n","|    total_timesteps | 3680712  |\n","| train/             |          |\n","|    actor_loss      | 3.81     |\n","|    critic_loss     | 2.24     |\n","|    ent_coef        | 0.00366  |\n","|    ent_coef_loss   | 1.92     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 153358   |\n","---------------------------------\n","Eval num_timesteps=3681600, episode_reward=-101.84 +/- 0.79\n","Episode length: 64.40 +/- 2.94\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 64.4     |\n","|    mean_reward     | -102     |\n","| time/              |          |\n","|    total_timesteps | 3681600  |\n","| train/             |          |\n","|    actor_loss      | 2.33     |\n","|    critic_loss     | 1.46     |\n","|    ent_coef        | 0.00358  |\n","|    ent_coef_loss   | -3.64    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 153395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15570    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7318     |\n","|    total_timesteps | 3681960  |\n","| train/             |          |\n","|    actor_loss      | 2.55     |\n","|    critic_loss     | 5.32     |\n","|    ent_coef        | 0.00348  |\n","|    ent_coef_loss   | -0.382   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 153410   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15580    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7320     |\n","|    total_timesteps | 3683064  |\n","| train/             |          |\n","|    actor_loss      | 2.19     |\n","|    critic_loss     | 17       |\n","|    ent_coef        | 0.00341  |\n","|    ent_coef_loss   | 0.379    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 153456   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15590    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7324     |\n","|    total_timesteps | 3686208  |\n","| train/             |          |\n","|    actor_loss      | 2.95     |\n","|    critic_loss     | 0.851    |\n","|    ent_coef        | 0.00361  |\n","|    ent_coef_loss   | 1.89     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 153587   |\n","---------------------------------\n","Eval num_timesteps=3686400, episode_reward=-103.37 +/- 0.55\n","Episode length: 88.60 +/- 19.11\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 88.6     |\n","|    mean_reward     | -103     |\n","| time/              |          |\n","|    total_timesteps | 3686400  |\n","| train/             |          |\n","|    actor_loss      | 2.31     |\n","|    critic_loss     | 0.558    |\n","|    ent_coef        | 0.00364  |\n","|    ent_coef_loss   | 0.298    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 153595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15600    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7327     |\n","|    total_timesteps | 3687648  |\n","| train/             |          |\n","|    actor_loss      | 3.02     |\n","|    critic_loss     | 26.5     |\n","|    ent_coef        | 0.00369  |\n","|    ent_coef_loss   | 2.5      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 153647   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15610    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7328     |\n","|    total_timesteps | 3688656  |\n","| train/             |          |\n","|    actor_loss      | 2.61     |\n","|    critic_loss     | 14.4     |\n","|    ent_coef        | 0.00374  |\n","|    ent_coef_loss   | 1.05     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 153689   |\n","---------------------------------\n","Eval num_timesteps=3691200, episode_reward=-109.51 +/- 3.26\n","Episode length: 216.00 +/- 7.35\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 216      |\n","|    mean_reward     | -110     |\n","| time/              |          |\n","|    total_timesteps | 3691200  |\n","| train/             |          |\n","|    actor_loss      | 3.12     |\n","|    critic_loss     | 1.1      |\n","|    ent_coef        | 0.00378  |\n","|    ent_coef_loss   | 0.145    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 153795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15620    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7333     |\n","|    total_timesteps | 3692328  |\n","| train/             |          |\n","|    actor_loss      | 2.92     |\n","|    critic_loss     | 0.872    |\n","|    ent_coef        | 0.00366  |\n","|    ent_coef_loss   | -0.353   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 153842   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15630    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7334     |\n","|    total_timesteps | 3692736  |\n","| train/             |          |\n","|    actor_loss      | 2.97     |\n","|    critic_loss     | 7.21     |\n","|    ent_coef        | 0.00364  |\n","|    ent_coef_loss   | 0.827    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 153859   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15640    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7335     |\n","|    total_timesteps | 3694752  |\n","| train/             |          |\n","|    actor_loss      | 2.88     |\n","|    critic_loss     | 22.9     |\n","|    ent_coef        | 0.00343  |\n","|    ent_coef_loss   | -2.17    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 153943   |\n","---------------------------------\n","Eval num_timesteps=3696000, episode_reward=-108.67 +/- 2.47\n","Episode length: 77.00 +/- 7.35\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 77       |\n","|    mean_reward     | -109     |\n","| time/              |          |\n","|    total_timesteps | 3696000  |\n","| train/             |          |\n","|    actor_loss      | 3.29     |\n","|    critic_loss     | 3.23     |\n","|    ent_coef        | 0.00328  |\n","|    ent_coef_loss   | -0.239   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 153995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15650    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7340     |\n","|    total_timesteps | 3696864  |\n","| train/             |          |\n","|    actor_loss      | 2.38     |\n","|    critic_loss     | 3.99     |\n","|    ent_coef        | 0.00322  |\n","|    ent_coef_loss   | 0.0765   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154031   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15660    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7341     |\n","|    total_timesteps | 3697512  |\n","| train/             |          |\n","|    actor_loss      | 3.78     |\n","|    critic_loss     | 3.36     |\n","|    ent_coef        | 0.00326  |\n","|    ent_coef_loss   | 1.69     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154058   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15670    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7343     |\n","|    total_timesteps | 3699072  |\n","| train/             |          |\n","|    actor_loss      | 3.06     |\n","|    critic_loss     | 3.61     |\n","|    ent_coef        | 0.00331  |\n","|    ent_coef_loss   | 0.0176   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154123   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15680    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7344     |\n","|    total_timesteps | 3699960  |\n","| train/             |          |\n","|    actor_loss      | 2.89     |\n","|    critic_loss     | 19.1     |\n","|    ent_coef        | 0.00331  |\n","|    ent_coef_loss   | -0.0159  |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154160   |\n","---------------------------------\n","Eval num_timesteps=3700800, episode_reward=-108.53 +/- 1.31\n","Episode length: 79.00 +/- 2.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 79       |\n","|    mean_reward     | -109     |\n","| time/              |          |\n","|    total_timesteps | 3700800  |\n","| train/             |          |\n","|    actor_loss      | 2.91     |\n","|    critic_loss     | 8.12     |\n","|    ent_coef        | 0.00336  |\n","|    ent_coef_loss   | 0.405    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15690    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7346     |\n","|    total_timesteps | 3700848  |\n","| train/             |          |\n","|    actor_loss      | 2.37     |\n","|    critic_loss     | 5.41     |\n","|    ent_coef        | 0.00336  |\n","|    ent_coef_loss   | 0.88     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154197   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15700    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7348     |\n","|    total_timesteps | 3702360  |\n","| train/             |          |\n","|    actor_loss      | 3.4      |\n","|    critic_loss     | 1.6      |\n","|    ent_coef        | 0.00346  |\n","|    ent_coef_loss   | 2.58     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154260   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15710    |\n","|    fps             | 503      |\n","|    time_elapsed    | 7348     |\n","|    total_timesteps | 3702792  |\n","| train/             |          |\n","|    actor_loss      | 3.8      |\n","|    critic_loss     | 25.7     |\n","|    ent_coef        | 0.00353  |\n","|    ent_coef_loss   | 2.66     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154278   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15720    |\n","|    fps             | 504      |\n","|    time_elapsed    | 7350     |\n","|    total_timesteps | 3704568  |\n","| train/             |          |\n","|    actor_loss      | 2.62     |\n","|    critic_loss     | 0.833    |\n","|    ent_coef        | 0.00376  |\n","|    ent_coef_loss   | -1.65    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154352   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15730    |\n","|    fps             | 504      |\n","|    time_elapsed    | 7350     |\n","|    total_timesteps | 3705360  |\n","| train/             |          |\n","|    actor_loss      | 2.06     |\n","|    critic_loss     | 12.3     |\n","|    ent_coef        | 0.00373  |\n","|    ent_coef_loss   | 0.369    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154385   |\n","---------------------------------\n","Eval num_timesteps=3705600, episode_reward=-110.91 +/- 3.42\n","Episode length: 104.40 +/- 34.78\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 104      |\n","|    mean_reward     | -111     |\n","| time/              |          |\n","|    total_timesteps | 3705600  |\n","| train/             |          |\n","|    actor_loss      | 2.96     |\n","|    critic_loss     | 21.9     |\n","|    ent_coef        | 0.00373  |\n","|    ent_coef_loss   | -0.652   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15740    |\n","|    fps             | 504      |\n","|    time_elapsed    | 7353     |\n","|    total_timesteps | 3707040  |\n","| train/             |          |\n","|    actor_loss      | 2.81     |\n","|    critic_loss     | 1.01     |\n","|    ent_coef        | 0.00365  |\n","|    ent_coef_loss   | -1.16    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154455   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15750    |\n","|    fps             | 504      |\n","|    time_elapsed    | 7353     |\n","|    total_timesteps | 3707424  |\n","| train/             |          |\n","|    actor_loss      | 3        |\n","|    critic_loss     | 1.15     |\n","|    ent_coef        | 0.00365  |\n","|    ent_coef_loss   | 3.44     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154471   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15760    |\n","|    fps             | 504      |\n","|    time_elapsed    | 7354     |\n","|    total_timesteps | 3708144  |\n","| train/             |          |\n","|    actor_loss      | 3.17     |\n","|    critic_loss     | 1.99     |\n","|    ent_coef        | 0.0037   |\n","|    ent_coef_loss   | 0.0861   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154501   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15770    |\n","|    fps             | 504      |\n","|    time_elapsed    | 7355     |\n","|    total_timesteps | 3708960  |\n","| train/             |          |\n","|    actor_loss      | 3.34     |\n","|    critic_loss     | 2        |\n","|    ent_coef        | 0.00374  |\n","|    ent_coef_loss   | 0.757    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154535   |\n","---------------------------------\n","Eval num_timesteps=3710400, episode_reward=-114.00 +/- 5.66\n","Episode length: 119.80 +/- 18.13\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 120      |\n","|    mean_reward     | -114     |\n","| time/              |          |\n","|    total_timesteps | 3710400  |\n","| train/             |          |\n","|    actor_loss      | 2.7      |\n","|    critic_loss     | 2.25     |\n","|    ent_coef        | 0.00376  |\n","|    ent_coef_loss   | -1.21    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15780    |\n","|    fps             | 504      |\n","|    time_elapsed    | 7359     |\n","|    total_timesteps | 3710496  |\n","| train/             |          |\n","|    actor_loss      | 3.14     |\n","|    critic_loss     | 3.15     |\n","|    ent_coef        | 0.00376  |\n","|    ent_coef_loss   | 1.31     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154599   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15790    |\n","|    fps             | 504      |\n","|    time_elapsed    | 7363     |\n","|    total_timesteps | 3713112  |\n","| train/             |          |\n","|    actor_loss      | 2.9      |\n","|    critic_loss     | 18.3     |\n","|    ent_coef        | 0.00378  |\n","|    ent_coef_loss   | 0.716    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154708   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15800    |\n","|    fps             | 504      |\n","|    time_elapsed    | 7364     |\n","|    total_timesteps | 3714168  |\n","| train/             |          |\n","|    actor_loss      | 3.03     |\n","|    critic_loss     | 1.88     |\n","|    ent_coef        | 0.00381  |\n","|    ent_coef_loss   | 0.352    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154752   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15810    |\n","|    fps             | 504      |\n","|    time_elapsed    | 7365     |\n","|    total_timesteps | 3714840  |\n","| train/             |          |\n","|    actor_loss      | 2.47     |\n","|    critic_loss     | 1.83     |\n","|    ent_coef        | 0.00385  |\n","|    ent_coef_loss   | -3.22    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154780   |\n","---------------------------------\n","Eval num_timesteps=3715200, episode_reward=-108.96 +/- 3.58\n","Episode length: 95.80 +/- 1.47\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 95.8     |\n","|    mean_reward     | -109     |\n","| time/              |          |\n","|    total_timesteps | 3715200  |\n","| train/             |          |\n","|    actor_loss      | 3.24     |\n","|    critic_loss     | 2.22     |\n","|    ent_coef        | 0.0038   |\n","|    ent_coef_loss   | 0.33     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15820    |\n","|    fps             | 504      |\n","|    time_elapsed    | 7369     |\n","|    total_timesteps | 3717264  |\n","| train/             |          |\n","|    actor_loss      | 2.74     |\n","|    critic_loss     | 1.91     |\n","|    ent_coef        | 0.00356  |\n","|    ent_coef_loss   | 2.55     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154881   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15830    |\n","|    fps             | 504      |\n","|    time_elapsed    | 7369     |\n","|    total_timesteps | 3717408  |\n","| train/             |          |\n","|    actor_loss      | 2.26     |\n","|    critic_loss     | 18.2     |\n","|    ent_coef        | 0.00358  |\n","|    ent_coef_loss   | 1.97     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154887   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15840    |\n","|    fps             | 504      |\n","|    time_elapsed    | 7369     |\n","|    total_timesteps | 3718104  |\n","| train/             |          |\n","|    actor_loss      | 3.19     |\n","|    critic_loss     | 10.7     |\n","|    ent_coef        | 0.0037   |\n","|    ent_coef_loss   | 0.663    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154916   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15850    |\n","|    fps             | 504      |\n","|    time_elapsed    | 7371     |\n","|    total_timesteps | 3719832  |\n","| train/             |          |\n","|    actor_loss      | 2.71     |\n","|    critic_loss     | 1.16     |\n","|    ent_coef        | 0.00385  |\n","|    ent_coef_loss   | 0.339    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154988   |\n","---------------------------------\n","Eval num_timesteps=3720000, episode_reward=-117.94 +/- 0.51\n","Episode length: 110.00 +/- 14.70\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 110      |\n","|    mean_reward     | -118     |\n","| time/              |          |\n","|    total_timesteps | 3720000  |\n","| train/             |          |\n","|    actor_loss      | 2.45     |\n","|    critic_loss     | 0.811    |\n","|    ent_coef        | 0.00387  |\n","|    ent_coef_loss   | 0.307    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 154995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15860    |\n","|    fps             | 504      |\n","|    time_elapsed    | 7374     |\n","|    total_timesteps | 3721560  |\n","| train/             |          |\n","|    actor_loss      | 3.14     |\n","|    critic_loss     | 0.874    |\n","|    ent_coef        | 0.00389  |\n","|    ent_coef_loss   | 0.603    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155060   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15870    |\n","|    fps             | 504      |\n","|    time_elapsed    | 7375     |\n","|    total_timesteps | 3722928  |\n","| train/             |          |\n","|    actor_loss      | 3.69     |\n","|    critic_loss     | 3.41     |\n","|    ent_coef        | 0.00393  |\n","|    ent_coef_loss   | 0.933    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155117   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15880    |\n","|    fps             | 504      |\n","|    time_elapsed    | 7376     |\n","|    total_timesteps | 3724584  |\n","| train/             |          |\n","|    actor_loss      | 3.93     |\n","|    critic_loss     | 1.44     |\n","|    ent_coef        | 0.00397  |\n","|    ent_coef_loss   | 0.518    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155186   |\n","---------------------------------\n","Eval num_timesteps=3724800, episode_reward=-67.10 +/- 54.14\n","Episode length: 266.00 +/- 191.06\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 266      |\n","|    mean_reward     | -67.1    |\n","| time/              |          |\n","|    total_timesteps | 3724800  |\n","| train/             |          |\n","|    actor_loss      | 3.3      |\n","|    critic_loss     | 11.4     |\n","|    ent_coef        | 0.00396  |\n","|    ent_coef_loss   | -2.03    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15890    |\n","|    fps             | 504      |\n","|    time_elapsed    | 7386     |\n","|    total_timesteps | 3725256  |\n","| train/             |          |\n","|    actor_loss      | 3.15     |\n","|    critic_loss     | 27.2     |\n","|    ent_coef        | 0.0039   |\n","|    ent_coef_loss   | -0.691   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155214   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15900    |\n","|    fps             | 504      |\n","|    time_elapsed    | 7387     |\n","|    total_timesteps | 3726384  |\n","| train/             |          |\n","|    actor_loss      | 3.6      |\n","|    critic_loss     | 5.48     |\n","|    ent_coef        | 0.00378  |\n","|    ent_coef_loss   | 2.69     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155261   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15910    |\n","|    fps             | 504      |\n","|    time_elapsed    | 7389     |\n","|    total_timesteps | 3728760  |\n","| train/             |          |\n","|    actor_loss      | 3        |\n","|    critic_loss     | 1.33     |\n","|    ent_coef        | 0.00367  |\n","|    ent_coef_loss   | 0.446    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155360   |\n","---------------------------------\n","Eval num_timesteps=3729600, episode_reward=-107.12 +/- 3.28\n","Episode length: 175.80 +/- 47.52\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 176      |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 3729600  |\n","| train/             |          |\n","|    actor_loss      | 3.03     |\n","|    critic_loss     | 1.62     |\n","|    ent_coef        | 0.0037   |\n","|    ent_coef_loss   | 2.17     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15920    |\n","|    fps             | 504      |\n","|    time_elapsed    | 7394     |\n","|    total_timesteps | 3731112  |\n","| train/             |          |\n","|    actor_loss      | 3.21     |\n","|    critic_loss     | 1.46     |\n","|    ent_coef        | 0.0037   |\n","|    ent_coef_loss   | 0.425    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155458   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15930    |\n","|    fps             | 504      |\n","|    time_elapsed    | 7395     |\n","|    total_timesteps | 3732144  |\n","| train/             |          |\n","|    actor_loss      | 2.7      |\n","|    critic_loss     | 1.81     |\n","|    ent_coef        | 0.00364  |\n","|    ent_coef_loss   | -1.09    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155501   |\n","---------------------------------\n","Eval num_timesteps=3734400, episode_reward=-113.81 +/- 1.35\n","Episode length: 96.40 +/- 6.86\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 96.4     |\n","|    mean_reward     | -114     |\n","| time/              |          |\n","|    total_timesteps | 3734400  |\n","| train/             |          |\n","|    actor_loss      | 2.73     |\n","|    critic_loss     | 18.3     |\n","|    ent_coef        | 0.00358  |\n","|    ent_coef_loss   | -0.434   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15940    |\n","|    fps             | 504      |\n","|    time_elapsed    | 7398     |\n","|    total_timesteps | 3734472  |\n","| train/             |          |\n","|    actor_loss      | 3.37     |\n","|    critic_loss     | 2.37     |\n","|    ent_coef        | 0.00357  |\n","|    ent_coef_loss   | -0.00604 |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155598   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15950    |\n","|    fps             | 504      |\n","|    time_elapsed    | 7399     |\n","|    total_timesteps | 3735360  |\n","| train/             |          |\n","|    actor_loss      | 2.59     |\n","|    critic_loss     | 11.6     |\n","|    ent_coef        | 0.00362  |\n","|    ent_coef_loss   | -0.408   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155635   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15960    |\n","|    fps             | 504      |\n","|    time_elapsed    | 7400     |\n","|    total_timesteps | 3736416  |\n","| train/             |          |\n","|    actor_loss      | 3.14     |\n","|    critic_loss     | 18.3     |\n","|    ent_coef        | 0.00368  |\n","|    ent_coef_loss   | -0.838   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155679   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15970    |\n","|    fps             | 504      |\n","|    time_elapsed    | 7402     |\n","|    total_timesteps | 3737304  |\n","| train/             |          |\n","|    actor_loss      | 2.9      |\n","|    critic_loss     | 3.77     |\n","|    ent_coef        | 0.00382  |\n","|    ent_coef_loss   | 4.59     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155716   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15980    |\n","|    fps             | 504      |\n","|    time_elapsed    | 7404     |\n","|    total_timesteps | 3738960  |\n","| train/             |          |\n","|    actor_loss      | 3.83     |\n","|    critic_loss     | 1.93     |\n","|    ent_coef        | 0.00427  |\n","|    ent_coef_loss   | 0.495    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155785   |\n","---------------------------------\n","Eval num_timesteps=3739200, episode_reward=-111.82 +/- 0.20\n","Episode length: 138.00 +/- 2.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 138      |\n","|    mean_reward     | -112     |\n","| time/              |          |\n","|    total_timesteps | 3739200  |\n","| train/             |          |\n","|    actor_loss      | 2.97     |\n","|    critic_loss     | 20.6     |\n","|    ent_coef        | 0.00429  |\n","|    ent_coef_loss   | 1.32     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 15990    |\n","|    fps             | 504      |\n","|    time_elapsed    | 7407     |\n","|    total_timesteps | 3739560  |\n","| train/             |          |\n","|    actor_loss      | 3.29     |\n","|    critic_loss     | 1.76     |\n","|    ent_coef        | 0.00428  |\n","|    ent_coef_loss   | -1.35    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155810   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16000    |\n","|    fps             | 504      |\n","|    time_elapsed    | 7408     |\n","|    total_timesteps | 3740760  |\n","| train/             |          |\n","|    actor_loss      | 3.07     |\n","|    critic_loss     | 1.41     |\n","|    ent_coef        | 0.00426  |\n","|    ent_coef_loss   | -3.23    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155860   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16010    |\n","|    fps             | 504      |\n","|    time_elapsed    | 7409     |\n","|    total_timesteps | 3741000  |\n","| train/             |          |\n","|    actor_loss      | 3.18     |\n","|    critic_loss     | 12.6     |\n","|    ent_coef        | 0.00424  |\n","|    ent_coef_loss   | -1.73    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155870   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16020    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7410     |\n","|    total_timesteps | 3742536  |\n","| train/             |          |\n","|    actor_loss      | 3.8      |\n","|    critic_loss     | 1.99     |\n","|    ent_coef        | 0.00414  |\n","|    ent_coef_loss   | 0.257    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155934   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16030    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7410     |\n","|    total_timesteps | 3742872  |\n","| train/             |          |\n","|    actor_loss      | 2.79     |\n","|    critic_loss     | 2.02     |\n","|    ent_coef        | 0.00416  |\n","|    ent_coef_loss   | 0.027    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155948   |\n","---------------------------------\n","Eval num_timesteps=3744000, episode_reward=-1.12 +/- 1.11\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.12    |\n","| time/              |          |\n","|    total_timesteps | 3744000  |\n","| train/             |          |\n","|    actor_loss      | 3.32     |\n","|    critic_loss     | 1.02     |\n","|    ent_coef        | 0.00427  |\n","|    ent_coef_loss   | 1.25     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 155995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16040    |\n","|    fps             | 504      |\n","|    time_elapsed    | 7416     |\n","|    total_timesteps | 3744480  |\n","| train/             |          |\n","|    actor_loss      | 2.58     |\n","|    critic_loss     | 3.03     |\n","|    ent_coef        | 0.00435  |\n","|    ent_coef_loss   | 1.6      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156015   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16050    |\n","|    fps             | 504      |\n","|    time_elapsed    | 7418     |\n","|    total_timesteps | 3746424  |\n","| train/             |          |\n","|    actor_loss      | 2.96     |\n","|    critic_loss     | 0.692    |\n","|    ent_coef        | 0.00458  |\n","|    ent_coef_loss   | -2.98    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156096   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16060    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7421     |\n","|    total_timesteps | 3748104  |\n","| train/             |          |\n","|    actor_loss      | 3.01     |\n","|    critic_loss     | 1.98     |\n","|    ent_coef        | 0.00435  |\n","|    ent_coef_loss   | -0.374   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156166   |\n","---------------------------------\n","Eval num_timesteps=3748800, episode_reward=-109.65 +/- 0.28\n","Episode length: 74.00 +/- 2.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 74       |\n","|    mean_reward     | -110     |\n","| time/              |          |\n","|    total_timesteps | 3748800  |\n","| train/             |          |\n","|    actor_loss      | 3.03     |\n","|    critic_loss     | 1.24     |\n","|    ent_coef        | 0.00437  |\n","|    ent_coef_loss   | -0.125   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16070    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7424     |\n","|    total_timesteps | 3749520  |\n","| train/             |          |\n","|    actor_loss      | 3.59     |\n","|    critic_loss     | 4.15     |\n","|    ent_coef        | 0.00432  |\n","|    ent_coef_loss   | -0.234   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156225   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16080    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7425     |\n","|    total_timesteps | 3750168  |\n","| train/             |          |\n","|    actor_loss      | 3.12     |\n","|    critic_loss     | 1.3      |\n","|    ent_coef        | 0.0043   |\n","|    ent_coef_loss   | 0.86     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156252   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16090    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7427     |\n","|    total_timesteps | 3751608  |\n","| train/             |          |\n","|    actor_loss      | 3.49     |\n","|    critic_loss     | 23.8     |\n","|    ent_coef        | 0.00435  |\n","|    ent_coef_loss   | -0.455   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156312   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16100    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7428     |\n","|    total_timesteps | 3752280  |\n","| train/             |          |\n","|    actor_loss      | 2.97     |\n","|    critic_loss     | 1.45     |\n","|    ent_coef        | 0.00435  |\n","|    ent_coef_loss   | -1.22    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156340   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16110    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7429     |\n","|    total_timesteps | 3753240  |\n","| train/             |          |\n","|    actor_loss      | 3.79     |\n","|    critic_loss     | 4.89     |\n","|    ent_coef        | 0.00411  |\n","|    ent_coef_loss   | -1.68    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156380   |\n","---------------------------------\n","Eval num_timesteps=3753600, episode_reward=-114.17 +/- 2.48\n","Episode length: 112.40 +/- 0.49\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 112      |\n","|    mean_reward     | -114     |\n","| time/              |          |\n","|    total_timesteps | 3753600  |\n","| train/             |          |\n","|    actor_loss      | 3.34     |\n","|    critic_loss     | 1.26     |\n","|    ent_coef        | 0.00403  |\n","|    ent_coef_loss   | -1.19    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16120    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7431     |\n","|    total_timesteps | 3754944  |\n","| train/             |          |\n","|    actor_loss      | 3.79     |\n","|    critic_loss     | 1.13     |\n","|    ent_coef        | 0.00398  |\n","|    ent_coef_loss   | 1.82     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156451   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16130    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7433     |\n","|    total_timesteps | 3757176  |\n","| train/             |          |\n","|    actor_loss      | 3.28     |\n","|    critic_loss     | 7.87     |\n","|    ent_coef        | 0.00413  |\n","|    ent_coef_loss   | -0.356   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156544   |\n","---------------------------------\n","Eval num_timesteps=3758400, episode_reward=-2.22 +/- 2.69\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -2.22    |\n","| time/              |          |\n","|    total_timesteps | 3758400  |\n","| train/             |          |\n","|    actor_loss      | 4.01     |\n","|    critic_loss     | 2.4      |\n","|    ent_coef        | 0.00411  |\n","|    ent_coef_loss   | 1.48     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16140    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7444     |\n","|    total_timesteps | 3761616  |\n","| train/             |          |\n","|    actor_loss      | 2.45     |\n","|    critic_loss     | 4.97     |\n","|    ent_coef        | 0.00403  |\n","|    ent_coef_loss   | -1.62    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156729   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16150    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7446     |\n","|    total_timesteps | 3762624  |\n","| train/             |          |\n","|    actor_loss      | 3.24     |\n","|    critic_loss     | 20.4     |\n","|    ent_coef        | 0.00404  |\n","|    ent_coef_loss   | -0.0511  |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156771   |\n","---------------------------------\n","Eval num_timesteps=3763200, episode_reward=-112.90 +/- 0.80\n","Episode length: 103.80 +/- 11.27\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 104      |\n","|    mean_reward     | -113     |\n","| time/              |          |\n","|    total_timesteps | 3763200  |\n","| train/             |          |\n","|    actor_loss      | 3.14     |\n","|    critic_loss     | 1.02     |\n","|    ent_coef        | 0.00405  |\n","|    ent_coef_loss   | -0.59    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16160    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7450     |\n","|    total_timesteps | 3765144  |\n","| train/             |          |\n","|    actor_loss      | 4.18     |\n","|    critic_loss     | 17.9     |\n","|    ent_coef        | 0.00422  |\n","|    ent_coef_loss   | 4.07     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156876   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16170    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7452     |\n","|    total_timesteps | 3767064  |\n","| train/             |          |\n","|    actor_loss      | 3.4      |\n","|    critic_loss     | 1.36     |\n","|    ent_coef        | 0.00426  |\n","|    ent_coef_loss   | -0.154   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156956   |\n","---------------------------------\n","Eval num_timesteps=3768000, episode_reward=-44.88 +/- 53.40\n","Episode length: 355.20 +/- 177.34\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 355      |\n","|    mean_reward     | -44.9    |\n","| time/              |          |\n","|    total_timesteps | 3768000  |\n","| train/             |          |\n","|    actor_loss      | 3.76     |\n","|    critic_loss     | 2.5      |\n","|    ent_coef        | 0.00421  |\n","|    ent_coef_loss   | -1.67    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 156995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16180    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7458     |\n","|    total_timesteps | 3769056  |\n","| train/             |          |\n","|    actor_loss      | 3.57     |\n","|    critic_loss     | 2.18     |\n","|    ent_coef        | 0.00408  |\n","|    ent_coef_loss   | 0.0893   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157039   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16190    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7460     |\n","|    total_timesteps | 3770520  |\n","| train/             |          |\n","|    actor_loss      | 3.24     |\n","|    critic_loss     | 3.09     |\n","|    ent_coef        | 0.00404  |\n","|    ent_coef_loss   | -0.881   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157100   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16200    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7462     |\n","|    total_timesteps | 3771648  |\n","| train/             |          |\n","|    actor_loss      | 3.65     |\n","|    critic_loss     | 23.5     |\n","|    ent_coef        | 0.00391  |\n","|    ent_coef_loss   | -0.215   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157147   |\n","---------------------------------\n","Eval num_timesteps=3772800, episode_reward=-110.64 +/- 0.46\n","Episode length: 94.80 +/- 8.82\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 94.8     |\n","|    mean_reward     | -111     |\n","| time/              |          |\n","|    total_timesteps | 3772800  |\n","| train/             |          |\n","|    actor_loss      | 2.99     |\n","|    critic_loss     | 20.1     |\n","|    ent_coef        | 0.00394  |\n","|    ent_coef_loss   | -0.333   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16210    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7466     |\n","|    total_timesteps | 3773088  |\n","| train/             |          |\n","|    actor_loss      | 3.69     |\n","|    critic_loss     | 1.69     |\n","|    ent_coef        | 0.00394  |\n","|    ent_coef_loss   | 0.797    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157207   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16220    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7468     |\n","|    total_timesteps | 3774864  |\n","| train/             |          |\n","|    actor_loss      | 3.32     |\n","|    critic_loss     | 3.62     |\n","|    ent_coef        | 0.00376  |\n","|    ent_coef_loss   | 0.841    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157281   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16230    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7468     |\n","|    total_timesteps | 3775056  |\n","| train/             |          |\n","|    actor_loss      | 3.25     |\n","|    critic_loss     | 1.12     |\n","|    ent_coef        | 0.00377  |\n","|    ent_coef_loss   | 1.53     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157289   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16240    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7470     |\n","|    total_timesteps | 3777240  |\n","| train/             |          |\n","|    actor_loss      | 3.78     |\n","|    critic_loss     | 20       |\n","|    ent_coef        | 0.00408  |\n","|    ent_coef_loss   | 1.12     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157380   |\n","---------------------------------\n","Eval num_timesteps=3777600, episode_reward=-106.07 +/- 3.64\n","Episode length: 175.60 +/- 48.50\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 176      |\n","|    mean_reward     | -106     |\n","| time/              |          |\n","|    total_timesteps | 3777600  |\n","| train/             |          |\n","|    actor_loss      | 3.38     |\n","|    critic_loss     | 1.02     |\n","|    ent_coef        | 0.00412  |\n","|    ent_coef_loss   | 2.48     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16250    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7473     |\n","|    total_timesteps | 3777720  |\n","| train/             |          |\n","|    actor_loss      | 3.68     |\n","|    critic_loss     | 7.76     |\n","|    ent_coef        | 0.00414  |\n","|    ent_coef_loss   | 1.65     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157400   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16260    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7474     |\n","|    total_timesteps | 3779016  |\n","| train/             |          |\n","|    actor_loss      | 3.42     |\n","|    critic_loss     | 19.3     |\n","|    ent_coef        | 0.00413  |\n","|    ent_coef_loss   | 0.0394   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157454   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16270    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7476     |\n","|    total_timesteps | 3781488  |\n","| train/             |          |\n","|    actor_loss      | 3.79     |\n","|    critic_loss     | 21.7     |\n","|    ent_coef        | 0.00395  |\n","|    ent_coef_loss   | -1.48    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157557   |\n","---------------------------------\n","Eval num_timesteps=3782400, episode_reward=-114.95 +/- 8.09\n","Episode length: 289.40 +/- 144.03\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 289      |\n","|    mean_reward     | -115     |\n","| time/              |          |\n","|    total_timesteps | 3782400  |\n","| train/             |          |\n","|    actor_loss      | 3.46     |\n","|    critic_loss     | 14.7     |\n","|    ent_coef        | 0.0039   |\n","|    ent_coef_loss   | 1.03     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16280    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7486     |\n","|    total_timesteps | 3784632  |\n","| train/             |          |\n","|    actor_loss      | 3.14     |\n","|    critic_loss     | 1.27     |\n","|    ent_coef        | 0.00393  |\n","|    ent_coef_loss   | -0.358   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157688   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16290    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7489     |\n","|    total_timesteps | 3786408  |\n","| train/             |          |\n","|    actor_loss      | 3.52     |\n","|    critic_loss     | 17.6     |\n","|    ent_coef        | 0.00387  |\n","|    ent_coef_loss   | 0.825    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157762   |\n","---------------------------------\n","Eval num_timesteps=3787200, episode_reward=-105.23 +/- 0.41\n","Episode length: 118.80 +/- 13.23\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 119      |\n","|    mean_reward     | -105     |\n","| time/              |          |\n","|    total_timesteps | 3787200  |\n","| train/             |          |\n","|    actor_loss      | 3.96     |\n","|    critic_loss     | 1.78     |\n","|    ent_coef        | 0.00395  |\n","|    ent_coef_loss   | 3.03     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16300    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7491     |\n","|    total_timesteps | 3788160  |\n","| train/             |          |\n","|    actor_loss      | 3.51     |\n","|    critic_loss     | 2.44     |\n","|    ent_coef        | 0.00411  |\n","|    ent_coef_loss   | 1.5      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157835   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16310    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7493     |\n","|    total_timesteps | 3789456  |\n","| train/             |          |\n","|    actor_loss      | 4.34     |\n","|    critic_loss     | 23.6     |\n","|    ent_coef        | 0.00437  |\n","|    ent_coef_loss   | 1.88     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157889   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16320    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7494     |\n","|    total_timesteps | 3790704  |\n","| train/             |          |\n","|    actor_loss      | 3.13     |\n","|    critic_loss     | 24.6     |\n","|    ent_coef        | 0.00453  |\n","|    ent_coef_loss   | 0.458    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157941   |\n","---------------------------------\n","Eval num_timesteps=3792000, episode_reward=-62.13 +/- 50.95\n","Episode length: 336.80 +/- 133.25\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 337      |\n","|    mean_reward     | -62.1    |\n","| time/              |          |\n","|    total_timesteps | 3792000  |\n","| train/             |          |\n","|    actor_loss      | 3.41     |\n","|    critic_loss     | 0.929    |\n","|    ent_coef        | 0.00458  |\n","|    ent_coef_loss   | -0.283   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 157995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16330    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7505     |\n","|    total_timesteps | 3795432  |\n","| train/             |          |\n","|    actor_loss      | 3.72     |\n","|    critic_loss     | 20.9     |\n","|    ent_coef        | 0.00504  |\n","|    ent_coef_loss   | 0.733    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158138   |\n","---------------------------------\n","Eval num_timesteps=3796800, episode_reward=-0.28 +/- 0.64\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.283   |\n","| time/              |          |\n","|    total_timesteps | 3796800  |\n","| train/             |          |\n","|    actor_loss      | 3.53     |\n","|    critic_loss     | 3.16     |\n","|    ent_coef        | 0.00503  |\n","|    ent_coef_loss   | -0.693   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16340    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7515     |\n","|    total_timesteps | 3799488  |\n","| train/             |          |\n","|    actor_loss      | 4.22     |\n","|    critic_loss     | 26.3     |\n","|    ent_coef        | 0.00486  |\n","|    ent_coef_loss   | -0.117   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158307   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16350    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7515     |\n","|    total_timesteps | 3799656  |\n","| train/             |          |\n","|    actor_loss      | 3.72     |\n","|    critic_loss     | 2.08     |\n","|    ent_coef        | 0.00486  |\n","|    ent_coef_loss   | 0.576    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158314   |\n","---------------------------------\n","Eval num_timesteps=3801600, episode_reward=-106.15 +/- 2.15\n","Episode length: 196.20 +/- 40.17\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 196      |\n","|    mean_reward     | -106     |\n","| time/              |          |\n","|    total_timesteps | 3801600  |\n","| train/             |          |\n","|    actor_loss      | 2.79     |\n","|    critic_loss     | 0.989    |\n","|    ent_coef        | 0.005    |\n","|    ent_coef_loss   | -0.555   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16360    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7520     |\n","|    total_timesteps | 3802992  |\n","| train/             |          |\n","|    actor_loss      | 3.79     |\n","|    critic_loss     | 6.62     |\n","|    ent_coef        | 0.00526  |\n","|    ent_coef_loss   | 0.948    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158453   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16370    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7521     |\n","|    total_timesteps | 3803664  |\n","| train/             |          |\n","|    actor_loss      | 3.47     |\n","|    critic_loss     | 1.57     |\n","|    ent_coef        | 0.0053   |\n","|    ent_coef_loss   | 1.35     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158481   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16380    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7523     |\n","|    total_timesteps | 3805128  |\n","| train/             |          |\n","|    actor_loss      | 2.53     |\n","|    critic_loss     | 39.7     |\n","|    ent_coef        | 0.00566  |\n","|    ent_coef_loss   | -2.33    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158542   |\n","---------------------------------\n","Eval num_timesteps=3806400, episode_reward=-65.62 +/- 55.09\n","Episode length: 464.00 +/- 29.39\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 464      |\n","|    mean_reward     | -65.6    |\n","| time/              |          |\n","|    total_timesteps | 3806400  |\n","| train/             |          |\n","|    actor_loss      | 3.82     |\n","|    critic_loss     | 1.94     |\n","|    ent_coef        | 0.00566  |\n","|    ent_coef_loss   | -0.535   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16390    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7532     |\n","|    total_timesteps | 3806832  |\n","| train/             |          |\n","|    actor_loss      | 3.97     |\n","|    critic_loss     | 4.6      |\n","|    ent_coef        | 0.00554  |\n","|    ent_coef_loss   | 1.11     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158613   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16400    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7534     |\n","|    total_timesteps | 3809016  |\n","| train/             |          |\n","|    actor_loss      | 3.6      |\n","|    critic_loss     | 41.1     |\n","|    ent_coef        | 0.00546  |\n","|    ent_coef_loss   | -2.77    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158704   |\n","---------------------------------\n","Eval num_timesteps=3811200, episode_reward=-0.66 +/- 0.17\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.66    |\n","| time/              |          |\n","|    total_timesteps | 3811200  |\n","| train/             |          |\n","|    actor_loss      | 3.96     |\n","|    critic_loss     | 4.66     |\n","|    ent_coef        | 0.0055   |\n","|    ent_coef_loss   | -0.194   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158795   |\n","---------------------------------\n","--------------------------------\n","| time/              |         |\n","|    episodes        | 16410   |\n","|    fps             | 505     |\n","|    time_elapsed    | 7541    |\n","|    total_timesteps | 3811200 |\n","--------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16420    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7546     |\n","|    total_timesteps | 3815016  |\n","| train/             |          |\n","|    actor_loss      | 2.95     |\n","|    critic_loss     | 2.11     |\n","|    ent_coef        | 0.00507  |\n","|    ent_coef_loss   | 2.47     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158954   |\n","---------------------------------\n","Eval num_timesteps=3816000, episode_reward=-64.35 +/- 52.17\n","Episode length: 267.80 +/- 189.59\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 268      |\n","|    mean_reward     | -64.3    |\n","| time/              |          |\n","|    total_timesteps | 3816000  |\n","| train/             |          |\n","|    actor_loss      | 3.46     |\n","|    critic_loss     | 17.8     |\n","|    ent_coef        | 0.00527  |\n","|    ent_coef_loss   | 2.1      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 158995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16430    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7555     |\n","|    total_timesteps | 3818208  |\n","| train/             |          |\n","|    actor_loss      | 3.9      |\n","|    critic_loss     | 13.8     |\n","|    ent_coef        | 0.00551  |\n","|    ent_coef_loss   | 0.933    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159087   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16440    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7556     |\n","|    total_timesteps | 3819792  |\n","| train/             |          |\n","|    actor_loss      | 3.72     |\n","|    critic_loss     | 3.73     |\n","|    ent_coef        | 0.00539  |\n","|    ent_coef_loss   | -0.627   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159153   |\n","---------------------------------\n","Eval num_timesteps=3820800, episode_reward=-116.08 +/- 4.65\n","Episode length: 131.00 +/- 22.05\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 131      |\n","|    mean_reward     | -116     |\n","| time/              |          |\n","|    total_timesteps | 3820800  |\n","| train/             |          |\n","|    actor_loss      | 3.31     |\n","|    critic_loss     | 1.39     |\n","|    ent_coef        | 0.00529  |\n","|    ent_coef_loss   | -1.72    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16450    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7560     |\n","|    total_timesteps | 3822048  |\n","| train/             |          |\n","|    actor_loss      | 3.04     |\n","|    critic_loss     | 8.35     |\n","|    ent_coef        | 0.00519  |\n","|    ent_coef_loss   | 0.154    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159247   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16460    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7561     |\n","|    total_timesteps | 3823344  |\n","| train/             |          |\n","|    actor_loss      | 3.97     |\n","|    critic_loss     | 3.56     |\n","|    ent_coef        | 0.00528  |\n","|    ent_coef_loss   | 0.663    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159301   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16470    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7564     |\n","|    total_timesteps | 3825024  |\n","| train/             |          |\n","|    actor_loss      | 3.79     |\n","|    critic_loss     | 7.23     |\n","|    ent_coef        | 0.00513  |\n","|    ent_coef_loss   | -1.05    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159371   |\n","---------------------------------\n","Eval num_timesteps=3825600, episode_reward=-111.79 +/- 5.01\n","Episode length: 144.20 +/- 30.86\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 144      |\n","|    mean_reward     | -112     |\n","| time/              |          |\n","|    total_timesteps | 3825600  |\n","| train/             |          |\n","|    actor_loss      | 3        |\n","|    critic_loss     | 1.98     |\n","|    ent_coef        | 0.00505  |\n","|    ent_coef_loss   | -2.8     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16480    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7569     |\n","|    total_timesteps | 3827136  |\n","| train/             |          |\n","|    actor_loss      | 3.63     |\n","|    critic_loss     | 33.3     |\n","|    ent_coef        | 0.00506  |\n","|    ent_coef_loss   | 1.07     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159459   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16490    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7573     |\n","|    total_timesteps | 3830328  |\n","| train/             |          |\n","|    actor_loss      | 3.5      |\n","|    critic_loss     | 1.66     |\n","|    ent_coef        | 0.00483  |\n","|    ent_coef_loss   | -2.01    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159592   |\n","---------------------------------\n","Eval num_timesteps=3830400, episode_reward=-107.49 +/- 1.41\n","Episode length: 157.40 +/- 43.60\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 157      |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 3830400  |\n","| train/             |          |\n","|    actor_loss      | 4.01     |\n","|    critic_loss     | 2.32     |\n","|    ent_coef        | 0.00481  |\n","|    ent_coef_loss   | -0.87    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16500    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7576     |\n","|    total_timesteps | 3831672  |\n","| train/             |          |\n","|    actor_loss      | 3.69     |\n","|    critic_loss     | 1.12     |\n","|    ent_coef        | 0.00486  |\n","|    ent_coef_loss   | -0.824   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159648   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16510    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7577     |\n","|    total_timesteps | 3832776  |\n","| train/             |          |\n","|    actor_loss      | 3.64     |\n","|    critic_loss     | 1.3      |\n","|    ent_coef        | 0.00495  |\n","|    ent_coef_loss   | 1.68     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159694   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16520    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7578     |\n","|    total_timesteps | 3834240  |\n","| train/             |          |\n","|    actor_loss      | 3.7      |\n","|    critic_loss     | 13.7     |\n","|    ent_coef        | 0.00491  |\n","|    ent_coef_loss   | -0.76    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159755   |\n","---------------------------------\n","Eval num_timesteps=3835200, episode_reward=-111.49 +/- 6.94\n","Episode length: 110.80 +/- 23.03\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 111      |\n","|    mean_reward     | -111     |\n","| time/              |          |\n","|    total_timesteps | 3835200  |\n","| train/             |          |\n","|    actor_loss      | 4.69     |\n","|    critic_loss     | 4.44     |\n","|    ent_coef        | 0.00484  |\n","|    ent_coef_loss   | 1.67     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16530    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7580     |\n","|    total_timesteps | 3835272  |\n","| train/             |          |\n","|    actor_loss      | 3.09     |\n","|    critic_loss     | 0.974    |\n","|    ent_coef        | 0.00484  |\n","|    ent_coef_loss   | -3.05    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159798   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16540    |\n","|    fps             | 506      |\n","|    time_elapsed    | 7582     |\n","|    total_timesteps | 3836880  |\n","| train/             |          |\n","|    actor_loss      | 3.98     |\n","|    critic_loss     | 9.9      |\n","|    ent_coef        | 0.00474  |\n","|    ent_coef_loss   | 1.09     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159865   |\n","---------------------------------\n","Eval num_timesteps=3840000, episode_reward=-42.86 +/- 52.76\n","Episode length: 400.00 +/- 122.47\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 400      |\n","|    mean_reward     | -42.9    |\n","| time/              |          |\n","|    total_timesteps | 3840000  |\n","| train/             |          |\n","|    actor_loss      | 3.41     |\n","|    critic_loss     | 1.21     |\n","|    ent_coef        | 0.00529  |\n","|    ent_coef_loss   | -0.489   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 159995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16550    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7593     |\n","|    total_timesteps | 3840144  |\n","| train/             |          |\n","|    actor_loss      | 3.54     |\n","|    critic_loss     | 8.22     |\n","|    ent_coef        | 0.00533  |\n","|    ent_coef_loss   | -1.31    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160001   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16560    |\n","|    fps             | 506      |\n","|    time_elapsed    | 7597     |\n","|    total_timesteps | 3844656  |\n","| train/             |          |\n","|    actor_loss      | 3.82     |\n","|    critic_loss     | 22.5     |\n","|    ent_coef        | 0.00526  |\n","|    ent_coef_loss   | 1.1      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160189   |\n","---------------------------------\n","Eval num_timesteps=3844800, episode_reward=-49.06 +/- 60.41\n","Episode length: 356.00 +/- 176.36\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 356      |\n","|    mean_reward     | -49.1    |\n","| time/              |          |\n","|    total_timesteps | 3844800  |\n","| train/             |          |\n","|    actor_loss      | 3.59     |\n","|    critic_loss     | 9.84     |\n","|    ent_coef        | 0.00529  |\n","|    ent_coef_loss   | 0.292    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16570    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7606     |\n","|    total_timesteps | 3847488  |\n","| train/             |          |\n","|    actor_loss      | 3.36     |\n","|    critic_loss     | 1.43     |\n","|    ent_coef        | 0.00494  |\n","|    ent_coef_loss   | -1.08    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160307   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16580    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7608     |\n","|    total_timesteps | 3849432  |\n","| train/             |          |\n","|    actor_loss      | 3.35     |\n","|    critic_loss     | 1.04     |\n","|    ent_coef        | 0.00475  |\n","|    ent_coef_loss   | -0.85    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160388   |\n","---------------------------------\n","Eval num_timesteps=3849600, episode_reward=-0.47 +/- 0.56\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.474   |\n","| time/              |          |\n","|    total_timesteps | 3849600  |\n","| train/             |          |\n","|    actor_loss      | 3.81     |\n","|    critic_loss     | 1.71     |\n","|    ent_coef        | 0.00474  |\n","|    ent_coef_loss   | 0.0207   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16590    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7617     |\n","|    total_timesteps | 3852744  |\n","| train/             |          |\n","|    actor_loss      | 3.69     |\n","|    critic_loss     | 2.81     |\n","|    ent_coef        | 0.00475  |\n","|    ent_coef_loss   | -0.00101 |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160526   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16600    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7619     |\n","|    total_timesteps | 3854256  |\n","| train/             |          |\n","|    actor_loss      | 3.54     |\n","|    critic_loss     | 48.2     |\n","|    ent_coef        | 0.00454  |\n","|    ent_coef_loss   | 0.371    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160589   |\n","---------------------------------\n","Eval num_timesteps=3854400, episode_reward=-107.41 +/- 0.09\n","Episode length: 216.40 +/- 28.90\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 216      |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 3854400  |\n","| train/             |          |\n","|    actor_loss      | 3.56     |\n","|    critic_loss     | 1.51     |\n","|    ent_coef        | 0.00454  |\n","|    ent_coef_loss   | 0.0966   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16610    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7622     |\n","|    total_timesteps | 3855168  |\n","| train/             |          |\n","|    actor_loss      | 3.93     |\n","|    critic_loss     | 1.63     |\n","|    ent_coef        | 0.00463  |\n","|    ent_coef_loss   | 1.35     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160627   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16620    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7623     |\n","|    total_timesteps | 3855672  |\n","| train/             |          |\n","|    actor_loss      | 3.41     |\n","|    critic_loss     | 1.22     |\n","|    ent_coef        | 0.00464  |\n","|    ent_coef_loss   | -1.38    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160648   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16630    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7627     |\n","|    total_timesteps | 3858624  |\n","| train/             |          |\n","|    actor_loss      | 3.64     |\n","|    critic_loss     | 1        |\n","|    ent_coef        | 0.00442  |\n","|    ent_coef_loss   | -2.02    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160771   |\n","---------------------------------\n","Eval num_timesteps=3859200, episode_reward=-109.71 +/- 1.39\n","Episode length: 97.80 +/- 23.03\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 97.8     |\n","|    mean_reward     | -110     |\n","| time/              |          |\n","|    total_timesteps | 3859200  |\n","| train/             |          |\n","|    actor_loss      | 5.08     |\n","|    critic_loss     | 1.88     |\n","|    ent_coef        | 0.00435  |\n","|    ent_coef_loss   | 0.531    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16640    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7632     |\n","|    total_timesteps | 3860472  |\n","| train/             |          |\n","|    actor_loss      | 3.85     |\n","|    critic_loss     | 3.56     |\n","|    ent_coef        | 0.00436  |\n","|    ent_coef_loss   | 0.169    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160848   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16650    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7632     |\n","|    total_timesteps | 3860928  |\n","| train/             |          |\n","|    actor_loss      | 3.74     |\n","|    critic_loss     | 22.4     |\n","|    ent_coef        | 0.00433  |\n","|    ent_coef_loss   | -0.104   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160867   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16660    |\n","|    fps             | 506      |\n","|    time_elapsed    | 7634     |\n","|    total_timesteps | 3863352  |\n","| train/             |          |\n","|    actor_loss      | 4.29     |\n","|    critic_loss     | 4.91     |\n","|    ent_coef        | 0.00464  |\n","|    ent_coef_loss   | 2.07     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160968   |\n","---------------------------------\n","Eval num_timesteps=3864000, episode_reward=-109.62 +/- 1.14\n","Episode length: 103.00 +/- 2.45\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 103      |\n","|    mean_reward     | -110     |\n","| time/              |          |\n","|    total_timesteps | 3864000  |\n","| train/             |          |\n","|    actor_loss      | 4.65     |\n","|    critic_loss     | 7.02     |\n","|    ent_coef        | 0.00473  |\n","|    ent_coef_loss   | 3.01     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 160995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16670    |\n","|    fps             | 506      |\n","|    time_elapsed    | 7636     |\n","|    total_timesteps | 3864216  |\n","| train/             |          |\n","|    actor_loss      | 3.79     |\n","|    critic_loss     | 8.85     |\n","|    ent_coef        | 0.00478  |\n","|    ent_coef_loss   | 3.06     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161004   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16680    |\n","|    fps             | 506      |\n","|    time_elapsed    | 7638     |\n","|    total_timesteps | 3866184  |\n","| train/             |          |\n","|    actor_loss      | 3.06     |\n","|    critic_loss     | 12.6     |\n","|    ent_coef        | 0.00502  |\n","|    ent_coef_loss   | -3.43    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161086   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16690    |\n","|    fps             | 506      |\n","|    time_elapsed    | 7639     |\n","|    total_timesteps | 3867312  |\n","| train/             |          |\n","|    actor_loss      | 4.21     |\n","|    critic_loss     | 4.36     |\n","|    ent_coef        | 0.00494  |\n","|    ent_coef_loss   | 1.56     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161133   |\n","---------------------------------\n","Eval num_timesteps=3868800, episode_reward=-111.36 +/- 0.26\n","Episode length: 218.80 +/- 54.87\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 219      |\n","|    mean_reward     | -111     |\n","| time/              |          |\n","|    total_timesteps | 3868800  |\n","| train/             |          |\n","|    actor_loss      | 3.96     |\n","|    critic_loss     | 3.31     |\n","|    ent_coef        | 0.00482  |\n","|    ent_coef_loss   | -1.16    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16700    |\n","|    fps             | 506      |\n","|    time_elapsed    | 7644     |\n","|    total_timesteps | 3869328  |\n","| train/             |          |\n","|    actor_loss      | 4.08     |\n","|    critic_loss     | 34.6     |\n","|    ent_coef        | 0.00475  |\n","|    ent_coef_loss   | -2.01    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161217   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16710    |\n","|    fps             | 506      |\n","|    time_elapsed    | 7648     |\n","|    total_timesteps | 3872496  |\n","| train/             |          |\n","|    actor_loss      | 4        |\n","|    critic_loss     | 1.08     |\n","|    ent_coef        | 0.00435  |\n","|    ent_coef_loss   | -0.0748  |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161349   |\n","---------------------------------\n","Eval num_timesteps=3873600, episode_reward=-108.37 +/- 4.39\n","Episode length: 191.20 +/- 30.37\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 191      |\n","|    mean_reward     | -108     |\n","| time/              |          |\n","|    total_timesteps | 3873600  |\n","| train/             |          |\n","|    actor_loss      | 3.26     |\n","|    critic_loss     | 18.7     |\n","|    ent_coef        | 0.00441  |\n","|    ent_coef_loss   | -0.382   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16720    |\n","|    fps             | 506      |\n","|    time_elapsed    | 7655     |\n","|    total_timesteps | 3876312  |\n","| train/             |          |\n","|    actor_loss      | 4.16     |\n","|    critic_loss     | 26.6     |\n","|    ent_coef        | 0.00438  |\n","|    ent_coef_loss   | 0.275    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161508   |\n","---------------------------------\n","Eval num_timesteps=3878400, episode_reward=-0.74 +/- 0.02\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.738   |\n","| time/              |          |\n","|    total_timesteps | 3878400  |\n","| train/             |          |\n","|    actor_loss      | 3.44     |\n","|    critic_loss     | 1.06     |\n","|    ent_coef        | 0.00429  |\n","|    ent_coef_loss   | 0.199    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16730    |\n","|    fps             | 506      |\n","|    time_elapsed    | 7667     |\n","|    total_timesteps | 3882576  |\n","| train/             |          |\n","|    actor_loss      | 4        |\n","|    critic_loss     | 31.2     |\n","|    ent_coef        | 0.00445  |\n","|    ent_coef_loss   | 0.602    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161769   |\n","---------------------------------\n","Eval num_timesteps=3883200, episode_reward=-108.81 +/- 1.71\n","Episode length: 85.40 +/- 17.64\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 85.4     |\n","|    mean_reward     | -109     |\n","| time/              |          |\n","|    total_timesteps | 3883200  |\n","| train/             |          |\n","|    actor_loss      | 4.39     |\n","|    critic_loss     | 2.57     |\n","|    ent_coef        | 0.00454  |\n","|    ent_coef_loss   | 1.8      |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16740    |\n","|    fps             | 506      |\n","|    time_elapsed    | 7671     |\n","|    total_timesteps | 3883680  |\n","| train/             |          |\n","|    actor_loss      | 4.4      |\n","|    critic_loss     | 22       |\n","|    ent_coef        | 0.00461  |\n","|    ent_coef_loss   | 1.54     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161815   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16750    |\n","|    fps             | 506      |\n","|    time_elapsed    | 7673     |\n","|    total_timesteps | 3885840  |\n","| train/             |          |\n","|    actor_loss      | 3.58     |\n","|    critic_loss     | 4.05     |\n","|    ent_coef        | 0.00485  |\n","|    ent_coef_loss   | -0.778   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161905   |\n","---------------------------------\n","Eval num_timesteps=3888000, episode_reward=-48.95 +/- 48.79\n","Episode length: 428.40 +/- 87.69\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 428      |\n","|    mean_reward     | -49      |\n","| time/              |          |\n","|    total_timesteps | 3888000  |\n","| train/             |          |\n","|    actor_loss      | 3.94     |\n","|    critic_loss     | 2.59     |\n","|    ent_coef        | 0.00519  |\n","|    ent_coef_loss   | 0.971    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 161995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16760    |\n","|    fps             | 506      |\n","|    time_elapsed    | 7682     |\n","|    total_timesteps | 3890328  |\n","| train/             |          |\n","|    actor_loss      | 4.58     |\n","|    critic_loss     | 14.8     |\n","|    ent_coef        | 0.00527  |\n","|    ent_coef_loss   | 0.000355 |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162092   |\n","---------------------------------\n","Eval num_timesteps=3892800, episode_reward=-74.33 +/- 60.35\n","Episode length: 386.00 +/- 93.08\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 386      |\n","|    mean_reward     | -74.3    |\n","| time/              |          |\n","|    total_timesteps | 3892800  |\n","| train/             |          |\n","|    actor_loss      | 4.11     |\n","|    critic_loss     | 1.28     |\n","|    ent_coef        | 0.00505  |\n","|    ent_coef_loss   | -0.663   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162195   |\n","---------------------------------\n","Eval num_timesteps=3897600, episode_reward=-0.17 +/- 0.82\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.171   |\n","| time/              |          |\n","|    total_timesteps | 3897600  |\n","| train/             |          |\n","|    actor_loss      | 3.21     |\n","|    critic_loss     | 1.35     |\n","|    ent_coef        | 0.0051   |\n","|    ent_coef_loss   | -1.8     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16770    |\n","|    fps             | 506      |\n","|    time_elapsed    | 7705     |\n","|    total_timesteps | 3900528  |\n","| train/             |          |\n","|    actor_loss      | 3.74     |\n","|    critic_loss     | 3.47     |\n","|    ent_coef        | 0.00504  |\n","|    ent_coef_loss   | -1.27    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162517   |\n","---------------------------------\n","Eval num_timesteps=3902400, episode_reward=-0.39 +/- 0.91\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.387   |\n","| time/              |          |\n","|    total_timesteps | 3902400  |\n","| train/             |          |\n","|    actor_loss      | 3.6      |\n","|    critic_loss     | 3.84     |\n","|    ent_coef        | 0.005    |\n","|    ent_coef_loss   | -0.71    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162595   |\n","---------------------------------\n","Eval num_timesteps=3907200, episode_reward=-108.69 +/- 0.09\n","Episode length: 118.00 +/- 22.05\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 118      |\n","|    mean_reward     | -109     |\n","| time/              |          |\n","|    total_timesteps | 3907200  |\n","| train/             |          |\n","|    actor_loss      | 3.77     |\n","|    critic_loss     | 2.49     |\n","|    ent_coef        | 0.00481  |\n","|    ent_coef_loss   | -1.33    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162795   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16780    |\n","|    fps             | 506      |\n","|    time_elapsed    | 7721     |\n","|    total_timesteps | 3907392  |\n","| train/             |          |\n","|    actor_loss      | 3.68     |\n","|    critic_loss     | 4.66     |\n","|    ent_coef        | 0.00478  |\n","|    ent_coef_loss   | -0.923   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162803   |\n","---------------------------------\n","Eval num_timesteps=3912000, episode_reward=-107.70 +/- 1.29\n","Episode length: 148.00 +/- 9.80\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 148      |\n","|    mean_reward     | -108     |\n","| time/              |          |\n","|    total_timesteps | 3912000  |\n","| train/             |          |\n","|    actor_loss      | 2.98     |\n","|    critic_loss     | 0.768    |\n","|    ent_coef        | 0.0047   |\n","|    ent_coef_loss   | -2.52    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 162995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16790    |\n","|    fps             | 506      |\n","|    time_elapsed    | 7730     |\n","|    total_timesteps | 3913848  |\n","| train/             |          |\n","|    actor_loss      | 3.65     |\n","|    critic_loss     | 1.84     |\n","|    ent_coef        | 0.00453  |\n","|    ent_coef_loss   | -1.83    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163072   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16800    |\n","|    fps             | 506      |\n","|    time_elapsed    | 7734     |\n","|    total_timesteps | 3916512  |\n","| train/             |          |\n","|    actor_loss      | 3.35     |\n","|    critic_loss     | 17.9     |\n","|    ent_coef        | 0.00438  |\n","|    ent_coef_loss   | 0.538    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163183   |\n","---------------------------------\n","Eval num_timesteps=3916800, episode_reward=-107.40 +/- 3.70\n","Episode length: 309.80 +/- 153.34\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 310      |\n","|    mean_reward     | -107     |\n","| time/              |          |\n","|    total_timesteps | 3916800  |\n","| train/             |          |\n","|    actor_loss      | 4        |\n","|    critic_loss     | 1.16     |\n","|    ent_coef        | 0.00442  |\n","|    ent_coef_loss   | 0.923    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16810    |\n","|    fps             | 506      |\n","|    time_elapsed    | 7740     |\n","|    total_timesteps | 3918360  |\n","| train/             |          |\n","|    actor_loss      | 3.91     |\n","|    critic_loss     | 26.5     |\n","|    ent_coef        | 0.00448  |\n","|    ent_coef_loss   | -0.0545  |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163260   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16820    |\n","|    fps             | 506      |\n","|    time_elapsed    | 7743     |\n","|    total_timesteps | 3921288  |\n","| train/             |          |\n","|    actor_loss      | 3.09     |\n","|    critic_loss     | 1.94     |\n","|    ent_coef        | 0.0043   |\n","|    ent_coef_loss   | 1.48     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163382   |\n","---------------------------------\n","Eval num_timesteps=3921600, episode_reward=1.36 +/- 0.27\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.36     |\n","| time/              |          |\n","|    total_timesteps | 3921600  |\n","| train/             |          |\n","|    actor_loss      | 4.46     |\n","|    critic_loss     | 20.8     |\n","|    ent_coef        | 0.00436  |\n","|    ent_coef_loss   | 0.998    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163395   |\n","---------------------------------\n","Eval num_timesteps=3926400, episode_reward=-114.67 +/- 9.36\n","Episode length: 258.40 +/- 41.15\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 258      |\n","|    mean_reward     | -115     |\n","| time/              |          |\n","|    total_timesteps | 3926400  |\n","| train/             |          |\n","|    actor_loss      | 3.36     |\n","|    critic_loss     | 8.19     |\n","|    ent_coef        | 0.00456  |\n","|    ent_coef_loss   | -1.15    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16830    |\n","|    fps             | 506      |\n","|    time_elapsed    | 7760     |\n","|    total_timesteps | 3927720  |\n","| train/             |          |\n","|    actor_loss      | 3.66     |\n","|    critic_loss     | 2.68     |\n","|    ent_coef        | 0.00469  |\n","|    ent_coef_loss   | -1.35    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163650   |\n","---------------------------------\n","Eval num_timesteps=3931200, episode_reward=-0.39 +/- 0.31\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.391   |\n","| time/              |          |\n","|    total_timesteps | 3931200  |\n","| train/             |          |\n","|    actor_loss      | 4.58     |\n","|    critic_loss     | 1.7      |\n","|    ent_coef        | 0.0044   |\n","|    ent_coef_loss   | 1.95     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163795   |\n","---------------------------------\n","Eval num_timesteps=3936000, episode_reward=0.59 +/- 1.01\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 0.593    |\n","| time/              |          |\n","|    total_timesteps | 3936000  |\n","| train/             |          |\n","|    actor_loss      | 3.42     |\n","|    critic_loss     | 13.3     |\n","|    ent_coef        | 0.00459  |\n","|    ent_coef_loss   | 1.41     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 163995   |\n","---------------------------------\n","Eval num_timesteps=3940800, episode_reward=-112.61 +/- 3.35\n","Episode length: 173.80 +/- 10.78\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 174      |\n","|    mean_reward     | -113     |\n","| time/              |          |\n","|    total_timesteps | 3940800  |\n","| train/             |          |\n","|    actor_loss      | 4.14     |\n","|    critic_loss     | 1.56     |\n","|    ent_coef        | 0.00414  |\n","|    ent_coef_loss   | 1.02     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164195   |\n","---------------------------------\n","Eval num_timesteps=3945600, episode_reward=-78.55 +/- 59.54\n","Episode length: 371.60 +/- 104.84\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 372      |\n","|    mean_reward     | -78.5    |\n","| time/              |          |\n","|    total_timesteps | 3945600  |\n","| train/             |          |\n","|    actor_loss      | 3.64     |\n","|    critic_loss     | 20.3     |\n","|    ent_coef        | 0.00431  |\n","|    ent_coef_loss   | 0.76     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164395   |\n","---------------------------------\n","Eval num_timesteps=3950400, episode_reward=-112.66 +/- 3.24\n","Episode length: 167.80 +/- 52.42\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 168      |\n","|    mean_reward     | -113     |\n","| time/              |          |\n","|    total_timesteps | 3950400  |\n","| train/             |          |\n","|    actor_loss      | 3.27     |\n","|    critic_loss     | 15.6     |\n","|    ent_coef        | 0.00419  |\n","|    ent_coef_loss   | -2.62    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16840    |\n","|    fps             | 506      |\n","|    time_elapsed    | 7806     |\n","|    total_timesteps | 3950472  |\n","| train/             |          |\n","|    actor_loss      | 4.65     |\n","|    critic_loss     | 1.86     |\n","|    ent_coef        | 0.00419  |\n","|    ent_coef_loss   | -0.0708  |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164598   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16850    |\n","|    fps             | 506      |\n","|    time_elapsed    | 7812     |\n","|    total_timesteps | 3954912  |\n","| train/             |          |\n","|    actor_loss      | 3.36     |\n","|    critic_loss     | 38.2     |\n","|    ent_coef        | 0.00464  |\n","|    ent_coef_loss   | 1.79     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164783   |\n","---------------------------------\n","Eval num_timesteps=3955200, episode_reward=-1.67 +/- 0.04\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.67    |\n","| time/              |          |\n","|    total_timesteps | 3955200  |\n","| train/             |          |\n","|    actor_loss      | 4.23     |\n","|    critic_loss     | 1.63     |\n","|    ent_coef        | 0.00468  |\n","|    ent_coef_loss   | 3.24     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164795   |\n","---------------------------------\n","Eval num_timesteps=3960000, episode_reward=-109.76 +/- 2.08\n","Episode length: 153.80 +/- 33.31\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 154      |\n","|    mean_reward     | -110     |\n","| time/              |          |\n","|    total_timesteps | 3960000  |\n","| train/             |          |\n","|    actor_loss      | 3.61     |\n","|    critic_loss     | 2.45     |\n","|    ent_coef        | 0.00445  |\n","|    ent_coef_loss   | 1.38     |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 164995   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16860    |\n","|    fps             | 506      |\n","|    time_elapsed    | 7826     |\n","|    total_timesteps | 3961080  |\n","| train/             |          |\n","|    actor_loss      | 3.54     |\n","|    critic_loss     | 7.44     |\n","|    ent_coef        | 0.0045   |\n","|    ent_coef_loss   | 0.331    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165040   |\n","---------------------------------\n","Eval num_timesteps=3964800, episode_reward=-42.19 +/- 50.35\n","Episode length: 393.20 +/- 130.80\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 393      |\n","|    mean_reward     | -42.2    |\n","| time/              |          |\n","|    total_timesteps | 3964800  |\n","| train/             |          |\n","|    actor_loss      | 3.53     |\n","|    critic_loss     | 1.64     |\n","|    ent_coef        | 0.00432  |\n","|    ent_coef_loss   | -0.359   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16870    |\n","|    fps             | 506      |\n","|    time_elapsed    | 7841     |\n","|    total_timesteps | 3967968  |\n","| train/             |          |\n","|    actor_loss      | 3.39     |\n","|    critic_loss     | 7.63     |\n","|    ent_coef        | 0.00447  |\n","|    ent_coef_loss   | -0.581   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165327   |\n","---------------------------------\n","Eval num_timesteps=3969600, episode_reward=-0.73 +/- 0.18\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.729   |\n","| time/              |          |\n","|    total_timesteps | 3969600  |\n","| train/             |          |\n","|    actor_loss      | 3.47     |\n","|    critic_loss     | 2.13     |\n","|    ent_coef        | 0.00437  |\n","|    ent_coef_loss   | -1.53    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165395   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16880    |\n","|    fps             | 506      |\n","|    time_elapsed    | 7851     |\n","|    total_timesteps | 3973080  |\n","| train/             |          |\n","|    actor_loss      | 3.63     |\n","|    critic_loss     | 1.5      |\n","|    ent_coef        | 0.00411  |\n","|    ent_coef_loss   | -0.524   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165540   |\n","---------------------------------\n","Eval num_timesteps=3974400, episode_reward=-0.98 +/- 0.13\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -0.977   |\n","| time/              |          |\n","|    total_timesteps | 3974400  |\n","| train/             |          |\n","|    actor_loss      | 3.43     |\n","|    critic_loss     | 4.51     |\n","|    ent_coef        | 0.00408  |\n","|    ent_coef_loss   | -0.619   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165595   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16890    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7862     |\n","|    total_timesteps | 3977688  |\n","| train/             |          |\n","|    actor_loss      | 3.51     |\n","|    critic_loss     | 3.15     |\n","|    ent_coef        | 0.00373  |\n","|    ent_coef_loss   | 0.424    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165732   |\n","---------------------------------\n","Eval num_timesteps=3979200, episode_reward=1.08 +/- 0.25\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | 1.08     |\n","| time/              |          |\n","|    total_timesteps | 3979200  |\n","| train/             |          |\n","|    actor_loss      | 3.77     |\n","|    critic_loss     | 3.2      |\n","|    ent_coef        | 0.00381  |\n","|    ent_coef_loss   | 0.446    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165795   |\n","---------------------------------\n","Eval num_timesteps=3984000, episode_reward=-2.38 +/- 0.92\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -2.38    |\n","| time/              |          |\n","|    total_timesteps | 3984000  |\n","| train/             |          |\n","|    actor_loss      | 3.47     |\n","|    critic_loss     | 22.4     |\n","|    ent_coef        | 0.00416  |\n","|    ent_coef_loss   | 0.0302   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 165995   |\n","---------------------------------\n","Eval num_timesteps=3988800, episode_reward=-64.18 +/- 51.43\n","Episode length: 303.80 +/- 160.20\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 304      |\n","|    mean_reward     | -64.2    |\n","| time/              |          |\n","|    total_timesteps | 3988800  |\n","| train/             |          |\n","|    actor_loss      | 3.69     |\n","|    critic_loss     | 12.3     |\n","|    ent_coef        | 0.00413  |\n","|    ent_coef_loss   | -1.21    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166195   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16900    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7893     |\n","|    total_timesteps | 3988992  |\n","| train/             |          |\n","|    actor_loss      | 3.14     |\n","|    critic_loss     | 0.723    |\n","|    ent_coef        | 0.00413  |\n","|    ent_coef_loss   | -0.432   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166203   |\n","---------------------------------\n","---------------------------------\n","| time/              |          |\n","|    episodes        | 16910    |\n","|    fps             | 505      |\n","|    time_elapsed    | 7897     |\n","|    total_timesteps | 3992064  |\n","| train/             |          |\n","|    actor_loss      | 3.77     |\n","|    critic_loss     | 23.4     |\n","|    ent_coef        | 0.004    |\n","|    ent_coef_loss   | -2.23    |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166331   |\n","---------------------------------\n","Eval num_timesteps=3993600, episode_reward=-1.32 +/- 0.05\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.32    |\n","| time/              |          |\n","|    total_timesteps | 3993600  |\n","| train/             |          |\n","|    actor_loss      | 3.91     |\n","|    critic_loss     | 21.8     |\n","|    ent_coef        | 0.00391  |\n","|    ent_coef_loss   | -0.505   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166395   |\n","---------------------------------\n","Eval num_timesteps=3998400, episode_reward=-1.01 +/- 0.51\n","Episode length: 500.00 +/- 0.00\n","---------------------------------\n","| eval/              |          |\n","|    mean_ep_length  | 500      |\n","|    mean_reward     | -1.01    |\n","| time/              |          |\n","|    total_timesteps | 3998400  |\n","| train/             |          |\n","|    actor_loss      | 3.82     |\n","|    critic_loss     | 8.3      |\n","|    ent_coef        | 0.00396  |\n","|    ent_coef_loss   | 0.0462   |\n","|    learning_rate   | 0.001    |\n","|    n_updates       | 166595   |\n","---------------------------------\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"s_YTQjcU9ANN"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-0UGmQMn30y8"},"outputs":[],"source":["################# Evaluación del entrenamiento"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rqOXrP3OGAky"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1dyUHGMjGKfH"},"outputs":[],"source":["# Read the CSV file into a DataFrame\n","data = pd.read_csv('/content/gdrive/MyDrive/TFM/policy_log/progress.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":564},"executionInfo":{"elapsed":1005,"status":"ok","timestamp":1696530691024,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"Cn-2KiRoGMNn","outputId":"495545e0-4aab-4f36-e536-8868be1308d2"},"outputs":[{"data":{"image/png":"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\n","text/plain":["<Figure size 1000x600 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# Create a line plot\n","plt.figure(figsize=(10, 6))  # Set the figure size (optional)\n","\n","# Assuming your CSV has columns named 'x' and 'y'\n","plt.plot(data['time/total_timesteps'], data['train/loss'], marker='o', linestyle='-')\n","\n","plt.title('Line Plot')\n","plt.xlabel('X-axis Label')\n","plt.ylabel('Y-axis Label')\n","plt.grid(True)  # Add grid lines (optional)\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":390},"executionInfo":{"elapsed":591,"status":"error","timestamp":1696502224565,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"O-9TpmTqYowr","outputId":"cb50addc-adac-4149-f542-608d2087ccf4"},"outputs":[{"ename":"AssertionError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-4d3d2004ebc7>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Helper from the library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m results_plotter.plot_results(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mtmp_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_plotter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_TIMESTEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"knights_archers_zombies_v10\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/results_plotter.py\u001b[0m in \u001b[0;36mplot_results\u001b[0;34m(dirs, num_timesteps, x_axis, task_name, figsize)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0mdata_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mdata_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnum_timesteps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mdata_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_frame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mnum_timesteps\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/monitor.py\u001b[0m in \u001b[0;36mload_results\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile_handler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0mfirst_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mfirst_line\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"#\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m             \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_line\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0mdata_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: "]}],"source":["from stable_baselines3.common import results_plotter\n","\n","# Helper from the library\n","results_plotter.plot_results(\n","    [tmp_path], 1e5, results_plotter.X_TIMESTEPS, \"knights_archers_zombies_v10\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"67j9x-584pXR"},"outputs":[],"source":["from stable_baselines3.common import results_plotter\n","\n","# Helper from the library\n","results_plotter.plot_results(\n","    [log_dir], 1e5, results_plotter.X_TIMESTEPS, \"TD3 LunarLander\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LEonRdbqbfCT"},"outputs":[],"source":["################# Evaluación de las policies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VtcozO1GbiMn"},"outputs":[],"source":["import glob\n","import os\n","import time\n","\n","import supersuit as ss\n","from stable_baselines3 import PPO\n","from stable_baselines3.ppo import CnnPolicy, MlpPolicy\n","\n","from pettingzoo.butterfly import knights_archers_zombies_v10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rpxGI28Jbiyf"},"outputs":[],"source":["def eval(env_fn, trained_policy, num_games: int = 100, render_mode: str | None = None, **env_kwargs):\n","    # Evaluate a trained agent vs a random agent\n","    env = env_fn.env(render_mode=render_mode, **env_kwargs)\n","\n","    # Pre-process using SuperSuit\n","    # visual_observation = not env.unwrapped.vector_state\n","    # if visual_observation:\n","\n","    # If the observation space is visual, reduce the color channels, resize from 512px to 84px, and apply frame stacking\n","    env = ss.color_reduction_v0(env, mode=\"B\")\n","    env = ss.resize_v1(env, x_size=84, y_size=84)\n","    env = ss.frame_stack_v1(env, 3)\n","\n","    print(\n","        f\"\\nStarting evaluation on {str(env.metadata['name'])} (num_games={num_games}, render_mode={render_mode}), policy={trained_policy}\"\n","    )\n","\n","    model = PPO.load(trained_policy)\n","\n","    rewards = {agent: 0 for agent in env.possible_agents}\n","\n","    # Note: we evaluate here using an AEC environments, to allow for easy A/B testing against random policies\n","    # For example, we can see here that using a random agent for archer_0 results in less points than the trained agent\n","    for i in range(num_games):\n","        env.reset(seed=i)\n","        env.action_space(env.possible_agents[0]).seed(i)\n","\n","        for agent in env.agent_iter():\n","            obs, reward, termination, truncation, info = env.last()\n","\n","            for agent in env.agents:\n","                rewards[agent] += env.rewards[agent]\n","\n","            if termination or truncation:\n","                break\n","            else:\n","                if agent == env.possible_agents[0]:\n","                    act = env.action_space(agent).sample()\n","                else:\n","                    act = model.predict(obs, deterministic=True)[0]\n","            env.step(act)\n","    env.close()\n","\n","    avg_reward = sum(rewards.values()) / len(rewards.values())\n","    avg_reward_per_agent = {\n","        agent: rewards[agent] / num_games for agent in env.possible_agents\n","    }\n","    print(f\"Avg reward: {avg_reward}\")\n","    print(\"Avg reward per agent, per game: \", avg_reward_per_agent)\n","    print(\"Full rewards: \", rewards)\n","    return avg_reward"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39096,"status":"ok","timestamp":1696269142153,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"E176uRuXbuR9","outputId":"2c55dc2c-2a0f-459c-84d8-8d0437a197d0"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Starting evaluation on knights_archers_zombies_v10 (num_games=10, render_mode=None), policy=policy_new4\n","Avg reward: 0.0\n","Avg reward per agent, per game:  {'archer_0': 0.0, 'archer_1': 0.0, 'knight_0': 0.0, 'knight_1': 0.0}\n","Full rewards:  {'archer_0': 0, 'archer_1': 0, 'knight_0': 0, 'knight_1': 0}\n"]},{"data":{"text/plain":["0.0"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["env_fn = knights_archers_zombies_v10\n","# Set vector_state to false in order to use visual observations (significantly longer training time)\n","env_kwargs = dict(max_cycles=700, max_zombies=10, vector_state=False)\n","\n","eval(env_fn, \"policy_new4\", num_games=10, render_mode=None, **env_kwargs)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38500,"status":"ok","timestamp":1696269047624,"user":{"displayName":"Víctor Manuel Arroyo Martín","userId":"10424599158381870205"},"user_tz":-120},"id":"CIh2EodMgk2E","outputId":"065629d8-63a6-4520-de42-9f2bfecf5137"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Starting evaluation on knights_archers_zombies_v10 (num_games=10, render_mode=None), policy=policy10\n","Avg reward: 0.0\n","Avg reward per agent, per game:  {'archer_0': 0.0, 'archer_1': 0.0, 'knight_0': 0.0, 'knight_1': 0.0}\n","Full rewards:  {'archer_0': 0, 'archer_1': 0, 'knight_0': 0, 'knight_1': 0}\n"]},{"data":{"text/plain":["0.0"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["env_fn = knights_archers_zombies_v10\n","# Set vector_state to false in order to use visual observations (significantly longer training time)\n","env_kwargs = dict(max_cycles=700, max_zombies=10, vector_state=False)\n","\n","eval(env_fn, \"policy10\", num_games=10, render_mode=None, **env_kwargs)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"mount_file_id":"1tNH0xWOkEmzP-ic_xuZPO9HHOk_62LLx","authorship_tag":"ABX9TyOe9mcmXaY9MtU+R6yOpQGf"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}